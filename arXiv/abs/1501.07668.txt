  Large scale models of physical phenomena demand the development of new
statistical and computational tools in order to be effective. Many such models
are `sloppy', i.e., exhibit behavior controlled by a relatively small number of
parameter combinations. We review an information theoretic framework for
analyzing sloppy models. This formalism is based on the Fisher Information
Matrix, which we interpret as a Riemannian metric on a parameterized space of
models. Distance in this space is a measure of how distinguishable two models
are based on their predictions. Sloppy model manifolds are bounded with a
hierarchy of widths and extrinsic curvatures. We show how the manifold boundary
approximation can extract the simple, hidden theory from complicated sloppy
models. We attribute the success of simple effective models in physics as
likewise emerging from complicated processes exhibiting a low effective
dimensionality. We discuss the ramifications and consequences of sloppy models
for biochemistry and science more generally. We suggest that the reason our
complex world is understandable is due to the same fundamental reason: simple
theories of macroscopic behavior are hidden inside complicated microscopic
processes.
