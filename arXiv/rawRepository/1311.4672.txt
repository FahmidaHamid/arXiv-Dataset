Properties of networks with partially structured and partially random connectivity
Yashar Ahmadian,1, 2, âˆ— Francesco Fumarola,1 and Kenneth D. Miller1, 2

arXiv:1311.4672v6 [q-bio.NC] 26 Jan 2015

2

1
Center for Theoretical Neuroscience, Department of Neuroscience,
Swartz Program in Theoretical Neuroscience, and Kavli Institute for Brain Science,
College of Physicians and Surgeons, Columbia University, NY, NY 10032
(Dated: January 28, 2015)

Networks studied in many disciplines, including neuroscience and mathematical biology, have
connectivity that may be stochastic about some underlying mean connectivity represented by a
nonnormal matrix. Furthermore the stochasticity may not be i.i.d. across elements of the connectivity matrix. More generally, the problem of understanding the behavior of stochastic matrices with
nontrivial mean structure and correlations arises in many settings. We address this by characterizing
large random N Ã— N matrices of the form A = M + LJR, where M , L and R are arbitrary deterministic matrices and J is a random matrix of zero-mean independent and identically distributed
elements. M can be nonnormal, and L and R allow correlations that have separable dependence
on row and column indices. We first provide a general formula for the eigenvalue density of A. For
A nonnormal, the eigenvalues do not suffice to specify the dynamics induced by A, so we also provide general formulae for the transient evolution of the magnitude of activity and frequency power
spectrum in an N -dimensional linear dynamical system with a coupling matrix given by A. These
quantities can also be thought of as characterizing the stability and the magnitude of the linear response of a nonlinear network to small perturbations about a fixed point. We derive these formulae
and work them out analytically for some examples of M , L and R motivated by neurobiological
models. We also argue that the persistence as N â†’ âˆ of a finite number of randomly distributed
outlying eigenvalues outside the support of the eigenvalue density of A, as previously observed, arises
in regions of the complex plane â„¦ where there are nonzero singular values of Lâˆ’1 (z1 âˆ’ M )Râˆ’1 (for
z âˆˆ â„¦) that vanish as N â†’ âˆ. When such singular values do not exist and L and R are equal to
the identity, there is a correspondence in the normalized Frobenius norm (but not in the operator
norm) between the support of the spectrum of A for J of norm Ïƒ and the Ïƒ-pseudospectrum of M .
PACS numbers: 87.18.Sn, 87.19.L-, 02.10.Yn, 89.75.-k
Journal reference: Physical Review E, 91, 012820 (2015).

I.

INTRODUCTION

Knowledge of the statistics of eigenvalues and eigenvectors of random matrices has applications in the modeling of phenomena relevant to a wide range of disciplines
[1â€“3]. In many applications, however, the matrices of interest are not entirely random, but feature substantial
deterministic structure. Furthermore this structure, as
well as the disorder on top of it, are in general described
by nonnormal matrices.
In neuroscience, for example, connections between neurons typically have restricted spatial range and show
specificity with respect to neuronal type, location and response properties. Experience-based synaptic plasticity,
which underlies learning and memory, naturally gives rise
to synaptic connectivity matrices that encode aspects of
the statistical structure of the sensory environment, while
containing significant randomness partly due to the inherent stochasticity of particular histories of sensory experience. Another simple example of structured neural
connectivity is due to what is known as Daleâ€™s principle [4â€“6]: neurons come in two main types, excitatory

âˆ— Corresponding

author.
ya2005@columbia.edu

and inhibitory. This empirical principle imposes a certain structure on the synaptic connectivity matrix, forcing all elements in each column of the matrix, describing
the synaptic projections of a certain neuron, to have the
same sign. Particularly when the typical weight magnitude is much larger than typical differences between the
magnitudes of excitatory and inhibitory weights, such a
matrix can be extremely nonnormal by some measures,
much more so than a fully random matrix [7]. Similarly,
biological knowledge imparts a great deal of structure
to models of biochemical [8â€“11] or ecological networks
[12â€“16], and matrices characterizing such interactions are
typically nonnormal. Yet our knowledge of connectivity
or interactions is at best probabilistic. To describe realistic biological behavior, we must generalize from the
behavior of a fixed, regular connectivity to the expected
behavior of a typical sample from an appropriate connectivity ensemble.
Furthermore, nonnormality can lead to important dynamical properties not seen for normal matrices [17]. In
general, networks with a recurrent connectivity pattern
described by a nonnormal matrix can be described as
having a hidden feedforward connectivity structure between orthogonal activity patterns, each of which can also
excite or inhibit itself [7, 18, 19]. In neural networks such
hidden feedforward connectivity arises from the natural
separation of excitatory and inhibitory neurons, yielding

2
so-called â€œbalanced amplificationâ€ of patterns of activity without any dynamical slowing [7]. Underlying this
is the phenomenon of â€œtransient amplificationâ€: a small
perturbation from a fixed point of a stable system with
nonnormal connectivity can lead to a large transient response over finite time [17]. Transient amplification also
yields unexpected results in ecological networks [20â€“22],
and has been conjectured to play a key role in many
biochemical systems [23]. Networks that yield long hidden feedforward chains can also generate long time scales
and provide a substrate for working memory [18, 19].
Systems with nonnormal connectivity can also exhibit
pseudo-resonance frequencies in their power-spectrum at
which the system responds strongly to external inputs,
even though the external frequency is not close to any
of the systemâ€™s natural frequencies as determined by its
eigenvalues [17].
While Hermitian random matrices and fully random
non-Hermitian matrices with zero-mean, independent
and identically distributed (iid) elements have been
widely studied, there is a shortage of results on quantities
of interest for nonnormal matrices that fall in between
the two extremes of fully random or fully deterministic. A natural departure from a nonnormal deterministic
structure, described by a connectivity matrix M , is to
additively perturb it with a fully random matrix J with
zero-mean, iid elements. In many important examples,
however, the strength of disorder (deviations from the
mean structure) is not uniform and itself has some structure (e.g. for each connection it can depend on the types
of the connected nodes or neurons). Moreover, the deviations of the strength of different connections or interactions from their average need not be independent. Hence
it is important to move beyond a simple iid deviation
from the mean structure. Here, we study ensembles of
large N Ã— N random matrices of the form A = M + LJR
where M , L and R are arbitrary (M ) or arbitrary invertible (L and R) deterministic matrices that are in general
nonnormal, and J is a completely random matrix with
zero-mean iid elements of variance 1/N . The matrix M is
thus the average of A, and describes average connectivity.
Note that when L and R are diagonal, they specify variances that depend separably on the row and column of A;
while when they are not diagonal, the elements of A are
not statistically independent. As we show in Sec. II C 3,
this form arises naturally, for example, in linearizations
of dynamical systems involving simple classes of nonlinearities. This type of ensemble is also natural from the
random matrix theory viewpoint, as it describes a classical fully random ensemble â€“ an iid random matrix J â€“
modified by the two basic algebraic operations of matrix
multiplication and addition.
We study the eigenvalue distribution of such matrices, but also directly study the dynamics of a linear system of differential equations governed by such matrices.
Specifically, for matrices of the above type, using the
Feynman diagram technique in the large N limit (we follow the particular version of this method developed by

Refs. [24, 25]), we have derived a general formula for
the density of their eigenvalues in the complex plane,
which generalizes the well-known circular law for fully
random matrices [26â€“30]. It also generalizes a result [31]
obtained for the case where L and R are scalar multiples of 1, the N -dimensional identity matrix (the same
result was obtained in [32] using the methods and language of free probability theory; the eigenvalue density
for the case L âˆ R âˆ 1 and a normal M was also calculated in Ref. [24] in the limit N â†’ âˆ, and that result
was extended to finite N in Ref. [33]). Apart from generalization to arbitrary invertible L and R, we also provide
a correct regularizing procedure for finding the support
of the eigenvalue density in the limit N â†’ âˆ, in certain
highly nonnormal cases of M ; the naive interpretation of
the formulae fails in these cases, which were not previously discussed. Furthermore, with the aim of studying
dynamical signatures of nonnormal connectivity, we focused on the dynamics directly, deriving general formulae
for the magnitude of the response of the system to a delta
function pulse of input (which provides a measure of the
time-course of potential transient amplification), as well
as the frequency power spectrum of the systemâ€™s response
to external time-dependent inputs.
These general results are presented in the next section.
There, we also present the explicit results of analytical
or numerical calculations based on these general formulae
for some specific examples of M , L and R. Sections III
and IV contain the detailed derivations of our general formulae for the eigenvalue density and the response magnitude formulae, respectively. Section V contains the detailed analytical calculations of these quantities for the
specific examples presented in Sec. II, based on the general formulae. We conclude the paper in Sec. VI.

II.

SUMMARY OF RESULTS

We study ensembles of large N Ã— N random matrices
of the form
A = M + LJR,

(2.1)

where M , L and R are arbitrary (M ) or arbitrary invertible (L and R) deterministic matrices [59], and J is a random matrix of independent and identically distributed
(iid) elements with zero mean and variance 1/N . Since
J and therefore LJR have zero mean, M is the ensemble
average of A. The random fluctuations of A around its
average are given by the matrix LJR, which for general L
and/or R has dependent and non-identically distributed
elements, due to the possible mixing and non-uniform
scaling of the rows (columns) of the iid J by L (R).
We are firstly interested in the statistics of the eigenvalues of Eq. (2.1). While the statistics of the eigenvalues
and eigenvectors of A are of interest in their own right,
we also directly consider certain properties of the linear

3
dynamical system
dx(t)
= âˆ’Î³x(t) + Ax(t) + I(t),
(2.2)
dt
for an N -dimensional state vector x(t), when A is a sample of the ensemble Eq. (2.1). Here, Î³ is a scalar and I(t)
is an external, time-dependent input. In studying this
system, we generally assume that Eq. (2.2) is asymptotically stable. This means that M , L, R and Î³ must
be chosen such that for any typical realization of J, no
eigenvalue of âˆ’Î³1 + M + LJR has a positive real part;
this can normally be achieved, for example, by choosing
a large enough Î³ > 0.
Using the diagrammatic technique in the non-crossing
approximation, which is valid for large N , we have derived general formulae for several useful properties of
such matrices involving their eigenvalues and eigenvectors (see Sec. IIIâ€“IV for the details of the derivations and
the definition of the non-crossing approximation). We
present these results in in this section. In our derivation
of these results, we assume the random J belongs to the
complex Ginibre ensemble [26], i.e. the distribution of the
elements of J is complex Gaussian. However, we emphasize that universality theorems ensure that, for given M ,
L and R, the obtained result for the eigenvalue density
in the limit N â†’ âˆ will not depend on the exact choice
of the distribution of the elements of J, beyond its first
two moments, and extend to any iid J (including, e.g., J
with real binary or log-normal elements) whose elements
have the same first two moments, i.e. zero mean and variance 1/N ; the universality of the eigenvalue density for
general M , L and R was established in Ref. [34], following earlier work on the universality of the circular law
established and successively strengthened in Refs. [26â€“
30]. Furthermore, empirically, from limited simulations,
we have thus far found (but have not proved) such universal behavior to also hold for the other quantities we
compute here (however, it is possible that universality
for these quantities might require the existence of some
higher moments beyond the second, as has been found for
universality of certain other properties of random matrices; see e.g. Ref. [35]). To demonstrate the universality
of our results, we have used non-Gaussian and/or real Jâ€™s
in most of the numerical examples below.
Hereinafter, we adopt the following notations. For any
matrix B, we denote its operator norm (its maximum
singular value) by kBk and we define its (normalized)
Frobenius norm via
1
1 X
kBk2F â‰¡
|Bij |2 = Tr(BB â€  )
(2.3)
N ij
N
(equivalently, kBkF is the root mean square of the singular values of B). For general matrices, A and B,
1
1
A
Tr(A), Aâˆ’â€  â‰¡ (Aâ€  )âˆ’1 ,
â‰¡ Aâˆ’1 ,
â‰¡ AB âˆ’1 ,
N
A
B
and when adding a scalar to a matrix, it is implied that
the scalar is multiplied by the appropriate identity matrix. We denote the identity matrix in any dimension
tr(A) â‰¡

(deduced from the context) by 1. For a complex variable
z = x + iy, the Dirac delta function is defined by Î´ 2 (z) â‰¡
Î´(x)Î´(y), and we define âˆ‚zÌ„ â‰¡ âˆ‚/âˆ‚ zÌ„ = (âˆ‚/âˆ‚x + iâˆ‚/âˆ‚y)/2,
and âˆ‚z â‰¡ âˆ‚/âˆ‚z = (âˆ‚/âˆ‚x âˆ’ iâˆ‚/âˆ‚y)/2. For simplicity,
we use the notation f (z) (instead of f (z, zÌ„)) for general,
nonholomorphic functions on the complex plane. We say
a quantity is O(f (N )) (resp. Î˜(f (N ))) when, for large
enough N , the absolute value of that quantity is bounded
above (resp. above and below) by a fixed positive multiple of |f (N )|. Finally, we say a quantity is o(f (N )) when
its ratio to |f (N )| vanishes as N â†’ âˆ.
The only conditions we impose on M , L and R are
that kM kF , kLkF , kRkF , kLâˆ’1 M Râˆ’1 kF and k(LR)âˆ’1 k
are bounded as N â†’ âˆ. We use the bound on k(LR)âˆ’1 k
in Appendices A and B; the Frobenius norm conditions
are assumptions in the universality theorem of Ref. [34]
which we use as discussed above. Finally, we assume
that for all z âˆˆ C, the distribution of the eigenvalues of
Mz Mzâ€  , where Mz is defined below in Eq. (2.6), tends
to a limit distribution as N â†’ âˆ. This last condition
simply makes precise the requirement that M , L and
R are defined consistently as functions of N , such that
a limit spectral density for M + LJR is meaningful; in
particular, it does not impose any further limits on the
growth of the eigenvalues of Mz Mzâ€  with N , beyond the
various norm bounds imposed above.

A.

Spectral density

1.

Summary of results

The density of the eigenvalues of M +LJR in the complex plane for a realization of J (also known as the empirical spectral distribution) is defined by
ÏJ (z) =

1 X 2
Î´ (z âˆ’ Î»Î± ),
N Î±

(2.4)

where Î»Î± are the eigenvalues of M + LJR. It is known
[34] that ÏJ (z) is asymptotically self-averaging, in the
sense that with probability one ÏJ (z) âˆ’ Ï(z) converges
to zero (in the distributional sense) as N â†’ âˆ, where
Ï(z) â‰¡ hÏJ (z)iJ is the ensemble average of ÏJ (z). Thus
for large enough N , any typical realization of J yields an
eigenvalue density ÏJ (z) that is arbitrarily close to Ï(z).
Our general result is that for large N , with certain
cautions and excluding certain special cases as described
below (Eqs. (2.19)â€“(2.20) and preceding discussion), Ï(z)
is nonzero in the region of the complex plane satisfying


tr (Mz Mzâ€  )âˆ’1 â‰¥ 1
(2.5)
where we defined

Mz â‰¡ Lâˆ’1 (z âˆ’ M )Râˆ’1 .

(2.6)

Using the definition Eq. (2.3), we can also express

4
Eq. (2.5) as


R




1
â‰¥ 1.
L
zâˆ’M 

(2.7)

F

Inside this region, Ï(z) is given by
#
"
1 âˆ‚
(RL)âˆ’1 Mzâ€ 
Ï(z) =
,
tr
Ï€ âˆ‚ zÌ„
Mz Mzâ€  + g(z)2

(2.8)

where g(z) is a real, scalar function found by solving


1
tr
= 1,
(2.9)
Mz Mzâ€  + g 2
for g for each z. As a first example, for the wellknown case of M = 0, L = 1, and R = Ïƒ1, we have
Mz = z/Ïƒ and the circular law follows immediately from
Eq. (2.5), which yields Ïƒ 2 /|z|2 â‰¥ 1 or |z| â‰¤ Ïƒ for the support, and from Eqs. (2.8)â€“(2.9) which yield the uniform
Ï(z) = 1/(Ï€Ïƒ 2 ) within that support. As we noted in the
introduction, formulae (2.5)â€“(2.9) generalize the results
of Refs. [31] and [32] for the special case L âˆ R âˆ 1 to
arbitrary invertible L and R (the eigenvalue density for
the case L âˆ R âˆ 1 and a normal M was also calculated
in Ref. [24]).
It is possible and illuminating to express Eqs. (2.7)â€“
(2.9) exclusively in terms of the singular values of Mz ,
which we denote by si (z) (we include possibly vanishing
singular values among si (z), so that we always have N of
them). First, noting that the squared singular values of
Mz are the eigenvalues of the Hermitian Mz Mzâ€  , we can
evaluate the trace in Eq. (2.9) in the eigen-basis of the
latter matrix, and rewrite this equation as
N
1 X
1
= 1.
N i=1 si (z)2 + g 2

(2.10)

Similarly, Eq. (2.5) can be equivalently rewritten as
N
1 X
si (z)âˆ’2 â‰¥ 1.
N i=1

(2.11)

As we prove at the end of Sec. III, Eq. (2.8) can also be
written in a form that makes it explicit that the dependence of Ï(z) on M , L and R is only through the singular
values of Mz and their derivatives with respect to z and
zÌ„. We have
"
#
N
1
1 X âˆ‚z (si (z)2 )
Ï(z) = âˆ‚zÌ„
.
(2.12)
Ï€
N i=1 si (z)2 + g(z)2
For the special case of M = 0 and general L and R, our
formulas can be simplified considerably. The spectrum is
isotropic around the origin in this case, i.e. Ï(z) depends
only on r â‰¡ |z|, and its support is a disk centered at the
origin with radius
"
#1/2
N
1 X 2
r0 = kRLkF =
Ïƒ
,
(2.13)
N i=1 i

where Ïƒi are the singular values of RL (this follows from
Eq. (2.11) by noting that for M = 0, the singular values of Mz = z(RL)âˆ’1 are si (z) = |z|/Ïƒi ). Within this
support the spectral density is given by

1
Ï(r) = âˆ’
âˆ‚r g(r)2 ,
(2.14)
2Ï€r
where g(r)2 > 0 is found by solving
1=

N
1
1 X
.
N i=1 Ïƒiâˆ’2 r2 + g(r)2

(2.15)

Integrating Eq. (2.14), we see that the proportion of
eigenvalues lying a distance larger than r from the origin
is, in this case, given by

g(r)2
(r < r0 )
n> (r) =
(2.16)
0
(r â‰¥ r0 ).
In Sec. III we prove that the eigenvalue density, given
by Eqs. (2.14)â€“(2.15), is always a decreasing function of
r = |z|, i.e. for r > 0 its derivative with respect to r is
strictly negative, as long as the limit distribution of the
{Ïƒi } as N â†’ âˆ has nonzero variance (otherwise Ï(z) is
given by the circular law with radius Eq. (2.13)). The
values of spectral density at r = 0 and r = r0 can be
calculated explicitly for general L and R:
N
1 1 X âˆ’2
Ïƒ
(2.17)
Ï€ N i=1 i
"
#âˆ’1
N
1 2 1 X 4
Ï(r = r0 ) = r0
Ïƒ
â‰¤ Ï(r = 0). (2.18)
Ï€
N i=1 i

Ï(r = 0) =

As noted above, certain cautions apply in using the
above formulae for the eigenvalue density and its boundary ((2.5)â€“(2.9), or equivalently Eqs. (2.10)â€“(2.12), and
for M = 0, Eqs. (2.14)â€“(2.15)). We have written these
formulas for finite N (assuming it is large). However,
the non-crossing approximation used in deriving these
formulas is only guaranteed to yield the correct result
for the eigenvalue density in the limit, i.e. limN â†’âˆ Ï(z)
(see Appendix A); finite-size corrections obtained from
Eqs. (2.5)â€“(2.9) are not in general correct, and o(1) contributions to g(z)2 or Ï(z) obtained from Eqs. (2.9) and
(2.8) should be discarded.
Furthermore, in general, the correct way of finding the
support of limN â†’âˆ Ï(z) using Eq. (2.5) is by setting the
left side of the inequality (2.5) to lim g 2 â†’ 0+ lim N â†’
âˆ of the left side of Eq. (2.9), as discussed in Sec. III
and Appendix A. However, in writing Eq. (2.5) we have
simply set g = 0 in Eq. 2.9, and thus implicitly taken
the limit g 2 â†’ 0+ before the N â†’ âˆ limit. To correctly
express the support, we must first define the function


1
K(g, z) â‰¡ lim tr
N â†’âˆ
Mz Mzâ€  + g 2

N 
1 X
1
= lim
(2.19)
N â†’âˆ N
si (z)2 + g 2
i=1

5
for fixed, strictly positive g, which serves to regularize
the denominators in Eq. (2.19) for si (z) which are zero
or vanishing in the limit N â†’ âˆ. The generally correct
way of expressing Eq. (2.5) or Eq. (2.11) is then
K(0+ , z) â‰¡ lim+ K(g, z) â‰¥ 1.
gâ†’0

(2.20)

Let us denote the the support of limN â†’âˆ Ï(z), given by
Eq. (2.20), by S0+ and the region specified by the limit
N â†’ âˆ of Eq. (2.5) or Eq. (2.11) by S0 . For many examples of M , L and R, the limits N â†’ âˆ and g â†’ 0+
commute everywhere and hence S0+ = S0 . However, if
there are zâ€™s at which some of the smallest si (z) are either zero or vanish in the limit N â†’ âˆ, the two limits
may fail to commute, and the naive use of Eq. (2.5) can
yield a region, S0 , strictly larger than and containing S0+ ,
the correct support of limN â†’âˆ Ï(z). For example, at zâ€™s
for which a Î˜(1) number of si (z) are zero or o(1), these
singular values do not make a contribution to K(g, z)
for g > 0 (their contribution to the sum in Eq. (2.19)
is O(N âˆ’1 )) and hence to K(0+ , z), but if they vanish
sufficiently fast as N â†’ âˆ they can make a nonzero contribution to the left side of Eq. (2.11); such z may fall
within S0 , but not within S0+ . For finite N , the si (z)
can vanish exactly when z coincides with an eigenvalue
of M ; thus the above situation can, e.g., arise close to
eigenvalues of M that are isolated and far from the rest
of M â€™s spectrum, so that they fall outside the support of
limN â†’âˆ Ï(z). In such cases, the spectrum of M + LJR
will nonetheless typically also contain isolated eigenvalues (which do not contribute to limN â†’âˆ Ï(z)) with effectively deterministic location, i.e. within o(1) distance
of corresponding isolated eigenvalues of M ; examples of
this phenomenon, for which S0 âˆ’ S0+ is not empty but
has zero measure, have been studied in Refs. [36, 37]
(for symmetric matrices, outlier eigenvalues corresponding to eigenvalues of the mean matrix were first studied
in Ref. [38]). For some choices of M , L and R, however, a
more interesting case can arise such that for z in a certain
region of the complex plane with nonzero measure, all
si (z) are nonzero at finite N (hence M has no eigenvalue
there), but a few si (z) are o(1) and vanish sufficiently
fast as N â†’ âˆ; in particular when L âˆ R âˆ 1, this
can occur for certain highly nonnormal M [60]. In such
cases the non-commutation of the two limits can lead to
a difference S0 âˆ’ S0+ with nonzero measure. In cases
we have examined this signifies that there exists a finite,
non-vanishing region outside the support of limN â†’âˆ Ï(z)
(typically surrounding it) where, although Ï(z) is o(1),
it nonetheless converges to zero sufficiently slowly that
a Î˜(1) number of â€œoutlierâ€ eigenvalues lie there (note
that the vast majority of eigenvalues, i.e. (1 âˆ’ o(1))N of
them, lie within the support of the limit density). We
will discuss examples of this phenomenon in Sec. II C below; in one of the examples (discussed in Sec. II C 2), the
existence of such outlier eigenvalues was first noted in
Ref. [39], and their distribution was quantitatively characterized in Ref. [36]. However, the connection between

such outlier eigenvalues and nonzero but o(1) singular
values of Mz , which arise, e.g., for highly nonnormal M ,
were not noted before to the best of our knowledge. We
have observed in simulations (and also supported by [36])
that the distribution of these outliers remains random as
N â†’ âˆ, is in general less universal than limN â†’âˆ Ï(z)
(e.g. it could depend on the choice of real vs. complex
ensembles for J), and its average behavior may not be
correctly given by the non-crossing approximation.

2.

Relationship to pseudospectra

Finally, we note a remarkable connection between our
general result for the support of the spectrum Eq. (2.5)
and the notion of pseudospectra, in the case in which
the limits g 2 â†’ 0+ and N â†’ âˆ commute (so that
Eq. (2.5) correctly describes the support). Pseudospectra
are generalizations of eigenvalue spectra, which are particularly useful in the case of nonnormal matrices (see
Ref. [17] for a review). The eigenvalue spectrum of matrix M can be thought of as the set of points, z, in
the complex plane where (z âˆ’ M )âˆ’1 is singular, i.e. it
has infinite norm. Given a fixed choice of matrix norm,
k Â· k, the pseudospectrum of M at level Ïƒ, or its â€œÏƒpseudospectrumâ€ in the given norm, is the set of points z
for which k(z âˆ’ M )âˆ’1 k â‰¥ Ïƒ âˆ’1 (thus as Ïƒ â†’ 0 we recover
the spectrum). For the specific choice of the operator
norm (i.e. when kAk is taken to be the maximum singular value of A), the Ïƒ-pseudospectrum can equivalently
be characterized as the set of points, z, for which there
exists a matrix perturbation âˆ†M , with kâˆ†M k â‰¤ Ïƒ, such
that z is in the eigenvalue spectrum of M + âˆ†M [17][61].
In words, in the operator norm, the Ïƒ-pseudospectrum of
M is the set to which its spectrum can be perturbed by
adding to it arbitrary perturbations of size Ïƒ or smaller.
In our setting we can think of LJR as a perturbation
of M . Let us focus on the case where L and R are proportional to the identity, i.e., we have âˆ†M = ÏƒJ, with
a positive scalar Ïƒ. Our result Eq. (2.7), in this case
reads kÏƒ(z âˆ’ M )âˆ’1 kF â‰¥ 1 or k(z âˆ’ M )âˆ’1 kF â‰¥ Ïƒ âˆ’1 . In
other words, as N â†’ âˆ, the spectrum of M + ÏƒJ, for
an iid random J with zero mean and variance 1/N , is
the Ïƒ-pseudospectrum of M in the normalized Frobenius
norm defined by Eq. 2.3. Interestingly, the perturbation, âˆ†M = ÏƒJ, has normalized
Frobenius norm Ïƒ as
qP
2 /N , which, by the law
N â†’ âˆ: this norm is Ïƒ
J
ij ij
of large numbers, converges to Ïƒ for large N . That is, as
N â†’ âˆ, the spectrum in response to the random perturbation ÏƒJ, which has size Ïƒ (in normalized Frobenius
norm), is the Ïƒ-pseudospectrum of M in the normalized
Frobenius norm.
This result sounds similar to the equivalence of the two
definitions of pseudospectra for the operator norm which
we noted above (one based on the norm of (z âˆ’ M )âˆ’1 ,
and one based on the spectra of bounded perturbations),
but it differs in two key respects. First, unlike in the

6
case of the operator norm, the general equivalence of the
two notions of pseudospectra noted above does not hold
for the normalized Frobenius norm. Second, for the operator norm, it is not in general the case that the Ïƒpseudospectrum of M is equivalent to the spectrum obtained from a single random perturbation of M of size
Ïƒ, even in the limit N â†’ âˆ (although the spectra arising from such random perturbations are sometimes used
as a â€œpoor manâ€™s versionâ€ or approximation of the pseudospectra [17]). This can be seen as follows. The operator norm of the random iid perturbation, ÏƒJ, i.e. its
maximum singular value, converges almost surely to 2Ïƒ
as N â†’ âˆ [40]. Condition 2.7 for zto be in thespectrum
under this random perturbation is (z âˆ’ M )âˆ’1  â‰¥ Ïƒ âˆ’1 ,
F

or rms({si (z)âˆ’1 }) â‰¥ 1 where the si (z) are the singular
and rms({xi }) represents the root-meanvalues of zâˆ’M
Ïƒ
square of the set of values {xi }. This is not equivalent to
the condition that z be in the 2Ïƒ-pseudospectrum
of M


in the operator norm, i.e. that (z âˆ’ M )âˆ’1  â‰¥ (2Ïƒ)âˆ’1
or smin (z)âˆ’1 â‰¥ 12 , where smin (z) is the minimum of the
si (z); in fact, noting that smin (z)âˆ’1 â‰¥ rms({si (z)âˆ’1 }), it
is easy to see that the spectrum under random iid perturbations with operator norm kÏƒJk = 2Ïƒ is strictly a
proper subset of the 2Ïƒ-pseudospectrum in the operator
norm. For example, for M = 0, the â€œpoor manâ€™s 2Ïƒpseudospectrumâ€ in the limit N â†’ âˆ is a ball of radius
Ïƒ about the origin (the circular law), while the true 2Ïƒpseudospectra of the zero matrix is the ball of radius 2Ïƒ
about the origin.
In sum, in the operator norm, the Ïƒ-pseudospectrum of
M for any N is equivalent to the set of points z for which
some perturbation âˆ†M with kâˆ†M k â‰¤ Ïƒ can be found
such that z is in the spectrum of M +âˆ†M [17]. In the normalized Frobenius norm in the limit N â†’ âˆ, however,
the Ïƒ-pseudospectrum of M is equivalent to the spectrum
of M + âˆ†M where âˆ†M is any random perturbation with
zero-mean iid elements with kâˆ†M kF = Ïƒ. This statement for the normalized Frobenius norm holds when the
two limits N â†’ âˆ and g â†’ 0+ commute; when the two
limits do not commute, the support of the spectral distribution of M + âˆ†M is a subset of the Ïƒ-pseudospectrum
of M in the normalized Frobenius norm.

B.

Average norm squared and power spectrum

As we mentioned in the introduction, an important
phenomenon encountered in dynamics governed by nonnormal matrices, as described by Eq. (2.2) with I(t) = 0,
is transient amplification in asymptotically stable systems. In any stable system, the size of the response to
an initial perturbation eventually decays to zero, with an
asymptotic rate set by the systemâ€™s eigenvalues. In stable
nonnormal systems, however, after an initial perturbation, the size of the network activity, as measured, e.g.,
by its norm squared kx(t)k2 = x(t)T x(t), can nonetheless exhibit transient, yet possibly large and long-lasting

growth, before it eventually decays to zero. By contrast,
in stable normal systems, kx(t)k2 can only decrease with
time. The strength and even the time scale of transient
amplification are set by properties of the matrix A beyond its eigenvalues; they depend on the degree of nonnormality of the matrix, as measured, e.g., by the degree
of non-orthogonality of its eigenvectors, or alternatively
by its hidden feedforward structure (see Eq. (2.34) for
the latterâ€™s definition).
Nonnormal systems can also exhibit pseudo-resonances
at frequencies that could be very different from their natural frequencies as determined by their eigenvalues; such
pseudo-resonances will be manifested in the frequency
power spectrum of the response of the system to time
dependent inputs. kx(t)k2 and the power spectrum of
response are examples of quantities that depend not only
on the eigenvalues of M + LJR but also on its eigenvectors.
Here, we present a few closely related formulas

 for gen
eral M , L and R. These include a formula for kx(t)k2 J ,
i.e. the ensemble average of the norm squared of the state
vector, x(t), as it evolves under Eq. (2.2) with I(t) = 0,
as well as a formula for the ensemble average of the power
spectrum of the response of the network to time-varying
inputs. The results of this section are valid, and in the
case of the power spectrum meaningful, when the system
Eq. (2.2) is asymptotically stable. As we mentioned after
Eq. (2.2), this means that M , L, R and Î³ must be chosen
such that for any typical realization of J, all eigenvalues
of âˆ’Î³1+M +LJR have negative real part. In particular,
the entire support of the eigenvalue density of M + LJR,
as determined by Eq. (2.5), must fall to the left of the
vertical line of zâ€™s with real part Î³; this is a necessary
condition, but may not be sufficient either at finite N
or in cases where an O(1) number of eigenvalues remain
outside this region of support even as N â†’ âˆ.
First, we consider the time evolution of the squared
norm, kx(t)k2 , of the response of the system to an impulse input, I(t) = x0 Î´(t), at t = 0, before which we
assume the system was in its stable fixed point x = 0
(for t > 0 this is equivalent to the squared norm of the
activity as it evolves according to Eq. (2.2) with I(t) = 0,
starting from the initial condition x(0) = x0 ). We provide a formula for the ensemble average of the more
general quadratic function, x(t)T Bx(t), where B is any
N Ã— N symmetric matrix; the norm squared corresponds
to B = 1. The result for general B, M , L and R is given
as a double inverse Fourier transform
hx(t)T Bx(t)iJ =
(2.21)
ZZ
dÏ‰1 dÏ‰2 it(Ï‰1 âˆ’Ï‰2 )
e
Tr[B C x (Ï‰1 , Ï‰2 ; x0 xT0 )] ,
2Ï€ 2Ï€
in terms of the N Ã— N Fourier-domain


â€œcovariance matrix,â€ C x (Ï‰1 , Ï‰2 ; x0 xT0 ) â‰¡ xÌƒ(Ï‰1 )xÌƒ(Ï‰2 )â€  J (where xÌƒ(Ï‰) is
the Fourier transform of x(t)). The expression for the

7
latter is given by
C x (Ï‰1 , Ï‰2 ; C I ) = C0x (Ï‰1 , Ï‰2 ; C I ) + âˆ†C x (Ï‰1 , Ï‰2 ; C I )
(2.22)
where
C0x (Ï‰1 , Ï‰2 ; C I ) â‰¡

1
1
, (2.23)
CI
Î³ + iÏ‰1 âˆ’ M
Î³ âˆ’ iÏ‰2 âˆ’ M â€ 

yields the result obtained by ignoring the randomness in
the connectivity (i.e. by setting A = M ), and
1
1
âˆ†C x (Ï‰1 , Ï‰2 ; C I ) â‰¡
LLâ€ 
Î³ + iÏ‰1 âˆ’ M
Î³ âˆ’ iÏ‰2 âˆ’ M â€ 


tr Râ€  R Î³+iÏ‰11 âˆ’M C I Î³âˆ’iÏ‰12 âˆ’M â€ 
 , (2.24)

Ã—
1 âˆ’ tr Râ€  R Î³+iÏ‰11 âˆ’M LLâ€  Î³âˆ’iÏ‰12 âˆ’M â€ 

is the contribution of the random part of connectivity
LJR. For later use, we have provided these expressions for a general third argument in C x (Â·, Â·; Â·); for use
T
in Eq. (2.21) C 
I must be
 substituted with x0 x0 . In the
2
special case of kx(t)k J corresponding to B = 1, and
iid disorder (L = 1, R = Ïƒ1), the contributions from
Eqs. (2.23)â€“(2.24) can be more compactly combined into
ZZ



dÏ‰1 dÏ‰2 it(Ï‰1 âˆ’Ï‰2 )
2
kx(t)k J =
e
(2.25)
2Ï€ 2Ï€
xT0 Î³âˆ’iÏ‰12 âˆ’M â€  Î³+iÏ‰11 âˆ’M x0


1 âˆ’ Ïƒ 2 tr Î³+iÏ‰11 âˆ’M Î³âˆ’iÏ‰12 âˆ’M â€ 
1
1
1
1
T
(we used Tr( z1 âˆ’M
x0 xT0 z2 âˆ’M
â€  ) = x0 z âˆ’M â€  z âˆ’M x0 to
1
2
write the numerator in Eq. (2.25)).
Next, we look at the power spectrum of the response
of the system to a noisy input, I(t), that is temporally
white, with zero mean and covariance
I
Ii (t1 )Ij (t2 ) = Î´(t1 âˆ’ t2 )Cij
.

(2.26)

Here the bar indicates averaging over the input noise (or
by ergodicity, over a long enough time). Our general result for the ensemble average of the matrix power spectrum of the response, which by definition is the Fourier
transform of the steady-state response covariance,
Z
x
Cij (Ï‰) â‰¡ dÏ„ eâˆ’iÏ‰Ï„ xi (t + Ï„ )xj (t),
(2.27)
is given by
hC x (Ï‰)iJ = C0x (Ï‰) + âˆ†C x (Ï‰).

(2.28)

Here we defined
C0x (Ï‰) â‰¡ C0x (Ï‰, Ï‰; C I )

(2.29)

and
âˆ†C x (Ï‰) â‰¡ âˆ†C x (Ï‰, Ï‰; C I )

(2.30)

are the power spectrum matrices obtained by ignoring
the randomness in connectivity (i.e. by setting A = M ),
and the contribution of quenched randomness LJR, respectively.
A closely related quantity is the total power of the
steady-state response
of the âˆšsystem to a sinusoidal inâˆš
Ï‰t
put I(t) = I0 2 cos
âˆš (the 2 serves to normalize the
average power of 2 cos Ï‰t to unity, so that the total
power in the input is kI0 k2 ). For such an input, the
steady-state activity, which we denote by xÏ‰ (t), is also
sinusoidal (with a possible phase shift). By total power
of the steady-state response we mean the time average of
the squared norm of the activity, kxÏ‰ (t)k2 , where now
the bar indicates temporal averaging (we call this total power, because the squared norm sums the power
in all components of xÏ‰ (t)). As in Eqs. (2.21)â€“(2.24), we
present a formula for the ensemble average of the more
general quantity xTÏ‰ B xÏ‰ . We have



xTÏ‰ B xÏ‰ J = Tr(B hC x (Ï‰)iJ ) ,
(2.31)

where hC x (Ï‰)iJ is given by Eqs. (2.28)â€“(2.30) with C I
replaced by I0 IT0 . For the special case of B = 1, corresponding to the total power of the response at frequency
Ï‰, using Eq. (2.23)â€“(2.24) with Ï‰1 = Ï‰2 = Ï‰, this formula can be simplified into
D
E
kxÏ‰ k2 =
(2.32)
J

2 
2
 

1
1

2 
L R zâˆ’M
I0 
 zâˆ’M
 1

F


,
2

 z âˆ’ M I0  +


1
L
1 âˆ’ R zâˆ’M
F

where z = Î³ + iÏ‰, k Â· k denotes the vector norm, and k Â· kF
denotes the Frobenius norm defined in Eq. (2.3). Finally,
for the case that the random part of the matrix is iid, i.e.
L = Ïƒ1 and R = 1, we can further simplify Eq. (2.32)
into


E
D
(Î³ + iÏ‰ âˆ’ M )âˆ’1 I0 2
kxÏ‰ k2 =
2 . (2.33)
J
1 âˆ’ Ïƒ 2 k(Î³ + iÏ‰ âˆ’ M )âˆ’1 k
F

The stability of the x = 0 fixed point guarantees the positivity of the expressions Eqs. (2.32)â€“(2.33) for the power
spectrum. This is true because, as we noted above, stability requires that the support of the eigenvalue density
of A is entirely to the left of the vertical line Re(z) = Î³.
By our result Eq. (2.7) for that support, this can only be
true if the denominators of the last terms in Eq. (2.32)
â€“(2.33) are positive, which guarantees the positivity of
the full expressions.
Note that the first term in Eq. (2.32) and the numerator in Eq. (2.33) represent the power spectrum in the
absence of randomness, i.e. if A, in Eq. (2.2) is replaced
with M . Thus, formulae (2.32)â€“(2.33) show that the
correct average power spectrum is always strictly larger
than the naive power spectrum obtained by assuming
that random effects will â€œaverage outâ€. Furthermore,

8
due to the denominators of the last terms in Eqs. (2.32)â€“
(2.33), the power spectrum will be larger for frequencies
where the support of the eigenvalue density, Eq. (2.7),
is closer to the vertical line with Re(z) = Î³. Similar,
but less precise statements can also be made about the
strength of transient amplification using formulae (2.21)â€“
(2.25) for the squared norm of the impulse response. One
measure of the strength of transient amplification up to
RT
time T is 0 kx(t)k2 dt. Integrating formulae Eq. (2.21)
(with B = 1) or Eq. (2.25) over t, one obtains formuRT
lae for 0 kx(t)k2 dt that are the same as Eqs. (2.21)â€“
(2.25), except for the factor eit(Ï‰1 âˆ’Ï‰2 ) in the integrands of
iT (Ï‰1 âˆ’Ï‰2 +i)

Eqs. (2.21) and (2.25) being replaced by i[1âˆ’eÏ‰1 âˆ’Ï‰2 +i ]
(with  â†’ 0+ ). Due to the denominator in this factor (for
T sufficiently large the numerator is constant), the main
contribution to the integrals over Ï‰1 and Ï‰2 should typically arise for Ï‰1 â‰ˆ Ï‰2 . On the other hand, note that
for Ï‰1 = Ï‰2 the denominators in Eqs. (2.24)â€“(2.25) reduce to the those in Eqs. (2.32)â€“(2.33), with the connection to the support of the spectral density noted above.
RT
Thus this dominant contribution to 0 kx(t)k2 dt must be
larger, the closer the support of the eigenvalue density,
Eq. (2.7), is to the vertical line with Re(z) = Î³. This
also suggests that, as in the case of the power spectrum,
the strength of transient amplification would typically be
underestimated if randomness of connectivity is ignored
and only its ensemble average M is taken into account in
solving Eq. (2.2).

Numerical simulations indicate that the quantities
kx(t)k2 and kxÏ‰ k2 are self-averaging in the large N limit;
that is, for large N , kx(t)k2 or kxÏ‰ k2 for any typical random realization of J will be very close to their ensemble averages, given by Eq. (2.25) and Eq. (2.32) respectively, with the random deviations from these averages
approaching zero as N goes to infinity (see Figs. 1, 3
and 8, below). This conclusion is also corroborated by
rough estimations (not shown) based on Feynman diagrams (the diagrammatic method is introduced in Secs.
III and IV) of the variance of fluctuations of these quantities for different realizations of J.

Finally, we note that the general formulae presented
in this section are valid only for cases where the initial
condition, x0 , or the input structure, I0 or C I , are chosen
independently of the particular realization of the random
matrix J (e.g., cases where x0 is itself random but independent of J, or when x0 is chosen based on properties
of M , L or R). In particular, our results do not apply
to cases in which the initial condition or the input is tailored or optimized for the particular realization of the
quenched randomness, J, in which case the true result
could be significantly different from those given by the
formulae of this section.

C.

Some specific examples of M , R and L

In this section we present the results of explicit calculations of the eigenvalue density Eq. (2.8), the average squared norm of response to impulse Eqs. (2.21) and
(2.25), and the total power in response to sinusoidal input
Eq. (2.33), for specific examples of M , L and R (the details of the calculations for the results presented here can
be found in Sec. V). For many of the examples presented
here, L and R are both proportional to the identity matrix; thus in these examples the full matrix is of the form
M + ÏƒJ where Ïƒ > 0 determines the strength of disorder
in the matrix. In Secs. II C 2 and II C 3, we also present
examples with nontrivial L and/or R.
Any matrix, M , can be turned into an upper-triangular
form by a unitary transformation, i.e.
M = U T U â€ ,

(2.34)

where U is unitary and T is upper-triangular (i.e. Tij = 0
if i > j) with its main diagonal consisting of the eigenvalues of M . The difference between nonnormal and normal matrices is that for the latter, T can be taken to
be strictly diagonal. Equation (2.34) is referred to as a
Schur decomposition of M [41], and we refer to the orthogonal modes of activity represented by the columns
of U as Schur modes. The Schur decomposition provides
an intuitive way of characterizing the dynamical system
Eq. (2.2). Rewriting Eq. (2.2), with J and I(t) set to zero,
in the Schur basis by defining y = U â€  x (i.e. yi is the activity in the i-th Schur mode), we obtain dy
dt = âˆ’Î³y+T y.
We see that activity in the j-th Schur mode provides an
input to the equation for the i-th mode only when i â‰¤ j
(as Tij = 0 for i > j). Thus the coupling between modes
is feedforward, going only from higher modes to lower
ones, without any feedback. We refer to Tij â€™s for j > i as
feedforward weights. As these vanish for normal matrices, we can say a matrix is more nonnormal the stronger
its feedforward weights are.
Due to the invariance of the trace, the norm, and
the adjoint operation under unitary transforms, our general formulae for the spectral density Eq. (2.8) and
the average squared norm in time and frequency space,
Eqs. (2.25) and (2.33), take the same form in any basis, so
in particular we can work in the Schur basis of M . Hence
M can be replaced by T , provided L and R are also expressed in M â€™s Schur basis and x0 or I0 are replaced by
U â€  x0 or U â€  I0 , respectively [62]. Thus we use the feedforward structure of the Schur decomposition to characterize the different examples we consider below. Our
examples are chosen to demonstrate interesting features
of nonnormal matrices in the simplest possible settings.

1.

Single feedforward chain of length N

In the first example, each and every Schur mode is
only connected to its lower adjacent mode, forming a long

9
%

!"#

xÏ‰ 2



$"%

ImÎ»

3


Ï(r)

!#$

Ï(r)

"#&
!

r

$

"

r

"#$
$"#

$

Ã¯
Ã¯

Ã¯
Ã¯

0

Ï‰







"





$"#

"#$





!



!"#
!"#

5


ReÎ»

"#'

$"!

!


0

ImÎ»





!

$"#

!
$
&&&

$"#

!

!"#





!

"#$

"

"#$

!

%%%
ReÎ»

ReÎ»

0

Ã¯5
Ã¯

Ã¯
Ã¯

Ã¯
Ã¯

0

ImÎ»







FIG. 1: (Color online) Top panel: the total power spectrum of
steady state response kxÏ‰ k2 as a function of input frequency
Ï‰, Eq. (2.33), for the system Eq. (2.2) with A = M + ÏƒJ,
and M given by Eq. (2.35) with w = 1 and Î»n = Â±i (with +i
and âˆ’i alternating), respectively. Here, N = 700, Ïƒ = 0.5,
and Î³ = 0.8. The input was fed into the last component of
x (the beginning of the feedforward chain characterized by
Eq. (2.35)), which for the matrix
M has natural frequency
âˆš
-1. That is, the input was I0 2 cos Ï‰t where I0 was 1 for the
last component and 0 for all other components. The green
(thick dashed)
D curve
E is the ensemble average of the total power
spectrum, kxÏ‰ k2 , calculated numerically using the general
J

formula Eq. (2.33), which is compared with an empirical average over 100 realizations of real Gaussian J (solid red line,
mostly covered by the dashed green line). The pink (light
gray) area shows the standard deviation among these 100 realizations around this average. The blue (thin) line shows
the result when disorder, ÏƒJ, is ignored, i.e. A is replaced
by its ensemble average M . Bottom panel: the eigenvalue
spectrum of M + ÏƒJ (black dots). Red big dots at Â±i show
the eigenvalues of M . The red curve is the outer boundary of
the eigenvalue spectrum of A as computed numerically using
Eq. (2.5). The real and imaginary axes of the complex plane
are interchanged, so that the frequency axis in the top panel
can be matched with the imaginary part of the eigenvalues,
i.e. the natural frequencies of Eq. (2.2).

feedforward chain of length N . For simplicity, we take all
feedforward weights in this chain to have the same value
w, so that
ï£«
ï£¶
Î»1 w 0 Â· Â· Â·
ï£¬
ï£·
M = T = ï£­ 0 Î»2 w Â· Â· Â· ï£¸
(2.35)
.. .. .. . .
.
. . .
or more succinctly Mnm = w Î´n+1,m + Î»n Î´nm .
Figure 1 shows the power spectrum of response (top
panel) and the eigenvalue distribution (bottom panel) of
A = M + ÏƒJ for an example M of the form Eq. (2.35)

FIG. 2: (Color online) The eigenvalue spectra of A = M +
ÏƒJ for N = 2000 and M given by Eq. (2.35) with Î»n = 0,
w = 1 for single realizations of real Gaussian J. Ïƒ = 0.95
and 0.5 in the left and rights panels, respectively. The red
circles mark the circular boundaries of the spectral support
given by Eq. (2.36). The insets show a comparison of the
analytic formula Eq. (2.37) for the spectral density (black
smooth trace) and histograms corresponding to the particular
realization shown in the main plot (red jagged trace).

with alternating imaginary eigenvalues, Î»n = (âˆ’1)n+1 i.
The black dots in the bottom panel of Fig. 1 show the
eigenvalues of A for one realization of J, scattered around
the highly degenerate spectrum of M at Â±i (red dots).
The top panel shows the ensemble
average
of the total
E
D
power spectrum of response, kxÏ‰ k2 , of the system
J

Eq. (2.2) to sinusoidal stimuli as given by our general formula Eq. (2.33) (green curve), showing that it perfectly
matches the empirical average (red curve) over a set of
100 realizations of J (the latter was obtained by generating 100 realizations of J, calculating kxÏ‰ k2 for each
realization, which is given by the numerator of Eq. (2.33)
with M replaced by M + ÏƒJ, and then averaging the results over the 100 realizations). The pink (light gray)
shading shows the standard deviation of the power spectrum over these 100 realizations. This will shrink to zero
as N goes to infinity, so that for large N the power spectrum of any single realization of A = M + ÏƒJ will lie
very close to the ensemble average. The system (2.2) in
the zero disorder case, Ïƒ = 0, has two highly degenerate
resonant frequencies (imaginary parts of the eigenvalues
of M ), Ï‰0Â± = Â±1, leading to possible peaks in the power
spectrum at these frequencies. The smaller the decay of
these modes (in this case given by Î³) is, i.e. the closer
the eigenvalues of the combined matrix âˆ’Î³ + M are to
the imaginary axis, the sharper and stronger are the resonances. Comparing the zero disorder power spectrum
(blue curve) with that for A = M + ÏƒJ, we see that the
disorder has led to strong but unequal amplification of
the two resonances relative to the case without disorder.
This is partly due to the disorder scattering some of the
eigenvalues of âˆ’Î³ + A much closer to the imaginary axis,
creating larger resonances.

10
For M of the form (2.35) with all eigenvalues zero
we have analytically calculated the eigenvalue density, Eq. (2.8), the magnitude of response to impulse
Eq. (2.25), and the power-spectrum Eq. p
(2.33). In this
case, using Eq. (2.5) naively yields |z| â‰¤ |w|2 + Ïƒ 2 for
the support of the eigenvalue density. However, using the
correct procedure, Eqs. (2.19)â€“(2.20), we find that this
formula is only correct for Ïƒ â‰¥ |w|, while for Ïƒ < |w|,
the true support of the eigenvalue density in the limit
N â†’ âˆ is the annulus
p
p
|w|2 âˆ’ Ïƒ 2 â‰¤ |z| â‰¤ |w|2 + Ïƒ 2 ,
(2.36)
(this result was obtained in Ref. [31]). Within this support the eigenvalue density in either case is
"
#
|w|2
1
Ï(z) =
1âˆ’ p
.
(2.37)
Ï€Ïƒ 2
4|w|2 |z|2 + Ïƒ 4

Figure 2 demonstrates the close agreement of Eqs. (2.36)â€“
(2.37) with the empirical spectrum of M +ÏƒJ for a single
realization of J, for N = 2000 and two different values
of Ïƒ. The discrepancy between the results obtained by
the naive use of Eq. (2.5) and Eq. (2.36) is due to the
fact that for |z| < |w|, Mz = (z âˆ’ M )/Ïƒ has an exponentially small, O(eâˆ’cN ), singular value (see next paragraph)
which makes the result of Eqs. (2.19)â€“(2.20) dependent
on the order of the two limits N â†’ âˆ and g â†’ 0+ . As we
discussed after Eq. (2.12), such a discrepancy can signify
the existence of an O(1) number of outlier eigenvalues
outside the support of limN â†’âˆ
p Ï(z). Simulations show
that this is the case for |z| < |w|2 âˆ’ Ïƒ 2 (see Fig. 2).
The most striking aspect of these results is revealed in
the limit Ïƒ â†’ 0. For Ïƒ = 0, the spectrum is that of M ,
which is concentrated at the origin. Remarkably, however, as seen from Eqs. (2.36)â€“(2.37), for very small but
nonzero Ïƒ the bulk of the eigenvalues are concentrated
in the narrow ring with modulus |z| â‰ˆ |w|. Thus in the
limit N â†’ âˆ the spectrum has a discontinuous jump
at Ïƒ = 0. This is a consequence of the extreme nonnormality of M , which manifests itself in the extreme
sensitivity of its spectrum to small perturbations, which
is well-known (see Ref. [17], Ch. 7). The notion of pseudospectra quantifies this sensitivity: the (operator norm)
-pseudospectrum of M is the region of complex plane
to which its spectrum can be perturbed by adding to
M a matrix of operator norm no larger than . As we
mentioned in Sec. II A, this is precisely the set of complex values z for which k(z âˆ’ M )âˆ’1 k > âˆ’1 [17], and
therefore by the definition of the operator norm k Â· k,
the region in which k(z âˆ’ M )âˆ’1 kâˆ’1 = smin (z âˆ’ M ) < ,
where smin (z âˆ’ M ) is the least singular value of z âˆ’ M .
As noted above, for |z| < |w|, smin (z âˆ’ M ) is exponentially small: smin (z âˆ’ M ) â‰¤ |w|| wz |N (for a proof see after
Eq. (5.15) in Sec. V A). Thus the -pseudospectrum of
M contains the set of points z satisfying |w|| wz |N < ,
 1/N
i.e. the centered disk with radius |w|( |w|
)
which approaches |w| as N â†’ âˆ. In other words, for large enough

1

x(t)20.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

1

2

3

4

5

6

7

8

9

Î³t
FIG. 3: (Color online) The norm squared of the response to
impulse, kx(t)k2 , of the system Eq. (2.2), for A = M + ÏƒJ,
with binary J, and M given by Eq. (2.35) (with Î»n = 0)
describing a N -long feedforward chain with
âˆš uniform weights
w. Here, w = 1, Ïƒ = 0.5, Î³ = 1.005 Ïƒ 2 + w2 ' 1.124,
and N = 700. The green (thick dashed) curve shows our

result, Eq.
 (2.38), for the average squared impulse response,
kx(t)k2 J , which lies on top of the red (thick solid) curve
showing the empirical average of kx(t)k2 over 100 realizations
of binary J. The five thin dashed black curves show the result for five particular realizations of J, and the pink (light
gray) area shows the standard deviation among the 100 realizations. The standard deviation shrinks to zero as N â†’ âˆ,
and kx(t)k2 for any realization lies close to its average for
large N . For comparison the purple (thin, lowest) curve shows
kx(t)k2 obtained by ignoring the effect of quenched disorder,
i.e. by setting A = M .

N , any point |z| < |w| is in the -pseudospectrum for
any fixed , no matter how small. It has been stated [17]
that dense random perturbations, of the form ÏƒJ considered here, tend to trace out the entire -psuedospectrum
(where  = ÏƒkJk â‰ˆ 2Ïƒ). Our result shows that, for
, Ïƒ  |w|, the spectrum of such perturbations traces
out the -psuedospectrum in quite an uneven fashion; the
vast majority (Î˜(N )) of the perturbed eigenvalues only
trace out the boundary of the pseudospectrum, |z| â‰ˆ |w|,
while only a few (O(1)) eigenvalues lie in its interior.
Thus, dense random perturbations can fail as a way of
visualizing (operator norm based) pseudospectra.
We now turn to the dynamics. We have explicitly calculated the average evolution of the magnitude of x(t),
Eq. (2.25), and the total power spectrum of steadystate response, Eq. (2.33), for the case where the initial condition is (or the input is fed into) the last Schur
mode, i.e. the beginning of the feedforward chain: x0 =
(0, Â· Â· Â· , 0, 1)T ( or I0 âˆ (0, Â· Â· Â· , 0, 1)T ). For the evolution
of the average norm squared, with the initial condition
x0 = (0, Â· Â· Â· , 0, 1)T , we obtain
p



kx(t)k2 J = eâˆ’2Î³t I0 (2t |w|2 + Ïƒ 2 ),
(t â‰¥ 0)
(2.38)
where IÎ½ (x) is the Î½-th modified Bessel function. Fig-

11
ure 3 plots the function Eq. (2.38) and compares it with
the result obtained by ignoring the disorder (corresponding to Ïƒ = 0). The main difference between the two
curves is the slower asymptotic decay of the Ïƒ 6= 0 result (green) compared with the zero-disorder case (purple). This is the result of the disorder spreading some
of the eigenvalues of âˆ’Î³ + A closer to the imaginary
axis, creating modes with smaller decay. Importantly,
in neither case do we see transient amplification. By
contrast, in the Ïƒ = 0 and for small enough decay, i.e.
for Î³ < |w|, the system Eq. (2.2) exhibits very strong
transient amplification. In this case, starting from the
initial condition x0 = (0, Â· Â· Â· , 0, 1)T , the solution for the
n
âˆ’Î³t
(N âˆ’ n)-th Schur component is xN âˆ’n (t) = (wt)
n! e
(for 0 â‰¤ n â‰¤ N âˆ’ 1), which is maximized at t = n/Î³
n
with a value max |xN âˆ’n | âˆ¼ ( |w|
Î³ ) for n  1. Thus up
to time t âˆ¼ N/Î³ the norm of the activity grows expo2Î³t
for t . N/Î³. For larger
nentially; kx(t)k2 & ( |w|
Î³ )
times the activity reaches the end of the N -long feedforward chain and starts decaying to zero; asymptotically
kx(t)k2 âˆ¼ eâˆ’2Î³t for t  N/Î³. However, as we have seen,
the spectrum of M is extremely sensitive to perturbations; even for very small but nonzero Ïƒ, the spectrum of
âˆ’Î³1+A has eigenvalues with real part as large as |w|âˆ’Î³.
Therefore, in the limit N â†’ âˆ, the system Eq. (2.2) is
unstable for |w| > Î³, as soon as Ïƒ 6= 0. Conversely, in the
presence of disorder (even infinitesimally small disorder
in the N â†’ âˆ limit), as long as the
p system is stable
(which from Eq. (5.15) requires Î³ > |w|2 + Ïƒ 2 ) it exhibits no transient amplification for the initial condition
along the last Schur mode. Let us note, however, that
as we mentioned after Eq. (2.33), Eq. (2.25) and hence
Eq. (2.38) do not yield the correct answer when the direction of the impulse is optimized for the specific realization of the quenched disorder J; such disorder-tuned
initial conditions can yield significant transient amplification even for the stable Ïƒ 6= 0 system.
Incidentally, we can also read the result
for M = 0
from Eq. (2.38), by setting w = 0, obtaining kx(t)k2 J =
eâˆ’2Î³t I0 (2Ïƒt). Since in this case, all directions are equivalent, this is the answer for the (normalized) initial condition along any direction, again as long as the direction
is chosen independently of the specific realization of J.

Finally, the total power of response to a sinusoidal input with amplitude I0 = (0, Â· Â· Â· , 0, I0 )T is given by
D
E
kxÏ‰ k2 =
J

Ï‰2

+

kI0 k2
.
âˆ’ |w|2 âˆ’ Ïƒ 2

Î³2

(2.39)

The main effect of the disorder
D is to Ereduce the width
of the resonance (the peak of kxÏ‰ k2 at Ï‰ = 0) and
J
increase its height. This is partly a consequence of the
scattering of the eigenvalues of âˆ’Î³ +A closer to the imaginary line by the disorder, creating modes with smaller
decay.

2.

Examples motivated by Daleâ€™s law: 1 or N/2 feedforward
chains of length 2

In this section we consider examples motivated by
Daleâ€™s law [4â€“6] in neurobiology. Daleâ€™s law is the observation (which holds generally but with some exceptions [42, 43]) that individual neurons release the same
neurotransmitter at all of their synapses. In the context
of many theoretical papers including this one, it refers
more specifically to the fact that an individual neuron
either makes only excitatory synapses or only inhibitory
synapses; that is, each column of the synaptic connectivity matrix has a fixed sign, positive for excitatory neurons
and negative for inhibitory ones. We will first consider
two examples of connectivity matrices respecting Daleâ€™s
law which take the form Eq. (2.1) with L = Ïƒ âˆ’1 R = 1,
and a scalar Ïƒ. At the end of this subsection we consider
an example with nontrivial L and R.
In the first example, we consider a matrix M , which
as we will show, has a Schur form T that is composed
of N/2 disjoint feedforward chains, each connecting only
two modes (we assume N is even). For simplicity we will
focus on the case where all eigenvalues are zero. Thus in
the Schur basis we have
ï£¶
ï£«
0 w1 0 0 Â· Â· Â·
ï£¬0 0 0 0 Â· Â· Â· ï£·
 
ï£¬0 0 0 w Â· Â· Â· ï£·
ï£·=W âŠ— 01
2
(2.40)
T =ï£¬
ï£¬0 0 0 0 Â· Â· Â· ï£·
00
ï£¸
ï£­
.. .. .. .. . .
.
. . . .
where we defined W to be the N/2 Ã— N/2 diagonal matrix of Schur weights W = diag(w1 , w2 , . . . , wN/2 ). T in
Eq. (2.40) arises as the Schur form of a mean matrix of
the form




1 K âˆ’K
1 1 âˆ’1
M=
=
âŠ—K
(2.41)
2 K âˆ’K
2 1 âˆ’1

where K is a normal (but otherwise arbitrary) N/2Ã—N/2
matrix (note that M is nonetheless nonnormal). The
feedforward weights in Eq. (2.40) are then the eigenvalue
of K. When K has only positive entries, matrices
of the form Eq. (2.41) satisfy Daleâ€™s principle, and
were studied in Ref. [7], in the context of networks
of excitatory and inhibitory neurons. We imagine a
grid of N/2 spatial positions, with an excitatory and
an inhibitory neuron at each position. 21 K, a matrix
with positive entries, describes the mean connectivity
strength between spatial positions, which is taken to
be identical regardless of whether the projecting, or
receiving, neuron is excitatory or inhibitory. The sign of
the weight, on the other hand, depends on the excitatory
or the inhibitory nature of the projecting or presynaptic
neuron; the first (last) N/2 columns of M represent
the projections of the excitatory (inhibitory) neurons
and are positive (negative). Since K is normal it can
be diagonalized by a unitary transform: K = EW E â€  ,
where W is as above, and E = (e1 , e2 , . . .) is the matrix

12
ImÎ»

!"&

%

n< (r)

%

ImÎ»

!"&

!"(

5

n< (r)

4

!"(

!"#

ImÎ»

!"#

r

!

!"$

!"%

!"%

!

!


!"%

!"%


!"$

!"$


!"#

!"#




!"# !"$ !"%

!
'''
ReÎ»

!"%

!"$

!"#

r

!

!"$

3
2
1



0



âˆ’1


!"#



!"$



!"%



!

!"%

!"$

!"#

âˆ’2

'''
ReÎ»

FIG. 4: (Color online) The eigenvalue spectra of A = M + ÏƒJ
for a binary J with Ïƒ = 0.1 and M given by Eq. (2.41) with
K = 1 (corresponding to wb = 1 for all the diagonal 2 Ã— 2
blocks in Eq. (2.40)). The main panels show the eigenvalues
for single realizations of J, with N = 600 (left) and N = 60
(right). The red circles mark the boundaries of the spectral
support, Eq. (2.44). Since A is real in this case, its eigenvalues
are either exactly real, or come in complex conjugate pairs;
the spectrum is symmetric under reflections about the real
axis. However, such signatures of the reality of the matrix
appear only as subleading corrections to the spectral density
Ï(z); they are finite size effects which vanish as N â†’ âˆ. The
insets show a comparison of the analytic formula Eq. (2.45)
(black curve) and the empirical result, based on the eigenvalues of the realizations in the main panels, for the proportion,
n< (r), of eigenvalues lying within a radius r of the origin
(red dots). The random fluctuations and the average bias of
the empirical n< (r) are both already small for N = 60, and
negligible for N = 600.

of the orthonormal eigenvectors eb of K, b = 1, . . . , N/2
(with
eigenvalues
 
  wb ). Then
to the basis

 transforming

0
eN/2
e1
0
e2
0
,
,
,
,
,...,
eN/2
0
e1
0
e2
0
(where 0 represents the N/2-dimensional vector of 0â€™s)
transforms the matrix
 to being
 block-diagonal with the
w
âˆ’w
b
b
, b = 1, . . . , N/2, along
2 Ã— 2 matrices 12
wb âˆ’wb


0 wb
the diagonal. The bth block becomes
in its
0 0
  


eb
eb
Schur basis âˆš12
, âˆš12
, so the full matrix
eb
âˆ’eb
takes the form Eq. (2.40). Thus, the b-th difference
eb
mode âˆš12
feeds forward to the b-th sum mode
âˆ’eb
 
eb
âˆš1
with weight wb . This feedforward structure
2
eb
leads to a specific form of nonnormal transient amplification, which the authors of Ref. [7] dubbed â€œbalanced
amplificationâ€; small differences in the activity of
excitatory and inhibitory modes feedforward to and
cause possibly large transients in modes in which the
excitatory and inhibitory activities are balanced.
Another interesting example of Daleâ€™s law is that in
which M simply captures the differences between the

âˆ’3
âˆ’4
âˆ’5
âˆ’5

0

5

ReÎ»
FIG. 5: (Color online) The eigenvalue spectra of A = M + ÏƒJ
for the M given by Eq. (2.42) in the balanced case, vT u = 0.
Here, N = 800, Ïƒ = 1 and Âµ = 12 (see equation Eq. (2.43)).
The black dots are the superimposed eigenvalues of A for 20
different realizations of complex Gaussian J. The small red
circle enclosing the vast majority of the eigenvalues has radius
Ïƒ = 1, corresponding to the standard circular law Eq. (2.46).
âˆš
A Î˜(N ) number of eigenvalues lie within this circle. A Î˜( N )
number lie just outside of this circle in a thin boundary layer
which shrinks to zero as N â†’ âˆ. Finally, a Î˜(1) number of
eigenvalues lie at macroscopic distances outside the unit circle.
The dashed blue circle shows radius r0 given by Eq. (2.44);
outliers can even lie outside this boundary.

mean inhibitory and mean excitatory synaptic strengths
and between the numbers of excitatory and inhibitory
neurons, with no other structure assumed (uniform mean
connectivity), as studied in Ref. [39]. Thus,
âˆš all excitatory projections have the same mean
Âµ
/
âˆš E N , and all
inhibitory ones have the mean âˆ’ÂµI / N . If we assume
a fraction f of all neurons are excitatory, then we can
write M as
M = uvT

(2.42)

where u = N âˆ’1/2 (1, . . . , 1)T is a unit vector, and the vector v has components vi = ÂµE or vi = âˆ’ÂµI for i â‰¤ f N
and i > f N , respectively (for f = 1/2 and ÂµE = ÂµI ,
Eq. (2.42) is a special case of Eq. (2.41)). The singlerank matrix M P
has only one non-zero eigenvalue given
by v Â· u = âˆš1N i vi , with eigenvector u. The case in
which the excitatory and inhibitory
weights are balanced
P
on average, in the sense that i vi = 0, is of particular
interest; mathematically it is in a sense the least symmetric and most nonnormal case as v Â· u = 0. In this case all
eigenvalues of M are equal to zero. Furthermore, since
in this case u and v are orthogonal, we can readily read
off the Schur decomposition of M from Eq. (2.42). The
normalized Schur modes are given by u, v/kvk and N âˆ’2

13
N> (Ïƒ) 45
40
35
30
25
20
15
10
5
0

500

1000

1500

N

FIG. 6: (Color online) The number of eigenvalues of M + ÏƒJ,
for the M given by Eq. (2.42), lying outside the circle of radius
Ïƒ vs. N (red line). Here, Ïƒ = 1, Âµ = 12 and vT u = 0. The
numbers (red points connected by solid red lines) are obtained
by numerically calculating the eigenvalues and counting the
outliers for 200 realizations of J, and taking the average of the
counts over all realizations, for N = 100, 200, 400, 800, 1600
(error bars âˆš
show standard error of mean). The black dashed
line plots N for comparison with our theoretical result
Eq. (2.47); the (dashed)
blue line which includes subleadâˆš
ing corrections to N , is obtained by numerically solving
Eq. (5.42) and substituting the result in Eq. (5.43) (these formulae are in turn obtained from Eqs. (2.8)â€“(2.9) in Sec. V B).

other unit vectors spanning the subspace orthogonal to
both u and v. All feedforward Schur weights are
âˆš zero, except for one very large weight, equal to kvk âˆ N , which
feeds from v/kvk to u. Thus the Schur representation
of
âˆš
M has the form Eq. (2.40) with w1 = kvk â‰¡ Âµ N and
wb6=1 = 0, where we defined
Âµ2 â‰¡ tr (M â€  M ) = kvk2 /N = f Âµ2E + (1 âˆ’ f )Âµ2I . (2.43)
Note that this is again a case of balanced amplification:
differences between excitatory and inhibitory activity,
represented by v, feed forward to balanced excitatory
and inhibitory activity, represented by u, with a very
large weight. In the following we present results only for
this balanced case of Eq. (2.42), which as just noted is a
special case of Eqs. (2.40).
We start by presenting the results for the eigenvalue
density. For general diagonal W in Eq. (2.40) (or equivalently, for general normal K in Eq. (2.41)), the eigenvalue
density, Ï(z), of A = M +ÏƒJ is isotropic around the origin
z = 0, and depends only on r = |z|. The spectral support
is a disk centered at the origin. In cases in which all the
weights wb are O(1), the radius of this disk can be found
directly from Eq. (2.5), which yields
"
#1/2
r
1
1 h|wb |2 ib
r0 = Ïƒ
+
+
.
(2.44)
2
4
2Ïƒ 2
Here, h|wb |2 ib is the average of the squared feedfor-

ward weights over all blocks of Eq. (2.40); equivalently,
h|wb |2 ib = 2 tr(M â€  M ) â‰¡ 2Âµ2 . As long as some wb are
nonzero, r0 is larger than the radius of the circular law,
Ïƒ, with the difference an increasing function of h|wb |2 ib ;
thus the spreading of the spectrum of M (originally concentrated at the origin) after the random perturbation
by ÏƒJ, is larger the more nonnormal M is. In cases in
which the feedforward weights of some of the 2 Ã— 2 blocks
of Eq. (2.40) grow without bound as N â†’ âˆ, there is
a corresponding singular value of Mz âˆ z âˆ’ M for every
such block which is nonzero for z 6= 0 but vanishes in
|z|2
the limit, scaling like âˆ¼ |w
where wb is the unbounded
b|
weight of that block (see Eq. (5.37) and its preceding
paragraph). (Note p
that as stated after Eq. (2.3) we assume kM kF = Âµ = h|wb |2 ib /2 is O(1), so that at most
o(N ) number of weights can be unbounded, and each can
at most scale like O(N 1/2 ).) In line with the general discussion after Eq. (2.12), in such cases the naive use of
Eq. (2.5) may yield an area larger than the true support
of limN â†’âˆ Ï(z); the correct support must be found by
using Eqs. (2.19)â€“(2.20), which in this case can yield a
support radius strictly smaller than Eq. (2.44). We have
calculated the explicit results for limN â†’âˆ Ï(z) for two
specific examples of M with the Schur form Eq. (2.40).
The first example belongs to the first case (bounded wb â€™s)
where limN â†’âˆ Ï(z) is Î˜(1) within the entire disk r â‰¤ r0 ,
while the second belongs to the second case (unbounded
wb â€™s) where the limit density is only nonzero in a proper
subset of that disk.
In the first example, we take all the Schur weights in
Eq. (2.40) to have the same value, which we denote by
w. In this case, the eigenvalue density is given by Ï(r) =
1 âˆ‚n< (r)
2Ï€r
âˆ‚r , where n< (r) is the proportion of eigenvalues
within a distance r from the origin and is given by
"
#
r2
|w|2
p
n< (r) = 2 1 âˆ’
. (2.45)
Ïƒ
Ïƒ 2 + Ïƒ 4 + |w|4 + 4|w|2 r2
n< (r) reaches unity exactly at r = r0 given by Eq. (2.44),
and Ï(r) is Î˜(1) for any smaller r. Figure 4 shows the
close agreement of Eq. (2.45) with empirical results based
on single binary realizations of J, for N as low as 60.
The second example is that of the balanced Eq. (2.42)
with u Â· v = 0. As we saw, all wb are zero in this âˆš
case
except for one very large, unbounded weight w1 = Âµ N .
As discussed above, in this case Mz âˆ z âˆ’ M has an o(1)
2
âˆš .
smallest singular value, approximately given by Âµ|z|
N
Using Eqs. (2.19)â€“(2.20), we find that the support of
limN â†’âˆ Ï(z) is the disc with radius Ïƒ (within the annulus Ïƒ < |z| â‰¤ r0 the eigenvalue density is o(1)), and
solving Eqs. (2.8)â€“(2.9) for |z| â‰¤ Ïƒ, we find that the spectral density is in fact identical with the circular law (the
eigenvalue density for the M = 0 case), i.e.
ï£±
(r < Ïƒ)
ï£² Ï€Ïƒ1 2 + o(1),
Ï(r) =
(2.46)
ï£³ o(1)
(r > Ïƒ).

14
It was shown in Refs. [36, 44] that more generally, for
any M of rank o(N ) and bounded kM kF , the eigenvalue
density of A = M + ÏƒJ is given by the circular law in
the limit N â†’ âˆ. For single rank M (as in the present
case) and a diagonal R, it was shown in Ref. [45] that
the eigenvalue density of M + JR agrees with that of JR
as N â†’ âˆ. In the present example, it was observed in
Ref. [39] that even though the majority of the eigenvalues
are distributed according to the circular law, there also
exist a number of â€œoutlierâ€ eigenvalues spread outside the
circle |z| = Ïƒ, which unlike in the M = 0 case, may lie at
a significant distance away from it (see Fig. 5). As we
mentioned in Sec. II A, the non-crossing approximation
cannot be trusted to correctly yield the o(1) contributions to Ï(z) by these outliers for |z| > Ïƒ. However, we
found that if we ignore this warning and use Eqs. (2.8)â€“
(2.9), keeping track of finite-size, o(1) contributions, we
obtain results that agree surprisingly well (though not
completely) with simulations. First, for the total number of outlier eigenvalues lying outside the circle |z| = Ïƒ
we obtain
N> (Ïƒ) â‰¡ N n> (Ïƒ) =

âˆš

N + O(1)

(2.47)

(here we defined n> (r) = 1 âˆ’ n< (r) to be the proportion
of eigenvalues lying outside the radius r); see Fig. 6 for
a comparison of Eq. (2.47) with simulations. The vast
majority of the outlier eigenvalues counted in Eq. (2.47)
lie in a narrow boundary layer immediately outside the
circle |z| = Ïƒ, the width of which shrinks with growing
N . In addition to these, however, there are a Î˜(1) number of eigenvalues lying at macroscopic, Î˜(1) distances
outside the circle |z| = Ïƒ. Using Eqs. (2.8)â€“(2.9) we have
calculated N> (r), the number of outlier eigenvalues lying outside radius r for r > Ïƒ. Figure 7 shows a plot
of N> (r) and compares it with the results of simulations
for different N . For roughly the inner half of the annulus Ïƒ < |z| < r0 , N> (r) agrees well with simulations,
but as r increases it deviates significantly from the empirical averages. In particular, N> (r) calculated from
Eqs. (2.8)â€“(2.9) vanishes at r0 given by Eq. (2.44), while
the empirical average of the number of outliers is nonzero
well beyond r0 . Finally, we note that the distribution of
these eigenvalues is not self-averaging, and depends on
the real vs. complex nature of the random matrix J [36].
In the real case, their distribution has been recently characterized as that of the inverse roots of a certain random
power series with iid standard real Gaussian coefficients
[36].
As for the dynamics, we have analytically calculated
the magnitude of impulse response, Eq. (2.25), as well as
the power-spectrum of steady-state response Eq. (2.33),
for A = M + ÏƒJ with M given by Eqs. (2.40)â€“(2.41)
with general wb , when the (impulse or sinusoidal) input feeds into the second Schur mode in one of the N/2
chains/blocks of Eq. (2.40); we denote the index for this
block by a. For the average magnitude of impulse re-

5

N> (r)
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
1

2

3

4

5

r = |z|

FIG. 7: (Color online) The number, N> (r), of outlier eigenvalues of A = M + ÏƒJ, for the M given by Eq. (2.42), lying farther from the origin than r, as a function of r. Here,
Ïƒ = 1, Âµ = 12 and vT u = 0. The vertical line marks
|z| = r0 ' 3.54 where r0 is given by Eq. (2.44). The colored
(shades of gray) connected points are N> (r) for realizations
of A, based on 200 samples of J, each color for a different
N , for N = 100, 200, 400, 800, 1600 and 3200 (error bars show
standard error of sample mean). Note the lack of scaling of
N> (r) with N .

sponse we find





1 âˆ’ Ca
1 + Ca
2
I0 (2r0 t) +
J0 (2r1 t) eâˆ’2Î³t
kx(t)k J =
2
2
(2.48)
where J0 (x) (I0 (x)) is the (modified) Bessel function, r0
is given by Eq. (2.44), we defined r12 = r02 âˆ’ Ïƒ 2 , and
1 + 2|wa |2 /Ïƒ 2
,
Ca â‰¡ p
1 + 2h|wb |2 ib /Ïƒ 2

(2.49)

with h|wb |2 ib = 2 tr(M â€  M ) denoting the average squared
feedforward weight among all the blocks of Eq. (2.40).
In Fig. 8 we plot Eq. (2.48) and compare it with the
result obtained by ignoring the disorder (i.e. by setting
Ïƒ = 0); in the latter case, the block a is decoupled from
the rest of the network, and solving
the2 Ã— 2 linear

âˆ’Î³ wa
system governed by the matrix
, we obtain
0 âˆ’Î³
2
2 2 âˆ’2Î³t
kx(t)k = (1 + wa t )e
. From the figure, we see that
the Ïƒ 6= 0 result (green) has a slower asymptotic decay
compared with the zero-disorder case (purple); this is
due to the disorder having spread some eigenvalues closer
to the imaginary axis, creating modes with smaller decay, along with the fact that the coupling between the
2 Ã— 2 blocks induced by the disorder insures that these
more slowly decaying modes will be activated. Indeed,
for large t, kx(t)k decays like eâˆ’Î³t when Ïƒ = 0, while
in the Ïƒ > 0 case, based on Eq. (2.44) it must decay
like eâˆ’(Î³âˆ’r0 )t , i.e. by a rate set by the largest real part
of the spectrum shifted by âˆ’Î³ (this is indeed what we

15
x(t)2 1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0

1

2

3

4

5

6

7

8

9

10

Î³t
FIG. 8: (Color online) The squared norm of response to impulse, kx(t)k2 , of the system Eq. (2.2), for A = M + ÏƒJ, with
log-normal J, and M given by Eq. (2.40) describing
p N/2 doublet feedforward chains weights wb . Here, wa = h|wb |2 ib =
3, Ïƒ = 0.4, Î³ = 1, and N = 1400. The green (thick dashed)
curve shows our result, Eqs. (2.48)â€“(2.49), for the average
norm squared which, except for a small window around its
peak, lies on top of the red (thick solid) curve showing the
empirical average of kx(t)k2 over 100 realizations of binary J.
The five thin dashed black curves show the result for five particular realizations of J, and the pink (light gray) area shows
the standard deviation among the 100 realizations. The standard deviation shrinks to zero as N â†’ âˆ and kx(t)k2 for any
realization lies close to its average for large N . For comparison the purple (thin, lowest) curve shows kx(t)k2 obtained
by ignoring the effect of quenched disorder, i.e. by setting
A = M.

obtain from Eq. (2.48) using the asymptotics of Bessel
functions). In addition, both curves exhibit transient
amplification where the magnitude of activity initially
grows to a maximum, before it decays asymptotically to
zero. The Ïƒ 6= 0 curve shows larger and longer transient
amplification, which is most likely attributable both to
the eigenvalues being closer to the Re(z) = Î³ line and to
augmented nonnormal effects (e.g. larger effective feedforward weights, or longer chains). We also mention that,
as in our previous examples, if the input direction is optimized for the particular realization of J, significantly
larger transient amplification may be achieved.
Finally, the total power spectrum of response to a sinusoidal input, Eq. (2.32), is given by the explicit formula
D
E
kxÏ‰ k2 =

Ï‰ 2 + Î³ 2 + |wa |2
kI0 k2 .
(Ï‰ 2 + Î³ 2 )2 âˆ’ Ïƒ 2 (Ï‰ 2 + Î³ 2 + Âµ2 )
J
(2.50)
where Âµ2 â‰¡ tr(M â€  M ) = h|wb |2 ib /2 and, as noted above,
the direction of I0 is that of the second Schur mode in
block a.
The example Eq. (2.42) motivated by Daleâ€™s law with
neurons of either excitatory or inhibitory types, can be
generalized to a network of neurons belonging to one of C
different types (these could be subtypes of excitatory or
inhibitory neurons), in which not only the mean but also

the variance of connection strengths depends on the preand post-synaptic types. When this dependence is factorizable, in a way we will now describe, the connectivity
matrix of such a network will be of the form Eq. (2.1)
with non-trivial L and R. Let c(i) âˆˆ {1, . . . , C} denote
the type of neuron i, and let fc denote the fraction of neuPC
rons of type c (so c=1 fc = 1); we assume C and fc are
all Î˜(1). Assume further that each synaptic weight is a
product of a pre- and a post-synaptic factor, and that in
each synapse these factors are chosen independently from
the same distribution, except for a deterministic sign and
overall scale that depend only on the type of the pre and
post-synaptic neurons, respectively. Thus if Aij denotes
the weight of the synaptic projection from neuron j to
neuron i, we have
1
Aij = âˆš (lc(i) xij )(rc(j) yij )
N

(2.51)

where xij â€™s and yij are positive random variables chosen
iid from the distributions Px (x) and Py (y), respectively.
Here, lc and rc determine the sign and the scale (apart
from the overall âˆš1N ) of the pre and post-synaptic factors
of the neurons in cluster c, respectively. Note that when
all lc are positive, Aij satisfies Daleâ€™s law. By absorbing
appropriate constants into lc â€™s and rc â€™s we can assume
that Var[xy] = hx2 ihy 2 i âˆ’ hxi2 hyi2 = 1. Then it is easy
to see that A can be cast in the form Eq. (2.1) with
Lij = lc(i) Î´ij
Rij = rc(i) Î´ij
1
Jij = âˆš (xij yij âˆ’ Î¾)
N
M = s L uuT R
where u is the unit vector

(2.52)
(2.53)
(2.54)
(2.55)

âˆš1 (1, . . . , 1)T ,
N

âˆš
s â‰¡ Î¾ N,

(2.56)

and Î¾ â‰¡ hxihyi is dimensionless and Î˜(1) (note that J,
given by Eq. (2.54), indeed has iid elements with zero
mean and variance N âˆ’1 ). Being single-rank, M has N âˆ’1
zero eigenvalues; its only (potentially) non-null eigenvector is Lu, with a generically large eigenvalue
Î»M = suT RLu = s

N
âˆš
1 X
rc(i) lc(i) = Î¾ N hÏƒc ic (2.57)
N i=1

where we defined
Ïƒc â‰¡ lc rc ,
hXc ic â‰¡

C
X

fc Xc .

(2.58)
(2.59)

c=1

As for the example Eq. (2.42), we will focus on the
balanced case in which Î»M âˆ hÏƒc ic = 0. From Eq. (2.55),
M = uÌƒvÌƒT with uÌƒ = Lu and vÌƒ = sRu. The balanced

16
condition is equivalent to uÌƒÂ· vÌƒ = 0 (see Eq. (2.57)). Thus,
similar to Eq. (2.42), the Schur representation of M has
the form (2.40) with w1 = kuÌƒkkvÌƒk and wb = 0 for b > 1.
In Sec. V C we prove that, as for Eq. (2.42), for the
ensemble Eqs. (2.52)â€“(2.55) the limit of the eigenvalue
distribution, limN â†’âˆ Ï(z), is also not affected by the
nonzero mean matrix Eq. (2.55); hence we can obtain
limN â†’âˆ Ï(z) for that example by safely setting M to
zero, and using formulae Eqs. (2.13)â€“(2.16) with L and
R given by Eqs. (2.52)â€“(2.53). Thus limN â†’âˆ Ï(z) is
isotropic and its support is the disk with radius
p
r0 = kRLkF = hÏƒc2 ic .
(2.60)
As in the previous example, when the balance condition
hÏƒc ic = 0 holds, use of the naive formula Eq. (2.5) with
M = uÌƒvÌƒT would have yielded
rÌƒ0 = r0

"

1
+
2

r

1
+ Î¾2
4

#1/2

,

(2.61)

which is larger than the correct result Eq. (2.60). As
discussed above, this result is not correct, but it indicates the existence of Î˜(1) number of outlier eigenvalues lying outside the boundary of limN â†’âˆ Ï(z) given by
Eq. (2.60). For r < r0 , the N â†’ âˆ limit of the proportion, n> (r), of eigenvalues lying farther than distance r
of the origin is given by g 2 (r) which is found by solving
Eq. (2.15), or equivalently


1
= 1.
(2.62)
g 2 + Ïƒcâˆ’2 r2 c
The results Eqs. (2.17)â€“(2.18) also hold, wherein the normalized sums over i can be replaced with appropriate
averages hÂ·ic . In the case of two neuronal types a closed
solution can be obtained for n> (r) and Ï(r). Identifying the two types with excitatory and inhibitory neurons, and assuming that lc = 1, ÏƒE â‰¡ Ïƒ1 > 0 and
ÏƒI â‰¡ Ïƒ2 < 0 (we will use E and I as indices instead
of c = 1 and 2, respectively) the ensemble Eqs. (2.52)â€“
(2.55) describes a synaptic connectivity matrix in which
all excitatory (inhibitory) connections are iid with mean
1
1
2
Î¾ÏƒE N âˆ’ 2 (âˆ’Î¾|ÏƒI |N âˆ’ 2 ) and variance ÏƒE
N âˆ’1 (ÏƒI2 N âˆ’1 ).
In this case, Eq. (2.62) yields a quadratic equation. Differentiating the solution of that equation with respect to
r2 we obtain the explicit result
ï£®
ï£¹
âˆ’2
(ÏƒE
+ÏƒIâˆ’2 )r 2 âˆ’1

Ï(r) =

r02 âˆ’2r 2
2 +Ïƒ 2
ÏƒE
I

âˆ’2
+
ÏƒE
+ ÏƒIâˆ’2 ï£¯
2
ï£¯1 âˆ’ r
ï£°
2
2Ï€
((ÏƒEâˆ’2 +ÏƒIâˆ’2 )r2 âˆ’1)
+
4

r 2 (r02 âˆ’r 2 )
(ÏƒE ÏƒI )2

ï£º
ï£º
ï£»

(2.63)
This result was first obtained (in a less simplified form)
in Ref. [39]. Figure 9 shows two examples of spectra for
single realizations of matrices of the form Eq. (2.51), with
three neural types (C = 3), where xij and yij , and hence
Jij , have log-normal distributions. The insets compare

!
!#$

ImÎ»

!

n> (r) !#$

ImÎ»

n> (r)

"#$

"#$

!

!
"

"#$

"#$

"

"

Ã¯"#$

Ã¯"#$

Ã¯!

Ã¯!
Ã¯!

Ã¯"#$

"
%%%
ReÎ»

"

r

"#$

!

Ã¯!

Ã¯"#$

"

r

"#$

!

%%%
ReÎ»

FIG. 9: (Color online) The eigenvalue spectra of A = M +
LJR with M , L and R given by Eqs. (2.52)â€“(2.55) with neurons belonging to one of three different types (C = 3). The
main panels show the eigenvalues for two particular realizations of J. In both panels, N = 2000, f1 = 0.6, f2 = f3 = 0.2,
lc = 1, Ïƒ1 = r1 = 0.76,

 Ïƒ
 2 = r2 = âˆ’0.57, Ïƒ3 = r3 = âˆ’1.71 (so
hÏƒc ic = 0 and r02 = Ïƒc2 c = 1), and Jij had real entries with
log-normal distribution; in the left (right) panel, the normally
distributed log10 Jij had standard deviation 0.5 (0.75). The
solid red circles mark the boundaries of the spectral support
as given by Eq. (2.60), and the dashed blue circles show the
radii given by Eq. (2.61). The insets compare n> (r) based on
the numerically calculated eigenvalues shown in the main panels (connected red dots), with that found by solving Eq. (2.62)
(black curve). In the right panelâ€™s inset we have also plotted
(green connected dots lying slightly above the red connected
circles) the empirically calculated n> (r) for a single realization with the same ensemble parameters, but with N = 8000;
the convergence to the universal limit at N â†’ âˆ is significantly slower in the right panel in which the distribution of
Jij had a considerably heavier tail.

n> (r) based on the numerically calculated eigenvalues,
with those found by solving Eq. (2.62). In the right panel,
the normally distributed log Jij have a higher standard
deviation, and hence the distribution of Jij has a heavier
tail. The right panelâ€™s inset demonstrates that the convergence to the universal, N â†’ âˆ limit can be considerably slow when the distribution of Jij is heavy-tailed.
3.

Linearizations of nonlinear neural and ecological
networks

In neuroscience applications, Eq. (2.2) can arise as a
linearization of nonlinear firing rate equations for a recurrent neural network of N neurons, around some stationary background. The nonlinear dynamical equations
for the evolution of the network activity typically take
the form [63]
T

dv(t)
= âˆ’v(t) + W f (v(t)) + Iv (t).
dt

(2.64)

Here v(t) is the vector of state variables of all neurons at
time t; its i-th component, vi (t), is commonly thought of

17
as the voltage of the i-th neuron, or the total synaptic
input it receives. f (Â·) is the neuronal nonlinear inputoutput function, which is imposed element-by-element
on its vector argument, with f (v)i â‰¡ f (vi ) giving the
output, i.e. the firing rate, of neuron i; Iv (t) is the external input vector; T = diag(Ï„1 , Ï„2 , Â· Â· Â· , Ï„N ) is a N Ã— N
diagonal matrix whose diagonal elements are the positive
time-constants of the neurons (hence T is invertible); and
W is the N Ã— N synaptic connectivity matrix.
Suppose that for a constant external input, Ivâˆ— ,
Eq. (2.64) has a fixed point v âˆ— . Then, given a small perturbation in the input, Iv (t) = Ivâˆ— + Î´Iv (t), we can write
v(t) = v âˆ— + x(t), and linearize the dynamics around the
fixed point by expanding Eq. (2.64) to first order in x(t)
and Î´Iv (t). This yields the set of linear differential equations
dx(t)
T
= âˆ’x(t) + W Î¦ x(t) + Î´Iv (t),
dt

(2.65)

for the (small) deviations, where we defined the diagonal
Jacobian
Î¦ = diag(f 0 (v âˆ— )).

(2.66)

Now suppose that the original connectivity matrix can
be written as W = hW i + Î´W , with a quenched disorder
part that is an iid random matrix: Î´W = ÏƒJ. Then
multiplying Eq. (2.65) by T âˆ’1 , we can convert Eq. (2.65)
into the form Eq. (2.2) with Î³ = 0 and A = M + LJR
with
M = T âˆ’1 (âˆ’1 + hW iÎ¦)
L = T âˆ’1
R = ÏƒÎ¦

(2.67)
(2.68)
(2.69)

I(t) = T âˆ’1 Î´Iv (t).

(2.70)

and input

This observation is not limited to neuroscience applications, and can also apply to many other frameworks,
e.g. those used in mathematical biology. Generalized
Lotka-Volterra (GLV) equations [47] used in modeling
the dynamics of food webs provide an example. Let
n(t) = (n1 (t), . . . , nN (t))T denote the vector of population sizes of N species.
The GLV equations take the
P
i
=
n
(r
+
W
n
form dn
i i
ij j ) or
j
dt
dn
= diag(r + W n)n
dt

(2.71)

where ri > 0 are the speciesâ€™ intrinsic growth rates and W
is the interaction matrix. Linearizing Eq. (2.71) around a
fixed point, nâˆ— , yields again a linear system of the form
Eq. (2.2) with Î³ = I(t) = 0. Starting with the same
simple model W = hW i + ÏƒJ, we find that A can be
written in the form Eq. (2.1) with
R = Ïƒ1,
L = diag(nâˆ— ),
M = diag(r + W nâˆ— ) + LhW i.

(2.72)
(2.73)

âˆ’4

x 10
6

ImÎ»
4
2
0
âˆ’2
âˆ’4
âˆ’6
âˆ’12

âˆ’10

âˆ’8

âˆ’6

âˆ’4

âˆ’2

ReÎ»

0
âˆ’3

x 10

FIG. 10: (Color online) The eigenvalues (black dots) of
A = M + JR, with M and R given by Eqs. (2.75)â€“(2.76)
with g = 0.01, a = 1.02 and N = 2000. This matrix governs
the dynamics of small perturbations away from a non-trivial
random fixed point in a clustered network of neurons (see
Eq. (2.74)), studied in Ref. [48]. The cyan dots on the real
line are the eigenvalues of M , and the red curve is the boundary of support of the eigenvalue distribution, as calculated
numerically from Eq. (2.5).

Note that if no species is extinct in the fixed point, i.e.
if all niâˆ— > 0, then M = LhW i.
Assuming the linear systems thus obtained, i.e. the
fixed points v âˆ— or nâˆ— , are stable, we can therefore think
of our results for kx(t)k2 and kxÏ‰ k2 as characterizing
the temporal evolution and the spectral properties of
the linear response of the nonlinear system Eq. (2.64)
(Eq. (2.71)) in its fixed point v âˆ— (nâˆ— ) to perturbations.
The necessary and sufficient condition for the stability
of a fixed point (without any change in the external input) is that all eigenvalues of the corresponding A have
negative real parts. Our formula for the boundary of the
eigenvalue distribution, Eq. (2.5), can be applied in these
cases to map out the region in parameter space (parameters here mean the time constants or intrinsic growth
rates in T or r, or the connectivity parameters determining the random ensemble for W , i.e. Ïƒ and the parameters of hW i) in which a particular fixed point is stable.
Recently our general formula Eq. (2.5) was used in this
way by colleagues [48] to determine the phase diagram
of a clustered network of neurons, in which intra-cluster
connectivity is large, but inter-cluster connectivity is random and weak. Because of the strong intra-cluster connectivity, each cluster behaves as a unit with a single
self-coupling a. Letting the random inter-cluster couplings between N clusters have zero mean and variance
g 2 /N , their analysis starts from the equation
dv(t)
= âˆ’v(t) + a tanh(v(t)) + gJ tanh(v(t))
dt

(2.74)

where J is an iid random matrix as above. Here, v is
a vector whose i-th component is the mean voltage of

18
cluster i, while the nonlinear function tanh(v(t)) (with
the hyperbolic tangent acting component-wise) represents the vector of mean firing rates of the clusters. The
analysis of Ref. [48] shows that there is a region of the
phase plane (a, g) where the self-connectivity, a, is excitatory and sufficiently strong, in which the system eventually relaxes to non-zero random attractor fixed points
v âˆ— ; for smaller values of a, the dynamics is chaotic (chaos
in the a = 0 case was established in Ref. [49]). The form
of these fixed points (the distribution of the elements of
v âˆ— as N â†’ âˆ for a given (a, g)) can be obtained using
mean-field theory, and the linearization about v âˆ— leads to
an equation in the form of Eq. (2.2), with A = M + JR,
where M and R are the diagonal matrices
M = diag(âˆ’1 + a tanh0 (v âˆ— ))
R = diag(g tanh0 (v âˆ— )).

(2.75)
(2.76)

Given this form, it can be shown that the fixed point v âˆ—
is stable if z = 0 is outside and to the right of the spectrum of the Jacobian matrix of the linearization, A. The
mean field solution for v âˆ— determines the statistics of the
elements of R2 M âˆ’2 for a given (a, g). From these it can
be determined if z = 0 is outside the spectrum using our
formula for the boundaryof spectrum
Eq. (2.5), which

yields the requirement tr

R2
M2

DERIVATION OF THE FORMULA FOR
THE SPECTRAL DENSITY

In this section we will derive the formulae Eqs. (2.5)â€“
(2.8) for the average spectral density, Ï(z), of random
matrices of the form A = M +LJR where M , L and R are
deterministic matrices, and J is random with iid elements
of zero mean and variance 1/N . We will use the Hermitianized diagrammatic method developed in Refs. [24, 25]
(and reviewed in Ref. [50]), which we will recapitulate

1
Î´ac Î´bd .
N

âˆ—
hJab Jcd
i=

(3.1)

Thus h|Jab |2 i = N1 , and all other first and second mo2
ments of J (including hJab
i) vanish. The measure on J
can be written as
â€  Y
dÂµ(J) âˆ eâˆ’N Tr(JJ )
dImJab dReJab .
(3.2)
ab

In this form, and by the invariance of the trace, it is clear
that the measure is symmetric with respect to the group
U (N ) âŠ— U (N ), acting on J by J 7â†’ U JV â€  where U and
V are arbitrary N Ã— N unitary matrices.
For a particular realization of J, we define the â€œGreenâ€™s
functionâ€ G(z; J) by

< 1. In this way, the

region of stability of the fixed points in the (a, g) plane
can be mapped (see Ref. [48] for the results, and a complete discussion of the analysis outlined here). Figure 10
shows a numerical example of the eigenvalue distribution
for A for a given (a, g) and the superimposed boundary
calculated using Eq. (2.5).
In closing we note a potential caveat in the applicability of our formulae to the linearization analysis of systems
like Eq. (2.65) and Eq. (2.71). We have derived the general formulae of Secs. II Aâ€“II B assuming that M , L and
R are independent of J. However, M and R as given by
Eqs. (2.67) and (2.69) (or M and L in Eqs. (2.72)â€“(2.73))
depend on J via their dependence on v âˆ— (nâˆ— ). However,
in our experience this dependence is often too weak and
indirect to render our formulae inapplicable; an example
is provided by the excellent agreement of the empirical
spectrum and the red boundary given by our formula in
Fig. 10, which also held for other parameter choices of
the model of Ref. [48].

III.

here for completeness. As mentioned in Sec. II, the spectral density is self-averaging for large N . Furthermore,
as established in Ref. [34], it is also universal in the large
N limit, in the sense that it is independent of the details
of the distribution of the elements of J as long its mean
and variance are as stated. The same universality theorem also ensures that the real or complex nature of J
does not by itself affect Ï(z) to leading order. Therefore,
for simplicity we consider the case where J is a zero-mean
complex Gaussian random matrix with hJab Jcd i = 0, and

G(z; J) â‰¡

1
,
Mz âˆ’ J

(3.3)

where Mz = Lâˆ’1 (z âˆ’ M )Râˆ’1 (Eq. (2.6)). In the case
L, R âˆ 1, G(z; J) will be proportional to the resolvent of
1
A, zâˆ’A
. More generally we have
1
= Râˆ’1 G(z; J)Lâˆ’1 .
zâˆ’A
Following Ref. [24], we will use the identity
 
1
1
1
2
2
Î´ (z) = âˆ‚zÌ„ âˆ‚z ln |z| = âˆ‚zÌ„
Ï€
Ï€
z

(3.4)

(3.5)

where the first identity follows by noting that 4âˆ‚zÌ„ âˆ‚z =
âˆ‡2 , where âˆ‡2 is the 2-D Laplacian, and recalling from
electrostatics that the solution of Poissonâ€™s equation for
a point charge at origin, i.e. âˆ‡2 Ï†(z) = 4Ï€Î´ 2 (z), in 2-D
is given by the potential field Ï†(z) = ln |z|2 ; the second
identity follows from âˆ‚z ln |z|2 = âˆ‚z (ln z + ln zÌ„) = z1 +
0. Using Eq. (3.5) we can write the empirical spectral
density, defined in Eq. (2.4), as
ÏJ (z) =

1
1 X 1
1
1
âˆ‚zÌ„
= âˆ‚zÌ„ tr
.
Ï€ N Î± z âˆ’ Î»Î±
Ï€
zâˆ’A

(3.6)

Performing the ensemble average we obtain
Ï(z) â‰¡ hÏJ (z)iJ =

1
âˆ‚zÌ„ tr[(RL)âˆ’1 hG(z; J)iJ ],
Ï€

(3.7)

where we used Eq. (3.4), and the linearity and cyclicity
of the trace. Thus, to calculate Ï(z), our task boils down
to calculating hG(z; J)iJ .

19
The diagrammatic technique provides a method for
calculating averages of products of G(z; J)â€™s. However,
this method in its standard form relies on A being a Hermitian matrix. It starts by an expansion of G(z; J) in
powers of J, which is only valid when z is far enough
from the spectrum of A, i.e. away from the points we are
most interested in. For Hermitian matrices, this is no
problem as the spectrum is confined to the real line, and
therefore G(z; J) and hG(z; J)iJ will be analytic outside
the real line. Thus one can use the expansion for z far
away outside the real line, perform the averaging over J,
and sum up the most dominant contributions to obtain
a result analytic in z. This result can then be analytically continued to z arbitrarily close to the spectrum on
the real line, yielding information about the spectrum.
All this would seemingly fail in the case of a nonnormal
(and in particular non-Hermitian) A, with eigenvalues
that in general cover a two dimensional region in the
complex plane. However, using a trick introduced by
Ref. [24], we can turn this problem to an auxiliary problem of averaging the Greenâ€™s functions for a Hermitian
matrix. By doubling the degrees of freedom, one defines
a z-dependent, 2N Ã— 2N Hermitian â€œHamiltonianâ€


0
Mz âˆ’ J
,
(3.8)
H(z) â‰¡
0
Mzâ€  âˆ’ J â€ 
and the corresponding 2N Ã— 2N resolvent matrix or
Greenâ€™s function depending on a new complex variable
Î·:
âˆ’1

G(Î·, z; J) â‰¡ (Î· âˆ’ H(z))
=âˆ’

Î·
Mz âˆ’J
(Mz âˆ’J)(Mz âˆ’J)â€  âˆ’Î· 2 (Mz âˆ’J)â€  (Mz âˆ’J)âˆ’Î· 2
â€ 
(Mz âˆ’J)
Î·
(Mz âˆ’J)(Mz âˆ’J)â€  âˆ’Î· 2 (Mz âˆ’J)â€  (Mz âˆ’J)âˆ’Î· 2

For Î· â†’ i0, we see that


0
(Mz âˆ’ J)âˆ’â€ 
.
G(0, z; J) = âˆ’
(Mz âˆ’ J)âˆ’1
0

(3.9)
!
.

(3.10)

and thus from Eq. (3.3), for any realization of J
G(z; J) = âˆ’ lim G21 (Î·, z; J)
Î·â†’i0

Here, we have used the notation
 11

G (Î·, z; J) G12 (Î·, z; J)
G(Î·, z; J) =
,
G21 (Î·, z; J) G22 (Î·, z; J)

(3.11)

(3.12)

where GÎ±Î² (with Î±, Î² âˆˆ {1, 2}) are N Ã—N matrices, forming the four blocks of G. We have written the limit in
Eq. (3.11) as Î· â†’ i0 to emphasize that until the end of
our calculations Î· is to retain a nonzero imaginary part,
which serves to regularize the denominators in Eq. (3.9);
c.f. the discussion after Eq. (3.35). We will be carrying out a perturbation expansion in powers of J, so we
decompose the Hamiltonian according to
H(z) = H0 (z) âˆ’ J ,




0 J
0 Mz
J â‰¡
, H0 (z) â‰¡
.
Jâ€  0
Mzâ€  0

(3.13)
(3.14)

We will sometimes use a tensor product notation to
denote matrices in this doubled up space, e.g. writing
J = Ïƒ + âŠ— J + Ïƒ âˆ’ âŠ— J â€  , where we defined the 2 Ã— 2
matrices
 
 
01
00
Ïƒ+ =
Ïƒâˆ’ =
.
(3.15)
00
10
By a slight abuse of notation we also denote 2N Ã—2N matrices Ïƒ Â± âŠ— 1N Ã—N by Ïƒ Â± , and we will denote the identity
matrix in any space by 1. From Eqs. (3.11)
we obtain


tr [(RL)âˆ’1 G(z; J)] = âˆ’tr Ïƒ + âŠ— (RL)âˆ’1 G(i0+ , z; J) ,
and from Eq. (3.7)



1
âˆ‚zÌ„ tr Ïƒ + âŠ— (RL)âˆ’1 G(Î·, z) (3.16)
Î·â†’i0 Ï€

1
= âˆ’ lim âˆ‚zÌ„ tr (RL)âˆ’1 G 21 (Î·, z) ,
(3.17)
Î·â†’i0 Ï€

Ï(z) = âˆ’ lim

where we defined

G(Î·, z) â‰¡ hG(Î·, z; J)iJ .

(3.18)

Having expressed Ï(z) in terms of the ensemble average
of the Greenâ€™s function for a Hermitian matrix, we now
develop the diagrammatic method for calculating ensemble averages of products of G(Î·, z; J) (including G(Î·, z)).
Note that, being the Greenâ€™s function of a Hermitian
matrix, G(Î·, z; J) and hence G(Î·, z) = hG(Î·, z; J)iJ are
analytic functions of Î· for Î· outside the real line, and
therefore analytic continuation can be used to take the
limit Î· â†’ i0 after obtaining the average over J for Î·
sufficiently away from the real line.
We will denote the elements of a generic 2N Ã— 2N
matrix A by AÎ±Î²
ab , where the Greek indices range in {1, 2}
and the Latin indices range in {1, . . . , N }. Using this
notation, the definition Eq. (3.14), and Eq. (3.1), we can
write the covariance for the components of J as


D
E
1
Î±Î² Î³Î´
+ âˆ’
âˆ’ +
(3.19)
Î´ad Î´bc ÏƒÎ±Î²
ÏƒÎ³Î´ + ÏƒÎ±Î²
ÏƒÎ³Î´
Jab
Jcd =
N
J
(the terms proportional to Ïƒ + Ïƒ + and Ïƒ âˆ’ Ïƒ âˆ’ involve
hJab Jcd i, or its complex conjugate, which vanish for the
complex Gaussian ensemble). It will be more handy to
rewrite the parenthesis on the right side of Eq. (3.19) as
1
2
2
1
Ï€Î±Î´
Ï€Î³Î²
+ Ï€Î±Î´
Ï€Î³Î²
, where
 
 
10
00
1
2
Ï€ â‰¡
Ï€ â‰¡
,
(3.20)
00
01
yielding
2
D
E
1 X r
Î±Î² Î³Î´
3âˆ’r
Jab
Jcd =
(Ï€ Î´ad ) (Ï€Î³Î²
Î´cb ).
N r=1 Î±Î´
J

(3.21)

Also, since Jab have zero mean, we have hJ iJ = 0.
The starting point of the diagrammatic method is the
perturbation expansion of G(Î·, z; J) = (Î·âˆ’H0 (z)âˆ’J )âˆ’1
in powers of J
G(Î·, z; J) = G(Î·, z; 0)

âˆ
X

n=0

n

[J G(Î·, z; 0)]

(3.22)

20

a,Î±

b,Î²

:=

GÎ±Î²
ab (Î·, z; 0)

aÎ±

bÎ²

cÎ³

dÎ´

:=

Î±Î² Î³Î´
Jab
Jcd

Î£(Î·, z) :=

:= G(Î·, z)
G(Î·, z; J) =

+

+

+

=

+

+

+

Im Î· 6= 0, and so long as k(RL)âˆ’1 k remains bounded as
N â†’ âˆ, only non-crossing pairings need to be retained
in the large N limit, as crossing pairings are suppressed
by inverse powers of N and do not contribute in the limit
(a pairing diagram is non-crossing if it can be drawn on a
plane, with the wavy lines drawn only on the half-plane
above the straight arrow line, without any wavy lines
crossing). As the last two lines of Fig. 11 demonstrate,
all non-crossing diagrams can be generated by iterating
the equation
G(Î·, z) = G(Î·, z; 0) + G(Î·, z; 0)Î£(Î·, z)G(Î·, z),

=

+

FIG. 11: The first two lines define different elements of Feynman diagrams: the Greenâ€™s function for J = 0 (zero disorder),
GÎ±Î²
ab (Î·, z; 0), the covariance of two J elements, the ensemble
averaged Greenâ€™s function, G(Î·, z) â‰¡ hG(Î·, z; J)iJ , and the
self-energy Î£(Î·, z) , Eq. (3.24) (the matrix indices for G(Î·, z)
and Î£(Î·, z) are arranged as for GÎ±Î²
ab (Î·, z; 0)). The third line is
the diagrammatic representation of the expansion Eq. (3.22)
of G(Î·, z; J) before averaging over J, where the J â€™s are
represented by dashed lines. Averaging over Eq. (3.2) is performed by pairing all J â€™s and connecting them with the wavy
lines representing hJ J i. In the large N limit, the contribution of crossing pairings is suppressed by negative powers
of N ; the sum of all non-crossing diagrams, shown on the
fourth line, yields the leading contribution to G(Î·, z) for large
N . The last line shows the diagrammatic representation of
Eq. (3.23), which if iterated generates all the non-crossing diagrams. Alternatively, G(Î·, z) can be found by solving this
self-consistent equation directly.

where G(Î·, z; 0) is given by Eq. (3.9) with the Jâ€™s set
to zero. This equation is represented diagrammatically
in the third line of Fig. 11; the thin arrows defined in
the first line of the figure represents G(Î·, z; 0), and the
dashed lines represent a power of J before ensemble averaging. To obtain the average resolvent, G(Î·, z), we
then average Eq. (3.22), term by term, with respect to
the ensemble Eq. (3.2). Since the measure is Gaussian
with zero mean, according to Wickâ€™s formula, the average of each term of Eq. (3.22) involving n factors of J
is given by a sum over the contributions of all possible
complete pairings of the J â€™s in that term (in particular,
since hJ iJ = 0, terms in Eq. (3.22) with odd powers of
J vanish after averaging). Each pairing can be represented as a Feynman diagram, as shown in Fig. 11, the
first two lines of which define the diagram elements. For
example, the last diagram in the fourth line of Fig. 11
shows one possible pairing of the term in Eq. (3.22) corresponding to n = 6. The contribution of each pairing
diagram is given by a product of factors, one per each
pair, given by Eq. (3.21) (represented by wavy lines) with
the right indices for that pair, as well as the factors of
G(Î·, z; 0) (represented by thin arrows), with all the intervening Greek and Latin matrix indices summed over
their proper ranges. We show in Appendix A that for

(3.23)

starting from G(0) (Î·, z) = G(Î·, z; 0). This equation is
represented diagrammatically in the last line of Fig. 11,
with the â€œself-energyâ€ matrix, Î£(Î·, z), defined by the diagram in the second line of that figure, i.e.
Î£(Î·, z) â‰¡ hJ G(Î·, z)J iJ .

(3.24)

Using Eq. (3.21) we obtain
Î£Î±Î´
ad (Î·, z) = Î´ad

2
X
r=1

r
Ï€Î±Î´

1
Tr (Ï€ 3âˆ’r G(Î·, z)),
N

which using Eq. (3.20) we can write as


âˆ’ig2 (Î·, z)1
0
Î£(Î·, z) =
,
0
âˆ’ig1 (Î·, z)1

(3.25)

(3.26)

where we defined the scalar functions
gÎ± (Î·, z) â‰¡ i tr G Î±Î± (Î·, z).

(3.27)

Using Eq. (3.26) we can solve Eqs. (3.23)â€“(3.26) for
G(Î·, z) at once, in terms of gÎ± (Î·, z), and then use
Eq. (3.27) to obtain a self-consistency equation, which
can be solved for gÎ± (Î·, z). To this end, we multiply
Eq. (3.23) by Gâˆ’1 (Î·, z; 0) on the left, and by G âˆ’1 (Î·, z)
on the right, to obtain

âˆ’1
G(Î·, z) = Gâˆ’1 (Î·, z; 0) âˆ’ Î£(Î·, z)
âˆ’1

= [Î· âˆ’ H0 (z) âˆ’ Î£(Î·, z)]

.

(3.28)

Using this expression with Eqs. (3.14) and (3.26), it can
be easily checked that


(Î· + ig1 )K1âˆ’1 (z âˆ’ M )K2âˆ’1
G(Î·, z) = âˆ’
, (3.29)
(zÌ„ âˆ’ M â€  )K1âˆ’1 (Î· + ig2 )K2âˆ’1
where K1 â‰¡ Mz Mzâ€  + (g1 âˆ’ iÎ·)(g2 âˆ’ iÎ·) and K2 â‰¡
Mzâ€  Mz + (g1 âˆ’ iÎ·)(g2 âˆ’ iÎ·), and we dropped the arguments of gÎ± (Î·, z) for succinctness. Imposing Eq. (3.27)
we obtain the self-consistency equations

g1 = (g1 âˆ’ iÎ·) tr K1âˆ’1 ,
(3.30)

âˆ’1
g2 = (g2 âˆ’ iÎ·) tr K2 .
(3.31)
Before solving
for g1 and g2 , we first show
 these equations

that tr K1âˆ’1 = tr K2âˆ’1 . One way to see this is to use

21
the singular value decomposition (SVD) of Mz in the
form
Mz = Uz Sz Vzâ€  ,

(3.32)

where Sz is a nonnegative diagonal matrix with the
singular values of M , si (z) (i = 1, Â· Â· Â· , N ), on the
diagonal, and Uz and Vz are unitary matrices (as in
Sec. II A we include possibly vanishing singular values
among si (z), so that Sz , Uz and Vz are always N Ã— N
matrices). Using the invariance of trace
under simi

larity transforms, we obtain tr K1âˆ’1 = tr K2âˆ’1 =
âˆ’1
tr Sz2 + (g1 âˆ’ iÎ·)(g2 âˆ’ iÎ·)
. Given this equality, it is
not hard to see that Eqs. (3.30) cannot be simultaneously satisfied unless g1 (Î·, z) = g2 (Î·, z) â‰¡ g(Î·, z), with
g(Î·, z) satisfying


1
g = (g âˆ’ iÎ·) tr 2
,
(3.33)
Sz + (g âˆ’ iÎ·)2
or as written in the original basis
"
#
1
g = (g âˆ’ iÎ·)tr
.
Mz Mzâ€  + (g âˆ’ iÎ·)2

(3.34)

Noting from Eqs. (3.26), that the self-energy is thus
proportional to the 2N Ã— 2N identity matrix, from
Eqs. (3.28) and (3.9) (for J = 0) we obtain
G(Î·, z) = G(Î· + ig(Î·, z), z; 0)
ï£«
=

ï£¶

(3.35)

iÎ³
Mz
â€ 
M
M
+Î³ 2 Mzâ€  Mz +Î³ 2 ï£¸
z
âˆ’ ï£­ Mzzâ€ 
iÎ³
Mz Mzâ€  +Î³ 2 Mzâ€  Mz +Î³ 2

where Î³ â‰¡ g(Î·, z) âˆ’ iÎ·.
According to Eq. (3.11), for our case of interest we
must solve Eq. (3.34) in the limit Î· â†’ i0. Note, however, that as shown in Appendix A, the non-crossing approximation is in general guaranteed to work only for
Im Î· 6= 0; hence the limit Î· â†’ i0 must be taken after
the limit N â†’ âˆ (as already pointed out in Sec. II, taking the limits in this order is important in cases where
some of the singular values in Sz vanish in the limit
N â†’ âˆ). For our purposes, it suffices to let Î· = i
for some real positive , and take the limit  â†’ 0+ at
the end. In this case one must seek a positive solution for g(i, z) in Eq. (3.34); this is because by definition, g(Î·, z) = itr G11 (Î·, z) = htrDiG11 (Î·, z; J)iJ and from
E

Eq. (3.9) we obtain g(i, z) = tr (Mz âˆ’J)(M z âˆ’J)â€  +2 ,
J
which for  > 0, is the ensemble average of the trace of
a positive definite matrix and hence positive. Taking the
limit N â†’ âˆ while keeping  (and hence  + g) positive
and nonzero, we define




1
1
K(Î³, z) â‰¡ lim tr
=
lim
tr
N â†’âˆ
N â†’âˆ
Sz2 + Î³ 2
Mz Mzâ€  + Î³ 2
(3.36)

for Î³ = g +  > 0. We can then rewrite Eq. (3.34) as
Î³(1 âˆ’ K(Î³, z)) = ,

(3.37)

with Î³ = g + . Since  and Î³ = g +  are positive, it
follows that 1âˆ’K(Î³, z) must also be positive. In the limit
 â†’ 0+ there are two possible situations: 1) g, Î³ â†’ 0+ ,
in which case we must have
lim K(Î³, z) < 1,

Î³â†’0+

(3.38)

or limÎ³â†’0+ K(Î³, z) = 1, or 2) the solution for g stays
finite and positive in the limit, while K(Î³, z) â†’ 1âˆ’ as
Î³ â†’ g + . Thus in the second case g(z) â‰¡ limâ†’0+ g(, z)
must satisfy K(g(z), z) = 1, i.e.


1
.
(3.39)
1 = lim tr 2
N â†’âˆ
Sz + g(z)2
Note further that since K(Î³, z) is a decreasing function of
Î³, in the second case we have K(0+ , z) â‰¥ K(g(z), z) = 1,
i.e.
lim K(Î³, z) â‰¥ 1.

Î³â†’0+

(3.40)

Thus the two possible solutions are realized in complimentary regions (with a shared boundary) of the complex plane for z, respectively given by Eqs. (3.38) and
(3.40).
Let us substitute the g(z) = 0 solution for the case
(3.38) in Eq. (3.35), and naively set Î· = i (and thus Î³)
to zero, to obtain
G(Î· = i0+ , z) = G(Î· = i0+ , z; 0).

(3.41)

From Eqs. (3.10)â€“(3.11), this solution yields hG(z; J)iJ =
âˆ’G 21 (Î· = i0+ , z) = Mzâˆ’1 = R(z âˆ’ M )âˆ’1 L which is analytic outside the spectrum of M . Hence from Eq. (3.7),
it yields Ï(z) = 0, at least outside the spectrum of M ; a
more careful analysis presented in Appendix B, in which
we correctly take the limit N â†’ âˆ in Eq. (3.17) before
taking  â†’ 0+ , confirms that in the region Eq. (3.38),
limN â†’âˆ Ï(z) always vanishes. We conclude that the support of limN â†’âˆ Ï(z) is where Eq. (3.40) holds (which is
Eq. (2.20) of Sec. II); here g(z) is to be found by solving Eq. (3.39), or equivalently Eq. (2.9) or Eq. (2.10).
In this region, we obtain Ï(z) by substituting Eq. (3.35),
with the solution of Eq. (3.39), into Eqs. (3.17). This
yields Eq. (2.8), which we rewrite here as
1
âˆ‚zÌ„ E(z)
Ï€"
#
(RL)âˆ’1 Mzâ€ 
,
E(z) â‰¡ tr
Mz Mzâ€  + g(z)2
Ï(z) =

(3.42)
(3.43)

with g(z) given by Eq. (3.39), or equivalently Eq. (2.9).
We will now obtain an alternative expression for Ï(z),
equivalent to Eqs. (3.42)â€“(3.43), which explicitly shows
that it depends only on the singular values of Mz . Noting

22

that, from Eq. (2.6), âˆ‚z Mz Mzâ€  = (RL)âˆ’1 Mzâ€  , we can
write Eq. (3.43) as
"
 #
âˆ‚z Mz Mzâ€ 
E(z) = tr
.
(3.44)
Mz Mzâ€  + g(z)2

Differentiating Eq. (3.50) and using Eq. (2.15) we obtain
N
1 X
1
,
âˆ‚r Ï•(r) = 2r
N i=1 r2 + Ïƒi2 g(r)2

and

On the other hand, we have
"

#



âˆ‚z Mz Mzâ€  + g 2 (z)
,
âˆ‚z tr ln Mz Mzâ€  + g(z)2 = tr
Mz Mzâ€  + g(z)2

= E(z) + âˆ‚z g 2 (z) ,
(3.45)

where to write the last term we used Eq. (2.9). Thus we
obtain
E(z) = âˆ‚z Ï•(z),


Ï•(z) â‰¡ âˆ’g 2 (z) + tr ln Mz Mzâ€  + g(z)2 .

or using the SVD, Eq. (3.32),


Ï•(z) = âˆ’g 2 (z) + tr ln Sz2 + g(z)2 ,
= âˆ’g(z)2 +

1
N

N
X
i=1

(3.46)
(3.47)

(3.48)



ln si (z)2 + g(z)2 . (3.49)

Finally, substituting Eq. (3.49) in Eq. (3.46), and using
Eq. (2.10), we obtain Eq. (2.12).
For the special case of M = 0, we have Mz = z(RL)âˆ’1 .
If we let Ïƒi to be the singular values of RL, then the
singular values of Mz will be given by si (z) = |z|Ïƒiâˆ’1 .
Substituting this in Eq. (2.10) and multiplying both sides
by r2 = |z|2 , we obtain Eq. (2.15). We see immediately
that g(z), Ï•(z) and Ï(z) depend only on the radius r =
|z|. Similarly we can rewrite Eq. (3.49) as
N

1 X  2 âˆ’2
Ï•(r) = âˆ’g(r) +
ln r Ïƒi + g(r)2 .
N i=1
2

(3.50)

To find the spectral radius (boundary of the spectrum) r0
we have to solve Eq. (2.15) for r, setting g(r) = 0. This
PN
yields r02 = N1 i=1 Ïƒi2 = kRLk2F , yielding Eq. (2.13).
Let us define the proportion of eigenvalues lying outside
a radius r from the origin by n> (r). To obtain Eqs. (2.14)
and (2.16), first note that
1
1 2
1 âˆ‚
âˆ‚zÌ„ âˆ‚z Ï•(z) =
âˆ‡ Ï•(z) =
(râˆ‚r Ï•(r)) ,
Ï€
4Ï€
4Ï€r âˆ‚r
(3.51)
where we used the expression of Laplacian, âˆ‡2 = âˆ‚x2 +âˆ‚y2 ,
in 2-D polar coordinates in the last
R âˆequality. Using
this with the definition n> (r) = 2Ï€ r Ï(r)rdr, we ob
âˆ
tain n> (r) = 2r âˆ‚r Ï•(r) r . For the limit at r â†’ âˆ,
note that for r > r0 , g(r) = 0 and we have Ï•(r) =
PN
1
2
2 âˆ’2
i=1 ln(r Ïƒi ) = 2 ln r âˆ’ N ln det(RL), and hence
N
r
2 âˆ‚r Ï•(r) â†’ 1 as r â†’ âˆ. Thus we obtain
Ï(r) =

r
n> (r) = 1 âˆ’ âˆ‚r Ï•(r).
2

(3.52)

(3.53)

n> (r) = 1 âˆ’ r2
= g(r)2

N
1 X
1
,
2
N i=1 r + Ïƒi2 g(r)2

N
1 X
Ïƒi2
.
N i=1 r2 + Ïƒi2 g(r)2

(3.54)

(3.55)

Using Eq. (2.15) once again we obtain Eq. (2.16). Finally,
using the latter together with Eqs. (3.51)â€“(3.52) yields
Eq. (2.14).
We will prove further general properties for the eigenvalue density for M = 0. Let us first define


Ïƒ âˆ’k
In,k (g, r) â‰¡
(3.56)
(g 2 + Ïƒ âˆ’2 r2 )n Ïƒ
and
N
1 X
f (Ïƒi ).
N â†’âˆ N
i=1

hf (Ïƒ)iÏƒ â‰¡ lim

(3.57)

(We assume
R âˆÏƒi have a limit density, ÏÏƒ (Ïƒ), such that
hf (Ïƒ)iÏƒ = 0 f (Ïƒ)ÏÏƒ (Ïƒ)dÏƒ is well-defined for f (Ïƒ) with
sufficiently fast decay at infinity. Note that since we assumed that k(RL)âˆ’1 k = (mini Ïƒi )âˆ’1 = O(1), this density has no measure at Ïƒ = 0 and hence the averages in
Eq. (3.56) are non-singular for n, k â‰¥ 0. Also hf (Ïƒ)iÏƒ is
finite as long as f (Ïƒ) = O(Ïƒ 2 ) as Ïƒ â†’ âˆ, as we are as2

suming
 that the kRLkF = O(1) and limN â†’âˆ kRLkF =
2
Ïƒ Ïƒ .) First, we obtain general expressions for Ï(r = 0)
and Ï(r = r0 ), with r0 given by Eq. (2.13). From
> (r)
Eq. (2.14), Ï€Ï(r) = âˆ’ âˆ‚n
âˆ‚(r 2 ) , which using Eq. (2.15),
re-expressed as I1,0 (g, r) = 1, we can write as
Ï€Ï(r) =

âˆ‚I1,0
âˆ‚(r 2 )
âˆ‚I1,0
âˆ‚(g 2 )

=

I2,2 (g, r)
.
I2,0 (g, r)

(3.58)

Using the facts that at r = 0, g = 1, and at r = r0 , g = 0,
we obtain
1 I2,2 (1, 0)
1 
 âˆ’2 
=
Ïƒ Ïƒ
Ï€ I2,0 (1, 0)
Ï€

 
1 I2,2 (0, r0 )
1 Ïƒ2 Ïƒ
Ï(r = r0 ) =
=
.
Ï€ I2,0 (0, r0 )
Ï€ hÏƒ 4 iÏƒ
Ï(r = 0) =

(3.59)
(3.60)

Using the fact that Ïƒ 4 and Ïƒ âˆ’2

 are
 anti-correlated

  

 and
that Ïƒ 2 = Ïƒ 4 Ïƒ âˆ’2 , we see that Ïƒ 2 Ïƒ â‰¤ Ïƒ 4 Ïƒ Ïƒ âˆ’2 Ïƒ or
Ï(r = r0 ) â‰¤ Ï(r = 0),

(3.61)

with equality if and only if Ï(Ïƒ) is deterministic, i.e., a
delta-function. This can happen if all but an o(1) fraction

23
of the Ïƒi â€™s have the same limit as N â†’ âˆ; in that case
the eigenvalue distribution is given by the circular law.
More generally, we can prove that Ï(r) is a decreasing
function of r for any choice of L and R (with M = 0).
dÏ(r)
Using dÏ(r)
dr = 2r d(r 2 ) , and Eq. (3.58) we obtain

âˆ‚I

2

âˆ‚(g ) âˆ‚
âˆ‚(r 2 ) âˆ‚(g 2 )
âˆ‚I
âˆ’nIn+1,k and âˆ‚(rn,k
2) =

d
d(r 2 )

=

âˆ‚
âˆ‚(r 2 )

+

=

âˆ‚
âˆ‚(r 2 )

IV.

âˆ’ Ï€Ï(r) âˆ‚(gâˆ‚ 2 )

2
2
I2,0
I3,4 âˆ’ 2I2,2 I2,0 I3,2 + I2,2
I3,0
dÏ(r)
= âˆ’4r
3
dr
I2,0

(3.63)

Defining

0

0
hf (Ïƒ)iÏƒ â‰¡ D

f (Ïƒ)
(g 2 +Ïƒ âˆ’2 r 2 )2
1
(g 2 +Ïƒ âˆ’2 r 2 )2

E

EÏƒ ,

(3.64)

Ïƒ

(hf (Ïƒ)iÏƒ is a bonafide expectation operator) we can write
"
0
Ïƒ âˆ’4
dÏ(r)
= âˆ’4r
(3.65)
dr
g 2 + Ïƒ âˆ’2 r2 Ïƒ
#


0
0

 âˆ’2 0

 âˆ’2 02
1
Ïƒ âˆ’2
âˆ’2
Ïƒ Ïƒ+
Ïƒ Ïƒ
g 2 + Ïƒ âˆ’2 r2 Ïƒ
g 2 + Ïƒ âˆ’2 r2 Ïƒ
or


Ïƒ âˆ’2
dÏ(r)
= âˆ’4r Cov0 [ 2
, Ïƒ âˆ’2 ]
dr
g + Ïƒ âˆ’2 r2


 âˆ’2 0
1
0
âˆ’2
âˆ’ Ïƒ Ïƒ Cov [ 2
, Ïƒ ] (3.66)
g + Ïƒ âˆ’2 r2
0

0

0

where Cov0 [f, g] â‰¡ hf giÏƒ âˆ’ hf iÏƒ hgiÏƒ is the covariance un0
Ïƒ âˆ’2
âˆ’2
der hÂ·iÏƒ . Now since g2 +Ïƒ
are both strictly
âˆ’2 r 2 and Ïƒ
decreasing functions of Ïƒ (since g > 0 for r < r0 ), while
1
g 2 +Ïƒ âˆ’2 r 2 is a strictly increasing function of Ïƒ (for r > 0),
the first covariance on the right hand side of Eq. (3.66) is
positive, while the second one is negative, and therefore
dÏ(r)
â‰¤ 0.
dr

(3.67)

This slope is zero at r = 0 and strictly negative for r > 0
as long as Var[Ïƒ] > 0 (again when Var[Ïƒ] = 0 we obtain
the circular law). At r = r0 we obtain

4 
 âˆ’2 0 
 âˆ’2 0 
 2 0
Ïƒ Ïƒ Ïƒ Ïƒ Ïƒ Ïƒ âˆ’1
r0

 

 4 2 
4 Ïƒ 2 Ïƒ 
 2  
 6 
=âˆ’
Ïƒ
Ïƒ
âˆ’
Ïƒ Ïƒ (3.68)
Ïƒ
Ïƒ
r0 hÏƒ 4 iÏƒ3

Ï0 (r0 ) = âˆ’

Var[Ïƒ 2 ]
2

hÏƒ 4 iÏƒ

â‰¤ 0.

(3.69)

(3.62)

âˆ’nIn+1,k+2 (we will
and âˆ‚(gn,k
2) =
drop the explicit (g, r) dependence of In,k â€™s when convenient) we find

D

Ï00 (r = 0) = âˆ’4Var0 [Ïƒ âˆ’2 ] = âˆ’4

dI

dI

2,0
2,2
dÏ(r)
d(r 2 ) I2,0 âˆ’ I2,2 d(r 2 )
,
= 2r
2
dr
I2,0

and using

The curvature of Ï(r) at zero can also be evaluated by
taking the limit r â†’ 0 of the bracket in Eq. (3.66), noting
that g â†’ 1 as r â†’ 0. We obtain

DERIVATION OF THE FORMULA FOR
THE AVERAGE NORM SQUARED

In this section, we focus on the dynamics governed by
the matrix A = M + LJR, according to Eq. (2.2), and
derive the general formulae presented in Sec. II B. We will
first consider the systemâ€™s response to an impulse input,
I(t) = x0 Î´(t), at t = 0, before which we assume the
system was at rest in its fixed point x = 0. We assume
x = 0 is a stable fixed point, i.e. all eigenvalues of âˆ’Î³1+A
have negative real parts, or equivalently, all eigenvalues of
A have real parts less than Î³ (more precisely, we assume
that as N â†’ âˆ, this will be the case almost surely, i.e.
for any typical realization of J; in particular, the vertical
line of zâ€™s with real part Î³ must be to the right of the
support of Ï(z), the average eigenvalue density for A,
as found by solving Eq. (2.5)). This means that x(t)
decays exponentiallyRas t â†’ âˆ, and therefore
R âˆ its Fourier
âˆ
transform, xÌƒ(Ï‰) â‰¡ âˆ’âˆ eâˆ’iÏ‰t x(t)dt = 0 eâˆ’iÏ‰t x(t)dt,
is well-defined. Fourier transformation of Eq. (2.2) with
I(t) = x0 Î´(t) yields iÏ‰xÌƒ(Ï‰) = (âˆ’Î³ +A)xÌƒ(Ï‰)+x0 . Solving
algebraically for xÌƒ(Ï‰), we obtain xÌƒ(Ï‰) = (Î³+iÏ‰âˆ’A)âˆ’1 x0 ,
or using Eqs. (3.3)â€“(3.4), xÌƒ(Ï‰) = Râˆ’1 G(Î³
+ iÏ‰; J)Lâˆ’1 x0 .
Râˆ
The inverse Fourier transform, x(t) = âˆ’âˆ eitÏ‰ xÌƒ(Ï‰) dÏ‰
2Ï€ ,
then yields
Z âˆ
dÏ‰ itÏ‰ âˆ’1
x(t) =
e R G(Î³ + iÏ‰; J)Lâˆ’1 x0 . (4.1)
2Ï€
âˆ’âˆ
Our goal is to study the statistics of x(t) (e.g., its moments) under the distribution Eq. (3.2). Equation (4.1)
allows us to reduce this task to the calculation of various moments of G(z; J) and its adjoint, and these can
be found using the diagrammatic technique. Note that,
in general, these moments involve not only the statistics
of the eigenvalues, but also that of the eigenvectors of
A = M + LJR; this can be seen from the spectral representation Râˆ’1 G(z; J)Lâˆ’1 = (z âˆ’ A)âˆ’1 = V (z âˆ’ Î›)âˆ’1 V âˆ’1
where Î› is a diagonal matrix of the eigenvalues of A, and
V is the matrix whose columns are the eigenvectors of
A. Here we will look at the simplest interesting statistic
involving the eigenvectors:
square norm of

 the average

the state vector, namely, kx(t)k2 J . As we discussed in
Sec. II B, its study is also motivated by the fact that transient amplification due to nonnormality of A manifests itself in the transient growth of kx(t)k2 = x(t)T x(t). With
a slight generalization, we derive a formula for the average of a general quadratic function x(t)T Bx(t) where B
is any symmetric matrix; the norm squared corresponds
to B = 1. Using, x(t)T = x(t)â€  (x(t) is real), the identity

24
xâ€  Bx = Tr (Bxxâ€  ), and Eq. (4.1), we obtain
ZZ
dÏ‰1 dÏ‰2 it(Ï‰1 âˆ’Ï‰2 )
e
x(t)T Bx(t) =
2Ï€ 2Ï€
(4.2)

Tr BR G(Î³ + iÏ‰1 ; J) CL Gâ€  (Î³ + iÏ‰2 ; J) ,

where we defined CL â‰¡ Lâˆ’1 x0 xT0 Lâˆ’â€  and BR â‰¡
Râˆ’â€  BRâˆ’1 .
Using Eq. (3.11) and Gâ€  (z; J) =
12
âˆ’ limÎ·â†’i0+ G (Î·, z; J), and the 2 Ã— 2 matrices Ï€ r defined in Eq. (3.20), we can rewrite the trace in Eq. (4.2)
as Tr Ï€ 2 âŠ—BR G(0, z1 ; J) Ï€ 1 âŠ—CL G(0, z2 ; J) , with zi =
Î³ + iÏ‰i , where now the trace is performed over 2N Ã— 2N
matrices. Averaging over J we then obtain

where we also exploited IÌƒâˆ—j (Ï‰) = IÌƒj (âˆ’Ï‰) for a real I(t).
Substituting Eq. (4.9) into Eqs. (4.7)â€“(4.8) we obtain
Z
dÏ‰ iÏ‰(t1 âˆ’t2 ) x
xi (t1 )xj (t2 ) =
e
Cij (Ï‰),
(4.10)
2Ï€
where
C x (Ï‰) =
(4.11)
Râˆ’1 G(Î³ + iÏ‰; J)Lâˆ’1 C I Lâˆ’â€  Gâ€  (Î³ + iÏ‰; J)Râˆ’â€  .

Noting that Eq. (4.10) expresses the covariance of the response as an inverse Fourier transform, we see that C x (Ï‰)
is indeed the power spectrum of the response, as defined
in Eq. (2.27). Finally note that the element, Cij , of any
T
hx(t)T Bx(t)iJ =
(4.3) matrix can be expressed as Tr (ej ei C), where ei are the
ZZ
unit
basis
vectors
(i.e.
vectors
whose
a-th component is
dÏ‰1 dÏ‰2 it(Ï‰1 âˆ’Ï‰2 )
e
F(Î³ + iÏ‰1 , Î³ + iÏ‰2 ; B, x0 xT0 ),
Î´ia ). Using this trick with Eq. (4.11), and following the
2Ï€ 2Ï€
steps leading from Eq. (4.2)

 x to Eq.
 (4.3), we see that after
ensemble
averaging,
C
(Ï‰)
can be written in the
ij
where, for general matrix arguments B and C, we define
J
form

 x

F(z1 , z2 ; B, C) â‰¡
(4.4)
Cij (Ï‰) J = F(Î³ + iÏ‰, Î³ + iÏ‰; ej eTi , C I )
(4.12)
hTr(B G(0, z1 ; J)C G(0, z2 ; J))iJ .
where F was defined by Eqs. (4.4)â€“(4.6).
with
Next, consider the systemâˆšEq. (2.2) being driven âˆš
by
a sinusoidal input I(t) = I0 2 cos Ï‰t (theâˆšfactor of 2
B â‰¡ Ï€ 2 âŠ— BR ,
BR â‰¡ Râˆ’â€  BRâˆ’1 , (4.5)
serves to normalize the time average of ( 2 cos Ï‰t)2 to
1
C â‰¡ Ï€ âŠ— CL ,
CL â‰¡ Lâˆ’1 CLâˆ’â€  . (4.6)
one), and consider the steady state response, which will
also oscillate at frequency Ï‰. Decomposing the input,
Before proceeding to the calculation of F(z1 , z2 ; B, C)
I(t), and the steady-state response, xÏ‰ (t), into their posusing the diagrammatic technique, we will also express
itive and negative frequency components (proportional
the other quantities presented in Sec. II B in terms of
to eiÏ‰t and eâˆ’iÏ‰t , respectively), from Eq. (2.2) we obtain
F(Î³ + iÏ‰, Î³ + iÏ‰; B, C), with appropriate Bâ€™s and Câ€™s.
âˆš
xÏ‰ (t) = 2Râˆ’1 Re[eiÏ‰t G(Î³ + iÏ‰; J)]Lâˆ’1 I0 .
(4.13)
First, we obtain the desired expression for the matrix
power spectrum, Eq. (2.27), of the steady-state response
Thus the norm squared of the steady state response,
to a temporally white noisy input, I(t), with covariance
kx(t)k2 = x(t)â€  x(t), will have a zero frequency comEq. (2.26). Using the Fourier transform of Eq. (2.2), and
ponent, plus components oscillating at Â±2Ï‰. Averaging
following similar steps to those leading to Eq. (4.1), we
over time kills the latter, leaving the zero frequency comcan write the steady-state solution for x(t) as in Eq. (4.1)
ponent intact, yielding
with x0 replaced by the Fourier transform of the input,
IÌƒ(Ï‰). Using this and exploiting xj (t2 ) = xâˆ—j (t2 ) we can
xÏ‰ (t)T xÏ‰ (t) = IT0 Lâˆ’1 Gâ€  (z; J)Râˆ’â€  Râˆ’1 G(z; J)Lâˆ’1 I0

write (after averaging over the input noise)
= Tr Râˆ’â€  Râˆ’1 G(z; J)ÏI Gâ€  (z; J)
(4.14)
ZZ
dÏ‰1 dÏ‰2 it1 Ï‰1 âˆ’it2 Ï‰2
where z = Î³ + iÏ‰, the bar indicates temporal averagxi (t1 )xj (t2 ) =
e
Kij (Ï‰1 , Ï‰2 )
2Ï€ 2Ï€
ing, and we defined ÏI â‰¡ Lâˆ’1 I0 IT0 Lâˆ’â€  . Generalizing to
(4.7)
xÏ‰ (t)T BxÏ‰ (t), averaging over the ensemble, and followwhere
the
Fourier-domain
covariance
matrix,
ing the steps leading from Eq. (4.2) to Eq. (4.3), we obK(Ï‰1 , Ï‰2 ) â‰¡ xÌƒ(Ï‰1 )xÌƒ(Ï‰2 )â€  , is given by
tain
E
D
K(Ï‰1 , Ï‰2 ) â‰¡
(4.8)
xÏ‰ (t)T BxÏ‰ (t) = F(Î³ + iÏ‰, Î³ + iÏ‰; B, I0 IT0 ), (4.15)
J
Râˆ’1 G(Î³ + iÏ‰1 ; J)Lâˆ’1 C I (Ï‰1 , Ï‰2 )Lâˆ’â€  Gâ€  (Î³ + iÏ‰2 ; J)Râˆ’â€  .
where F is given by Eqs. (4.4)â€“(4.6). Comparing
Here, the bars indicate averaging over the input noise
Eq. (4.15) with Eq. (4.12), we also obtain
distribution, and we defined C I (Ï‰1 , Ï‰2 ) â‰¡ IÌƒ(Ï‰1 )IÌƒ(Ï‰2 )â€  .
D
E
On the other hand, the Fourier transform of Eq. (2.26)
xÏ‰ (t)T BxÏ‰ (t) = Tr(B hC x (Ï‰)iJ )
(4.16)
J
yields
which is Eq. (2.31) of Sec. II, it being understood that
C I (Ï‰1 , Ï‰2 ) â‰¡ IÌƒ(Ï‰1 )IÌƒ(Ï‰2 )â€  = 2Ï€Î´(Ï‰1 âˆ’ Ï‰2 )C I ,
(4.9)
C I in Eq. (4.12) is replaced by I0 IT0 as in Eq. (4.15).

25


m, n

=
N â†’âˆ

=


m, n NCP



ladders

Âµ1

Î½1

Î½2

Âµ2

Âµ1

Î½1

!

Ï1

Âµ2

Âµ1

Î½1

Î½2

D

Î½1

Î½2

Âµ2

Âµ1

Ï1

Î½2

Î»2

+

D

Î»1

Î½1

Ï2

Âµ2

Î»1

:=

+

+ Â·Â·Â·

+

Ï2

=

+

D

Âµ2

FIG. 12: Contribtutions to Eq. (4.17) in the non-crossing approximation. The first line shows Eq. (4.17) written using the
expansion Eq. (3.22). The diagram shows the contribution of
the m-th and n-th terms in the expansion for two Greenâ€™s
functions, respectively. Thus the top (bottom) solid line contains m (n) factors of J , shown by dashed lines. In the large
N limit, averaging each summand over J boils down to summing all non-crossing pairings (NCP) of the dashed lines. The
second row shows a specific non-crossing pairing for the diagram shown in the first line. Finally, summing over all m, and
n and all NCPâ€™s, is equivalent to replacing all solid lines (representing G(Î·i , zi ; J = 0)) with thick solid lines representing
the non-crossing average Greenâ€™s function, G(Î·i , zi ) (calculated according to Eqs. (3.28)â€“(3.26)), and summing over all
NCPâ€™s with every pairing connecting the straight lines on top
and bottom (and not each to itself). This procedure yields
the ladder diagrams, the sum over which is shown in the third
line.

Now that we have expressed all our quantities of interest in terms of the kernel F as defined in Eq. (4.4),
our task boils down to performing the average over J in
Eq. (4.4) to obtain a closed formula for F with general
arguments B and C. To this end, we now proceed to
calculate the more general object
FÂµ1 Î½2 ;Âµ2 Î½1 (1; 2) â‰¡ hGÂµ1 Î½1 (1; J)GÂµ2 Î½2 (2; J)iJ ,

D

Âµ1

=

ladders

J

Î»2

Î½2



(4.17)

using the diagrammatic technique. Here, we adopted the
abbreviated notation (1) â‰¡ (Î·1 , z1 ) and (2) â‰¡ (Î·2 , z2 ) for
the function arguments, and Âµi = (Î±i , ai ) (similarly for
Î½i ) for indices in the 2N dimensional space (as in Sec. III,
Î±, Î², . . ., and a, b, . . . denote indices in the 2 and N dimensional spaces, respectively). Once we have calculated
FÂµ1 Î½2 ;Âµ2 Î½1 (1; 2), we can obtain F(z1 , z2 ; B, C), with the
appropriate B and C, via
F(z1 , z2 ; B, C) = B Î½2 Âµ1 FÂµ1 Î½2 ;Âµ2 Î½1 (0, z1 ; 0, z2 )C Î½1 Âµ2 ,
(4.18)
where all indices are summed over, and B and C were
defined in Eqs. (4.5)â€“(4.6).
As before, we start by using the expansion Eq. (3.22)
for the two Greenâ€™s functions in Eq. (4.17). This is shown
diagrammatically in the first line of Fig. 12, for the contribution of m-th and n-th terms in the expansion of the
first and the second Greenâ€™s function, respectively. As

FIG. 13: The first row is the diagrammatic representation of
Eqs. (4.19)â€“(4.21). In the last term, Ïâ€™s and Î»â€™s are summed
over. It shows the sum of all ladder diagram contributing to
Eq. (4.17) (i.e. the last line of Fig. 12) in terms of D, which
is defined in the second row. The first term on the right side
of the first row equation (the ladder with zero rungs) is the
disconnected average Eq. (4.20); it corresponds to taking the
average of each Greenâ€™s function in Eq. (4.17) separately and
then multiplying. The last row shows an iterative form of the
equation in the second row, which can be solved to give the
expression Eqs. (4.23) and (4.26) for D.

before, for large N , averaging over J entails summing
the contribution of all non-crossing pairings. This is indicated in the second line of Fig. 12. Finally, the third
line of Fig. 12 shows that summing over all mâ€™s, and nâ€™s
and all non-crossing pairings, is equivalent to replacing
all solid lines with thick solid lines representing the average Greenâ€™s function in the non-crossing approximation,
G(Î·i , zi ) (defined diagrammatically in the third line of
Fig. Eq. (11), and given by Eq. (3.35) as we found in
the previous section), and summing over all non-crossing
pairings with every pairing connecting the thick arrow
lines on top and bottom (and not each to itself). This
procedure yields a sum over all ladder diagrams with different number of rungs, as shown in the third line of
Fig. 12.
As shown in the first row of Fig. 13, the sum of all
ladder diagrams can be written as a sum
F = F 0 + F D,

(4.19)

where
FÂµ01 Î½2 ;Âµ2 Î½1 (1; 2) â‰¡ GÂµ1 Î½1 (1)GÂµ2 Î½2 (2),

(4.20)

is the disconnected average of the two Greenâ€™s functions,
and FÂµD1 Î½2 ;Âµ2 Î½1 (1; 2) is the sum of ladder diagrams in
which the two Greenâ€™s function are connected by at least
one wavy line. The latter can be written in the form
FÂµD1 Î½2 ;Âµ2 Î½1 (1; 2) â‰¡
(4.21)
GÎ»2 Î½2 (2)GÂµ1 Ï1 (1)DÏ1 Î»2 ;Ï2 Î»1 (1; 2)GÎ»1 Î½1 (1)GÂµ2 Ï2 (2),
where all repeated indices are summed over, and the â€œdiffusonâ€, D, is given by the sum of all diagrams in the
second row of Fig. 13.

To calculate D, it helps to first rewrite Eq. (3.21) as

Î±Î´;Î³Î²
DÂµÏ;Î»Î½ (1; 2) = Dad;cb
(1; 2)

=

2
1 X r
s
(Ï€ Î´ad ) Drs (1; 2) (Ï€Î³Î²
Î´cb ),
N r,s=1 Î±Î´

1

D

1

D(1; 2) â‰¡ Ïƒ + Ïƒ Î  Ïƒ + Â· Â· Â· = Ïƒ

1

âˆ
X

n

, (4.24)

and the â€œpolarization matrixâ€ for the diffuson
r
s
rs
sr
Î D
rs (1; 2) â‰¡ tr(Ï€ G(1)Ï€ G(2)) = tr(G (1)G (2)).
(4.25)
Here, as before, with the trace performed over the 2N dimensional space, and we used Eq. (3.20) to write
the last form of Î D . Summing the geometric series in
Eq. (4.24) we obtain

D(1; 2) = Ïƒ 1 12Ã—2 âˆ’ Î D (1; 2)Ïƒ 1
The 2 Ã— 2 matrix inversion yields
D(1; 2) =
1
D
D
(1 âˆ’ Î D
)(1
âˆ’
Î D
12
21 ) âˆ’ Î 11 Î 22



âˆ’1

D

.

Î 22
1 âˆ’ Î D
21

Ï€t

Î±Î´;Î³Î²
FIG. 14: The contribution to Dad;cb
(1; 2) from the second
term in the series shown in the second row of Fig. 13, in more
detail. The covariance of J in the form Eq. (4.22) is used
to write this expression in a more manageable form. The
repeated indices, r, t, u, s, are summed over 1 and 2. The
matrices inside the loop multiply each other in cyclic order,
t
u
giving rise to the trace Tr (G(2)Ï€
whole dia 1 G(1)Ï€
 ). The
P
r
Î±Î´
D 1
1
gram gives N rs (Ï€ âŠ— 1)ad Ïƒ Î  Ïƒ rs (Ï€ s âŠ— 1)Î³Î²
cb where
the â€œpolarization matrixâ€ Î D
tu was defined in Eq. (4.25).

and

Î D Ïƒ 1

n=0

1
Ïƒus

(4.23)

where Âµ = (Î±, a), Î½ = (Î², b), Î» = (Î³, c), Ï = (Î´, d) and we
defined the 2 Ã— 2 matrices
1

1
Ïƒrt

G(2)



G(1)
Ï€u

(4.22)


01
is the first Pauli matrix. This helps
10
us because in the expansion of Fig. 13, the two factors
in Eq. (3.21) involving Ï€ r and Ï€ s decouple and get absorbed in adjacent loops, or contribute to form factors
in the left or right ends of the ladder diagrams. This is
demonstrated in Fig. 14 for the second term in the series
expansion of D shown in the second line of Fig. 13. Extending this similarly to all the terms in that expansion,
we obtain

where Ïƒ 1 =

r
Ï€Î±Î´
Î´ad

2
D
E
1 X r
Î±Î² Î³Î´
1
s
Jab Jcd =
(Ï€ Î´ad ) Ïƒrs
(Ï€Î³Î²
Î´cb ),
N r,s=1 Î±Î´
J

s
Ï€Î³Î²
Î´cb

26

(4.26)


1 X
Tr BR G 2r (0, z1 )G r2 (0, z2 ) Ã—
N r,s

Drs (0, z1 ; 0, z2 ) Tr G s1 (0, z1 )CL G 1s (0, z2 ) , (4.30)

âˆ†F(z1 , z2 ; B, C) =

where r and s are summed over {1, 2}.
According to Eq. (4.3) we are interested in zi = Î³ + iÏ‰i
(i = 1,2) for arbitrary real Ï‰i . As we mentioned before
Eq. (4.1), these trace a vertical line in the complex plane
that is entirely to the right of the support of the average
eigenvalue density, Ï(z), of A, i.e. they are in the region
where the the valid solution of Eq. (3.34) is the trivial
g(0, z) = 0. In this case, we have Eq. (3.41), and for Î· â†’
i0+ , from Eq. (3.10) (replacing A with M , corresponding
to J = 0) we have


0 Mzâˆ’â€ 
i
.
(4.31)
G(0, zi ) = âˆ’
0
Mzâˆ’1
i
Using this in Eqs. (4.29)â€“(4.30) we obtain

(4.27)

D
1 âˆ’ Î 12
,
Î D
11

where all Î D â€™s have arguments (1; 2) = (Î·1 , z1 ; Î·2 , z2 )
which were suppressed for clarity.
Going back to Eq. (4.18), we can also break up
F(z1 , z2 ; B, C) into a disconnected part and a connected
part mirroring the decomposition Eqs. (4.19)â€“(4.21):
F(z1 , z2 ; B, C) = F 0 (z1 , z2 ; B, C) + âˆ†F(z1 , z2 ; B, C),
(4.28)
where F 0 (z1 , z2 ; B, C) and âˆ†F(z1 , z2 ; B, C) are defined
as in Eq. (4.18), but with FÂµ1 Î½2 ;Âµ2 Î½1 on the right side
replaced by FÂµ01 Î½2 ;Âµ2 Î½1 and FÂµD1 Î½2 ;Âµ2 Î½1 , respectively. Using
Eqs. (4.20)â€“(4.21) and (4.23), we then obtain
F 0 (z1 , z2 ; B, C) = Tr (BG(0, z1 )CG(0, z2 ))
(4.29)

21
12
= Tr BR G (0, z1 )CL G (0, z2 ) ,

and


F 0 (z1 , z2 ; B, C) = Tr BR Mzâˆ’1
CL Mzâˆ’â€ 
,
1
2

(4.32)


âˆ†F(z1 , z2 ; B, C) = tr BR G 21 (0, z1 )G 12 (0, z2 ) Ã—

D12 (0, z1 ; 0, z2 ) Tr G 21 (0, z1 )CL G 12 (0, z2 ) . (4.33)

Using the definitions Eq. (2.6) and Eqs. (4.5)â€“(4.6) we
can simplify Eq. (4.32) to


1
1
F 0 (z1 , z2 ; B, C) = Tr B
C
. (4.34)
z1 âˆ’ M zÌ„2 âˆ’ M â€ 
From Eqs. (4.25) and (4.31) we see that (for zi of interest
and for Î·i going to zero) Î rr = 0 and Î 12 = Î 21 , and
from Eq. (4.27) we obtain
1
(4.35)
1 âˆ’ Î 21 (0, z1 ; 0, z2 )
1
=
.
1 âˆ’ tr(G 21 (0, z1 )G 12 (0, z2 ))

D12 (0, z1 ; 0, z2 ) =

D

27
Substituting this in Eq. (4.33) and using Eq. (4.31) once
again, we finally obtain


tr BR Mzâˆ’1
Mzâˆ’â€ 
Tr Mzâˆ’1
CL Mzâˆ’â€ 
1
2
1
2


âˆ†F(z1 , z2 ; B, C) =
,
âˆ’â€ 
1 âˆ’ tr Mzâˆ’1
1 Mz 2
and after simplification using Eqs. (2.6) and (4.5)â€“(4.6),

âˆ†F(z1 , z2 ; B, C) =
(4.36)

 

1
1
1
1
â€ 
â€ 
tr B z1 âˆ’M LL zÌ„2 âˆ’M â€  Tr R R z1 âˆ’M C zÌ„2 âˆ’M â€ 


.
=
1
1
1 âˆ’ tr Râ€  R z1 âˆ’M
LLâ€  zÌ„2 âˆ’M
â€ 

The general formulae of Sec. II B readily follow.
Equations (2.21)â€“(2.24), with C I replaced by x0 xT0 ,
for the case of response to an impulse input follow
from Eqs. (4.3), (4.28), (4.34) and (4.36), respectively.
Equations (2.28)â€“(2.30) (with C0x and âˆ†C x defined in
Eqs. (2.23)â€“(2.24)) for the power spectrum of the response to a temporally white noisy input, are similarly
obtained from Eq. (4.12) by using Eqs. (4.28), (4.34) and
Eq. (4.36), after setting B = ej eTi , C = C I and z1 = z2 =
Î³ + iÏ‰ (with the traces involving B = ej eTi turned into
matrices in Eqs. (2.28)â€“(2.30), using Tr (ej eTi X) = Xij ).
The result Eq. (2.31) for the steady state response to a
sinusoidal input was already derived in Eq. (4.16).
We see that according to Eqs. (4.3) and (4.28)
h
i
hx(t)T Bx(t)iJ = x(t)T Bx(t)
+ âˆ†fB (t), (4.37)
J=0

where the two terms on the right hand side are obtained
by replacing F(Â·, Â·; B) in Eq. (4.3) with Eq. (4.34) and
Eq. (4.36), respectively. The integrals over Ï‰1 and Ï‰2
decouple for the first term yielding the expected result
for J = 0,
h
i


â€ 
x(t)T Bx(t)
= eâˆ’2Î³t Tr BetM x0 xT0 etM ,
J=0

â€ 

= xT0 et(âˆ’Î³+M ) Bet(âˆ’Î³+M ) x0 . (4.38)

Unlike the J = 0 contribution, it is not possible to perform the double Fourier transform, Eq. (4.3), needed for
obtaining âˆ†fB (t) for arbitrary M , L and R. In the next
section, we will analytically calculate this for some special examples of M , with L and R proportional to the
identity matrix (i.e. for iid quenched randomness).
V.

CALCULATIONS FOR SPECIFIC
EXAMPLES OF M

In this section we give the detailed calculations of the
explicit expressions for the spectral density Eq. (2.8),
the power spectrum Eq. (2.31), and the average squared
norm Eqs. (2.21) and (2.25), for the specific examples of
M , L and R presented in Sec. II C.
In the examples worked out in the subsections V A
and V B, both R and L are proportional to the identity

matrix; we take L = 1 and R = Ïƒ1. Furthermore, for
such examples we will do the calculations by choosing the
unit of time such that Ïƒ = 1 (notice that given Eq. (2.2),
the elements of A and M have dimensions of frequency);
then at the end of our calculations using the replacements
t â†’ tÏƒ, z â†’ z/Ïƒ, Î³ â†’ Î³/Ïƒ, M â†’ M/Ïƒ, and Ï â†’ Ïƒ 2 Ï
(with the latter applying to both the eigenvalue density
and the power spectral density), we obtain the result for
general Ïƒ. The eigenvalue density and the norm squared
kxk2 are invariant with respect to unitary transforms,
and, for L and R proportional to the identity, so is the
distribution of the random part of A, Eq. (3.2). Thus
by effecting a unitary transform M â†’ U â€  M U , we can
assume M is already in its Schur form Eq. (2.40) without
loss of generality.

A.

Single feedforward chain of length N :
Mij = w Î´i+1,j âˆ’ Î³Î´ij

We start with the example in Sec. II C 1, where M is
ï£«
ï£¶
0 w 0 Â·Â·Â·
ï£·
ï£¬
(5.1)
M = T = ï£­0 0 w Â· Â· Â·ï£¸
.. .. .. . .
. . . .
or Mij = w Î´i+1,j . First we calculate the eigenvalue density. According to Eqs. (2.8)â€“(2.9), in order to calculate the spectral density, we need to calculate first the
inverse of Mz Mzâ€  + g 2 = (z âˆ’ M )(z âˆ’ M )â€  + g 2 (remember that we have set Ïƒ = 1, as we explained in
the beginning
of the section).
To this end, notice that


Kij â‰¡ (z âˆ’ M )(z âˆ’ M )â€  ij = Qij âˆ’ |w|2 Î´iN Î´jN where
Qij â‰¡ (|z|2 + |w|2 )Î´ij âˆ’ wzÌ„Î´i+1,j âˆ’ wÌ„zÎ´i,j+1 .

(5.2)

As the difference (z âˆ’ M )(z âˆ’ M )â€  âˆ’ Q = âˆ’|w|2 eN eTN is
single rank, we can use the Woodbury formula for matrix
inversion to write
1
1
=
+
(5.3)
K + g2
Q + g2


1
1
1
T
e
e
,
Q + g 2 N N Q + g 2 |w|âˆ’2 âˆ’ eTN (Q + g 2 )âˆ’1 eN
where eTN = (0, . . . , 0, 1). (The only conditions for the
validity of Eq. (5.3) is that the factor in parenthesis is not
singular, i.e. eTN (Q + g 2 )âˆ’1 eN 6= |w|âˆ’2 ; we will consider
the validity of this condition below.) Since Q is Toeplitz
and Hermitian, it can be diagonalized easily. Using standard methods, we find that the eigenvalues and eigenvectors of Q, satisfying Qv n = Î»n v n , are given by

2
Ï€n


Î»n = |z| âˆ’ |w|eiÏ†n  ,
Ï†n â‰¡
(5.4)
N +1
r
2  wÌ„z j/2
vnj =
sin Ï†n j
(5.5)
N + 1 wzÌ„

28
for n = 1, . . . , N . The eigenvectors are orthonormal
v â€ n v m = Î´nm , and we have the spectral representation
N
X
1
1
=
vâ€  .
vn
2
2 n
Q+g
Î»
+
g
n
n=1

(5.6)

Using Eqs. (5.4)â€“(5.6) in Eq. (5.3) we obtain
âˆ‚I2 (g,z)

âˆ’ âˆ‚g2
1
1
=
I
(g,
z)
+
tr
,
1
K + g2
N |w|âˆ’2 âˆ’ I2 (g, z)

(5.7)

where we defined
I1 (g, z) â‰¡ tr

N
1 X
1
1
=
2
Q+g
N n=1 Î»n + g 2

(5.8)

N
1 X 2 sin2 Ï†n
1
e =
. (5.9)
I2 (g, z) â‰¡ eN
Q + g2 N
N + 1 n=1 Î»n + g 2
T

In writing the numerator of the lastterm in Eq. (5.7), we

1
1
T
used Eqs. (5.5)â€“(5.6) to write Tr Q+g
=
2 eN eN Q+g 2
P
2
N
âˆ‚I
(g,z)
2
sin
Ï†
1
2
2
k Q+g
= N1+1 n=1 (Î»n +g2 )n2 = âˆ’ âˆ‚g
. In the
2 eN k
2
N â†’ âˆ limit, the sums in Eqs. (5.8)â€“(5.9) can be approximated by the integrals

I1 (g, z) =

Z

2Ï€

0

I2 (g, z) =

Z

0

2Ï€

dÏ†
1
,

2
|z| âˆ’ |w|eiÏ†  + g 2 2Ï€
2 sin2 Ï†
dÏ†
.

2
|z| âˆ’ |w|eiÏ†  + g 2 2Ï€

(5.10)
(5.11)

Some elementary contour integration then yields

âˆ’1/2
I1 (g, z) = (|z|2 + |w|2 + g 2 )2 âˆ’ 4|w|2 |z|2
, (5.12)

I2 (g, z) =

|z|2 + |w|2 + g 2 âˆ’ I1 (g, z)âˆ’1
.
2|w|2 |z|2

(5.13)

In particular, we see that I2 (0, z) = min(|w|âˆ’2 , |z|âˆ’2 ), so
that the condition for the validity of Eq. (5.3) would be
violated for |z| < |w|, if g turns out to be zero. However,
note that I2 (g, z) is a decreasing function of g 2 , so for
finite g 2 > 0, the denominator in Eq. (5.7) is always
positive (as is its numerator, for the same reason). Thus
if we follow the correct procedure of Eq. (2.19)â€“(2.20),
taking the N â†’ âˆ limit before sending g 2 to zero, we
are justified in using Eqs. (5.3) and (5.7). Furthermore,
for g 2 > 0 the second term in Eq. (5.7) is O(N âˆ’1 ), and
should be neglected. Solving Eq. (2.9) (with left hand
side correctly interpreted as Eq. (2.19)), which now takes
the form I1 (g, z) = 1, yields
p
g(z)2 = âˆ’|z|2 âˆ’ |w|2 + 4|w|2 |z|2 âˆ’ 1.
(5.14)
This is positive if and only if
p
p
|w|2 âˆ’ 1 â‰¤ |z| â‰¤ |w|2 + 1,

(5.15)

which after the proper rescaling yields Eq. (2.36) for
general Ïƒ. Note that Eq. (5.15) is precisely the region
given by Eq. (2.20), which in the present case reads
I1 (0, z) â‰¥ 1. It is instructive to compare this result
with what we would obtain by naively using Eq. (2.5),
i.e. tr (K âˆ’1 ) â‰¥ 1, wherein g is set to zero before taking the N â†’ âˆ limit; as we now show, that only yields
the right inequality in Eq. (5.15). To see this, first note
that for |w| > |z|, we can use Eq. (5.7) even for g 2 = 0
(since the denominator
of the

 last term does not vanish),
which yields tr (Mz Mzâ€  )âˆ’1 = tr (K âˆ’1 ) = I1 (0, z) + o(1),
and by Eq. (2.5), the right inequality in Eq. (5.15). For
|z| < |w|, however, we cannot set g = 0 in Eq. (5.7).
In fact, when |z| < |w|, the matrix z âˆ’ M has an exponentially small singular value; to see this, note that
the vector u with components ui = ( wz )iâˆ’1 satisfies
(z âˆ’ M )u = w( wz )N eN , so that k(z âˆ’ M )uk = |w|| wz |N ,
)uk
and since smin (z âˆ’ M ) â‰¤ k(zâˆ’M
and kuk â‰¥ 1, it folkuk
z N
lows that smin (z âˆ’ M ) â‰¤ |w|| w | , which is O(eâˆ’cN ) for
|z| < |w|. For large enough N , this singular value alone
suffices to make Eq. (2.11) (equivalent to Eq. (2.5)) hold
for any |z| < |w|, as N1 smin (z)âˆ’2 diverges despite its N1
prefactor.
Let us now calculate the eigenvalue density in the annulus Eq. (5.15). In order to use Eq. (2.8), we will first
calculate
tr

Mâ€ 
zÌ„ âˆ’ M â€ 
=
zÌ„
âˆ’
tr
K + g(z)2
K + g(z)2

(5.16)

where we used Eq. (2.9) to write the last expression.
Mâ€ 
To obtain tr K+g(z)
In
2 , we will again use Eq. (5.3).
the region Eq. (5.15), the contribution of the second
term in Eq. (5.3) is again suppressed by 1/N , and

âˆ’1
from Eq. (5.1)
and (5.6)we have tr M â€  Q + g 2
=

PN
PN âˆ’1 j j+1
1
1
wÌ„ N n=1
A straightforward
j=1 vn vÌ„n
Î»n +g 2 .

calculation using Eq. (5.5) (and the orthonormality of
1/2
PN âˆ’1 j j+1
= wzÌ„
v n ) yields
cos Ï†n . Using this
j=1 vn vÌ„n
wÌ„z
and approximating the sum over n with an integral, we
obtain


Z
|w||z| 2Ï€
cos Ï†
Mâ€ 
dÏ†
â‰ˆ
,
tr

2
iÏ†
2
Q + g(z)2
z


0
|z| âˆ’ |w|e
+ g(z) 2Ï€

1  2
=
(|z| + |w|2 + g(z)2 )I1 (g(z), z) âˆ’ 1 .
(5.17)
2z
Using Eq. (5.17) with I1 (g(z), z) = 1 (true in the region
Eq. (5.15)), differentiating Eq. (5.16) with respect to zÌ„,
and substituting in Eq. (2.8), we finally obtain
"
#
1
|w|2
Ï(z) =
1âˆ’ p
,
(5.18)
Ï€
4|w|2 |z|2 + 1

for z in the region Eq. (5.15). After the proper rescaling
this yields Eq. (2.37).



We now turn to the calculation of kx(t)k2 J , using
(2.25). To calculate the trace in the denominator of

29
Eq. (2.25), first note that for Eq. (5.1) the expansion
PN âˆ’1 n
(z âˆ’ M )âˆ’1 = n=0 zM
n+1 terminates and is exact, yielding


1  w jâˆ’i
1
,
(5.19)
=
z âˆ’ M i,j
z z
for j â‰¥ i, and zero otherwise. Turning the sums in the
trace into a sum over the nonzero diagonals of Eq. (5.19)
we obtain


N âˆ’1
1
1
n
1 X
tr
(1 âˆ’ )q n (5.20)
=
â€ 
zÌ„2 âˆ’ M z1 âˆ’ M
zÌ„2 z1 n=0
N
where q â‰¡ |w|2 /(zÌ„2 z1 ) and zi = Î³ + iÏ‰i . The condition
of stability of Eq. (2.2) requires the entire spectrum of
âˆ’Î³1+A = âˆ’Î³1+M +J to be to the leftp
of the imaginary
axis. By Eq. (5.15), this requires Î³ > |w|2 + 1 > |w|.
It follows that |q| < 1, and therefore the geometric series
Eq. (5.20) converges as N â†’ âˆ. Summing the series and
retaining terms of leading order as N â†’ âˆ, we obtain


1
1
1
tr
=
.
(5.21)
zÌ„2 âˆ’ M â€  z1 âˆ’ M
zÌ„2 z1 âˆ’ |w|2
If we set the initial condition x0 in Eqs. (2.25) (or the input amplitude I0 in Eq. (2.31)) to eN = (0, Â· Â· Â· , 0, 1)T ,
and use Eq. (5.19), we find that the numerator in
Eq. (2.25) is also given by the right hand side of
Eq. (5.21). Using this and Eqs. (5.21), we obtain
F (z1 , z2 ) =

1
,
zÌ„2 z1 âˆ’ |w|2 âˆ’ 1

(5.22)

for the integrand of Eq. (2.25) which we denoted by
F (z1 , z2 ), with zi = Î³ + iÏ‰i , (i = 1, 2). By comparing the integrand of Eq. (2.25) with Eq. (2.33), we see
that to obtain the total power spectrum for the input
amplitude I0 = I0 (0, Â· Â· Â· , 0, 1)T , we need to multiply
Eq. (5.22) by I20 = kI0 k2 , and substitute z1 = z2 = Î³ +iÏ‰.
With the proper rescaling, this yields
 Eq. (2.39)
 for general Ïƒ. To obtain the formula for kx(t)k2 J , we substitute Eq. (5.22) with zi = Î³ + iÏ‰i for the integrand
of Eq. (2.25). Changing the integration variables by
Ï‰1 = â„¦ + Ï‰/2 and Ï‰2 = â„¦ âˆ’ Ï‰/2, we obtain
Z
Z



dÏ‰ itÏ‰ dâ„¦
1
kx(t)k2 J =
e
,
2Ï€
2Ï€ â„¦2 + (Î³ + iÏ‰/2)2 âˆ’ |w|2 âˆ’ 1
Z
1 dÏ‰
eitÏ‰
p
=
.
(5.23)
2 2Ï€ (Î³ + iÏ‰/2)2 âˆ’ |w|2 âˆ’ 1
Finally consulting a table of Laplace transforms [51], we
obtain
p



kx(t)k2 J = eâˆ’2Î³t I0 (2t |w|2 + 1),
(t â‰¥ 0) (5.24)

where I0 (x) is the 0-th modified Bessel function. Implementing the rescalings t â†’ tÏƒ, Î³ â†’ Î³/Ïƒ and w â†’ w/Ïƒ,
we obtain Eq. (2.38).

B.

N/2 feedforward chains of length 2

Here we carry out the explicit calculations for the example of Sec. II C 2 where M is given by Eq. (2.40) (without loss of generality, we assume M has its Schur form),
using formulae 
(2.8)â€“(2.9)
 for the spectral density and
Eq. (2.25) for kx(t)k2 J . First we will calculate the
eigenvalue density. From Eq. (2.40), K â‰¡ Mz Mzâ€  =
(z âˆ’ M )(z âˆ’ M )â€  (we are setting L = R = 1 in Eq. (2.6);
see the comments at the beginning of this section) is a
block-diagonal matrix with 2 Ã— 2 diagonal blocks, with
the b-th block (b = 1, . . . , N/2) given by


  2

z âˆ’wb
zÌ„
0
|z| + |wb |2 âˆ’wb zÌ„
=
, (5.25)
0 z
âˆ’wÌ„b zÌ„
âˆ’wÌ„b z
|z|2
where wb is the corresponding Schur weight in Eq. (2.40).
Likewise, (K + g 2 )âˆ’1 whose trace appears in Eqs. (2.9) is
given by a block-diagonal
matrix with diagonal
blocks
 2

2
|z|
+
g
w
zÌ„
b
1
. Taking
(|z|2 +g 2 )2 +|wb |2 g 2
wÌ„b z |z|2 + g 2 + |wb |2
the normalized trace we thus obtain


|z|2 + g 2 + 12 |wb |2
2 âˆ’1
, (5.26)
tr (K + g ) =
(|z|2 + g 2 )2 + |wb |2 g 2 b
where hÂ·ib means averaging over the N/2 blocks, i.e.
PN/2
1
hf (wb )ib â‰¡ N/2
b=1 f (wb ).
Let us first calculate the support boundary of Ï(z).
As discussed in Sec. II A, when (for |z| =
6 0) all singular
values of Mz = z âˆ’ M are bounded from below as N â†’
âˆ, the support is correctly given by Eq. (2.5) (we will
discuss cases in which some si (z) are o(1) further below).
Setting g = 0 in Eq. (5.26), and substituting in Eq. (2.5),
this yields
1 â‰¤ tr K âˆ’1 = |z|âˆ’2 + Âµ2 |z|âˆ’4 ,

(5.27)

where we defined Âµ2 = 12 h|wb |2 ib = tr (M â€  M ). It follows
that the support is the disk |z| â‰¤ r0 , where
r
1
1
2
r0 = +
+ Âµ2 .
(5.28)
2
4
The replacements Âµ â†’ Âµ/Ïƒ and r0 â†’ r0 /Ïƒ then yield
Eq. (2.44).
From Eqs. (2.9) and (5.26), within the support, g 2 (z)
is found by solving the equation


|z|2 + g 2 + 21 |wb |2
1
tr
=
= 1,
(5.29)
K + g2
(|z|2 + g 2 )2 + |wb |2 g 2 b
while for |z| > r0 we have g(z) = 0. It is clear from
Eq. (5.29) that g 2 (z) depends on z and zÌ„ only through
|z| â‰¡ r. From Eq. (2.8), within its support the eigenvalue
density is given by

âˆ‚  â€ 
tr Mz (K + g 2 )âˆ’1
âˆ‚ zÌ„

âˆ‚  â€ 
= 1âˆ’
tr M (K + g 2 )âˆ’1 ,
âˆ‚ zÌ„

Ï€Ï(z) =

(5.30)

30
where we are now using the short-hand g 2 = g 2 (|z|) (the
solution of Eq. (5.29)), and in writing the second line we
used Mzâ€  = zÌ„ âˆ’ M â€  and Eq. (5.29). From Eqs. (2.40) and
(5.26) we see that M â€  (K + g 2 )âˆ’1 has the same blockdiagonal structure
as Eq. (2.40),

 and a short calculation
shows that tr M â€  (K + g 2 )âˆ’1 = zÌ„I3 (|z|), where we defined


1
2
2 |wb |
.
(5.31)
I3 (r) â‰¡
(r2 + g(r)2 )2 + |wb |2 g(r)2 b
I3 (r) is manifestly positive (assuming some wb are
nonzero), while when g 2 > 0, from Eq. (5.29) we have
I3 (r) â‰¤ 1, and thus
0 < I3 (r) â‰¤ 1.

(5.32)

Replacing this in Eq. (5.30), and using zÌ„ âˆ‚fâˆ‚(|z|)
=
zÌ„

2r âˆ‚fâˆ‚r(r)r=|z| , we obtain
Ï€Ï(z) =


1 âˆ‚  2
r âˆ’ r2 I3 (r) ,
2r âˆ‚r

(5.33)

for r = |z| â‰¤ r0 , and zero otherwise; the spectral
density is rotationally symmetric and depends only on
r = |z|. The advantage of writing the density as a
complete derivative is that it can be immediately integrated to yield n< (r), the proportion of eigenvalues
with modulusR smaller than some radius r. We have
r
n< (r) = 2Ï€ 0 Ï(r0 )r0 dr0 , which upon substitution of
Eq. (5.33), yields
n< (r) = r2 (1 âˆ’ I3 (r))

(r â‰¤ r0 ).

(5.34)

Likewise, we define n> (r) â‰¡ 1 âˆ’ n< (r) to be the proportion of eigenvalues with modulus larger than r. From
these definitions we have
Ï(r) =

1 âˆ‚n> (r)
1 âˆ‚n< (r)
=âˆ’
,
2Ï€r âˆ‚r
2Ï€r âˆ‚r

(5.35)

and from Eqs. (5.34) and n> (r) = 1 âˆ’ n< (r), after some
manipulation exploiting Eq. (5.29), we obtain
n> (r) = g(r)2 (1 + I3 (r)).

are given by the trace and determinant of Eq. (5.25), i.e.
by |wb |2 + 2|z|2 and |z|4 , respectively. It follows that
for blocks where the feedforward weight wb is O(1), both
sb,Â± (z) will be Î˜(1) for |z| =
6 0, while for blocks in which
wb â†’ âˆ as N â†’ âˆ, we have
(5.37)

s2bâˆ’ (z) â‰ˆ

(5.38)

4

|z|
= o(1).
|wb |2

(Note that as stated after Eq. (2.3) we assume kM k2F =
Âµ2 = h|wb |2 ib /2 is O(1), so that at most o(N ) number of
weights
âˆš can be unbounded, and each such wb can at most
be O( N ).) If all the wb are O(1), and hence all singular
values are Î˜(1) (for |z| =
6 0), Eq. (5.28) yields the correct
support radius as noted above, and for r â‰¤ r0 , Eq. (5.29)
yields a Î˜(1) solution for g(r)2 , which leads to a Î˜(1) solution for n> (r) and Ï(r) via Eqs. (5.36)â€“(5.35). In cases
in which some wb are unbounded, however, Eq. (5.28)
(derived from Eq. (2.5)) may not yield the correct support boundary. Such cases are examples of the highly
nonnormal cases mentioned in the general discussion after Eq. (2.12), for which the support of limN â†’âˆ Ï(z)
must be found by using Eqs. (2.19)â€“(2.20). This is equivalent to solving Eq. (5.29) after the limit N â†’ âˆ is
taken (assuming g 2 > 0), and then finding where the solution for g 2 (|z|) vanishes, which yields the correct support radius. From Eq. (5.36) this is indeed the radius at
which limN â†’âˆ n> (r) and hence limN â†’âˆ Ï(r) vanish as
well. This radius is in general smaller than r0 as given
by Eq. (5.28).
We now calculate Ï(z) for two specific examples of M
from each group. The first example is that of equal and
O(1) feedforward weights in all blocks, which we denote
by w (in terms of Eq. (2.41), this case corresponds to K =
w1). Here we can drop the block averages in Eqs. (5.29)
and (5.31), replacing wb with w. Solving Eqs. (5.29) for
g 2 (|z| = r) we find
1 w2
1p
g 2 (r) = âˆ’
âˆ’ r2 +
1 + w4 + 4w2 r2 . (5.39)
2
2
2
Substituting this into Eq. (5.31) and Eq. (5.34) yields

(5.36)

n< (r) = r2 âˆ’

2

We see that beyond the radius r at which g vanishes
(which when all wb â€™s are bounded is r = r0 ), n> (r) and
1 âˆ‚n>
Ï(r) = âˆ’ 2Ï€r
âˆ‚r vanish identically, while for smaller r
they are positive.
In cases in which some wb grow without bound as N â†’
âˆ, some singular values, si (z), of Mz = z âˆ’ M are o(1),
and more care is needed. First, to see this, note that by
definition si (z)2 are the eigenvalues of the block-diagonal
K = Mz Mzâ€  ; thus they come in pairs composed of the
eigenvalues of Kâ€™s 2 Ã— 2 blocks, given by Eq. (5.25). We
denote the pair of eigenvalues corresponding to block b
by sb,Â± (z)2 , with the plus and minus subscripts denoting
the larger and smaller singular value, respectively. The
sum sb+ (z)2 + sbâˆ’ (z)2 and the product sb+ (z)2 sbâˆ’ (z)2

s2b+ (z) = |wb |2 + O(1) â†’ âˆ

1+

âˆš

w2 r2
.
1 + w4 + 4w2 r2

(5.40)

The replacements w â†’ w/Ïƒ and r â†’ r/Ïƒ then yield
Eq. (2.45) for general Ïƒ, and Ï(r) can be caclulated using
Eq. (5.35).
The second case is that of Eq. (2.42). In this case only
one of the blocks has a nonzero Schur weight given by
|w1 |2 = Tr (M â€  M ) = N Âµ2 = O(N ), where Âµ = O(1) is
given by Eq. (2.43). Equation (5.29) now yields
1=

1
Âµ2
r2 âˆ’ g2
+
Â·
,
r2 + g2
(r2 + g 2 )2 + N Âµ2 g 2 r2 + g 2

(5.41)

r2 + g2 âˆ’ 1
Âµ2
=
.
r2 âˆ’ g2
(r2 + g 2 )2 + N Âµ2 g 2

(5.42)

or

31
The right hand side of this last equation is I3 (r), as follows from Eq. (5.31); thus using Eq. (5.42) we can rewrite
Eq. (5.36) as
n> (r) = g 2 (r)

2r2 âˆ’ 1
.
r2 âˆ’ g 2 (r)

(5.43)

Let us now solve Eq. (5.42) to find g(r)2 . As noted above,
and in accordance with the general prescription given after Eq. (2.12), for the purpose of obtaining limN â†’âˆ Ï(z)
we have to first take the N â†’ âˆ limit in Eq. (5.41), keeping g 2 > 0 fixed, and only then solve for g 2 . Doing so
makes the last term in Eq. (5.41) vanish, and we obtain
g 2 (r) = 1 âˆ’ r2 . This is positive for r â‰¤ 1 and vanishes at
r = 1, the correct support radius of limN â†’âˆ Ï(z), which
is strictly smaller than r0 given by Eq. (5.28). From
Eq. (5.43) we obtain n> (r) = g 2 (r) = 1 âˆ’ r2 . It then
follows from Eq. (5.35) that the N â†’ âˆ limit of the
eigenvalue density is identical with the circular law (the
result for the M = 0), i.e. limN â†’âˆ Ï(r) = Ï€1 for r â‰¤ 1
and zero otherwise. With the correct scaling, this yields
Eq. (2.46).
Contrary to the general prescription given after
Eq. (2.12), we will now solve equations Eqs. (5.29), (5.31)
and Eq. (5.36) for r > 1, without taking the limit N â†’ âˆ
first. As we will see, the obtained solution for g(r)2 ,
and by Eqs. (5.32) and (5.36) therefore the solutions for
n> (r) and Ï(r), will be nonzero but o(1) for 1 < r â‰¤ r0 .
As discussed in Sec. II C 2, these finite-size corrections,
which in general are not trustworthy, in the present case
are in surprisingly good agreement with simulations for
some range of râ€™s beyond rÎ˜(1) , but deviate from the true
n> (r) for larger r (see Fig. 7). At finite N , it can indeed
be checked that Eq. (5.29) has a positive solution for g 2 if
and only if r < r0 , with r0 given by Eq. (5.28). Simplifying Eq. (5.42) yields a cubic equation in g 2 . However, it
turns out that ignoring the cubic term in g 2 is harmless
for large N ; the quadratic approximation has the positive
solution
s
2
1 âˆ’ r2
1 âˆ’ r2
r2 (r2 + Âµ2 ) âˆ’ r6
g 2 (r) =
+
+
,
2
2
Âµ N
2
(5.44)
and for all r < r0 , corrections to Eq. (5.44) when the cubic term is reinstated decay faster than the leading contribution from Eq. (5.44) (nevertheless we numerically
solved the full cubic equation (5.29) to obtain the black
curve in Fig. 7, and the blue trace in Fig. 6). First, analyzing Eq. (5.44) we see that g 2 (r) is indeed Î˜(1) only for
r < 1, where as we already found g 2 (r) = 1 âˆ’ r2 + o(1).
Furthermore, for a fixed r > 1 (such that r âˆ’ 1 does not
vanish as N â†’ âˆ), the solution for g(r) is O(N âˆ’1 ). Thus
from Eq. (5.43) we thus see that N n> (r), i.e. the total
number of eigenvalues with modulus larger than r, for
1 < r < r0 (and r âˆ’ 1 = Î˜(1)) is only O(1); the solution
for N n> (r) is shown in Fig. 7. Correspondingly, from
Eqs. (5.43) and (5.35) we see that Ï(r) is o(1) in this region and vanishes in the limit N â†’ âˆ, as already found.

Now let us calculate the total number of eigenvalues lying outside the circle |z| = 1. This is given by N n> (1).
From Eq. (5.44) we find g 2 (1) = âˆš1N , and substituting in
Eq. (5.43) we obtain
âˆš
(5.45)
N> (1) â‰¡ N n> (1) = N + O(1).
With the proper rescaling this yields Eq. (2.47) for general Ïƒ. Note that, according to Eq. (5.44), g(r) (and
hence n> (r)) remains Î˜(N âˆ’1/2 ) (as opposed to O(N âˆ’1 ))
in a thin boundary layer outside of width Î˜(N âˆ’1/2 ) just
outside of the circle |z| = 1.



We will now work out the formula for kx(t)k2 J ,
Eqs. (2.25)â€“(2.21), when the initial condition x0 is the
second Schur-vector in block b = a, which we denote
by ea2 ; in the Schur representation, Eq. (2.40), we have
ea2 = (0, 1)T (we only write the components of ea2 in
block a). To calculate the numerator in Eq.
 (2.25)
 we
0 wa
âˆ’1
first calculate (z âˆ’ Ta ) ea2 where Ta =
de0 0
notes the a-th diagonal 2 Ã— 2 block of Eq. (2.40). Since
Ta2 = 0, we have (z âˆ’ Ta )âˆ’1 = z âˆ’1 + z âˆ’2 Ta , which yields
v a (z) â‰¡ (z âˆ’ Ta )âˆ’1 ea2 = (wa z âˆ’2 , z âˆ’1 )T . We thus obtain
1
1
1
|wa |2
x0 = v a (z2 )â€  v a (z1 ) =
+ 2 2.
â€ 
zÌ„2 âˆ’ M z1 âˆ’ M
z1 zÌ„2
z1 zÌ„2
(5.46)
On the other hand, we have
xT0

tr

1
1
=
â€ 
zÌ„2 âˆ’ M z1 âˆ’ M

(5.47)

1
1
h|wb |2 ib /2
h Tr 2Ã—2 (zÌ„2 âˆ’ Tbâ€  )âˆ’1 (z1 âˆ’ Tb )âˆ’1 ib =
.
+
2
z1 zÌ„2
z12 zÌ„22
Substituting Eqs. (5.46)â€“(5.47) in Eq. (2.25) we obtain
F (z1 , z2 ) =

z1 zÌ„2 + |wa |2
.
(z1 zÌ„2 )2 âˆ’ (z1 zÌ„2 + Âµ2 )

(5.48)

where we used Âµ2 = h|wb |2 ib /2, and we denoted the integrand of Eq. (2.25) by F (z1 , z2 ) with zi = Î³ + iÏ‰i ,
(i = 1, 2). By comparing the integrand of Eq. (2.25) with
Eq. (2.33), we see that substituting z1 = z2 = Î³D+ iÏ‰ into
E
Eq. (5.48) yields the total power spectrum,

kxÏ‰ k2 .
J

After the proper rescalings,
this


 yields Eq. (2.50) for
general Ïƒ. To obtain kx(t)k2 J , on the other hand,
we should substitute Eq. (5.48) into Eq. (2.21) with
zi = Î³+iÏ‰i . Let us use the change of variables Ï‰1 = â„¦+Ï‰
and Ï‰2 = â„¦ âˆ’ Ï‰. Then we have z1 zÌ„2 = â„¦2 + (Î³ + iÏ‰)2 ,
and from Eq. (2.21) we obtain
Z



dÏ‰ 2itÏ‰
2
kx(t)k J =
e
fa (Î³ + iÏ‰)
(5.49)
2Ï€
where we defined
Z
dâ„¦
â„¦2 + u2 + |wa |2
fa (u) â‰¡ 2
.
2Ï€ (â„¦2 + u2 )2 âˆ’ (â„¦2 + u2 + Âµ2 )

(5.50)

32
where we defined v â‰¡ (RL)âˆ’1 u. Using the Woodbury
matrix identity we can write

Let us rewrite the integrand in Eq. (5.50) as

(â„¦2

â„¦2 + u2 + |wa |2
â„¦2 + u2 + |wa |2
=
Ã—
2
2
2
2
2
+ u âˆ’ r0 )(â„¦ + u + r1 )
r02 + r12


1
1
âˆ’
,
(5.51)
â„¦2 + u2 âˆ’ r02
â„¦2 + u2 + r12

where r02 was defined in Eq. (5.28) and
r12

â‰¡

r02

âˆ’ 1 â‰¥ 0.

The integral of Eq. (5.53) in Eq. (5.49) is essentially the
inverse Laplace transform of Eq. (5.53). Consulting a
table of Laplace transforms [51] yields



kx(t)k2 J =
(5.54)
 2

2
2
2
r + |wa |
r1 âˆ’ |wa |
eâˆ’2Î³t 0 2
I
(2r
t)
+
J
(2r
t)
.
0
0
0
1
r0 + r12
r02 + r12

where J0 (x) (I0 (x)) is the 0-th Bessel function (modified
Bessel function).
p From Eqs. (5.28) and (5.52) it follows
2
2
that r0 + r1 = 1 + 4Âµ2 , and using Âµ2 = h|wb |2 ib /2 once
again, we obtain





1 + Ca
1 âˆ’ Ca
kx(t)k2 J =
I0 (2r0 t) +
J0 (2r1 t) eâˆ’2Î³t
2
2
(5.55)
where we defined
1 + 2|wa |2

1+

2h|wb |2 ib

.

1
g 2 + |z|2 (RL)âˆ’2
 2

s âˆ’zÌ„s
Dâ‰¡
.
âˆ’zs 0
Qâ‰¡

Network with different neural types and
independent, factorizable weights

Here we carry out the explicit calculations for the network with C neural types presented Sec. II C 2, with M ,
L and R are given by Eqs. (2.52)â€“(2.55). From Eqs. (2.6)
and (2.52)â€“(2.55) we obtain Mz = z(RL)âˆ’1 âˆ’ suuT , and
Mz Mzâ€  = |z|2 (RL)âˆ’2 âˆ’ zsvuT âˆ’ zÌ„suvT + s2 uuT , (5.57)

(5.59)

(5.60)

We will argue that for g > 0, tr (g 2 + Mz Mzâ€  )âˆ’1 = tr Q,
up to o(1) corrections. From Eq. (5.58), for the remainder
âˆ†(g, z) â‰¡ tr (g 2 + Mz Mzâ€  )âˆ’1 âˆ’ tr Q, we obtain


1
U â€  Q2 U
âˆ†(g, z) = âˆ’ Tr
(5.61)
N
Dâˆ’1 + U â€  QU
where the trace is now over 2 Ã— 2 matrices. We have


0 (zs)âˆ’1
âˆ’1
D =âˆ’
(5.62)
(zÌ„s)âˆ’1 |z|âˆ’2
and for n = 1, 2 we obtain
 â€  n
 

u Q u uâ€  Qn v
In,0 In,1
U â€  Qn U =
=
, (5.63)
In,1 In,2
uâ€  Qn v vâ€  Qn v
where
N
(lc(i) rc(i) )âˆ’k
1 X

 (5.64)
N i=1 g 2 + |z|2 (lc(i) rc(i) )âˆ’2 n
*
+
Ïƒcâˆ’k
=
(5.65)
n
g 2 + Ïƒcâˆ’2 |z|2
c

In,k (g, z) â‰¡

and we are using the notation Eq. (2.59) (we will drop
the explicit g and z dependence of In,k when convenient).
Note that all In,k (g, z) are O(1) and for even k are positive. Inverting Dâˆ’1 + U â€  QU we obtain

(5.56)

Effecting the proper rescalings we obtain the result for
general Ïƒ, Eqs. (2.48)â€“(2.49).

C.

= Q âˆ’ QU

(5.52)

One can calculate the integral over â„¦ in Eq. (5.50) by
contour integration, closing the contour, say, in the upper half of complex plane. The poles of the first and the
second terms p
on the second line of Eq. p
(5.51) are located
2
2
at â„¦0,Â± = Â±i u âˆ’ r0 and â„¦1,Â± = Â±i u2 + r12 , respectively. For u = Î³ + iÏ‰ (Î³ > 0) the roots falling in the
upper half plane are â„¦0,+ and â„¦1,+ , independently of Ï‰.
From their residues we obtain
"
#
1
r02 + |wa |2
r12 âˆ’ |wa |2
p
fa (u) = 2
+ p
. (5.53)
r0 + r12
u2 âˆ’ r02
u2 + r12

Ca â‰¡ p

1
U â€ Q
(5.58)
Dâˆ’1 + U â€  QU

where we defined the N Ã— 2 matrix U = u , v , and
1

g 2 + Mz Mzâ€ 

âˆ†(g, z) =

1
T (g, z)
N âˆ’ det(Dâˆ’1 + U â€  QU )

(5.66)

where



I2,0 I2,1
I1,2 âˆ’ |z|âˆ’2 (zs)âˆ’1 âˆ’ I1,1
T (g, z) â‰¡ Tr
I2,1 I2,2
I1,0
(zÌ„s)âˆ’1 âˆ’ I1,1




1
1
= I2,2 I1,0 + I2,0 I1,2 âˆ’ 2 âˆ’ 2I2,1 I1,1 âˆ’
|z|
sRe z
and
âˆ’ det(D

âˆ’1

2
 

1
1 

+ U QU ) = I1,0
âˆ’ I1,2 + I1,1 âˆ’ 
|z|2
sz

2
2


g 2
1
= 2 I1,0
+ I1,1 âˆ’  .
(5.67)
|z|
sz
â€ 



33
We see that both T (g, z) and âˆ’det(Dâˆ’1 + U â€  QU )
are O(1) (to obtain their limits as N â†’ âˆ we
can set sâˆ’1 = O(N âˆ’1/2 ) equal to zero) and since
g2 2
2
âˆ’det(Dâˆ’1 + U â€  QU ) â‰¥ |z|
2 I1,0 and I1,0 > 0, we see that
for g > 0, the denominator in Eq. (5.66) is bounded away
from zero, and hence âˆ†(g, z) = O(N âˆ’1 ) and can be safely
ignored for g > 0.
We will thus use tr (g 2 + Mz Mzâ€  )âˆ’1 = tr Q+o(1). From
Eq. (5.59) we obtain tr Q = I1,0 (g, z) and hence from
Eqs. (2.19),


1
K(g, z) = lim tr Q =
(5.68)
N â†’âˆ
g 2 + Ïƒcâˆ’2 r2 c
where r â‰¡ |z|.
Note that the approximation
tr (g 2 + Mz Mzâ€  )âˆ’1 = tr Q is equivalent to using Mz Mzâ€  =
|z|2 (RL)âˆ’2 instead of the full expression Eq. (5.57) and
hence to setting M = 0. Accordingly, the support of the
eigenvalue distribution is given by Eq. (2.13), or equivalently by Eq. (2.60), and within this support, g 2 is depends only on |z| = r and is found by solving Eq. (2.15),
or equivalently Eq. (2.62). Similar considerations show
that in using Eq. (2.8) to obtain limN â†’âˆ Ï(z) we can set
M = 0, yielding an isotropic eigenvalue density. From
Eqs. (2.14)â€“(2.16), the proportion, n> , of eigenvalues lying a distance larger than r is equal to g 2 (r), which is
found by solving Eq. (2.62). The results Eqs. (2.17)â€“
(2.18) also hold, wherein the normalized sums over i can
be replaced with appropriate averages hÂ·ic .
Let us now go back to the expression for âˆ†(g, z), and
consider the case g = 0. In this case
In,k (0, z) = |z|âˆ’2n hÏƒc2nâˆ’k ic ,

(5.69)

and we obtain

T (g, z) = |z|âˆ’6 hÏƒc2 i2c âˆ’ 2hÏƒc3 ic hÏƒc ic + 2hÏƒc3 ic

sâˆ’1
|z|4 Re z
(5.70)

and


2

sâˆ’1 
âˆ’ det(Dâˆ’1 + U â€  QU ) = |z|âˆ’2 hÏƒc ic âˆ’
.
z 

(5.71)

In the special case in which hÏƒc ic = 0 (this corresponds
to the special case of the example Eq. (2.42) with f ÂµE âˆ’
(1 âˆ’ f )ÂµI âˆ u Â· v = 0, which we considered above), the
determinant will have a vanishing limit as N â†’ âˆ (or
sâˆ’1 â†’ 0). This leads to a finite limit for âˆ†(0, z) and we
obtain
Î¾ 2 hÏƒc2 i2
s2 hÏƒc2 i2
=
,
(hÏƒc ic = 0). (5.72)
âˆ†(0, z) =
4
N |z|
|z|4
Adding this to tr Q in the right side of Eq. (5.68), and
using the naive formula Eq. (2.5) or K(0, z) = 1 for the
spectral boundary, we would have obtained the equation

 2
Ïƒc c
Î¾ 2 hÏƒc2 i2
+
.
(5.73)
1=
r2
r4
This in turn yields the radius Eq. (2.61) which is larger
than the true boundary of the support of limN â†’âˆ Ï(z)
given by Eq. (2.60).

VI.

CONCLUSIONS

We have provided a general formula for the eigenvalue
density of partly random matrices, i.e. matrices with
general mean and non-trivial covariance structure. General formulae have also been derived for the magnitude
of impulse response and frequency power spectrum in an
N -dimensional linear dynamical system with a coupling
given by such partly random matrices. Our theory makes
no requirement on the normality of matrices; its applications include therefore the stability and linear response
analysis of neural circuits, whose linearized dynamics is
always nonnormal. We have demonstrated our theory
by tackling analytically two specific neural circuits: a
feedforward chain of length N , and a set of randomly
coupled feedforward subchains of length 2. A connection
has also been revealed between the eigenvalue spectra of
dense random matrix perturbations, and the theory of
pseudospectra.
The non-crossing diagrammatic method can be used to
calculate other quantities of interest for matrix ensembles
of the form A = M + LJR, considered here as well; possible examples are direct statistics of eigenvectors [52], or
the correlations of the random fluctuations of the eigenvalue density hÎ´ÏJ (z)Î´ÏJ (z + w)iJ for macroscopic w (i.e.
for |w| = Î˜(1)). On the other hand, quantities such as
the microscopic structure of hÎ´ÏJ (z)Î´ÏJ (z + w)iJ , e.g. for
|w| = Î˜(N âˆ’1/2 ) with z inside the support, which could
be of interest in the study of eigenvalue repulsion are
not accessible to the non-crossing approximation. This
is also the case, in general, for the statistics of the â€œoutlierâ€ eigenvalues that we discussed after Eq. (2.12) and in
the examples of Sec. II C 1 and Sec. II C 2, which may be
of importance in practical applications. The calculation
of such quantities is possible, for example, by using the
replica technique (see e.g. Ref. [53]).
Finally, there are important forms of disorder which
are not covered by the general ensemble A = M + LJR
with iid, and hence dense, J. Examples of relevance
to neuroscientific applications include sparse A [54â€“56]
(note that, e.g., binary matrices with probability of a
nonzero weight, p, which is small but Î˜(1) as N â†’ âˆ are
covered by our formulae; by â€œsparseâ€ disorder we refer,
e.g. to the case p = o(1)), or more general structure of
correlations between the elements of A (in the ensemble
considered in this article, and for real J, the covariance
hÎ´Aij Î´Ai0 j 0 iJ = (LLT )ii0 (RT R)jj 0 is single rank); the latter is of importance in considering networks with local
topologies where, e.g., the matrix A has a banded structure. Generalization to other forms of random disorder
is thus an important direction for future research.

Acknowledgments

We thank Larry Abbott and Merav Stern for helpful discussions. Y.A. was supported by the Kavli Institute for Brain Science Postdoctoral Fellowship and

34
by the Swartz Program in Computational Neuroscience
at Columbia University, supported by a gift from the
Swartz Foundation. K.D.M. was supported by grant
R01-EY11001 from the NIH and by the Gatsby Charitable Foundation through the Gatsby Initiative in Brain
Circuitry at Columbia University.

(a)

!
"
#

Ïƒ + (RL)âˆ’1
Appendix A: Validity of the non-crossing
approximation

In this appendix we will give the justification for the
non-crossing approximation used in Sec. (III) and (IV).
That is, we will show that the only diagrams not suppressed by inverse powers of N are the non-crossing diagrams. We will limit our discussion to the case of
the eigenvalue density considered in Sec. (III), but
the generalization to the quantities calculated in (IV)
is straightforward. As explained after Eq. (3.22), averaging of G(Î·, z; J) over the disorder J involves summing over all complete pairings of the factors of J in
every term of the expansion Eq. (3.22), with each pairing of each term represented by a diagram as shown in
Fig. 11. Each such diagram is composed of a solid directed line (each segment of which represents a factor of
GÎ±Î²
ab (Î·, z; 0)), with a number of wavy lines (each representing the expression Eq. (3.21), with different indices)
connecting different points on the solid arrow line, and
all the internal matrix indices summed over. For the
purpose of calculating the eigenvalue density, according
to Eqs. (3.16)â€“(3.18), what we need to calculate is actually tr Ïƒ + (RL)âˆ’1 hG(Î·, z; J)iJ ; thus we can imagine the
solid arrow making a loop by closing-in on itself sandwiching Ïƒ + âŠ— (RL)âˆ’1 (see Fig. 15).
Given the structure of the Kronecker deltas in
Eq. (3.21), it is more convenient for our purpose here,
however, to think of each diagram as a number of â€œorbits,â€ each formed by starting somewhere on the solid line
and moving on it always along its arrow until the next
wavy line is encountered, whereby we leave the solid line,
continuing on the wavy line without crossing it (because
Eq. (3.21) is composed of two Kronecker deltas, one for
each side of the wavy line, enforcing index identification
at the corresponding ends on each side [64]) and return
somewhere else on the solid line, continuing as before
until we reach the initial point (see Fig. 15). As we go
around this orbit, for each solid line traversed we write
down, from right to left, a G(Î·, z; 0) and for each wavy
line a Ï€ ri (see Eqs. (3.20)) where i is the index of the
wavy line. Because all matrix indices are summed over,
such adjacent factors multiply like matrices, and since
the orbit forms a loop, in the end we obtain the trace
of the matrix product thus obtained. (This recipe for
assigning the contribution of each orbit accounts for the
Kronecker deltas and Ï€ r â€™s in Eq. (3.21), but not for the
factor N1 and the sum over râ€™s; we will account for these,
at the end, after Eq. (A2).) A generic orbit, which we
refer to as internal, closes on itself after traversing, say,

(b)

!

Ïƒ + (RL)âˆ’1
FIG. 15: (Color online) The orbits (shown by thin red
paths) for two diagrams for the spectral density in a
complex J ensemble. The non-crossing diagram on top
has three orbits: orbit (1) is the external orbit connecting the two ends of the Greenâ€™s function, while orbits
(2) and (3) are the internal orbits.
As in Eqs. (A1)

and (A2), they contribute tr Ïƒ + G(Î·, z; 0)Ï€ r1 G(Î·, z; 0) ,


Tr Ï€ 3âˆ’r1 G(Î·, z; 0)Ï€ r2 G(Î·, z; 0) and Tr Ï€ 3âˆ’r2 G(Î·, z; 0) ,
respectively, with r1 and r2 summed over 1 and 2 (cf.
Eq. (3.21)). The trace contributed by each of the three orbits
is O(N ), which when combined with the three factors of
1/N accounting for the two wavy lines and the normalization
of the external orbitâ€™s trace, yield an O(1) expression for
this diagram. By contrast, the crossing diagram on the
right has no internal orbits. Its only external orbit contributes
Tr Ïƒ + G(Î·, z; 0)Ï€ r2 G(Î·, z; 0)Ï€ 3âˆ’r1 G(Î·, z; 0)Ï€ 3âˆ’r2
r1
G(Î·, z; 0)Ï€ G(Î·, z; 0)) which after normalization is O(1).
Accounting for two factors of 1/N coming from the wavy
lines, we then see that this crossing diagram is O(N âˆ’2 ) and
hence is suppressed as N â†’ âˆ.

m wavy lines sanwiching m Greenâ€™s functions (e.g. the
orbits labeled 2 and 3 in panel (a) of Fig. 15), and thus
contributes a trace of the form
Im,r â‰¡ Tr(G(Î·, z; 0)Ï€ ri1 Â· Â· Â· G(Î·, z; 0)Ï€ rim ) ,

(A1)

where r is short-hand for {ri1 , . . . , rim }, and ik are the
indices of the wavy lines traversed in the orbit. In every
diagram, there is also exactly one orbit (e.g. the orbits
labeled 1 in both panels of Fig. 15) which in addition
includes the factor Ïƒ + (RL)âˆ’1 sandwhiched between the
two external Greenâ€™s functions. This orbit, which we call
the external orbit, contributes a trace of the form

En,rÌƒ â‰¡ Tr Ïƒ + (RL)âˆ’1 G(Î·, z; 0)Ï€ rj1 Â· Â· Â· Ï€ rjn G(Î·, z; 0)
(A2)
where n is the number wavy lines the orbit traverses and
rÌƒ is short for {rj1 , . . . , rjn }, and jk are the indices of the
wavy lines traversed in this orbit (in writing Eq. (A2) we
dropped the N1 that normalizes the trace in Eqs. (3.16),
but we will account for it below). For succinctness, in

35
Eqs. (A1)â€“(A2) we suppressed the arguments (Î·, z) for
Im,r and En,rÌƒ on which they depend. The full expression
for the diagram is obtained by multiplying all such trace
factors contributed by every orbit in the diagram, as well
as a factor of N âˆ’wâˆ’1 where w is the number of wavy lines
in the diagrams, to account for the N âˆ’1 in Eq. (3.21) for
each wavy line, as well as the extra N âˆ’1 which normalizes
the trace in the external orbit Eq. (A2) as dictated by
Eq. (3.16). The obtained expression is finally summed
over all the r-indices corresponding to each wavy line, as
required by Eq. (3.21).
The justification for the non-crossing approximation is
based on the claim that each trace contributed by a orbit (external or internal) as in Eqs. (A1)â€“(A2) is O(N ),
irrespective of Î·, z, m or r. We will provide justification
for this claim below. However, accepting it as true, we
see that any diagramâ€™s scaling with N solely depends on
the number of orbits and wavy lines it contains. A wellknown topological argument then shows that the contributions of crossing diagrams are suppressed by inverse
powers of N [57, 58]; for completeness we will summarize
this argument here. First note that, assuming the claim,
any diagram will yield an expression that is O(N Î± ) with
Î± = f âˆ’ w âˆ’ 1,

(A3)

where f is the number of orbits in the diagram (the sum
over at most 2w possible configurations of ri does not
contribute to the scaling with N ). Let V denote the total
number of vertices in the diagram (i.e. the number of
intersections of wavy lines and the solid line, plus an extra
one representing the insertion of Ïƒ + (RL)âˆ’1 in the solid
line loop) and let E denote its total number of edges, i.e.
E â‰¡ w + s, where s is the number of solid line segments
(s = 5 in both panels of Fig. 15). It is easy to see that
V = s. Thus we have E âˆ’ V = w. Formally defining the
number of â€œfacesâ€ in the diagram by F â‰¡ f + 1, and its
â€œEuler characteristicâ€ by
Ï‡ â‰¡ F âˆ’ E + V,

(A4)

we then find that Ï‡ = F âˆ’ (E âˆ’ V ) = f + 1 âˆ’ w. From
Eq. (A3) we then obtain
Î± = Ï‡ âˆ’ 2.

(A5)

Thus the contribution of a diagram is O(N Î± ), with Î±
determined solely by the diagramâ€™s formal â€œEuler characteristicâ€ via Eq. (A5). It can be shown that a diagram
with F formal â€œfacesâ€ and a formal â€œEuler characteristicâ€ Ï‡ as defined above, can be drawn on (embedded in)
a two-dimensional oriented surface with Euler characteristic Ï‡, such that no edges (solid or wavy) cross to create
new vertices, and each face created on the surface by its
partitioning by the drawn diagram, a) is topologically a
disk, and b) has a one-to-one correspondence with and is
encircled by an orbit in the diagram, where we now count
among the orbits, also the loop formed by the solid arrow
line. Thus the number of faces on the surface is indeed
F = f + 1, and the Ï‡, as defined above for the diagram,

indeed agrees with the Euler characteristic of the surface,
as conventionally defined. Topologically, such a surface
is a generalized torus with g holes, satisfying Ï‡ = 2 âˆ’ 2g;
the surface with zero holes is the sphere, or after decompactification, the plane (e.g. the diagram in panel (b) of
Fig. 15 can be drawn in this manner on a torus). We
thus see that
Î± = âˆ’2g,

(A6)

and therefore the only diagrams that are not suppressed
by inverse powers of N are those that can be drawn, as
described above, on the plane. Since we took the area
enclosed by the solid arrow line loop as a face by itself,
this means that the diagram should be drawable with
the wavy lines remaining outside this area (in order not
to partition it into several faces) without crossing each
other; this is the precise definition of the diagram being
non-crossing [65].
Let us now go back to justifying the claim that the
traces contributed by the orbits as in Eqs. (A1)â€“(A2) are
O(N ). For this purpose we will make use of the singular value decomposition of Mz introduced in Eq. (3.32).
Defining the unitary matrix


Uz 0
Uz â‰¡
,
(A7)
0 Vz
and using Eq. (3.32), we can write H0 (z), defined in
Eq. (3.14), as
H0 (z) = Uz HÌƒ0 (z) Uzâ€  ,

(A8)

where
HÌƒ0 (z) â‰¡




0 Sz
.
Sz 0

(A9)

Let us also define GÌƒ(Î·, z; 0) â‰¡ Uzâ€  G(Î·, z; 0)Uz , such that
G(Î·, z; 0) = Uz GÌƒ(Î·, z; 0)Uzâ€  .

(A10)

Then using the definition G(Î·, z; 0) = (Î· âˆ’ H0 (z))âˆ’1 , we
see that
!
Î·
Sz
1
Î· 2 âˆ’Sz2 Î· 2 âˆ’Sz2
GÌƒ(Î·, z; 0) =
=
,
(A11)
Î·
Sz
Î· âˆ’ HÌƒ0 (z)
Î· 2 âˆ’Sz2 Î· 2 âˆ’Sz2
where we used Eq. (A9) to write the last equality. Given
the block-diagonal nature of Eq. (A7) and the definitons
Eq. (3.20), we also have
Ï€ r = Uz Ï€ r Uzâ€  .

(A12)

We now substitute G(Î·, z; 0) and Ï€ ri in Eqs. (A1)â€“(A2)
with the right hand sides of Eqs. (A10) and (A12), respectively. After canceling the Uz â€™s we obtain


Im,r = Tr GÌƒ(Î·, z; 0)Ï€ ri1 Â· Â· Â· GÌƒ(Î·, z; 0)Ï€ rim ,
(A13)


En,rÌƒ = Tr Ïƒ +A(z)GÌƒ(Î·, z; 0)Ï€ rj1 Â· Â· Â· Ï€ rjn GÌƒ(Î·, z; 0) (A14)

36
where we defined
A(z) â‰¡ Uzâ€  (RL)âˆ’1 Vz ,
(A15)


such that Uzâ€  Ïƒ + âŠ— (RL)âˆ’1 Uz = Ïƒ + âŠ— A(z) â‰¡ Ïƒ + A(z).
For the internal orbits, we see from Eq. (A11) that each
GÌƒ(Î·, z; 0), depending on whether it is sandwiched between the same projectors Ï€ r , or between two opposite
projectors, Ï€ r and Ï€ 3âˆ’r , contributes a diagonal factor
equal to Î·/(Î· 2 âˆ’ Sz2 ) or Sz /(Î· 2 âˆ’ Sz2 ), respectively. Thus,
for any configuration of ri â€™s, if the number of Greenâ€™s
functions sandwiched the second way is k (1 â‰¤ k â‰¤ m),
we obtain
Im,r (Î·, z) =

N
X
Î· mâˆ’k si (z)k
(Î· 2 âˆ’ si (z)2 )m
i=1

(1 â‰¤ k â‰¤ m)

(A16)
for the internal orbits (in particular, we see that the sole
dependence of Im,r (Î·, z) on r is via the number k). We
therefore have
 mâˆ’k

 Î·
si (z)k 

|Im,r (Î·, z)| â‰¤ N max  2
. (A17)
i
(Î· âˆ’ si (z)2 )m 

When the imaginary part of Î· is nonzero, the denominator in the right hand side of Eq. (A17) cannot vanish
for any value of si (z) (while as Î· â†’ i0, which is the
limit we have to take after summing up the relevant diagrammatic series, si (z) that approach zero as N grows
can make this expression unbounded as N â†’ âˆ). Assuming Im Î· > 0, it will be sufficient for our purposes to
substitute Eq. (A17) with the weaker bound
 mâˆ’k k 
 Î·
s 
,
(Im Î· > 0)
|Im,r (Î·, z)| â‰¤ N max  2
s
(Î· âˆ’ s2 )m 
(A18)
where now the maximum is taken for s ranging over the
whole [0, âˆ). Since Im Î· > 0 the expression has no singularities at finite real s, and since 2m > k, it cannot
diverge as s â†’ âˆ either; thus it has a finite maximum
independent
precisely, it is easy to show that
 mâˆ’k of
N . hMore
âˆš im
 Î·
sk 
2
maxs  (Î·2 âˆ’s2 )m  â‰¤ |Im Î·| , irrespective of k as long as
1 â‰¤ k â‰¤ m, yielding
" âˆš #m
2
|Im,r (Î·, z)| â‰¤ N
,
(Im Î· > 0). (A19)
|Im Î·|

Similarly, the trace for the external orbit can be written in the new basis Eq. (A11) as
En,rÌƒ (Î·, z) =

N
X
i=1

Aii (z)

Î· nâˆ’kÌƒ si (z)kÌƒ
,
(Î· 2 âˆ’ si (z)2 )n

(1 â‰¤ kÌƒ â‰¤ n),

(A20)
where kÌƒ is the number of Greenâ€™s functions in Eq. (A2)
sandwiched between two Ï€ r â€™s with different superscripts;
this convention works correctly for the external orbit as
well, if we account for the presence of Ïƒ + by imagining

a Ï€ 2 (Ï€ 1 ) to the left (right) of the leftmost (rightmost)
Greenâ€™s function. From Eq. (A15), we can write Aii (z) =
ui (z)â€  (RL)âˆ’1 vi (z), where we defined the vectors ui (z)
and vi (z) to be the i-th column of Uz and Vz , respectively.
By the Cauchy-Schwartz inequality we then have
|Aii (z)| â‰¤ kui (z)kk(RL)âˆ’1 vi (z)k
â‰¤ kui (z)kkvi (z)kk(RL)âˆ’1 k,

(A21)

where k(RL)âˆ’1 k is the operator norm, or the maximum
singular value, of (RL)âˆ’1 . But since Uz and Vz are unitary matrices, ui (z) and vi (z) are unit vectors, and we
obtain
|Aii (z)| â‰¤ k(RL)âˆ’1 k.

(A22)

Going back to Eq. (A20), this yields the bound


 Î· nâˆ’kÌƒ s (z)kÌƒ 


i
âˆ’1
|En,rÌƒ (Î·, z)| â‰¤ N k(RL) k max  2
 . (A23)
i  (Î· âˆ’ si (z)2 )n 

The only difference with the inequality for Im,r is the
factor k(RL)âˆ’1 k. Repeating the same argument as for
the internal traces, we therefore see that
" âˆš #n
2
|En,rÌƒ (Î·, z)| â‰¤ N
k(RL)âˆ’1 k,
(Im Î· > 0),
|Im Î·|
(A24)
and thus a sufficient condition for En,rÌƒ to be O(N ) for
Im Î· > 0, is that k(RL)âˆ’1 k remains bounded as N â†’ âˆ,
i.e.
k(RL)âˆ’1 k = O(1).

(A25)

Combining Eqs. (A19) and (A24), and given the prescription after Eq. (A2), we can bound the absolute
value of the contribution of a diagram with genus g
(or h g crossings),
w wavy lines, and s solid lines, by
i
âˆš

s

2w |Im2Î·| k(RL)âˆ’1 kN âˆ’2g (the power of s is obtained
by noting that the powers of m and n in the bounds
Eqs. (A19) and (A24), when summed over all orbits must
equal s, since every Greenâ€™s function or solid line appears in exactly one orbit). Hence for a fixed, nonzero
Im Î·, the contribution of crossing diagrams (i.e. those
with g â‰¥ 1) goes to zero as N â†’ âˆ. Thus if we
take the limit N â†’ âˆ before the limit Î· â†’ i0+ , ignoring the crossing diagrams is safe, and the expression
for Ï(z) obtained from Eq. (3.16)
after analytic contin
uation of tr Ïƒ + (RL)âˆ’1 G(Î·, z) to Î· = i0, with G(Î·, z)
given by the contribtiuon of non-crossing diagrams to
hG(Î·, z; J)iJ , gives the correct result for limN â†’âˆ Ï(z).
We mention that when the smallest singular value si (z)
remains bounded away from zero as N â†’ âˆ, even at
Î· = 0 the traces Eqs. (A16) and (A20) are O(N ), as is
not hard to check, justifying the non-crossing approximation at Î· = 0. Thus it is only when some si (z) are
o(1) that it becomes important to send Î· to i0+ only after the limit N â†’ âˆ has been taken. In particular, in

37
such cases, applying the limit Î· â†’ i0+ to the results obtained using the non-crossing approximation before taking the limit N â†’ âˆ, may yield finite-size contributions
to limN â†’âˆ Ï(z), which in general may yield incorrect
subleading corrections.

where we defined
>

TN (Î³) â‰¡ tr
VN (Î³) â‰¡ tr

Appendix B: Ï(z) vanishes in the region Eq. (3.38)

In this appendix we prove more rigorously that
in the region Eq. (3.38), the eigenvalue density vanishes.
More precisely, we prove that Ï(z) â‰¡
limâ†’0+ limN â†’âˆ ÏN (z, ) = 0, where


1 âˆ‚
(RL)âˆ’1 Mzâ€ 
ÏN (z, ) â‰¡
tr
,
(B1)
Ï€ âˆ‚ zÌ„
Mz Mzâ€  + Î³ 2
is obtained by substituting Eq. (3.35) into Eqs. (3.17).
Here, Î³ = g(z, ) + , is the solution of Eq. (3.37), which
as we argued in Sec. III, vanishes as  â†’ 0+ when z is in
the region Eq. (3.38) (note that since Eq. (3.37) is defined
in the limit N â†’ âˆ, Î³ has no dependence on N ). Recall
that for  > 0, g(z, ) is positive and therefore Î³ >  > 0.
Expanding the derivative in Eq. (B1) we obtain


(RL)âˆ’1 (RL)âˆ’â€ 
Ï€ÏN (z, ) = tr
Mz Mzâ€  + Î³ 2


1
1
âˆ’â€ 
âˆ’1
â€ 
Mz (RL)
âˆ’tr (RL) Mz
Mz Mzâ€  + Î³ 2
Mz Mzâ€  + Î³ 2


(RL)âˆ’1 Mzâ€ 
1
âˆ’tr
âˆ‚zÌ„ (Î³ 2 )
Mz Mzâ€  + Î³ 2 Mz Mzâ€  + Î³ 2


(RL)âˆ’1 Q(RL)âˆ’â€ 
= tr
Mz Mzâ€  + Î³ 2


(RL)âˆ’1 Mzâ€ 
1
âˆ’tr
âˆ‚zÌ„ (Î³ 2 ),
(B2)
Mz Mzâ€  + Î³ 2 Mz Mzâ€  + Î³ 2
where we defined Q = 1 âˆ’ Mzâ€  M M1â€  +Î³ 2 Mz (we suppress
z
z
the explicit dependence of Î³ on z for simplicity). By the
2
Woodbury matrix identity Q = M â€  MÎ³ +Î³ 2 , which upon
z
z
substitution in Eq. (B2) yields


(RL)âˆ’1
(RL)âˆ’â€ 
Ï€ÏN (z, ) = tr
Î³2
(B3)
Mzâ€  Mz + Î³ 2 Mz Mzâ€  + Î³ 2


(RL)âˆ’1 Mzâ€ 
1
âˆ’tr
âˆ‚zÌ„ (Î³ 2 ).
â€ 
â€ 
2
2
Mz Mz + Î³ Mz Mz + Î³
Differentiating Eq. (3.37) with respect to zÌ„ yields
âˆ’ âˆ‚zÌ„ (Î³ 2 ) =

âˆ’2Î³ 2 âˆ‚zÌ„ K
,
1 âˆ’ K âˆ’ 2Î³ 2 âˆ‚Î³ 2 K

(B4)

with the partial derivatives of K(Î³, z) given by
>

>

âˆ’ âˆ‚Î³ 2 K = Tâˆ (Î³) â‰¡ lim TN (Î³)
N â†’âˆ

âˆ’âˆ‚zÌ„ K = Vâˆ (Î³) â‰¡ lim VN (Î³),
N â†’âˆ

(B5)

"


1
(Mz Mzâ€  + Î³ 2 )2

#

(B6)

Mz (RL)âˆ’â€ 

1



.

(B7)

2 Î³ 2 Vâˆ (Î³) VN (Î³)âˆ—
>
1 âˆ’ K(Î³) + 2Î³ 2 Tâˆ
(Î³)

(B8)

Mz Mzâ€  + Î³ 2 Mz Mzâ€  + Î³ 2

We thus obtain
Ï€ÏN (z, ) = Î³ 2 TN (Î³) +
where we defined
TN (Î³) â‰¡ tr



(RL)âˆ’1

(RL)âˆ’â€ 

Mzâ€  Mz + Î³ 2 Mz Mzâ€  + Î³ 2



.

(B9)

Having eliminated derivatives of Î³, we now simply need
to show that limÎ³â†’0+ limN â†’âˆ of the right side of
Eq. (B8) vanishes for z is in the region Eq. (3.38) (where
Î³ = 0+ is the solution of Eq. (3.37) as  â†’ 0+ ).
We will start by bounding the traces TN (Î³) and VN (Î³)
in Eq. (B8). For VN (Î³) we use the singular value decomposition Eq. (3.32):
 



(RL)âˆ’1 Mzâ€ 
1

 (B10)
|VN (Î³)| = tr

â€ 
â€ 
2
2
Mz Mz + Î³ Mz Mz + Î³


Sz
= tr Uzâ€  (RL)âˆ’1 Vz 2
(Sz + Î³ 2 )2


Sz
â‰¤ k(RL)âˆ’1 ktr
(Sz2 + Î³ 2 )2
(where in the last line we used Eq. (A22)), i.e.
>

|VN (Î³)| â‰¤ k(RL)âˆ’1 kVN (Î³)

(B11)

where we defined
>

VN (Î³) â‰¡

N
1 X
si (z)
.
N i=0 (si (z)2 + Î³ 2 )2

(B12)

Taking the limit N â†’ âˆ, we obtain from Eq. (B11)
>

>

|Vâˆ (Î³)| â‰¤ CVâˆ (Î³)

(B13)

>

where Vâˆ (Î³) â‰¡ limN â†’âˆ VN (Î³), and C is an upper bound
on k(LR)âˆ’1 k (which we have assumed is O(1) as N â†’
âˆ). To bound TN (Î³), we use the inequality
1

1

|tr(ABCD)| â‰¤ kAkkCktr (BB â€  ) 2 tr (DDâ€  ) 2 .

(B14)

This can be derived by first using the Cauchy-Schwartz
inequality, |tr (AB)|2 â‰¤ tr (AAâ€  )tr (BB â€  ), and then using
the inequality |tr (AB)| â‰¤ kBktr (A), valid for positive
semi-definite A (which in turn follows from the definition
of kBk after unitary diagonalization of A). Using (B14)
we obtain
 



(RL)âˆ’1
(RL)âˆ’â€ 
tr
â‰¤


â€ 
â€ 
2
2
M z Mz + Î³ Mz M z + Î³
"
2 #
1
âˆ’1 2
â‰¤ k(RL) k tr
(B15)
Mz Mzâ€  + Î³ 2

38
or
>

|TN (Î³)| â‰¤ k(RL)âˆ’1 k2 TN (Î³).

(B16)

Using the inequalities, (B11), (B13) and (B16) in
Eq. (B8) we obtain
"
#
>
>
2 Î³ 2 Vâˆ (Î³) VN (Î³)
2
2 >
Ï€|ÏN (z, )| â‰¤ C Î³ TN (Î³) +
>
1 âˆ’ K(Î³) + 2Î³ 2 Tâˆ
(Î³)
(B17)
Taking the N â†’ âˆ limit (while keeping Î³ finite), and
defining Ï(z, ) â‰¡ limN â†’âˆ ÏN (z, ), we obtain
"
#
>
2 (Î³ Vâˆ (Î³))2
2
2 >
Ï€Ï(z, ) â‰¤ C Î³ Tâˆ (Î³) +
>
1 âˆ’ K(Î³) + 2Î³ 2 Tâˆ
(Î³)
(B18)
where we defined
>

N
1 X
1
2 + Î³ 2 )2
N â†’âˆ N
(s
(z)
i
i=0

Tâˆ (Î³) â‰¡ lim
>

N
1 X
si (z)
.
N â†’âˆ N
(si (z)2 + Î³ 2 )2
i=0

Vâˆ (Î³) â‰¡ lim

(B19)

as the limit of the density of the singular values of Mz
>
>
[66]. Note that contributions to Tâˆ (Î³) and Vâˆ (Î³) from
integration on [s0 , âˆ) for any fixed, nonzero s0 remain finite as Î³ â†’ 0+ ; only singular contributions arising from
>
the region s = O(Î³)  1 can contribute to Î³ 2 Tâˆ (Î³)
>
and Î³ Vâˆ (Î³) as Î³ â†’ 0+ . Thus we only need concern
ourselves with the portion of integrals from 0 to some arR s Ï (s;z)ds
bitrary small, but fixed s0 , and show that Î³ 2 0 0 (sS2 +Î³ 2 )2
R s sÏ (s;z)ds
and Î³ 0 0 (sS2 +Î³ 2 )2 vanish as Î³ â†’ 0+ . Let us first consider the situation similar to that in the two examples
Eqs. (5.1) and (2.42). For those examples, there is a
region of z outside Eq. (3.38), where a single (more generally O(1)) singular value si (z) vanishes as N â†’ âˆ,
while all the other si (z) remain bounded from below.
But an O(1) set of (vanishing) singular values does not
contribute to the density Eq. (B23) and since the other
si (z) are bounded from below, there is an s0 below which
ÏS (s; z) identically vanishes. So the claim is clearly true
for such cases. More generally, we exploit the fact that z
is in the region Eq. (3.38), so that

(B20)

Thus, to show that limâ†’0+ Ï(z, ) = 0, it suffices to show
>
>
that Î³ 2 Tâˆ (Î³) and Î³ Vâˆ (Î³) vanish as Î³ â†’ 0+ (since z is in
the region Eq. (3.38), 1âˆ’K(Î³) and hence the denominator
in the last term in Eq. (B18) remains positive as Î³ â†’ 0+ ).
Let us rewrite Eq. (B19) as
Z âˆ
>
ÏS (s; z)ds
Tâˆ (Î³) =
(B21)
2 + Î³ 2 )2
(s
Z0 âˆ
>
sÏS (s; z)ds
Vâˆ (Î³) =
(B22)
(s2 + Î³ 2 )2
0
where we defined

lim+

Î³â†’0

Z

0

âˆ

ÏS (s; z)ds
< 1.
s2 + Î³ 2

(B24)

We conclude that as s â†’ 0+ the density, ÏS (s; z), must
vanish at least as fast as sÎ± , i.e. it must be O(sÎ± ), for
some Î± > 1; otherwise the integral in Eq. (B24) diverges
in the limit. Let us therefore choose s0 to be small enough
such that for s â‰¤ s0 , Ï(s; z) < csÎ± for some constant c
and Î± > 1. It is then an elementary exercise to show that
R s Î±+1
Rs
Î±
min(2,Î±âˆ’1)
) and
Î³ 2 0 0 (s2s+Î³ds2 )2 and Î³ 0 0 (ss2 +Î³ds
2 )2 are O(Î³

(B23)

O(Î³ min(1,Î±âˆ’1) ), respectively, as Î³ â†’ 0+ , and since Î± > 1,
they both vanish in the limit, proving the claim.

[1] M. L. Mehta, Random Matrices, Academic Press (2004).
[2] Z. Bai and J. W. Silverstein, Spectral Analysis of Large
Dimensional Random Matrices, Science Press (2006).
[3] T. Guhr, A. MuÌˆller-Groeling, and H. A. WeidenmuÌˆller,
Phys. Rep. 299, 189 (1998).
[4] H. Dale, Proc. R. Soc. Med. 28, 319 (1935).
[5] J. C. Eccles, P. Fatt, and K. Koketsu, J. Physiol. 126,
524 (1954).
[6] P. Strata and R. Harvey, Brain. Res. Bull. 50, 349 (1999).
[7] B. K. Murphy and K. D. Miller, Neuron 61, 635 (2009).
[8] H. Jeong, B. Tombor, R. Albert, Z. N. Oltvai, and A. L.
Barabasi, Nature 407, 651 (2000).
[9] A. L. Barabasi and Z. N. Oltvai, Nat. Rev. Genet. 5, 101
(2004).
[10] X. Zhu, M. Gerstein, and M. Snyder, Genes Dev. 21,
1010 (2007).

[11] M. Vidal, M. E. Cusick, and A.-L. BarabaÌsi, Cell 144,
986 (2011).
[12] R. M. May, Nature 238, 413 (1972).
[13] J. Camacho, R. GuimeraÌ€, and L. A. Nunes Amaral, Phys.
Rev. Lett. 88, 228102 (2002).
[14] F. S. Valdovinos, R. Ramos-Jiliberto, L. Garay-NarvaÌez,
P. Urbani, and J. A. Dunne, Ecology Letters 13, 1546
(2010).
[15] J. E. Vermaat, J. A. Dunne, and A. J. Gilbert, Ecology
90, 278 (2009).
[16] R. GuimeraÌ€, D. B. Stouffer, M. Sales-Pardo, E. A. Leicht,
M. E. J. Newman, and L. A. N. Amaral, Ecology 91, 2941
(2010).
[17] L. N. Trefethen and M. Embree, Spectra and Pseudospectra, Princeton University Press (2005).
[18] S. Ganguli, D. Huh, and H. Sompolinsky, Proceedings of

N
1 X
ÏS (s; z) = lim
Î´(s âˆ’ si (z))
N â†’âˆ N
i=0

39
the National Academy of Sciences 105, 18970 (2008).
[19] M. S. Goldman, Neuron 61, 621 (2009).
[20] M. G. Neubert and H. Caswell, Ecology 78, 653 (1997).
[21] X. Chen and J. E. Cohen, Proc. Biol. Sci. 268, 869
(2001).
[22] S. Tang and S. Allesina, Front. Ecol. Evol. 2 (2014).
[23] J. H. McCoy, New Journal of Physics 15, 113036 (2013).
[24] J. Feinberg and A. Zee, Nuclear Physics B 504, 579
(1997).
[25] J. Feinberg and A. Zee, Nuclear Physics B 501, 643
(1997).
[26] J. Ginibre, J. Math. Phys. 6, 440 (1965).
[27] V. L. Girko, Theory Probab. Appl. 29, 694 (1984).
[28] Z. D. Bai, Ann. Probab. 25, 494 (1997).
[29] T. Tao and V. Vu, Commun. Contemp. Math. 10, 261
(2008).
[30] F. GoÌˆtze and A. Tikhomirov, Ann. Probab. 38, 1444
(2010).
[31] B. Khoruzhenko, Journal of Physics A: Mathematical
and General 29, L165 (1996).
[32] P. Biane and F. Lehner, Colloq. Math. 90, 181 (2001).
[33] S. Hikami and R. Pnini, Journal of Physics A: Mathematical and General 31, L587 (1998).
[34] T. Tao, V. Vu, and M. Krishnapur, Ann. Probab. 38,
2023 (2010).
[35] T. Tao and V. Vu, Acta Mathematica 206, 127 (2011).
[36] T. Tao, Probab. Theory Related Fields 155, 231 (2013).
[37] S. Oâ€™Rourke and D. Renfrew, Electron. J. Probab. 19, 1
(2014).
[38] S. F. Edwards and R. C. Jones, Journal of Physics A:
Mathematical and General 9, 1595 (1976).
[39] K. Rajan and L. F. Abbott, Phys. Rev. Lett. 97, 188104
(2006).
[40] Y. Yin, Z. Bai, and P. Krishnaiah, Probab. Theory Related Fields 78, 509 (1988).
[41] R. A. Horn and R. G. Johnson, Matrix Analysis, Cambridge University Press (1990).
[42] P. Jonas, J. Bischofberger, and J. Sandkuhler, Science
281, 419 (1998).
[43] D. H. Root, C. A. Mejias-Aponte, S. Zhang, H.-L. Wang,
A. F. Hoffman, C. R. Lupica, and M. Morales, Nat Neurosci 17, 1543 (2014).
[44] D. ChafaÄ±Ìˆ, Journal of Theoretical Probability 23, 945
(2010).
[45] Y. Wei, Phys. Rev. E 85, 066116 (2012).
[46] K. D. Miller and F. Fumarola, Neural Comput. 24, 25
(2012).
[47] J. Hofbauer and K. Sigmund, Evolutionary Games
and Population Dynamics (Cambridge University Press,
Cambridge, England, 1998).
[48] M. Stern, H. Sompolinsky, and L. F. Abbott,
arXiv:1409.2535v1 (2014).
[49] H. Sompolinsky, A. Crisanti, and H. J. Sommers, Phys.
Rev. Lett. 61, 259 (1988).
[50] J. Feinberg, Journal of Physics A: Mathematical and
General 39, 10029 (2006).
[51] M. Abramowitz and I. A. Stegun, Handbook of mathematical functions with formulas, graphs, and mathematical tables, New York: Wiley-Interscience (1970).
[52] B. Mehlig and J. T. Chalker, J. Math. Phys. 41, 3233
(2000).
[53] S. M. Nishigaki and A. Kamenev, Journal of Physics A:
Mathematical and General 35, 4571 (2002).
[54] T. Rogers and I. P. Castillo, Phys. Rev. E 79, 012101

(2009).
[55] F. Slanina, Phys. Rev. E 83, 011118 (2011).
[56] I. Neri and F. L. Metz, Phys. Rev. Lett. 109, 030602
(2012).
[57] G. t. Hooft, Nuclear Physics B 72, 461 (1974).
[58] E. BreÌzin, C. Itzykson, G. Parisi, and J. B. Zuber, Communications in Mathematical Physics 59, 35 (1978).
[59] Since we will present results for the limit of the eigenvalue
density, etc, as N â†’ âˆ, M , L, R and J must each
be more precisely understood as an infinite sequence of
matrices dependent on N .
[60] The designation â€œhighly nonnormalâ€ can be motivated,
when L and R are proportional to the identity matrix,
as follows. Let us denote the (operator norm based) pseudospectrum of M , i.e. the region of zâ€™s over which
k(z âˆ’ M )âˆ’1 k > âˆ’1 , by Î£ (M ). For fixed N , the true
spectrum of M , which we denote by Î£(M ), is the set of
points over which the smallest singular value of (z âˆ’ M )
is exactly zero and hence k(z âˆ’M )âˆ’1 k = âˆ. For finite N ,
limâ†’0+ Î£ (M ) = Î£(M ) for any M . However, for nonnormal M this approach could be much slower than in
the normal case (see our discussion in Sec. II A 2, and the
book [17] for a complete discussion of pseudospectra and
their relationship with nonnormality). Now suppose that,
as in the atypical cases under discussion, in a finite region
of the complex plane the smallest singular value of Mz is
nonzero for finite N , but vanishes in the limit N â†’ âˆ.
This means that the operator norm of (z âˆ’ M )âˆ’1 âˆ
Mzâˆ’1 is finite over such a region but goes to infinity as
N â†’ âˆ. Hence, if we define Î£âˆ
 (M ) â‰¡ limN â†’âˆ Î£ (M )
and Î£âˆ (M ) â‰¡ limN â†’âˆ Î£(M ), we see that in such
6= Î£âˆ (M ) (or equivalently,
cases limâ†’0+ Î£âˆ
 (M )
limâ†’0+ limN â†’âˆ Î£ (M ) 6= limN â†’âˆ limâ†’0+ Î£ (M )).
More generally but less precisely, this indicates that at
finite but large N , the -pseudospectra of such matrices
can cover a significantly broader region than the spectrum even for very small , indicating extreme nonnormality.
[61] This equivalence is true more generally for any matrix
norm derived from a general vector norm; see Ref. [17]
for a proof.
[62] The unitary invariance of these formulae is in turn a
consequence of the invariance of both the corresponding
quantities (Ï(z) and kx(t)k2 ), as well as the statistical
ensemble for J, Eq. (3.2), and hence that of LJR when
L âˆ R âˆ 1, under unitary transforms like Eq. (2.34).
[63] It is also common to write the firing rate equations in the
= âˆ’r(t)+f (W r(t)+Ir (t)). At least
different form, T dr(t)
dt
in the case where all neurons have equal time constants,
i.e. T âˆ 1, the two formulations are equivalent and are
related by the change of variable v = W r + Ir [46].
[64] This structure is a consequence of using a complex ensemble for J, for which the covariances hJab Jcd i vanishes.
For the real Gaussian ensemble, by contrast, the latter do
Î±Î² Î³Î´
nothvanish; in this case Eq. (3.21)
becomes

 hJab Jcd i = i
+
âˆ’
âˆ’
+
+
+
âˆ’
âˆ’
1
Î´ad Î´bc ÏƒÎ±Î² ÏƒÎ³Î´ + ÏƒÎ±Î² ÏƒÎ³Î´ + Î´ac Î´bd ÏƒÎ±Î² ÏƒÎ³Î´ + ÏƒÎ±Î²
ÏƒÎ³Î´
=
N
h
i
P2
3âˆ’r
3âˆ’r
r
r
1
r=1 (Ï€Î±Î´ Î´ad )(Ï€Î²Î³ Î´bc ) + (Ï€Î±Î³ Î´ac )(Ï€Î²Î´ Î´bd ) .
N
[65] Notice that this is a more restrictive property than planarity of the diagram; for example the graph in panel (b)
of Fig. 15 is planar, as one of the wavy lines can be drawn
inside the solid loop without crossing any other line, but
it is not non-crossing as defined here.

40
[66] More precisely, we only need to define the limit Eq. (B23)
in the sense of distributions, i.e. such that for any
regular test function, f (s2 ), bounded at infinity and
regular everywhere,
includingR at s2 â†’ 0+ , we have
PN
âˆ
2
f
(s
(z)
) = 0 f (s2 )ÏS (s; z)ds. We do
limN â†’âˆ N1
i
i=1
not assume any smooth form for ÏS (s; z); in particular, ÏS (s; z) may have delta function singularities when

an O(N ) singular values converge to the same value as
N â†’ âˆ. Also note that this assumption does not forbid
the possibility that some si (z) diverge as N â†’ âˆ; our
requirement that kM kF remain bounded automatically
guarantees that these will not be numerous enough to
contribute to ÏS (s; z) at infinity.

