arXiv:1005.0517v1 [physics.bio-ph] 4 May 2010

Noise and Neuronal Heterogeneity
Michael J. Barber ∗
Universidade da Madeira, Centro de Ciências Matemáticas, Campus Universitário
da Penteada, 9000-390 Funchal, Portugal, michael.barber@arcs.ac.at
Manfred L. Ristig
Institut für Theoretische Physik, Universität zu Köln, D-50937 Köln, Germany,
ristig@thp.uni-koeln.de]

Abstract
We consider signal transaction in a simple neuronal model featuring intrinsic noise.
The presence of noise limits the precision of neural responses and impacts the quality
of neural signal transduction. We assess the signal transduction quality in relation to
the level of noise, and show it to be maximized by a non-zero level of noise, analogous
to the stochastic resonance effect. The quality enhancement occurs for a finite range
of stimuli to a single neuron; we show how to construct networks of neurons that extend the range. The range increases more rapidly with network size when we make
use of heterogeneous populations of neurons with a variety of thresholds, rather than
homogeneous populations of neurons all with the same threshold. The limited precision of neural responses thus can have a direct effect on the optimal network structure,
with diverse functional properties of the constituent neurons supporting an economical
information processing strategy that reduces the metabolic costs of handling a broad
class of stimuli.

1 Introduction
Neural network models are often constructed of simple units. Typically, model neurons
have a particular threshold or bias, and saturate to a fixed value for either strong or weak
inputs. Some such models can in fact be derived by systematic approximations of more
detailed models such as the Hodgkin-Huxley model (Abbott and Kepler, 1990); many other
models are derived from alternative heuristic or phenomenological assumptions. Networks
of even the simplest models are well known to be capable of representing complex functions.
∗

Present address: ARC systems research GmbH, Donau-City-Straße 1, 1220 Vienna, Austria

1

2

M. J. Barber and M. L. Ristig

In this chapter, we investigate the degree to which the simple dynamics of an individual
unit limit the inputs that can be processed, and how these limitations can be overcome. To
do this, we consider the response of model neurons to a variety of stimuli. The precision
of the responses for biological neurons have been shown to be rather limited, typically in
the range of a few bits for each action potential or “spike” (Rieke et al., 1997). We mimic
this limited precision in the model neurons by including noise in the systems. We focus
on intrinsic neuronal noise which has identical statistics for each of the units in the neural
network.
Noise is usually viewed as limiting the sensitivity of a system, but nonlinear systems can react to noise in surprising ways. Perhaps the best known of these is the phenomenon known as stochastic resonance (SR), wherein an optimal response to weak or
subthreshold signals is observed when a non-zero level of noise is added to the system (Gammaitoni et al., 1998). For example, a noise-free, subthreshold neuronal input
can occasionally become suprathreshold when noise is added, allowing some character of the input signal to be detected. SR has been observed and investigated in many
systems, ranging from resonant cavities to neural networks to the onset of ice ages
(see, e.g., Bezrukov and Vodyanoy, 1997; Gailey et al., 1997; Goychuk and Hänggi, 2000;
Jung and Shuai, 2001; Moss and Pei, 1995; Schmid et al., 2001; Wenning and Obermayer,
2003; Wiesenfeld and Moss, 1995).
Collins et al. (1995b) showed, in a summing network of identical Fitzhugh-Nagumo
model neurons, that an emergent property of SR in multi-component systems is that the
enhancement of the response becomes independent of the power of the noise. This allows
networks of elements with finite precision to take advantage of SR for diverse inputs. To
build upon the findings of Collins et al., we consider networks of simpler model neurons, but
these are allowed to have different dynamics. In particular, we examine noisy McCullochPitts (McP) neurons (Hertz et al., 1991) with a distribution of thresholds. We construct
heterogeneous networks that perform better than a homogeneous network with the same
number of noisy McP neurons and similar network architecture.

2 Neural Network Model
To investigate the effect of noise on signal transduction in networks of thresholding units,
we consider a network of noisy McCulloch-Pitts (McP) neurons. The McP neuron is perhaps the simplest neural model, being only a simple thresholding unit. When the total
input to a neuron (signal plus noise) exceeds its threshold, the neuron activates, firing an
action potential or “spike.” Formally, the activation state ai of neuron i in response to some
stimulus S can be expressed as
ai (S) = u (S − S0 ) ,

(1)

where S0 is the neuron’s threshold and u is the Heaviside step function, defined as
u (x) =

(

1 x>0
0 otherwise

.

(2)

Noise and Neuronal Heterogeneity

3

We model the limited precision of neurons by including noise as an intrinsic1 feature of the
neurons, so that eq. (1) becomes
ai (S) = u (S − S0 + η) ,

(3)

where η is zero-mean, i.i.d. (independent, identically distributed) Gaussian noise with variance σ 2 . We assume the noise distributions to be identical for all the McP neurons.
The network architecture is simple: an input layer of N noisy McP neurons is connected
to a single linear output neuron. Each synaptic weight is of identical and unitary strength, so
the output neuron calculates the sum of the N input neuron activation states as its response
RN (S):
RN (S) =

N
X

ai (S) .

(4)

i=1

Each input unit is presented the same analog signal, but with a different realization of the
intrinsic neuronal noise.
An important special case is when there is just a single input neuron (N = 1). Since
the output neuron is a summing unit, its response is just the response of the single input
neuron, i.e., RN (S) = a1 (S). In this chapter, we will use “single neuron” synonymously
with “network having only a single input neuron.”

3 Network Response
In this section, we consider the response RN of the network. Due to the specific choices
of neural model and network architecture made in section 2, the resulting neural networks
are quite tractable mathematically. A great deal of formal manipulation is thus possible,
including exact calculations of the expectation value and variance of the network response.
We initially focus on homogeneous networks with identical input neurons, including the
special case of a single input neuron. The stimuli can be chosen without loss of generality
so that S0 = 0. The results for S0 6= 0 can be recovered by a straightforward translation
along the S-axis.
The behavior for the standard, noise-free McP neurons is trivial, with all input neurons synchronously firing or remaining quiescent. However, considerably more interesting
behavior is possible for noisy McP neurons: subthreshold signals have some chance of
causing a neuron to fire, while suprathreshold signals have some chance of failing to cause
the neuron to fire.
For a network with N input neurons with the network architecture discussed above
section 2, the response RN of
the output neuron is just the number of input neurons that

fire. The probability p S; σ 2 of any neuron firing is


p S; σ

1

2



1
=p
2πσ 2

Z

∞

−S

x2
exp − 2
2σ

!

dx ,

(5)

Although we conceptually take the noise as intrinsic to the neuron, the model we use is formally equivalent
to a noise-free neuron subjected to a noisy stimulus.

4

M. J. Barber and M. L. Ristig

while the probability q S; σ 2 for the neuron to remain quiescent is




q S; σ 2



1
=p
2πσ 2

Z

−S

−∞

x2
exp − 2
2σ

!

dx .

(6)

Combining eqs. (5) and (6) gives q S; σ 2 + p S; σ 2 = 1, as expected.
Given eqs. (5) and (6), the expectation value hRi S; σ 2 and variance σZ2 (S; σ 2 ) when
N = 1 are




hRi S; σ 2








= p S; σ 2

(7)



= p S; σ 2 q S; σ 2



= N p S; σ 2

2
σR
S; σ 2





 



.

= N p S; σ 2 q S; σ 2



(8)

Since the noise is independent, the probability of different input neurons firing is also independent and the expected value and variance of the output neuron activation are seen to
be


hRN i S; σ 2


2
S; σ 2
σR
N

The dependence of hRi S; σ
is shown in fig. 1.


2





2 S; σ
and σR


2





 

(9)
.

(10)

on the stimulus S and the noise variance σ 2

4 Decoding the Network Output
In this section, we explore the ability of the neural circuit to serve as a signal transducer.
We identify limits on the signal transduction capability by decoding the state of the output
neuron to reproduce the input stimulus. Near the threshold value S0 , this gives rise to linear
decoding rules. The basic approach is similar to the “reverse reconstruction” using linear
filtering that has been applied with great effect to the analysis of a number of biological systems (see, e.g., Bialek and Rieke, 1992; Bialek et al., 1991; Prank et al., 2000; Rieke et al.,
1997; Theunissen et al., 1996).
We expand the expected output RN to first order near the threshold (i.e., S → 0), giving




hRN i S; σ 2 =

 
N
N
+p
S
+
O
S2 .
2
2πσ 2

(11)

An example of the linear approximation is shown in fig. 2.
Dropping the higher order terms and inverting eq. (11) gives a linear decoding rule of
the form


q
RN
1
−
,
(12)
ŜN = 2πσ 2
N
2

where ŜN is the estimate of the input stimulus. Combining eqs. (9) and (12), we can show
that
 
 1
 q
D E
2
2
2
.
(13)
ŜN S; σ = 2πσ p S; σ −
2

Noise and Neuronal Heterogeneity

5

The expected value of ŜN is thus
of N ; for notational simplicity, we
D Eseen to be independent
D E
drop the subscript and write Ŝ . Examples of Ŝ are shown in fig. 3 for several values
of the variance σ 2 . Note that, as the noise variance increases, the expected value of the
estimated stimulus closely matches the actual stimulus over a broader range.
We must also consider the uncertainty of the value decoded from the network response.
This leads to a total decoding error ∆ŜN with the form


2
∆ŜN
S; σ 2



=







σŜ2

N



S; σ 2







2 

= ε2 S; σ 2 + σŜ2

where
ε S; σ 2

ŜN − S

D E

Ŝ

=



=



N



S; σ 2

S; σ 2 − S
D E

ŜN − Ŝ



,

(14)

(15)

S; σ 2

2 

 

2πσ 2 
p S; σ 2 q S; σ 2 .
N

=

(16)

The expected difference ε S; σ 2 and the decoding variance σŜ2 S; σ 2 are shown in
N
figs. 4.




5 The Role of Noise
The noisy nature of the neurons has a striking and counter-intuitive effect on the properties of the activation state: increasing noise improves signal transmission, as seen in figs. 3
and 4(a). This effect is analogous to the stochastic resonance effect (Gammaitoni et al.,
1998). SR can be informally understood as the noise sometimes driving a nominally subthreshold signal to cross the threshold and producing a current. Signals close to the threshold will more frequently cross the threshold, giving a stronger response than signals far
from the threshold.
There are several properties of the activation probabilities that we can derive from
eqs. (5) and (6) and that we will find useful for understanding the role of noise in the
neural behavior. First, there is a scaling property with the form






p S; σ 2 = p αS; α2 σ 2



,

(17)

where α > 0. Second, there is a reflection property with the form






p S; σ 2 = q −S; σ 2



.

(18)

As the activation probabilities are at the core of essentially all the equations in this chapter,
the scaling and reflection properties will be broadly useful to us.

6

M. J. Barber and M. L. Ristig

The scaling and reflection properties of the activation probabilities can be used to derive
similar properties of the neural responses. The statistics of the neural responses obey the
relations








hRN i S; σ 2


hRN i S; σ 2
2
σR
S; σ 2
N



2
S; σ 2
σR
N







= 1 − hRN i −S; σ 2


= hRN i αS; α2 σ 2


2
= σR
−S; σ 2
N





2
αS; α2 σ 2
= σR
N







(19)
(20)
(21)

,

(22)

where α > 0. Important corollaries of these
relations are that hRN i S; σ 2 =

2
2
hRN i −1; (σ/V ) for all V <0, hRN i S; σ = hRN i 1; (σ/V )2 for all V > 0, and
2
2
σR
S; σ 2 = σR
1; (σ/V )2 for all V 6= 0. It is thus necessary to consider only one
N
N
subthreshold stimulus and one suprathreshold stimulus in order to understand the impact of
noise on the neural responses; see fig. 5.
Similarly, properties of the statistics for the estimated input Ŝ can be derived, giving
D E



= − Ŝ

S; σ 2

= α Ŝ

S; σ 2











Ŝ

D E

Ŝ

−S; σ 2

αS; α2 σ 2

ε −S; σ 2



ε αS; α2 σ 2
2
−S; σ 2
σR
N



2
σR
αS; α2 σ 2
N






D E

D E

= −ε S; σ 2


= αε S; σ 2






2
S; σ 2
= σR
N





(23)



(24)
(25)
(26)



(27)

2
= α2 σR
S; σ 2
N

D E





,

(28)
D E

where α > 0. Eqs. (23) through (26) imply that Ŝ S; σ 2 = S Ŝ 1; (σ/S)2 and


ε S; σ 2 = Sε 1; (σ/S)2 for all S 6= 0. Again, we can focus on one subthreshold
stimulus and one suprathreshold stimulus to understand the impact of noise (see fig. 6) for
the behavior in the two cases, and use straightforward transformation to obtain the exact
results for other stimuli.


Further, eq. (14) and eqs. (25) through (28) imply ε2 S; σ 2 = S 2 ε2 1; (σ/S)2 ,



2
2
2 S; σ 2 = S 2 ∆Ŝ 2 (1; (σ/S)) for all S 6= 0.
σR
S; σ 2 = S 2 σR
1; (σ/S)2 , and ∆ŜN
N
N
N
Thus, the noise dependence of these latter error sources can be understood with a single
2 1; (σ/S)2 has its minimum for a
stimulus; see fig. 7. Note that the total error ∆ŜN
nonzero value of the noise variance, analogous to the stochastic resonance effect; see fig. 7.




6 Networks of Heterogeneous Neurons
Thus far, we have focused on single neurons and networks of identical neurons.
D E The effect
of multiple neurons has generally been simple, either having no effect on— Ŝ , ε—or just

Noise and Neuronal Heterogeneity

7

2 , σ 2 —the single-channel values.
rescaling—hRN i, σR
N
Ŝ
N

2 . In fig. 8, we show how
A significant exception to this general trend is found in ∆ŜN
2
∆ŜN varies with N . The error curve flattens out into a broad range of similar values, so
that the presence of noise enhances signal transduction without requiring a precise relation
between S and σ 2 seen for smaller values of N . This effect is essentially the “stochastic
resonance without tuning” first reported by Collins et al. (1995a).
Informally stated, SR without tuning allows for a wider range of potentials to be accurately decoded from the channel states for any particular value of the noise variance. To
make this notion of “wider range” precise, we again focus our attention on the expected
response of the neurons (see fig. 2). The expected neural response hRN i saturates to zero
or one when S is far from the neuronal threshold. The width W of the intermediate range
can be defined, for example, by taking the boundaries of this range to be the points where
the first order approximation
p reaches the saturation values of zero and one. The width in
this case becomes W = 2πσ 2 .
Other definitions for the response width are, of course, possible, but we still should
observe that the width is proportional to σ, since the activation probability depends only on
the ratio of S and σ (eq. (5)). The same width is found for multiple identical input neurons,
because the output neuron response is proportional to the single neurons response, without
broadening the curve in fig. 2.
The response width can thus be increased by increasing the noise variance σ 2 . As seen
2.
in figs. 7 and 8, such an increase ultimately leads to a growth in the decoding error ∆ŜN
2
2
2
In the asymptotic limit as σ becomes large, ∆ŜN is dominated by σŜ and we have the
N
asymptotic behavior
!


σ2
2
,
(29)
∆ŜN
S; σ 2 = O
N
2 with increasing σ 2 thus can be overcome by further
based on eq. (16). The growth in ∆ŜN
increasing the number of neurons in the input layer. Therefore, the
√ response width W is
effectively constrained by the number of neurons N , with W = O( N ) for large N .
An arbitrary response width can be produced by assembling enough neurons. However, this approach is inefficient, and greater width increases can be achieved with the same
number of neurons. Consider instead dividing up the total width into M subranges. These
subranges can each be independently covered by a subpopulation of N neurons; all neurons
within a subpopulation are identical to one another, while neurons from different
subpop√
ulations differ only√in their thresholds. The width of each subrange is O( N ), but the
total width is O(M N ). Thus, the total response width can increase more rapidly as additional subpopulations of neurons are added. Conceptually, multiple thresholds are a way
to provide a wide range of accurate responses, with multiple neurons in each subpopulation
providing independence from any need to “tune” the noise variance to a particular value.
To describe the behavior of channels with different thresholds, much of the preceding
analysis can be directly applied by translating the functions along the potential axis to obtain
the desired threshold. However, system behavior was previously explored near the threshold

8

M. J. Barber and M. L. Ristig

value, but heterogeneous populations of neurons have multiple thresholds. Nonetheless, we
can produce a comparable system by simply assessing system behavior near the center of
the total response width.
To facilitate a clean comparison, we set the thresholds in the heterogeneous populations
so that a linear decoding rule can be readily produced. A simple approach that achieves this
is to space the thresholds of the subpopulations by 2W , with all neurons being otherwise
equal. The subpopulations with lower thresholds provide an upward shift in the expected
number of active neurons for higher threshold subpopulations, such that the different subpopulations are all approximated to first order by the same line. Thus, the expected total
number of active neurons leads to a linear decoding rule by expanding to first order and
inverting, as was done earlier for homogeneous populations. Note that this construction
requires no additional assumptions about how the neural responses are to be interpreted,
nor does it require alterations to the network architecture.
To illustrate the effect of multiple thresholds, we begin by investigating the response
of a homogeneous baseline to a stimulus S. The baseline network consists of M = 1
2
populations of N = 1000 neurons with
√ S0 = 0 and variance σ = 1. Using the definition
above, the response width is W = 2π. We then consider two cases, homogeneous and
heterogeneous, in each of which we increase the response width by doubling the number of
neurons while maintaining similar error expectations for the decoded stimuli.
In the homogeneous case, we have a single population (M = 1) with N = 2000
neurons. Doubling the number of neurons allows us to double the variance to σ 2 = 2 with
similar expected errors outside the response width. Thus, we observe an extended range,
relative to the baseline case, in which we can reconstruct the stimulus from the network
output (fig. 9).
In the heterogeneous case, we instead construct two subpopulations (M = 2) with
2
N = 1000 neurons. We leave the variance unchanged
p at σ = 1. One of the subpopulations
is modified so that the thresholdsp
lie at +W/2 = π/2, while the other is modified so that
the thresholds lie at −W/2 = − π/2. The resulting neural network has a broad range in
which we can reconstruct the stimulus from the network response, markedly greater than
the baseline and homogeneous cases (fig. 9).

7 Conclusion
We have constructed networks of heterogeneous McP neurons that outperform similar networks of homogeneous McP neurons. The network architectures are identical, with the only
difference being the distribution of neuronal thresholds. The heterogeneous networks are
sensitive to a wider range of signals than the homogeneous networks. Such networks are
easily implemented, and could serve as simple models of many diverse natural and artificial
systems.
The superior scaling properties of heterogeneous neuronal networks can have a profound metabolic impact; large numbers of neurons imply a large energetic investment, in
terms of both cellular maintenance and neural activity. The action potentials generated in

Noise and Neuronal Heterogeneity

9

neurons can require a significant energetic cost (Laughlin et al., 1998), making the tradeoff between reliably coding information and the metabolic costs potentially quite important.
Thus, we expect neuronal heterogeneity to be evolutionarily favored, even for quite simple
neural circuits.
Although we have used a specific model consisting of thresholding neurons with additive Gaussian noise, we expect that the key result is more widely applicable. The demonstration of the advantage of neuronal heterogeneity largely follows from two factors that
are not specific to the model neurons. First, the distance of the input stimulus from the
threshold is proportional to the standard deviation of the Gaussian noise, and, second, the
total variance of the network response is proportional to the number of input neurons. Ultimately, the heterogeneous thresholds are favorable because the independently distributed
noise provides a natural scale for the system.

Acknowledgments
We would like to acknowledge support from the Portuguese Fundação para a Ciência e a
Tecnologia under Bolsa de Investigação SFRH/BPD/9417/2002 and Plurianual CCM. The
writing of this chapter was supported in part by ARC systems research GmbH.

References
L. F. Abbott and T. Kepler. Model neurons: From Hodgkin-Huxley to Hopfield. In
L. Garrido, editor, Statistical Mechanics of Neural Networks, pages 5–18, Berlin, 1990.
Springer-Verlag.
S. M. Bezrukov and I. Vodyanoy. Stochastic resonance in non-dynamical systems without
response thresholds. Nature, 385(6614):319–21, 1997.
W. Bialek and F. Rieke. Reliability and information transmission in spiking neurons. Trends
Neurosci., 15(11):428–434, 1992.
W. Bialek, F. Rieke, R. R. de Ruyter van Steveninck, and D. Warland. Reading a neural
code. Science, 252(5014):1854–1857, 1991.
J. J. Collins, C. C. Chow, and T. T. Imhoff. Stochastic resonance without tuning. Nature,
376:236–238, 1995a.
J. J. Collins, C. C. Chow, and T. T. Imhoff. Aperiodic stochastic resonance in excitable
systems. Pys. Rev. E, 52(4):R3321–4, 1995b.
Paul C. Gailey, Alexander Neiman, James J. Collins, and Frank Moss. Stochastic resonance in ensembles of nondynamical elements:
The role of internal noise.
Physical Review Letters, 79(23):4701–4704, 1997.
URL
http://link.aps.org/abstract/PRL/v79/p4701.

10

M. J. Barber and M. L. Ristig

L. Gammaitoni, P. Hänggi, P. Jung, and F. Marchesoni. Stochastic resonance. Rev. Mod.
Phys., 70(1):223–87, 1998.
I. Goychuk and P. Hänggi. Stochastic resonance in ion channels characterized by information theory. Phys. Rev. E., 61(4):4272–80, 2000.
J. Hertz, A. Krogh, and R. G. Palmer. Introduction to the Theory of Neural Computation.
Addison-Wesley Publishing Company, Reading, MA, 1991.
P. Jung and J. W. Shuai. Optimal sizes of ion channel clusters. Europhys. Lett., 56(1):
29–35, 2001. doi: 10.1209/epl/i2001-00483-y.
Simon B. Laughlin, Rob R. de Ruyter van Steveninck, and John C. Anderson. The
metabolic cost of neural information. Nat Neurosci, 1(1):36–41, 1998. doi: 10.1038/236.
F. Moss and X. Pei. Neurons in parallel. Nature, 376:211–2, 1995.
K. Prank, F Gabbiani, and G Brabant. Coding efficiency and information rates in transmembrane signaling. Biosystems, 55(1–3):15–22, 2000.
F. Rieke, D. Warland, R. R. de Ruyter van Steveninck, and W. Bialek. Spikes: Exploring
the Neural Code. MIT Press, Cambridge, MA, 1997.
G. Schmid, I. Goychuk, and P. Hänggi. Stochastic resonance as a collective property
of ion channel assemblies. Europhys. Lett., 56(1):22–28, 2001. doi: 10.1209/epl/
i2001-00482-6.
F. Theunissen, J. C. Roddey, S. Stufflebeam, H. Clague, and J. P. Miller. Information
theoretic analysis of dynamical encoding by four identified primary sensory interneurons
in the cricket cercal system. J. Neurophysiol., 75(4):1345–64, 1996.
G.

Wenning and K. Obermayer.
Activity driven adaptive
tic resonance.
Physical Review Letters, 90(12):120602, 2003.
http://link.aps.org/abstract/PRL/v90/e120602.

stochasURL

K. Wiesenfeld and F. Moss. Stochastic resonance and the benefits of noise: From ice ages
to crayfish and SQUIDs. Nature, 373:33–36, 1995.

Noise and Neuronal Heterogeneity

11

1

hRi

0.8
0.6
σ 2 = 1.0
σ 2 = 10.0
σ 2 = 100.0
σ 2 = 1000.0

0.4
0.2
0
-10

-5

0

5

10

S
(a) Mean

0.25
0.2

2
σR

0.15
σ 2 = 1.0
σ 2 = 10.0
σ 2 = 100.0
σ 2 = 1000.0

0.1
0.05
0
-10

-5

0

5

10

S
(b) Variance

Figure 1: Statistics of single neuron
activation. As the noise variance σ 2 increases, (a) the

2
mean activation state hRi
S; σ takes longer to saturate to the extreme values, while (b)

2
2
the variance σR S; σ of the activation state increases with the noise variance.

12

M. J. Barber and M. L. Ristig

1
0.8
hRi

0.6
0.4
0.2

Exact
1st order

0
-4

-2

0

2

4

S
Figure 2: First order approximation of the expected activation of a single neuron. Near the
threshold (S0 = 0), the expected activation is nearly linear. Further from the threshold, the
activation saturates at either zero or one and diverges from the linear approximation. The
values shown here are based on noise variance σ 2 = 1.

8
6
4
D E
Ŝ

2
0

Exact
σ 2 = 1.0
σ 2 = 10.0
σ 2 = 100.0
σ 2 = 1000.0

-2
-4
-6
-8
-8

-6

-4

-2

0

2

4

6

8

S
Figure 3: Expectation value of the stimulus decoded from the output of a single neuron. As
the noise variance σ 2 increases, the expectation value of the decoded stimulus approximates
the true value of the stimulus over an interval of increasing width.

Noise and Neuronal Heterogeneity

8

σ 2 = 1.0
σ 2 = 10.0
σ 2 = 100.0
σ 2 = 1000.0

6
4
2
ε

13

0
-2
-4
-6
-8
-8

-6

-4

-2

0

2

4

6

8

S
(a) Expected difference

1.6
1.4

σŜ2 /σ 2

1.2
1
0.8
σ 2 = 1.0
σ 2 = 10.0
σ 2 = 100.0
σ 2 = 1000.0

0.6
0.4
0.2
-10

-5

0

5

10

S
(b) Estimate variance

Figure 4: (a) Expected difference between the stimulus and the value decoded from the
single-neuron response. The decoded value systematically diverges from the true value as
the input gets farther from the threshold value at zero. (b) Variance of the decoded stimulus
values. Again, the variances shown here are based on decoding the single-neuron response.
The variance of the neuronal noise has been used to scale the variances of the estimates into
a uniform range.

14

M. J. Barber and M. L. Ristig

1

hRi

0.8
0.6
0.4
0.2

Subthreshold
Suprathreshold

0
0

2

4

6

8

10

12

14

16

18

20

6

7

8

9

10

(σ/S)2
(a) Mean

0.3
0.25

2
σR

0.2
0.15
0.1
0.05
0
0

1

2

3

4

5
(σ/S)2

(b) Variance

Figure 5: (a) Noise dependence of single-neuron mean activation. For large values of
(σ/S)2 , the expected activation state asymptotically approaches 1/2. (b) Noise dependence
of single-neuron activation variance. For large values of (σ/V )2 , the variance asymptotically approaches 1/4.

Noise and Neuronal Heterogeneity

15

1

D E
Ŝ /S

0.5
0
-0.5

Subthreshold
Suprathreshold

-1
0

0.5

1

1.5

2

2.5

3

3.5

4

3.5

4

(σ/S)2
(a) Estimated stimulus

1

ε/S

0.5
0
-0.5

Subthreshold
Suprathreshold

-1
0

0.5

1

1.5

2

2.5

3

(σ/S)2
(b) Expected difference

Figure 6: (a) Noise dependence of the single-neurons estimated stimulus. For large values
of (σ/S)2 , the estimates for the subthreshold and suprathreshold signals asymptotically
approach −1 and +1, respectively. (b) Noise dependence of the expected difference. For
large values of (σ/S)2 , the expected differences asymptotically approach 0 for both the
subthreshold and suprathreshold signals.

16

M. J. Barber and M. L. Ristig

Reconstruction errors

1
0.8
0.6
0.4

(ε/S)2
(σŜ2 /S)2

0.2

(∆Ŝ/S)2

0
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

(σ/S)2
Figure 7: Comparison of decoding error sources. The values shown here are calculated
from the response of a single neuron. The minimum in ∆Ŝ 2 occurs for a nonzero noise
variance of the signal, as with stochastic resonance.

1

(∆ŜN /S)2

0.8
0.6

N =1
N = 10
N = 100
N = 1000

0.4
0.2
0
0

1

2

3

4

5

6

7

8

9

10

(σ/S)2
Figure 8: Effect of the number of neurons on the decoding error. As N becomes large,
the error curve flattens out, indicating a broad range of noise values that all give similar
accuracy in the decoding process.

Noise and Neuronal Heterogeneity

17

3
2

D E
Ŝ

1
0
Exact
M = 1, N = 1000
M = 1, N = 2000
M = 2, N = 1000

-1
-2
-3
-3

-2

-1

0

1

2

3

1

2

3

S
(a) Decoded stimulus

0.5
M = 1, N = 1000
M = 1, N = 2000
M = 2, N = 1000

0.4

∆Ŝ

0.3
0.2
0.1
0
-3

-2

-1

0
S

(b) Decoding error

Figure 9: (a) Expectation value of the decoded output in homogeneous and heterogeneous
networks. The response of the heterogeneous neural network (M = 2, N = 1000) can be
accurately decoded over a broader range than the responses of the baseline (M = 1, N =
1000) and homogeneous (M = 1, N = 2000) networks. (b) Total decoding error for homogeneous and heterogeneous networks. The heterogeneous neural network has a broader
basin of low error values than the baseline and homogeneous networks.

