The Statistics of Fixation Times for Systems with Recruitment
Tommaso Biancalani∗
Department of Physics, University of Illinois at Urbana-Champaign,
Loomis Laboratory of Physics, 1110 West Green Street, Urbana, Illinois 61801-3080, USA

Louise Dyson∗ and Alan J. McKane
Theoretical Physics Division, School of Physics and Astronomy,
University of Manchester, Manchester M13 9PL, United Kingdom

arXiv:1408.5004v3 [cond-mat.stat-mech] 23 Jan 2015

We investigate the statistics of the time taken for a system driven by recruitment to reach fixation.
Our model describes a series of experiments where a population is confronted with two identical
options, resulting in the system fixating on one of the options. For a specific population size, we show
that the time distribution behaves like an inverse Gaussian with an exponential decay. Varying the
population size reveals that the timescale of the decay depends on the population size and allows the
critical population number, below which fixation occurs, to be estimated from experimental data.
PACS numbers: 05.40.-a, 87.23.Cc, 02.50.Ey

∗

These authors contributed equally to this work.

2
I.

INTRODUCTION

When the same phenomenon is discovered in diverse areas, it indicates that there may be an elegant and simple
shared explanation. One such phenomenon is found in the following experiments in varied fields: in foraging colonies [1]
(reviewed in [2]); queueing dynamics [3]; herd investment behaviour [4]; and the evolution of language [5] (modelling
reviewed in [6]). All these studies have an underlying recruitment mechanism and display similar behaviours. To
illustrate the general phenomenon we take, as an example, the evolution of language.
Language is a complex and evolving system for human communication. Consider two linguistic variants, competing
to become the single shared convention (i.e. reach fixation). This competition occurs via a recruitment process,
where an individual using a particular variant may induce conversational partners to also use this variant. Thus the
more people using a certain variant, the more are recruited to also use it. If, by random chance, a particular variant
becomes more popular, then recruitment can amplify the disparity, until fixation is reached. Other systems also
display an analogous behaviour [1, 3, 4, 7] and all share three basic traits: a population-based system, two (equally
favourable) options, and a recruitment mechanism leading to autocatalytic amplification. In [8] it was argued that
the experiments in [1, 3, 4] share a single explanation.
In the language of birth-death processes [9], recruitment can be described using the terminology of chemical reactions:
r=1

r=1

X + Y −−→ 2X,

X + Y −−→ 2Y,

(1)

where X and Y indicate an individual choosing one of the two options. An additional term,


X←
→ Y,

(2)

describes the presence of spontaneous changes and its strength, , is supposed small compared to that of the previous
reactions. We have obtained this reaction scheme as a simplification of the Togashi-Kaneko four species model [10]
and proposed it as a description of systems with recruitment [11]. An alternative method to our analytical treatment,
based on the discrete time Markov chain, has been recently proposed to study these schemes in the two and three
species variants [12].
To gain intuitive understanding, one can approximate the reaction scheme by means of an expansion in the inverse
of the population size, which yields the following stochastic differential equation as  → 0 [11, 13]:
r
1 − x2
ẋ = −x +
η(t),
(3)
λ
where x denotes the difference in the concentration of individuals choosing each of the two options X and Y , and
where the dot denotes differentiation with respect to time. Here η(t) is Gaussian white noise with zero mean and
correlator
hη(t)η(t0 )i = δ(t − t0 ).

(4)

The parameter λ > 0 is proportional to the population size and time t has been rescaled by  [11]. Under the change
of variable, x = (1 − z)/2, we recover the usual equation describing the Moran process with mutations [14]. The
model also has similarities to the voter model but
√ with the additional deterministic term, −x. The voter model in
its most similar form can be described by ẋ = 1 − x2 η(t) [28], so that x = ±1 are absorbing states of the system.
Interestingly, when the system grows at a constant rate, Eq. (3) can describe the voter model in rescaled time [29].
Equation (3) exhibits a type of bistability in which the bistable states do not correspond to fixed points [15][16].
Other models with similar mechanisms have been discussed in the recent literature [17]. The deterministic part of
the equation has a unique stable fixed point at x = 0, and for large values of λ (i.e. large populations) the second
term becomes negligible, and the system resides at this point. However, when λ is smaller than some critical value
(λ < λc ≡ 1), a noise-induced transition takes place [11]: the noise term becomes dominant, and is maximal at x = 0.
The system is therefore driven away from the deterministic steady state at x = 0 and towards x = ±1, where the noise
term vanishes and the population consists of individuals of a single species. Once at these states, the system either
displays metastability or is absorbed, according to the boundary conditions in use. If Eq. (3) is being studied as a
representation of the individual-based system described in (1) and (2) then x is constrained to lie within the interval
[−1, 1] as values outside this interval would correspond to negative population numbers. For some systems, such as
in the evolution of language, a fixed population may not subsequently re-introduce a rejected linguistic variant and
so absorbing boundary conditions halts the dynamical process when fixation is reached. In other systems, such as in
foraging ant colonies, the process continues to be of interest after the boundary is reached, so that reflecting boundary
conditions are more appropriate, and the movement between bistable states may be studied.

3
In this paper, we investigate the distribution of times taken for a system initialised with no bias (at x = 0) to
reach fixation (at x = ±1). For λ = λc /2, Eq. (3) is exactly solvable (solution given in Eq. (8)) and thus the full
distribution of times may be found in terms of the derivative of a Jacobi theta function (Eq. (10) or Eq. (14)). For
general λ, we calculate an analogous, but approximate, solution (Eq. (23)), which is checked against simulations of
the reaction scheme. Our analytical treatment indicates that the distribution of times taken to reach fixation decays
exponentially at long times (Eq. (12) and Eq. (24)), with a timescale dependent on the population size. At short
times, for λ = λc /2, the distribution can be approximated by an inverse Gaussian distribution (Eq. (15)), which
captures the initial growth and its skewed maximum.
II.

ANALYTICAL TREATMENT FOR THE CASE λ = λc /2
A.

The time-dependent distribution

We begin by finding P (x, t), which is defined as the solution of the Fokker-Planck equation corresponding to Eq. (3)
for λ = λc /2. To proceed, we find a change of variables in Eq. (3) under which the noise becomes purely additive.
Since the equation is defined in the Itō sense, the change of variables must be performed using the Itō formula [9]
which is given in this case by
r


1 − x(t)2 00
1 − x(t)2
0
0
y (x(t)) + y (x(t))
η(t),
(5)
ẏ[x(t)] = −x(t)y (x(t)) +
2λ
λ
so that taking y = arcsin(x), the equation becomes
ẏ =

1 1
1
( − 2) tan(y) + √ η(t).
2 λ
λ

Thus, for λ = 1/2, the deterministic part vanishes so that Eq. (6) reduces to ẏ =
Fokker-Planck equation is the diffusion equation:
∂t Q(y, t) = ∂y2 Q(y, t).

(6)
√

2 η(t). The corresponding
(7)

Note that since x ∈ [−1, 1], the y-variable is constrained to lie in the interval [−π/2, π/2].
The strategy now consists of solving the diffusion equation before reversing the transformation via P (x, t) =
Q(y, t)dy/dx, to obtain the solution to Eq. (3). We take the system to be initially localised, P (x, 0) = δ(x − x0 )
and impose reflective boundary conditions at y = ±π/2. Note that the solution of the diffusion equation cannot be
simply a Gaussian because of the constraint y ∈ [−π/2, π/2]. Even though the localised system initially spreads in a
Gaussian way, once it reaches y ≈ ±π/2 it accumulates at the impassable boundaries. The solution to the diffusion
equation for these particular boundary conditions has been discussed in the literature both by physicists [18, 19] and
mathematicians [20, 21], but for convenience we reproduce it in Appendix A (see also Ref. [22]). One sees that the
probability density function corresponding to Eq. (3) is given by

θ3 arcsin(x) − arcsin (x0 ) , e−4 t
√
P (x, t) =
,
(8)
π 1 − x2
where θn (s, q) is the n-th Jacobi elliptic theta function [24].
The distribution (8) is positive and normalised in the interval [−1, 1]. It diverges at x = ±1 but this is not a sign
of singular behaviour as the probability P (x, t)∆x, evaluated on x = ±1, tends to zero as ∆x → 0, meaning that the
system is well behaved at these points. As t → ∞, P (x, t) relaxes to the stationary distribution
Ps (x) = π −1 1 − x2

−1/2

,

(9)

which indicates that the system spends most of its time in proximity to x ≈ ±1. The dynamics from the localised
initial condition to the final stationary distribution is shown in Fig. 1.
B.

Time statistics

The change of variables, y = arcsin(x), is also instrumental in finding an exact solution for the distribution f (T )
(still in the case where λ = λc /2) of the time T taken for the system, initialised at x = 0, to reach one of the two states,

4

1.5

P(x,t)

1

0.5

0
-1

-0.5

0
x

0.5

1

FIG. 1. The analytical expression of P (x, t) (solid lines) defined in Eq. (8), is displayed against Gillespie simulations [23] of
the reaction scheme (symbols) with N = λc /2 particles and  = 10−2 . The initial condition is x0 = 0.5 and we show results
for three different times: t = 0.05 (red line, squares), t = 0.2 (purple line, circles), t = 2 (blue line, triangles).

x = ±1. This is equivalent to initialising the system in the y variable at y = 0, and imposing absorbing boundary
conditions at y = ±π/2. These latter conditions ensure that the dynamics cease when the system reaches one of the
states x = ±1 for the first time. Performing these calculations in an analogous way to before (see Appendix B) gives
f (T ) =

2 0
θ (0, e−4T ),
π 1

(10)

where we used the notation θ10 (s, q) ≡ ∂s θ1 (s, q).

0.8

f(T)
f0(T)
f∞(t)

0.6

0.4

0.2

0
0

1

2

3

4

5

T
FIG. 2. The function f (T ) (solid line), Eq. (10), is shown against its asymptotic approximations: f0 (T ) (black dots), Eq. (15),
and f∞ (T ) (red dots), Eq. (12). The vertical line indicates the maximum point, Tm = π 2 /24, of f0 (T ).

The distribution (10) (Fig. 2) is unimodal, so that the dominant timescale is given by the single maximum of
the distribution, and skewed, suggesting that the maximum can significantly differ from the mean. We may further
understand this equation by carrying out an asymptotic analysis. We use an expansion for the derivative of the Jacobi

5
elliptic theta function, θ10 (s, q), which holds for s = 0 and small q. At first order this is [24]
1

θ10 (0, q) ≈ 2q 4 .

(11)

Applying this to Eq. (10) leads to an approximation for large times
f (T ) ≈ f∞ (T ) =

4 −T
e ,
π

(12)

which indicates that the decay is exponential.
Asymptotics for short times can also be obtained, but we need to first apply the Jacobi imaginary transform [24]
so that we may again use the previous expansion for small q. The transform is
r
 2 
iπ
y
−i
iπα
θ1 (y, e ) =
exp
θ1 (−α−1 y, e− α ).
(13)
α
πiα
Setting α = 4iT /π, taking the derivative with respect to y, and evaluating it at y = 0, yields an alternative expression
for Eq. (10):
√
π2
π −3 0
f (T ) =
T 2 θ1 (0, e− 4T ).
(14)
4
The advantage of this form is that T now appears in the denominator of the exponential allowing us to again apply
the expansion, Eq. (11), to Eq. (14) to give, for short times,
r


π2
π
exp −
f (T ) ≈ f0 (T ) =
4T 3
16T
 2 
z̄
z̄
∝√
.
(15)
exp −
3
4DT
DT
This last expression, when normalised, is already known in a different context. It is the inverse Gaussian distribution
and gives the statistics of times taken by a one-dimensional Brownian particle (here with diffusion coefficient D = 1)
starting from the origin in a semi-infinite system, [z̄, ∞), to reach an absorbing boundary at z̄ [25] (here z̄ = π/2).
The factor T −3/2 is a consequence of the one-dimensional nature of the Brownian motion [25] and gives the leading
order behaviour for absorbing states z̄ far from the origin. The presence of the exponential reveals that the absorption
times are of order T ∼ O(z̄ 2 /D) and a computation of the position of the maximum value gives
Tm =

z̄ 2
≈ 0.41.
6D

(16)

The asymptotic approximations (12) and (15) and the position of the maximum, Tm , are shown in Fig. 2.
III.

ANALYTICAL TREATMENT FOR A GENERAL λ

Returning to Eq. (3) with general λ, we wish to investigate the time taken for an unbiased system to pick one of
the two states x = ±1. We cannot approach this in a similar way to the earlier λ = λc /2 case as the deterministic
part of Eq. (6) does not vanish in this regime. However, the distribution of times can still be obtained by separation
of variables [16]. We begin by writing down the backwards Fokker-Planck equation [9] corresponding to Eq. (3):

1 − x2 ∂ 2 G
∂G
∂G
= −x
+
,
(17)
∂T
∂x
2λ
∂x2
with absorbing boundary conditions, G(x = 1, T ) = 0, and G(x = −1, T ) = 0. The function G(x, T ) denotes
the density of probability that the system has not escaped from the interval [−1, 1] after a time T . The required
distribution f (T ) is thus given by the rate −∂G/∂T that the system leaves the domain [9]. The initial condition is
given by G(x, 0) = 1, since a system initialised in the domain must by definition remain in the domain at time T = 0.
Equation (17) may be solved giving rise to series solutions in terms of the eigenfunctions of the right-hand side. We
describe the method here, and refer the reader to the attached CDF / Mathematica file for the explicit expressions
of some of the resulting quantities. Searching for separable solutions and using the boundary conditions reveals a

6
discrete set of eigenvalues, Fn , which give the inverse of the timescales at which the profiles given by the corresponding
(unnormalised) eigenfunctions vn decay. The eigenvalues and eigenfunctions are given by
n(1 + n − 2λ)
,
2λ
∞
X
vn =
Lk,n,λ sin((n + 2k)φ),

Fn =

(18)

k=0

for n ∈ N and φ = arccos(x). The expression for the coefficients Lk,n,λ is given in the attached CDF / Mathematica
file.
The solutions are therefore given by
G(φ, T ) =

∞
X

Cn e−Fn T vn (φ),

(19)

n=0

where the Cn are constants that are determined using the initial condition G(φ, 0) = 1. The initial condition is applied
by using the orthogonality of the eigenfunctions [16], vn (φ), of the backward Fokker-Planck equation, under the inner
product defined by
Z
hvn , vm i =

π

sin φPs (cos φ)vn (φ)vm (φ) dφ,

(20)

0

where
Ps (cos φ) = N (sin φ)2(λ−1)

(21)

is the stationary distribution of Eq. (3) [11], and N is a normalisation constant. Since the eigenfunctions are orthogonal
with respect to the inner product (20), hvn , vm i = 0 for n 6= m. Thus taking the inner product of the initial condition
(Eq. (19) at T = 0) with vn reveals that
hG(φ, 0), vn i = Cn hvn , vn i,

(22)

and gives an expression for Cn .
Thus the distribution of fixation times is given by
f (T ) =

∞
X
X h1, vn i
n−1
Fn
(−1)k+ 2 Lk,n,λ e−Fn T .
hvn , vn i
n

(23)

k=0

Whilst h1, vn i may be found exactly, and is zero for even values of n, the normalisation terms, hvn , vn i, are given
by an infinite summation. However, since the summand is small for all except the first term, we may take only the
very first term in the sum and neglect the others. Expressions for these quantities are given in the attached CDF /
Mathematica file. The infinite n summation in Eq. (23) is also not exactly computable and we therefore truncate the
summation and observe that increasing the number of terms taken in the sum leads to a better approximation for
small values of T (Fig. 3).
Taking the first two terms in the sum gives a good fit to simulations for a range of λ values (Fig. 4). A better fit
is given for smaller values of λ, i.e. where there are fewer individuals in the population, since our approximation of
taking  → 0 to give Eq. (3) is more accurate at smaller population sizes. The distribution found in the general λ case
is qualitatively similar to that in the exactly solvable case, and is skewed for all values of λ, similarly to the λ = λc /2
case.
Taking only the leading order term in the summation, we obtain

2Γ 25 − λ λ−1 T
f (T ) ≈ − √ 2
e λ ,
πλ Γ(−λ)

(24)

which fits the tail of the distribution (Fig. 3) and recapitulates the long-time approximation found in Eq. (10). It is
therefore clear that the distribution has an exponentially decaying tail, with a faster decay for smaller λ.

7

1.5

f(T)

1

0.5

0
0

0.5

1
T

1.5

2

FIG. 3. Equation (23) (solid lines) is compared to simulations as described in Fig. 1 (symbols) for one (blue line), two (red line)
and three (black line) terms in the summation. Simulations are averaged over 2500 runs with  = 10−3 and λ = 0.6, initialised
at x = 0 and run until x = ±1.

3
2.5

f(T)

2
1.5
1
0.5
0
0

0.5

1
T

1.5

2

FIG. 4. The first two terms of Eq. (23) (solid lines) is compared to simulations [23] as described in Fig. 1 (symbols) for λ = 0.2
(red), λ = 0.5 (blue) and λ = 0.7 (black). Simulations are averaged over 2500 runs with  = 10−3 , initialised at x = 0 and
stopped when x = ±1.

IV.

CONCLUSION

In this paper, we have shown that for λ = λc /2, Eq. (3) can be solved and the statistics of fixation times can
be found as a differentiated Jacobi elliptic theta function. An asymptotic analysis reveals that, for short times, this
distribution behaves as an inverse Gaussian distribution but with an exponential decay at long times. Our analysis for
a general λ shows that the timescale of the decay is λ/(λ − 1) (Eq. (24)), which provides an experimental prediction
for estimating the rescaled population size, λ, thus quantifying the distance of the system from the critical size, λc .
Since the model we have studied is very simple, it is worth asking if these predictions are applicable to experimental
results [1, 3, 4, 7, 26]. Equation (3) displays some general features that we expect to be ubiquitous: the skewness of

8
the distribution, the presence of a single maximum and the exponential tail. For short times, we have shown that for
a specific choice of λ the times are distributed according to an inverse Gaussian distribution. Do we expect this to
hold for general λ? Our series solution provides a more general expression to fit the data. Furthermore, agreement
between theory and experiment might be obtained by using generalisations of the inverse Gaussian distribution, such
as the Levy or Gamma distributions.
Future research directions include exploring new fields of applicability such as the chiral symmetry-breaking observed in chemical and biological molecules [27]. This has an autocatalytic mechanism and is relevant in the field of
astrobiology.

ACKNOWLEDGMENTS

T.B. ackowledges Nigel Goldenfeld for useful discussions concerning homochirality. L.D. was supported under
EPSRC grant EP/H02171X. T.B. acknowledges partial support from the National Aeronautics and Space Administration through the NASA Astrobiology Institute under Cooperative Agreement No. NNA13AA91A issued through
the Science Mission Directorate.
Appendix A: Solution of the diffusion equation

We begin by computing the time-dependent distribution, P (x, t), of the equation:
√ p
ẋ = −x + 2 1 − x2 η(t),

(A1)

where η(t) is Gaussian noise with zero mean and correlator,
hη(t)η(t0 )i = δ(t − t0 ),

(A2)

and Eq. (A1) follows from the main text by assigning λ = 1/2. The corresponding Fokker-Planck equation is

∂P
∂
∂2
=
(xP ) +
(1 − x2 )P .
2
∂t
∂x
∂x

(A3)

We imagine that we start with the system localised at a certain x0 , and look for a solution that is positive and
normalised in the x domain [9]:
Z

1

dx P (x, t) = 1,

P (x, t) > 0,

−1

(A4)

P (x, 0) = δ(x − x0 ).
Under the change of variable y = arcsin(x), Eq. (A3) becomes the diffusion equation. Translated into the y-variable
these conditions read:
Z π2
dy Q(y, t) = 1, Q(y, t) > 0,
(A5)
−π
2

and
dx
=
dy
= δ(sin(y) − sin(y0 )) cos(y) = δ(y − y0 ),

Q(y, 0) = P (x, 0)

(A6)

where y0 = arcsin(x0 ) and the last equality holds because y is restricted to [−π/2, π/2].
To solve the diffusion equation, we go over to a Fourier representation. We start by recalling that any function
f (y), defined in an interval of length l, admits a representation as a Fourier series [22]:
f (y) =

+∞
X
k=−∞



2π
exp ik y fk .
l

(A7)

9
Since in our case the domain is an interval of length π, the probability density function Q(y, t) can be rewritten as:
Q(y, t) =

+∞
X

ei2ky ak (t).

(A8)

k=−∞

Inserting this into the diffusion equation gives a linear equation for the Fourier coefficients. Once solved, we have:
2

ak (t) = ak (0)e−4k t .

(A9)

The coefficients at the initial time, ak (0), are determined using the initial condition and the identity:
δ (y − y0 ) =

+∞
1 X
exp(ik (y − y0 )).
2π

(A10)

k=−∞

This shows that ak (0) = π −1 e−2iky0 . The series for Q(y, t) can now be summed and becomes:
Q(y, t) =π −1

+∞
X

eik2(y−y0 ) e−4k

2

t

(A11)

k=−∞

=π

−1

−4t

θ 3 y − y0 , e



,

where the function θn (x, q) is the n-th elliptic theta function, with the conventions adopted in [24]. Changing back
to the x variable, by P (x, t) = Q(y, t)dy/dx, yields the time-dependent probability density function:

θ3 arcsin(x) − arcsin (x0 ) , e−4 t
√
P (x, t) =
.
(A12)
π 1 − x2
Appendix B: The time statistics for λ = λc /2

We need to solve the diffusion equation with absorbing boundary conditions at y = π/2 and y = −π/2. The system
is initialised at y0 = 0. We can carry out an analogous calculation to the one of Appendix A. More simply, the
solution of the diffusion equation for these boundary conditions can be found in the literature [22]. In either case:
Q(y, t) =

∞
h 
 π
2
π i
2X
sin n y +
sin n
e−n t =
π n=1
2
2

∞
h

2X
π i −4(n+ 12 )2 t
=
(−1)n sin (2n + 1) y +
e
,
π n=1
2

(B1)

and following [24], we immediately recognise that:
Q(y, t) =


1 
π
θ1 y + , e−4t .
π
2

(B2)

The integral of Q(y, t) over the y-domain represents the probability that y remains in [−π/2, π/2) after a time t.
This is the probability that y has not yet reached the absorbing boundary after a time t. Rephrased again, it is the
probability that the time at which y is absorbed, T , is greater than t. We denote this probability by:
Z π/2
Prob(T > t) =
dy Q(y, t).
(B3)
−π/2

Since Prob(T > t) is the cumulative distribution function of T , the statistics of jumps is readily obtained by differentiation:


Z π/2



f (T ) = − ∂t Prob(T > t)
=−
dy ∂t Q(y, t)
t=T

−π/2

,
=∂y Q(y, T )
π/2

−π/2

t=T

(B4)

10
where to simplify the last integral we have used the fact that ∂t Q = ∂y2 Q. Noting from Eqs. (B1) and (B2) that
θ10 (π, e−4t ) = −θ10 (0, e−4t ), we see that
f (T ) =

2 0
θ (0, e−4T ).
π 1

(B5)

[1] J. Pasteels, J. Deneubourg, and S. Goss, Experientia Supplementum 54, 155 (1987).
[2] C. Detrain and J. Deneubourg, Phys. Life Rev. 3, 162 (2006); D. J. Sumpter, Collective Animal Behavior (Princeton
University Press, 2010).
[3] G. S. Becker, J. Polit. Econ. 99, 1109 (1991).
[4] D. S. Scharfstein and J. C. Stein, Am. Econ. Rev. 80, 465 (1990).
[5] W. Croft, Explaining Language Change: An Evolutionary Approach (Pearson Education, 2000).
[6] R. Blythe, J. Stat. Mech. 2009, 02059 (2009).
[7] C. Saloma, G. J. Perez, G. Tapang, M. Lim, and C. Palmes-Saloma, Proc. Natl. Acad. Sci. USA 100, 11947 (2003).
[8] A. Kirman, Q. J. Econ. 108, 137 (1993).
[9] C. W. Gardiner, Handbook of Stochastic Methods for Physics, Chemistry and the Natural Sciences, 4th ed. (Springer, New
York, 2009); N. G. van Kampen, Stochastic Processes in Physics and Chemistry, 3rd ed. (Elsevier Science, Amsterdam,
2007).
[10] Y. Togashi and K. Kaneko, Phys. Rev. Lett. 86, 2459 (2001).
[11] T. Biancalani, L. Dyson, and A. J. McKane, Phys. Rev. Lett. 112, 038101 (2014).
[12] N. Saito and K. Kaneko, arXiv preprint arXiv:1403.6222 (2014).
[13] T. Biancalani, T. Rogers, and A. J. McKane, Phys. Rev. E 86, 010106(R) (2012).
[14] W. J. Ewens, Mathematical Population Genetics: I. Theoretical Introduction, Vol. 27 (Springer, 2004).
[15] C. R. Doering, Phys. Rev. A 34, 2564 (1986).
[16] W. Horsthemke and R. Lefever, Noise-Induced Transitions (Springer-Verlag, Berlin, 1984); J. K. McSweeney and
L. Popovic, Ann. Appl. Prob. 24, 1226 (2014).
[17] O. Al Hammal, H. Chaté, I. Dornic, and M. A. Muñoz, Phys. Rev. Lett. 94, 230601 (2005); D. Russell and R. Blythe, ibid.
106, 165702 (2011); M. Assaf, E. Roberts, Z. Luthey-Schulten, and N. Goldenfeld, ibid. 111, 058102 (2013); T. Rogers
and T. Gross, Phys. Rev. E 88, 030102 (2013); D. Remondini, E. Giampieri, A. Bazzani, G. Castellani, and A. Maritan,
Physica A 392, 336 (2013); M. Parker, A. Kamenev, and B. Meerson, Phys. Rev. Lett. 107, 180603 (2011).
[18] M. Smoluchowski, Bull. Int. l’Acad. Sci. Cracovie, Math-Naturw. Klasse A , 418 (1913).
[19] S. M. Soskin, Radiophys. Quantum Electron. 30, 456 (1987).
[20] S. Karlin and J. McGregor, J. Math. Anal. Appl. 1, 163 (1960).
[21] P. Biane, J. Pitman, and M. Yor, Bull. Amer. Math. Soc. 38, 435 (2001).
[22] A. D. Polyanin, Handbook of Linear Partial Differential Equations for Engineers and Scientists (CRC press, 2002).
[23] D. T. Gillespie, J. Phys. Chem. 81, 2340 (1977); D. T. Gillespie, A. Hellander, and L. R. Petzold, J. Chem. Phys. 138,
170901 (2013).
[24] J. V. Armitage and W. F. Eberlein, Elliptic Functions (Cambridge University Press, 2006).
[25] S. Redner, A Guide to First-Passage Processes (Cambridge University Press, 2001).
[26] D. Helbing, M. Isobe, T. Nagatani, and K. Takimoto, Phys. Rev. E 67, 067101 (2003).
[27] Y. Saito and H. Hyuga, Rev. Mod. Phys. 85, 603 (2013).
[28] O. Al Hammal, H. Chaté, I. Dornic, and M. A. Muñoz, Phys. Rev. Lett. 94, 230601 (2005).
[29] R. G. Morris and T. Rogers, J. Phys. A: Math. Theor 47, 342003 (2014).

