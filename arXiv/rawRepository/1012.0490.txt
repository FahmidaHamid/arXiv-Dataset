arXiv:1012.0490v2 [q-bio.NC] 10 Jan 2011

Testing of information condensation in a model
reverberating spiking neural network1

A. K. Vidybida
Department of Synergetics, Bogolyubov Institute for Theoretical Physics
Metrologichna str., 14-B, 03680 Kyiv, Ukraine
E-mail: vidybida@bitp.kiev.ua

Abstract
Information about external world is delivered to the brain in the form of structured in time spike trains.
During further processing in higher areas, information is subjected to a certain condensation process, which
results in formation of abstract conceptual images of external world, apparently, represented as certain uniform
spiking activity partially independent on the input spike trains details. Possible physical mechanism of
condensation at the level of individual neuron was discussed recently. In a reverberating spiking neural
network, due to this mechanism the dynamics should settle down to the same uniform/periodic activity in
response to a set of various inputs. Since the same periodic activity may correspond to different input spike
trains, we interpret this as possible candidate for information condensation mechanism in a network. Our
purpose is to test this possibility in a network model consisting of five fully connected neurons, particularly,
the influence of geometric size of the network, on its ability to condense information. Dynamics of 20 spiking
neural networks of different geometric sizes are modelled by means of computer simulation. Each network was
propelled into reverberating dynamics by applying various initial input spike trains. We run the dynamics
until it becomes periodic. The Shannon’s formula is used to calculate the amount of information in any
input spike train and in any periodic state found. As a result, we obtain explicit estimate of the degree of
information condensation in the networks, and conclude that it depends strongly on the net’s geometric size.

1

Introduction

The ability of a biological object to obtain enough
information about external world is essential for
the object’s survival. The information is delivered to the central nervous system through various sensory pathways in the form of spike trains.
The pathways’ information throughput has been
studyed for a long time in theoretical [22] and experimental [25] research. The paradigm of those
research is consistent with the assumption that the
1 Accepted

best situation is when the brain receives maximum
information from sensory systems. But compare
this with [10]. Another paradigm, which is as well
mature, [23], concentrates on self-organization of
spike trains when primary sensory activity spreads
to higher brain areas. Self-organization is accompanied with information loss, [15], Indeed, a
kind of standartization of activity evoked by various primary sensory inputs is obsereved experimentally in higher brain areas during olfactory
[6, 38] and auditory [4] perception. In visual sys-

in the International Journal of Neural Systems, http://www.worldscinet.com/ijns/ijns.shtml

2

A. Vidybida

tem, simple examples are the transformation of
scene representation to viewpoint-invariant, [7] or
retinotopic-invariant [32] coordinates. Here, spike
trains in the optic nerve must depend on certain
information (retinal position, viewpoint), which
is removed in higher areas of conceptual representation. In the context of cognitive physiology, the process of reduction of information aimed
at conceptual representation/recognition of external objects is known as information condensation,
[21]. Usually pattern recognition phenomenon,
which is closely related to information condensation, is considered in parallel with training, see
[12, 17, 24, 28, 30, 31, 39], learning, [1, 11, 13, 21],
or other plasticity, [16], in the corresponding network. In a biological network, the learning mechanism involves biosynthesis [27] and is therefore very
slow process, which requires seconds or minutes,
[3]. At the same time, recognition of objects in visual scene can be accomplished within 150 ms, or
faster, citeThorpe1,Thorpe. During this short period of time, the network has constant structure,
but it is the spiking activity which evolves within
it from information-rich at the sensory periphery
into information-poor, representing concrete entities/concepts at higher brain areas.
We now put a question: What could be the
physical mechanism of information condensation in
a spiking neural system? One possible mechanism,
[37] which operates at the level of single neuron,
was discussed, see Fig. 1, 2 and n. 2.1.12.1.1, below. In a reverberating spiking network, due to
this mechanism the spiking dynamics should settle down to the same definite periodic activity in
response to any stimulus from a definite set of various inputs, and to another periodic activity in response to members of another set of inputs. If so,
then the definite periodic dynamical state can be
considered as an abstract representation of a feature, which all stimuli from the definite set have in
common. And this is just what is expected from
the condensation of information.
Our purpose in this work is to study how the
ability of a simple spiking neural net to condense
information in the above described sense depends
on its physical parameter — the net’s geometric size. For this purpose we simulate dynamics
of a net composed of five binding neurons placed
equidistantly on a circle. The circle’s radius, R,
characterizes the net’s geometric size. The net is
fully connected, and propagation velocity is taken
the same for all connections and all values of R.
Thus, the variations in R are expressed exclusively
in the variations in the interneuronal propagation
times. Initially, the net is stimulated by a spike
train of input impulses triggering each of five neurons at times {t0 , t1 , . . . , t4 }. Afterwards, the spik-

ing dynamics is allowed to go freely until it settles
down to a periodic one. This allows to figure out
sets of input stimuli bringing about the same periodic dynamics. The number of different periodic
dynamics and the number of input stimuli in a set,
which corresponds to a definite periodic dynamics,
characterize the net’s ability to condense information. We found that this ability depends strongly
on the net’s size.

2
2.1

Methods
The Binding Neuron Model

The understanding of mechanisms of higher brain
functions expects a continuous reduction from
higher activities to lower ones, eventually, to activities in individual neurons, expressed in terms
of membrane potentials and ionic currents. While
this approach is correct scientifically and desirable
for applications, the complete range of the reduction is unavailable to a single researcher/engineer
due to human brain limited capacity. In this connection, it would be helpful to abstract from the
rules by which a neuron changes its membrane potentials to rules by which the input impulse signals are processed into its output impulses. The
coincidence detector, and temporal integrator are
the examples of such an abstraction, see discussion by König et al., [20]. One more abstraction,
the binding neuron (BN) model, is proposed as signal processing unit, [34] which can operate as either coincidence detector, or temporal integrator,
depending on quantitative characteristics of stimulation applied. This conforms with behavior of
real neurons, see, e.g. work by Rudolph & Destexhe, [26]. The BN model describes functioning
of a neuron in terms of discret events, which are
input and output impulses, and degree of temporal coherence between the input events, see Fig.
1. Mathematically, this can be realized as follows.
Each input impulse is stored in the BN for a fixed
time, τ . The τ is similar to the tolerance interval discussed by MacKay, [23]. All input lines are
excitatory and deliver identical impulses. The neuron fires an output impulse if the number of stored
impulses, Σ, is equal or higher than the threshold
value, N0 . In this model, inhibition is expressed in
decreased τ value. It is clear, that BN is triggered
when a bunch of input impulses is received in a
narrow temporal interval. In this case, the bunch
could be considered as compound event, and the
output impulse — as an abstract representation of
this compound event. One could treat this mechanism as binding of individual input events into
a single output event, provided the input events
are coherent in time. Such interpretation is sug-

3

Testing of Information Condensation
elementary
event
elementary
event
..
..
..
elementary
event

-

-

-

Binding of the
elementary
elementary event
secondary neurons
events based for
on their
(represents the bound event)
temporal
coherence

>

HHH
j

{

inhibition controls binding

Figure 1: Signal processing in the binding neuron model.[35, 36]
gested by binding of features/events in largescale
neuronal circuits, [5, 8, 9, 36].
It would be interesting to characterize the BN
input-output relations in the form of transfer function, which allows exact calculation of output in
terms of input. In our case, input is the sequence of discrete arriving moments of standard
impulses: Tin = {l1 , l2 , l3 , l4 , . . . }. The output is
the sequence of discrete firing moments of BN:
Tout = {f1 , f2 , . . . }. It is clear that Tout ⊂ Tin . The
transfer function in our case could be the function
σ(l), l ∈ Tin , which equals 1 if l is the firing moment, l ∈ Tout , and 0 otherwise. For BN with
threshold N0 the required function can be constructed as follows. It is clear that the first N0 − 1
input impulses are unable to trigger neuron, therefore σ(l1 ) = 0, . . . , σ(lN0 −1 ) = 0. The next input
is able to trigger if and only if all N0 inputs are
coherent in time:
σ(lN0 ) = 1 if and only if lN0 − l1 ≤ τ.
In order to determine σ(lN0 +k ), k ≥ 1, one must
take into acount all previous input moments, therefore we use notation σTin instead of σ. The values
of σTin (lN0 +k ) can be determined recursively:
σTin (lN0 +k ) = 1 if and only if lN0 +k − lk+1 ≤ τ and
σTin (li ) = 0 for all i ∈ {k + 1, . . . , N0 + k − 1}.
The function σTin describes completely the BN
model for arbitrary threshold value N0 ≥ 2 .

2.1.1

Information Condensation in a
Single Neuron

It is worth noticing, that any firing (triggering)
moment of a spiking neuron is determined by the
moment of last input impulse, which just ensures

that the triggering condition is satisfied. In a neuron, which needs more than one input impulse to
fire, variations of temporal position of impulses, received just before the triggering one, do not influence the moment of emitting the output impulse,
provided those variations are in resonable limits
and arrival moment of the triggering impulse remains the same.

τ
t25 t24 t23

t15 t14

t22

t21

t13 t12 t11

Figure 2: Example of two different inputs into
a single neuron, which produce identical outputs.
Thus, different input spike trains can produce exactly the same output. This looks like if some detailes of the input stimulus, which is composed of
several impulses, were reduced/condensed in the
output. In the BN, the triggering condition is
that the number of impulses in the BN’s internal memory equals to N0 . Consider BN with
N0 = 4, which is stimulated with two different
spike trains with input impulses arrival times S1 =
{t11 , t12 , t13 , t14 , t15 }, t11 < t12 < t13 < t14 < t15 ,
and S2 = {t21 , t22 , t23 , t24 , t25 }, t21 < t22 < t23 <
t24 < t25 . Let the arrival moments satisfy the following conditions:
t14 − t11 < τ,
t24 − t21 > τ,

t25 − t22 < τ,
t14 = t25 = to .

4

A. Vidybida

In this case, both S1 and S2 , if fed to the BN, will
produce exactly the same output, namely, the single impulse at moment to . This is illustrated in
Fig. 2.

2.2

The Network Model

As a reverberating spiking neural net we take the
net of five neurons placed equidistantly at a circle
of radius R, see Fig, 3. Each neuron has threshold
N0 = 4, and internal memory, τ = 10 ms. The
net is fully connected. The connection lines are
characterized with length and propagation velocity, v, which is the same in all lines. For R fixed,
there are two types of connection line, the short
one, with propagation delay d, and the long one
with propagation delay D. Each neuron has additionally the external stimulus input line, which is
used to start the net dynamics. Single impulse in
the stimulus input line delivers to its target neuron
just threshold excitation. This causes firing at the
moment of the stimulus impulse arrival. For numerical simulations we use 20 networks of different
sizes, see Table 1. The propagation velocity in any
interconnection line is taken v = 0.1 m/s.

2.2.1

Numerical Simulation

As programming language we use Python under
Linux operating system. The dynamics was modelled by advancing time with step dt = 200 µs.
The delay values d and D, when measured in the
dt units, were rounded to the nearest from below
integers, see Table 1. As a result, the simulating
program operates in whole numbers with no rounding errors involved.
Each single tick of the program, the network’s
tick, advances time by dt, and consists of three partial ticks, which are performed in the given order.
Namely, (i) input tick, which advances time in the
input lines, (ii) axonal tick, which advances time
in the internal connection lines, (iii) neuronal tick,
which advances time in the neurons.
This manner of updating states can be treated
as synchronous in a sense that each component of
the network has the same physical time when the
network’s tick is complete. On the other hand,
viewed as interneuronal communication process,
the dynamics should be treated as asynchronous
due to nonzero propagation delays.
During the step (ii), a neuron can get impulse
into its internal memory. If a neuron appears in the
state “Fire” as a result of the network tick, then
the output impulse it produces can appear in the
connection lines only during the next network tick.
This introduces effective delay of one dt between
delivering the triggering impulse to a neuron and
emitting output impulse by that neuron.

2.3
2.3.1

Data Acquisition Algorithm
Set of Stimuli

The net was entrained to reverberating dynamics
by applying initial input spike train of five impulses, one triggering impulse per neuron, at times
(in dt units) {t0 = 1, t1 , t2 , t3 , t4 }. The triggering
moment of neuron # 0 is taken 1 for all stimuli
in order to exclude rotational symmetry between
the stimuli applied. Other four triggering moments
run independently through the set {1, 2, . . . , tmax },
where tmax is choosen proportional to R for each
net size. In choosing tmax , we follow two different
paradigms. In the first paradigm of short stimuli we restrict the overall duration of the stimulus train with the value tmax = d. Thus, the
set of stimuli has d4 different stimuli. If ti ≤ d,
i = 0, . . . , 4, then any neuron in the net never
obtains impulse from other neurons before it obtains its external input stimulation. In the second
paradigm of extended stimuli, we restrict the overall duration of the stimulus train with the value
tmax = M , which is about three times longer than
d for each network (see Table 1). Here, all stimuli {t0 = 1, t1 , t2 , t3 , t4 }, which were presented to a
network, cover the set of M 4 different trains, which
equals from 625 different stimuli for net #1 to
100 000 000 different stimuli for net #20 (see Table
1). The stimuli were sampled in accordance with
standard algorithm of 4-digit counter. Namely, we
started from stimulus {1, 1, 1, 1, 1}, the next stimulus is obtained by advancing t1 by 1, and so on.
The stimulus next to {1, M, 1, 1, 1} is {1, 1, 2, 1, 1},
the one next to {1, M, M, 1, 1} is {1, 1, 1, 2, 1} and
so on, until stimulus {1, M, M, M, M } is presented.
In the extended paradigm, the late external input impulse can enter corresponding neuron after
it received impulses from neurons already triggered
by early external input impulses.
The second paradigme is in concordance with
visual information processing, [2] where activity
from higher brain areas, which was invoked due to
visual stimulation at earlier time, is retroinjected
to areas V1 and V2 in the primary visual cortex,
where it interacts with activity invoked by visual
input at later time during perception.

2.3.2

Figuring out Periodic States

After the last input impulse from the train {t0 =
1, t1 , t2 , t3 , t4 } reachs its target neuron, the program begins appending at each time step the instantaneous state of the net to a Python list. The
instantaneous state consists of states of all 20 connection lines and all 5 neurons (see Fig. 4). Before
appending, the program checks if the current instantaneous state was already included in the list.

5

t1

Testing of Information Condensation

t2

n1

d
n2

D
n0

t0 = 1

n3

t3
n4

t4
Figure 3: The network, used for simulations. Here {t0 , t1 , t2 , t3 , t4 } — is the input spike train, d, D — are the
propagation delays in the connection lines. Any line can be either empty, or propagating one impulse.
net #
R, mm
d, dt
D, dt
M , dt
net #
R, mm
d, dt
D, dt
M , dt

1
0.029
1
2
5
11
0.314
18
29
55

2
0.057
3
5
10
12
0.343
20
32
60

3
0.086
5
8
15
13
0.371
21
35
65

4
0.114
6
10
20
14
0.400
23
38
70

5
0.143
8
13
25
15
0.429
25
40
75

6
0.171
10
16
30
16
0.457
26
43
80

7
0.200
11
19
35
17
0.486
28
46
85

8
0.229
13
21
40
18
0.514
30
48
90

9
0.257
15
24
45
19
0.543
31
51
95

10
0.286
16
27
50
20
0.571
33
54
100

Table 1: Dimensions of networks used for simulations; dt = 200 µs.
net #

1
31

2
61
104
123

net #

11
301
494

12
331
544

3
91
154
187
246
13
361
584

4
111
184
227
296
14
391
634

5
141
234
287
376
15
411

6
171
284
347
456
16
441

7
8
9
10
201 221 251 281
324 364 414 454
407 447 507 567
526
17
18
19
20
471 491 521 551

Table 2: Distinct periods in dt units of found periodic states in the short stimuli paradigm. Superscript denotes
the number of different periodic states with this period.

6

A. Vidybida

If it was, then the periodic dynamical state is found
with its complete cyclic trajectory covered by instantaneous states between the inclusion and the
last record in the list, inclusively. Measures are
taken in order not to count the same cyclic trajectory, which was entrained at its different points, as
different periodic states.
The data for each net were stored in two
MySQL tables. Table STATES included the serial number of any periodic state found, one element from the corresponding cyclic trajectory, and
period of the state (see Fig. 4). Single record
in the INPUTS table included the input stimulus
{t0 = 1, t1 , t2 , t3 , t4 }, the serial number of the periodic state it leads to (this number is 0 for fading
dynamics), and relaxation time, namely, the time,
which is spent between the last external input impulse is delivered and the net’s entrance moment
into the periodic regime.

and k3 > ki , i = 0, 1, 2 leads to contradiction, and
so on.
This number of triggering is either 1 or 2 for
trajectories found, see examples in Fig. 6. Some
nets have only one periodic state, which coresponds
to synchronous firing of all 5 neurons and symmetrical states of connection lines at any moment of
time. This is the case for nets number 1 and from
15 to 20 for short stimuli, and for nets number 19
and 20 for extended stimuli.

3.2

Condensation of Information

In order to estimate the degree of information condensation in the course of transformation of an external spike train into a certain periodic state of the
net, one needs to calculate information amount,
which is delivered by specifying a spike train, and
which is delivered by specifying the state, it leads
to.
0, 1 4

3 2

state #319
50

0

3

Results

2, 3

0, 4

1
state #107
50
0, 3 1, 2 4
state #199
66

0

3.1

Characterization of Periodic
States Found

After initial stimulation, networks from #1 to #7
entrain to periodic activity after any stimulation,
and networks #8 to #20 either entrain to periodic dynamics, or stops from any activity after
some time. This takes place for both short and
extended stimuli.The number of different periodic
states found for each network is shown in Fig. 5.
Here, the maximal number of periodic states obtained with short stimuli is 18, and this number is
achieved for net numbers from 3 to 7. Exact values
of periods, and number of different periodic states
with this period is shown in Table 2 for short stimuli. We omit similar table for extended stimuli.
The maximal number of different periodic states
obtained with extended stimuli is 485, which is
achieved in the net number 9. The maximal number of different periodic states with the same period is 294 for period 50·dt in net #9. It should be
mentioned that two periodic states, which can be
turned into eachother by suitable renumeration of
neurons, were considered as different.
It is evident that in the network of five neurons
with threshold 4, each neuron is triggered the same
number of times during period. Indeed, expect
that neuron n4 fires k4 times, and any of the other
four fires less during period: k4 > ki , i = 0, 1, 2, 3.
In order to be triggered k4 times, n4 must obtain
not less than 4k4 input impulses during period.
But it can obtain only k0 +k1 +k2 +k3 , which is less
than required. Similarly, situation when k4 = k3

0, 3

1, 2

4

0

Figure 6: Examples of periodic states found
for net number 9 in the extended stimuli
paradigm. Spikes indicate the firing moments,
labels near each spike give numbers of neurons,
firing at this moment. The two upper trains
show states with period 10 ms, the lower one with period 13.2 ms.
This can be done by wellknown Shannon’s formula
[29]
X
pi log2 pi ,
(1)
H=−
i

where pi is the probability to obtain case number i from a set of cases. At the input end we
have the set of d4 , or M 4 different external input
spike trains. In our statement of the problem, it
is natural to consider all external input trains as
equally probable. If so, then information delivered
by specifying certain train is
Hs = 4 log 2 d,

(2)

for the short stimuli paradigm, and
He = 4 log 2 M,

(3)

for the extended stimuli paradigm.
While estimating information, delivered by
specifying certain periodic state, one should take
into account that probabilities of different periodic

7

Testing of Information Condensation

mysql> select * from STATES_5_9 where num=269;
+------+---------------------------------------------------------------------------------------------------------------------------+--------+
| num | state
| period |
+------+---------------------------------------------------------------------------------------------------------------------------+--------+
| 269 | 0 1 1 0 8 8 17 17 24 15 15 24 8 8 0 0 8 17 17 8
0 False False
0 False False
0 False False
0 False False
0 False True 49 1
|
82 |
+------+---------------------------------------------------------------------------------------------------------------------------+--------+
1 row in set (0.02 sec)
mysql>
Figure 4: Example of single record in the MySQL table STATES. The first field (num) is numerical, and gives
the serial number of periodic state found. The second field (state) is a string, which describes instantaneous
state from the periodic state found (a point from the cyclic trajectory, which represents the whole trajectory, or
periodic state). The first 20 numbers in the string describe states of all connection lines: ’0’ means that the line
is empty, positive number specifies after how many ticks the propagating impulse will rich the targeted neuron.
The next five chunks confined between the ”next line” symbols describe states of neurons. The first number in
each chunk is the ”kick” — the total number of impulses obtained by neuron after the axonal tick was complete.
During the neuronal tick, corresponding to that axonal tick, the ”kick” is utilized and set to zero. The next
boolean in the chunk indicates if the neuron is in the ”Fire” state. The next boolean indicates if the neuron has
any impulses in its internal memory. If it has, then next couples of numbers (up to three couples) describe those
impulses. In this example, neuron #4 has 1 impulse with time to leave 49·dt. The third field (period) specifies
period (in dt units) of this periodic state.

number of periodic states

number of periodic states

20

15

10

5

0
2

4

6

8

10
12
net #

14

16

18

20

500
450
400
350
300
250
200
150
100
50
0
2

4

6

8

10
12
net #

14

16

18

20

Figure 5: Number of different periodic states found for each net with short (left panel), and extended (right
panel) stimuli.

8

A. Vidybida

Tn
pn = 4 ,
d

(4)

8
information of periodic states

states are not the same. In order to calculate probability pn of a periodic state Cn , we calculate the
number of input spike trains leading to the Cn ,
namely, Tn , and divide this number by the total
amount of different input stimuli (see histogram
for Tn in Fig. 7)

7

e

6
5
4
s

3
2
1
0
2

for the short stimuli paradigm, and
pn =

Tn
,
M4

6

8

10

12

14

16

18

net #

(5)

for the extended stimuli paradigm. Then we use
Eq. (1) with probabilities of individual periodic
states found in accordance with (4), (5) to calculate information which should be ascribed to
any periodic state. In this calculations, we treat
uniformly with others the external input stimuli,
which lead to fading dynamics. Correspondingly,
the state with no activity is treated uniformly with
periodic states. This is in the contrast with data
presented in Fig. 5, and Table 2, where the state
with no activity is excluded.

90
number of domains in one bin

4

80
70
60
50
40
30
20

Figure 8: Dependence of information amount,
which is ascribed to periodic state, on the net
size. Curve ’e’ corresponds to extended stimuli
paradigm, ’s’ — to the short stimuli one.
information of a periodic state, calculated in accordance with Eq. (1) with probabilities found due to
Eqs. (4), (5), varies between 6.93 and 7.33 bits
for extended stimuli, and between 3.17 and 3.46
for short stimuli. In the plateau, the degree of information condensation calculated as input information divided by the periodic state information,
varies between 9.02 and 12.27 for extended stimuli, and between 11.2 and 19.31 for short stimuli.
Out of a plateau range, for greater net sizes the
amount of information in a periodic state drops
sharply due to simplification of the set of periodic states. Namely, the total number of periodic
states decreases to one, with the second one with
no activity, and the probability is distributed very
unevenly between this two states. This leads to
the degree of information condensation as high as
41000 for extended stimuli, and 690 for short ones.

10
0
0

10000

20000
30000
size of conceptual domain

40000

Figure 7: Histogram of conceptual domain
sizes for net #9 in extended stimuli paradigm.
The bin size is 518. 23 domains with sizes from
50764 to 193732 are not presented.
As it could be expected, the information
amount in an external input spike train increases
as logarithm of the net size, in accordance with
Eqs. (2), (3), varying from 25 to 81 bits for short
paradigm, and from 37 to 106 bits for extended
paradigm. Information, which could be ascribed
to a periodic state, depends on the net size in a
more complicated manner, see Fig. 8. A remarkable feature is a kind of plateau between net #3
and #9 for both short, and extended stimulation
paradigme. In the plateau, the

4

Discussion

Any reverberating spiking neural net can represent complicated dynamical behavior. If the net’s
instantaneous states can be described with whole
numbers, then the net will inevitably either entrain to periodic dynamics, or stop its activity at
all. In this study, it is appeared that a very simple
net of Fig. 3 can be engaged into a considerably
large set of different periodic activities. It is not
clear which part of all possible in this net periodic
states was discovered in our simulation. As it follows from comparison between short and extended
stimuli paradigm, the number of different periodic
states found increases with increasing range of input stimuli. Certainly, this increase must saturate
somewhere. This is because any two different periodic regimes are represented by their cyclic trajectories, which has no common points (instantaneous
states). On the other hand, the total number of
instantaneous states the network can have is finite

20

9

Testing of Information Condensation

due to finiteness of the set of states of each element
the network is composed of.
The number of periodic states found in a net
depends on the net’s geometric size. Variations
in the net’s size display themselves exclusively in
variations of interneuronal propagation delays d
and D. On the other hand, the duration of neuronal internal memory, τ , is the same for net of any
size. Thus, it is namely the relationships between
the times an impulse spends for travelling between
neurons, and time it is allowed to spend in a neuron waiting for additional impulses, which controls
possible number of periodic states.
It is worth noticing that each net has one completely synchronized periodic state. The completely synchronozed state is stable and achieved
during finite time. This is in the contrast to the
case of pulse-coupled oscillators with delayed excitatory coupling, (see, e.g. Ref. [40]).
In the four-dimensional set of stimuli we used,
the neighbouring stimuli differ from eachother by
one dt in one of four dimensions. This can be
treated as analogous representation of some reality.
The set of periodic states should be considered as
a set of discrete entities due to qualitative difference between any two states. This conforms with
a paradigm discussed in cognitive physiology, [21].
The process of transformation of initial analogous
inputs into a discret set of periodic states implies a
loss of information and can be treated as condensation of information.
If we take a set of input stimuli, any of which
leads to the same periodic state, then that periodic
state can be considered as an abstract/conceptual
representation of a feature, which all stimuli from
the set have in common, and the corresponding set
could be named as “conceptual domain”. What
kind of feature or concept does the conceptual domain represent? If our net was trained to recognize
a certain real feature, then it would be that feature.
In the context of this study, the common feature is
that all stimuli from the conceptual domain engage
namely this net into namely this periodic dynamics.
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

1

44

45
0

2

1

349

4

5

5

6

6

8

7

125

2

9

8

262

42

8

10

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45
0

42

349

125 262

300
42

8

10

273

11

12

12

13

2

15

13

42

8

14

172 182

125
0

17
18
19

41

352

133

302

20

262

133

270

41
256 348

203

22

212

178

202 92

24
25
26

27

201

259

301

40

178

27

28
178

295

301

31

297

32
33

264

34

221

35

220

36

221

265
264

36

221

256

35

270

220

37

221

38

264

41

270

42

44

295

45 358

330

178

265
266
267

228

268
269
270

43
44

270

270

264

270

42
270

35

265

256

295

41

267

269
256

267
266
92

40

266

268

43

259

39

265

40

269
268

30

35

39

260

266

34

38

259

29

267

33

37

28

268

269

92

32

35

23

178
40

26

31

273

41 133

92

21

270

42

371

350

19

201

25

354

18

23

125

172 182
356

17

35

22

300

15
16

270

21

24

273

133

41

20

262

42

8

14
42

371

16

30

4

9

273

11

29

3

3

4

7

2

1
2

42

3

45 132

295

256

0

177

178

270
170

228

178 181 186 190 193 276 275 274 240 239

Figure 9: 2-dimensional cross-section of the 4dimensional space of inputs for network #9 in

extended stimuli paradigm. left, cross-section
is made with plane (t3 = 23, t4 = 23), right,
(t3 = 21, t4 = 23). Origin for (t1 , t2 ) is in
the upper left corner. Both t1 and t2 run
through the set {1, 2, . . . , 45} of values. Numbers in polygons indicate serial numbers of corresponding periodic states.
It would be interesting to have a look on
the topology of the conceptual domains in the 4dimensional space of all stimuli. For this purpose we figured out 2-dimensional cross-section
of the input stimuli (see Fig. 9). In the crosssection, a typical conceptual domain is composed
of several coherent clusters disconnected with eachother. The histogram of sizes of conceptual domains found is given in Fig. 7.
Decomposition of the whole set of input stimuli
into a number of conceptual domains represented
by corresponding periodic dynamics resembles approach in analyses of multivariate datasets, see review in [14]. The difference is that here the 4dimensional dataset (a conceptual domain) is represented by unidimensional cyclic trajectory, which
corresponds to the domain, and the trajectory is
composed of points/vectors, which have other dimension than the dataset vectors (see Fig. 4).
Nevertheless, having the network, the whole cyclic
trajectory can be reproduced starting from its any
single point. Thus, here the datasets are reduced
down to individual points. This is in concordance
with the information condensation idea.
Why do we stick ourselves with namely the periodic dynamics? The answer is related to the
memory/learning problem, even if we do not consider any plasticity in this study. It is known [3]
that modification of synaptic strength may happen due to repetitive delivery of impulses to those
synapses. Periodic dynamical states are just well
suited for such repetitive delivery. All other dynamical behaviors are of transient type, and have
less chances to cause plastic changes in biological
network. On the other hand, successful perception
expects ability to report about what was perceived,
which is impossible without memory.
Unfortunately, we cannot draw this biological
analogy too far. Real biological network includes
not only excitatory, but about 4% of inhibitory
neurons, [18]. Representing this characteristic in
a model network requires to have at least 25 neurons in it. This dramatically increases the number
of possible external stimuli, which requires a qualitative change in our approach as regards the speed
of simulations and analysis of obtained data.
Finally, what happens if we use another neuronal model in the network? Our opinion is that results will be qualitatively similar. Using the binding neuron here is natural, since it represents in

10

A. Vidybida

refined form what a spiking neuron does with signals it receives. Additionally, the BN model easily
allows to develop a program operating in whole
numbers. This excludes possible dynamical artefacts due to rounding errors.
In future work, it would be interesting to compare results, if another spiking neuron model is
used in the network, to study the topology of
conceptual domains and how the topology could
change if a plasticity is introduced in the network
model.

5

Conclusions

A network composed of spiking neurons is able to
condense information due to the fact that different initial stimuli could lead the network to the
same periodic dynamics. This happens by means
of initial/basic condensation of information in spiking neurons, as it is described in n. 2.1.12.1.1,
above. The network’s geometric size, which determines the interneuronal transmission delays, has
considerable influence on the net’s ability to condense information, mainly due to influence on the
number of different periodic states the network can
have, see Fig. 5. The latter has influence on the
amount of information, which should be ascribed
to a single periodic state, see Fig. 8. As a result, the degree of information condensation varies
between 9 and 41 000, see n. 3.23.2 for details.
The networks considered here are too primitive to have reliable biological implications. At the
same time, numerical parameters, see Table 1 and
n. 2.22.2, such as network sizes and spike propagation velocity, are taken corresponding to biological
data. The threshold value 4 does not contradict
to biological reality, as experimentally registered
thresholds are between 1 and 300. The BN internal memory duration, τ , is commensurable with
halfdecay time of the excitatory postsynaptic potentials (EPSP). Thus, in the framework of this
extremely simple model, one could expect that the
ability of biological neural network to condense information should depend on its geometric size, or
on the relationships between interneuronal transmission delays and the EPSP halfdecay time.
Acknowledgments This work was supported
by the Program of basic research of the National
Academy of Science of Ukraine.
Content of this work was partially published
in an abstract form in the abstract book of
the 2nd International Biophysics Congress and
Biotechnology at GAP & 21th National Biophysics
Congress, (5-9 Oct. 2009) Diyarbakır, Turkey,
http://www.ibc2009.org/

References
[1] Acharya, R., Chua, E.C.P., Chua, K.C.,
Min, L.C., and Tamura, T. (2010), Analysis and Automatic Identification of Sleep
Stages using Higher Order Spectra, International Journal of Neural Systems, 20:6.
[2] J. Bullier, “Integrated model of visual processing,” Brain Res. Rev., 36, 96–107 (2001).
[3] D. V. Buonomano and M. M. Merzenich,
“Cortical plasticity:
from synapses to
maps,” Annu. Rev. Neurosci., 21, 149–186
(1998).
[4] P. Cariani, “Temporal codes, timing nets,
and music perception,” J. New Music Res.,
30, 107–135 (2001).
[5] A. R. Damasio, “The brain binds entities and
events by multiregional activation from convergence zones,” Neural Comput., 1, 123–
132 (1989).
[6] P. Duchamp-Viret and A. Duchamp, “Odor
processing in the frog olfactory system,”
Prog. Neurobiol., 53, 561–602 (1997).
[7] R. Durbin and G. Mitchison, “A dimension
reduction framework for understanding cortical maps,” Nature, 343, 644–647 (1990).
[8] R. Eckhorn, R. Bauer, W. Jordan, M.
Brosch, W. Kruse, M. Munk and H. J. Reitboeck, “Coherent oscillations: a mechanism
for feature linking in the visual cortex?,”
Biol. Cybern., 60, 121–130 (1988).
[9] A. K. Engel, P. König, A. K. Kreiter, C.
M. Gray and W. Singer, “Temporal coding
by coherent oscillations as a potential solution to the binding problem: physiological evidence,” In Schuster, H.G., Singer,
W. (ed), Nonlinear Dynamics and Neuronal
Networks, 3–25. VCH Weinheim (1991).
[10] J. Feldman, “Ecological expected utility and
the mythical neural code,” Cognitive Neurodynamics, 4, 25–35 (2010).
[11] Ghosh-Dastidar, S. and Adeli, H. (2007), Improved Spiking Neural Networks for EEG
Classification and Epilepsy and Seizure Detection, Integrated Computer-Aided Engineering, Vol. 14, No. 3, pp. 187-212.
[12] S. Ghosh-Dastidar and H. Adeli, “Spiking
neural networks,” Intern. J. Neural Sys. 19,
295–308 (2009).
[13] S. Ghosh-Dastidar and H. Adeli, A New
Supervised Learning Algorithm for Multiple
Spiking Neural Networks with Application in
Epilepsy and Seizure Detection, Neural Networks 22, 1419–1431 (2009).

11

Testing of Information Condensation

[14] A. Gorban and A. Zinovyev, “Principal manifolds and graphs in practice: from molecular biology to dynamical systems,” Intern. J.
Neural Sys., 20, 219–232 (2010).
[15] H. Haken, “Brain Dynamics. Synchronization and Activity Patterns in Pulse-Coupled
Neural Nets with Delays and Noise,”
Springer, Berlin 2007.
[16] J. Iglesias and A. E. P. Villa, “Emergence
of preferred firing sequences in large spiking neural networks during simulated neuronal development,” Intern. J. Neural Sys.,
18, 267–277 (2008).
[17] Johnston, S.P., Prasad, G.,Maguire, L. and
McGinnity, T.M. (2010), An FPGA Hardware/software co-design methodology - towards evolvable spiking networks for robotics
application, Intern. J. Neural Sys., 20:6.
[18] E. R. Kandel, J. H. Schwartz and T. M. Jessell, “Principles of Neural Science,” (Fourth
ed.), New York: McGraw-Hill (2000).
[19] H. Kirchner and S. J. Thorpe, “Ultra-rapid
object detection with saccadic eye movements: visual processing speed revisited,”
Vision Res., 46, 1762–1776 (2006).
[20] P. König, A. K. Engel and W. Singer, “Integrator or coincidence detector? The role of
the cortical neuron revisited,” Trends Neurosci., 19, 130–137 (1996).
[21] P. König and N. Krüger, “Symbols as selfemergent entities in an optimization process
of feature extraction and predictions,” Biol.
Cybern., 94(4), 325–334 (2006).
[22] D. M. MacKay and W. S. McCulloch, “The
limiting information capacity of a neuronal
link,” Bull. Math. Biophys., 14, 127–135,
(1952).
[23] D. M. MacKay, “Self-organization in the
time domain,” In M. C. Yovitts, G. T. Jacobi
and G. D. Goldstein (ed), Self-Organizing
Systems, Spartan Books, Washington, 37–48
(1962).

[27] M. T. Scharf, N. H. Woo, K. M. Lattal, Z.
Young, P. V. Nguyen and T. Abel, “Protein
synthesis is required for the enhancement of
long-term potentiation and long-term memory by spaced training,” J. Neurophysiol., 87
2770–2777 (2002).
[28] Schliebs, S., Kasabv, N., and Defoin-Platel,
M. (2010), On the Probabilistic Optimization of Spiking Neural Networks, Intern. J.
Neural Sys., 20:6.
[29] C. E. Shannon, “A mathematical theory of
communication,” Bell System Technical J.,
27, 379–423 and 623–656, July and October,
(1948).
[30] Soltic and S. Kasabov, N. (2010), Knowledge
extraction from evolving spiking neural networks with rank order population coding ,
Intern. J. Neural Sys., 20:6.
[31] Strain, T.J., McDaid, L.J., Maguire, L.P.,
and T.M. McGinnity, T.M. (2010), An
STDP Training Algorithm for a Spiking Neural Network with Dynamic Threshold Neurons, Intern. J. Neural Sys., 20:6.
[32] S. Tang, R. Wolf, S. Xu and M. Heisenberg,
“Visual pattern recognition in Drosophila is
invariant for retinal position,” Science, 305,
1020–1022 (2004).
[33] S. Thorpe, D. Fize and C. Marlot, “Speed
of processing in the human visual system,”
Nature, 381, 520–522 (1996).
[34] A. K. Vidybida, “Neuron as time coherence
discriminator,” Biol. Cybern., 74, 539–544
(1996).
[35] A. K. Vidybida, “Information processing in a pyramidal-type neuron,” Proc.
BioNet’96
Biologieorientierte Informatik
und pulspropagierende Netze, 3-d Workshop.
Ed.: Heinz G., pp. 96–99. (1996).
[36] A. K. Vidybida, “Inhibition as binding controller at the single neuron level,” BioSystems, 48, 263–267 (1998).

[24] Nichols, E., McDaid, L.J., and Siddique,
N.H. (2010), Case Study on Self-organizing
Spiking Neural Networks for Robot Navigation, Intern. J. Neural Sys., 20:6.

[37] A. K. Vidybida, “Output stream of binding
neuron with instantaneous feedback,” Eur.
Phys. J. B, 65, 577–584 (2008); Eur. Phys.
J. B, 69, 313 (2009).

[25] Ch. L. Passaglia and J. B. Troy, “Information transmission rates of cat retinal ganglion cells,” J. Neurophysiol., 91, 1217–1229
(2004).

[38] M. Wehr and G. Laurent, “Odor encoding
by temporal sequences of firing in oscillating neural assemblies” Nature, 384, 162–166
(1996).

[26] M. Rudolph and A. Destexhe, “Tuning neocortical pyramidal neurons between integrators and coincidence detectors, ” J. Comput.
Neurosci., 14, 239–251 (2003).

[39] B. Widrow, “Generalization and information
storage in networks of adaline ‘neurons’ ” in:
Yovitz,M.C., Jacobi,G.T., Goldstein,G.(ed),
Self-Organizing Systems, 435–461 (1962).

12

A. Vidybida

[40] W. Wu and T. Chen, “Impossibility
of asymptotic synchronization for pulsecoupled oscillators with delayed excitatory

coupling,” Intern. J. Neural Sys., 19, 425–
435 (2009).

number of domains

5
4
3
2
1
0
1

10

100
1000
10000
size of conceptual domain

100000

1e+06

