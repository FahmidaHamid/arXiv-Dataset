The mathematics of the genetic code reveal that frequency recurrence leads to
nonlinear scaling in the DNA codon distribution of Homo sapiens
Bohdan B. Khomtchouk∗

arXiv:1402.4095v3 [q-bio.GN] 5 Jan 2015

University of Miami Miller School of Medicine
Center for Therapeutic Innovation and Department of Psychiatry and Behavioral Sciences
1120 NW 14th ST Suite 1463, Miami, FL, USA 33136
The nature of the quantitative distribution of the 64 DNA codons in the human genome has been
an issue of debate for over a decade. Some groups have proposed that the quantitative distribution
of the DNA codons ordered as a rank-frequency plot follows a well-known power law called Zipf’s
law. Others have shown that the DNA codon distribution is best fitted to an exponential function.
However, the reason for such scaling behavior has not yet been addressed. In the present study,
we demonstrate that the nonlinearity of the DNA codon distribution is a direct consequence of the
frequency recurrence of the codon usage (i.e., the repetitiveness of codon usage frequencies at the
whole genome level). We discover that if frequency recurrence is absent from the human genome,
the frequency of occurrence of codons scales linearly with the codon rank. We also show that DNA
codons of both low and high frequency of occurrence in the genome are best fitted by an exponential
function and provide strong evidence to suggest that the coding region of the human genome does
not follow Zipf’s law. Information-theoretic methods and entropy calculations are applied to the
DNA codon distribution and a new approach, called the lariat method, is proposed to quantitatively
analyze the DNA codon distribution in Homo sapiens.
Keywords: Genetic code, information theory, entropy, nonlinearity, exponential scaling, power law scaling,
Zipf’s law, mathematical genetics, computational genomics

INTRODUCTION

From the days of its inception, information theory has
served to explain how communicative systems function.
The mathematician Claude E. Shannon launched the
field of information theory with his seminal 1948 paper, A
Mathematical Theory of Communication [1], which was
later published as a book The Mathematical Theory of
Communication [2]. The theory laid the groundwork for
the development of data compression and storage methods (e.g., ZIP files, JPEGs, MP3s), channel coding (e.g.,
DSL), national security measures (e.g., cryptographically
secure ciphers), and various other commercial applications (e.g., seismic oil exploration). The applications of
information theory have also expanded into various academic fields such as quantum computing, neurobiology,
linguistics, and ecology.
The application of information theory to the study
of biological phenomena dates back to the 1970s [3–5],
where it was used to obtain quantitative measures such
as redundancy and divergence of DNA sequences [6].
A resurgence of the application of information-theoretic
methods to the study of DNA sequences was prompted
by the availability of sequence data in publicly available
online databases from the late 1980s to the late 1990s and
early 2000s [7–12]. More recently, information theory has
reappeared in the genetics and genomics field in the study
of alignment-free DNA sequence analysis and comparison, genome entropy estimation, and the identification of
allergens in sequenced genomes [13, 14]. The resurgence
of the use of information-theoretic methods in genetics
and genomics is predicted to infuse promising results into
next-generation sequencing projects and gene mapping,
metagenomics, and communication theory-based models

of information transmission in organisms [13].
In this article, we build a communication theory-based
model of the DNA genetic code as a communicative system, where the speaker is modeled as the 64 codons and
the receiver is modeled as the 20 amino acids and a stop
signal. In information theory terminology, the codons
are the signals and the amino acids and stop signal are
the objects. The 64 signals comprise the entirety of the
lexicon and the 21 objects are the targets to be mapped
into from this list of signals through the communicative
channel, the RNA intermediate.

BACKGROUND AND METHODS

There has been a considerable range of analytical approaches [9, 13, 15–39] performed to investigate the statistical and scale invariant features of long-range correlations in DNA to gain insight into questions such as
whether or not the rank-frequency distribution of the
codons follows a power law scaling law known as Zipf’s
law [40]:

f∝

1
rα

(1)

where f is the frequency, r is the rank, and α is a statistical scaling coefficient that is classically seen to be ≈ 1
for many sources examined, such as texts [41], where a
text can be a string of DNA nucleotides.
We analyze Homo sapiens data from the Codon Usage Database and known amino acid residue frequencies
sampled from the primary structures of 207 unrelated
proteins of known sequence [42, 43].

2
(B)

ln(p(si ))

(A)

(C)

0

0

0

−2

−2

−2

−4

−4

−4

−6

−6

−6

−8

0

20

40

60

−8

0

20

Rank r

40

60

−8

0

Rank r

20

40

60

Rank r

FIG. 1: Panel A: Linear regression analysis of exponential fit data shows that linear regression analysis over the entire set
(64 codons) of the lexicon of DNA codons demonstrates the superiority of exponential scaling (R2 =0.9454) in fitting DNA
sequences as compared to power law scaling (R2 =0.6823, Panel A: Fig. 3). Panel B: Linear regression analysis over a subset of
the lexicon size of DNA codons (r=1, . . . , 32) shows the superiority of exponential fits at data of low rank (R2 =0.9074) over
power law fits to the respective data (R2 =0.8707, Panel B: Fig. 3). Panel C: Linear regression analysis over a subset of the
lexicon size of DNA codons (r=33, . . . , 64) also shows the superiority of exponential fits at data of high rank (R2 =0.9403) over
power law fits to the respective data (R2 =0.9114, Panel C: Fig. 3).

Let p(si ) be the probability of use of a specific codon si
(i=1, . . . , 64), generally for any amino acid or stop signal
rj (j=1, . . . , 21):
p(si ) =

X

p(si , rj )

(2)

RESULTS AND DISCUSSION

We graph p(si ) versus the rank r and examine the nature of both a power law fit and an exponential fit to the
data (Fig. 2).

j

DNA codon frequency distribution

where p(si , rj ) is the probability of using si for rj [44]:
(3)

That is, p(si , rj ) is the probability of si mapping onto rj ,
p(rj ) is the probability of use of amino acid rj in protein
sequences, and p(si |rj ) is the probability of using codon
for amino acid rj .
The frequency of occurrence of a codon can be expressed in terms of a probability:

data
power law fit
exponential fit

0.1
8 · 10−2
p(si )

p(si , rj ) = p(rj )p(si |rj )

6 · 10−2
4 · 10−2

L(si )
(4)
L
where L(si ) is the frequency of occurrence of the specific
codon si , and L is the total frequency of occurrence of
all the 64 codons in the lexicon. Therefore:
p(si ) =

64
X

p(si ) = 1

(5)

i=1

In the linear regression analysis, least squares fitting is applied using a slope no-intercept model to logtransformed data to examine the respective power law
and exponential fits to the DNA codon distribution,
where the fits are conducted with a one parameter model
using least squares.

2 · 10−2

10

20

30
40
Rank r

50

60

FIG. 2: Probability of use of a specific codon si as a function
of rank demonstrates frequency of codon usage across the entire distribution spectrum of the lexicon L, comprised of the
64 DNA codons. Power law (R2 =0.6859) and exponential distributions (R2 =0.9489) are fitted to the data. Exponential fit:
p(si ) = 0.0503e−0.055r Power law fit: p(si ) = 0.1176r −0.811

We discover that the value of the scaling coefficient (6)

3

log(p(si ))

(A)

(B)

(C)

0

0

0

−1

−1

−1

−2

−2

−2

−3

−3

−3

−4

0

0.5

1

1.5

2

−4

0

0.5

log(Rank r)

1
log(Rank r)

1.5

2

−4

0

0.5

1

1.5

log(Rank r)

FIG. 3: Panel A: Linear regression analysis of power law data shows that linear regression analysis over the entire set (64
codons) of the lexicon of DNA codons demonstrates the superiority of exponential scaling (R2 =0.9454, Panel A: Fig. 1) in
fitting DNA sequences as compared to power law scaling (R2 =0.6823). Panel B: Linear regression analysis over a subset of the
lexicon size of DNA codons (r=1, . . . , 32) shows the superiority of exponential fits at data of low rank (R2 =0.9074, Panel B:
Fig. 1) over power law fits to the respective data (R2 =0.8707). Panel C: Linear regression analysis over a subset of the lexicon
size of DNA codons (r=33, . . . , 64) also shows the superiority of exponential fits at data of high rank (R2 =0.9403, Panel C:
Fig. 1) over power law fits to the respective data (R2 =0.9114).

is α ≈ 0.8, in significant contrast to Zipf’s law where
α ≈ 1 for texts and non-technical natural languages
[41, 45], and the power law fit (R2 = 0.6859) is considerably weaker than the exponential fit (R2 = 0.9489).
Therefore, these results strongly suggest that Zipf’s law
is not applicable to the coding regions of the human
genome. The findings support [38, 39, 46] that exponential scaling captures the nature of the DNA codon
distribution much more accurately than power law scaling, as evidenced in this analysis by the coefficient of
determination, R2 . A prior study [38] also demonstrated
that the DNA codon frequency distribution as a function of rank r of Homo sapiens (Fig. 2), as well as other
eukaryotic species, is best modeled as the sum of an exponential function, a linear function, and a constant, not
Zipf’s law, i.e.:
f (r) = αe−ηr − βr + γ

(6)

Where α, η, β, and γ are scaling factors that are constant depending on the biological species under analysis.
Furthermore, we extend our analysis to examine highranked codons and low-ranked codons with a linear regression approach to determine whether an exponential
fit over the respective region of the DNA codon distribution provides a better fit for DNA sequences. We find
that exponential fits (Fig. 1) provide a better representation of DNA codons of lower rank (r=1, . . . , 32) as well as
DNA codons of higher rank (r=33, . . . , 64) than do the
respective power law fits (Fig. 3), as shown by the higher
respective R2 values. Hence, we validate that exponential scaling behavior best governs the distribution of the
lexicon size of the DNA codons across both low and high

ranks. We also conclude from the coefficients of determination obtained in the linear regression analysis that the
exponential fit over the entire DNA codon distribution
is better equipped to capture the global topology of the
frequency distribution than is the power law fit (Fig. 1,
Fig. 2, Fig. 3).
Next, we visualize the lexicon as a function of rank (low
versus high) to create an intuitive handle for the number of signals (codons) of a certain rank that are present
within an entire lexicon (e.g., how many codons of a specific frequency exist in the lexicon, where the lexicon is
defined as the 64 DNA codons). This new approach we
introduce, which we call the lariat method, poses a natural improvement over existing methodologies to quantify
rank based on the frequency when one is interested at
examining the distribution of all the different frequencies of the DNA codons at the whole-genome level. Most
prior studies were designed such that DNA codons of recurrent frequency are assigned different rank numbers,
where if two codons have the same frequency of occurrence they belong to two different ranks, one following
the other sequentially [39]. Another study investigating a
physical phenomenon in a different academic field wholly
unrelated to DNA also assigned sequential ranks to recurrent frequencies [47]. The alternative and physically
more meaningful scenario to examine a rank-frequency
distribution is to gather the codons into bins of different recurrent frequencies and then assign rank numbers
to each bin, where one bin may contain multiple codons.
This procedure partitions the lexicon size of the 64 DNA
codons into non-overlapping subsets of signals, or codons,
of recurrent frequency:

2

4
DNA codon lariat frequency distribution

{L} =

[

L(r)

0.12

(7)

binned data
power law fit
exponential fit
linear fit

r

Binned p(si )

0.1
8 · 10−2
6 · 10−2
4 · 10−2
2 · 10−2

5

10

15

20

Rank r

FIG. 4: If the frequency recurrence of codons is omitted, the frequency of occurrence of codons scales linearly
with the rank. Any nonlinear deviations from linear scaling are the result of recurrent codon frequencies. Power
law (R2 =0.6122), exponential (R2 =0.8537), and linear fits
(R2 =0.9506) are applied to this binned data. Exponential fit:
p(si ) = 0.0813e−0.161r Linear fit: p(si ) = −0.0024r + 0.0462
Power law fit: p(si ) = 0.1225r −0.992

·10−2

Residuals plot of lariat data

5

Residual

It has been shown that power-law fits and exponential fits that connect frequency to rank as applied to the
codon distribution have been promising sources of fit to
DNA sequences [10, 39, 46]. Observations that exponential fits have consistently lower χ2 [39] than power-law fits
and, hence, provide a better fit [46] for DNA sequences
have opened interesting new questions about the nature
of rank-frequency distributions and the parameters that
govern them.
By employing the lariat method, where codons of recurrent frequency are binned into a single rank value
(Fig. 4), we discover that the recurrence of the codon
usage explains the nonlinearity of the spatial distribution of the DNA codons. If the recurrence is omitted,
as performed in this binning procedure, p(si ) scales linearly with the rank r. We verify from the residuals plot
(Fig. 5) that the linear fit residuals are distributed randomly about zero, signifying that, apart from random
uncertainty, the linear model correctly predicts the data.
On the contrary, the exponential and particularly the
power law fit residuals show systematic, non-random deviation of the data from the respective models. It should
be noted that the number of bins in the lariat method is
uniquely determined based upon how many unique codon
frequencies exist in the codon pool of the organism under study. For example, as an extreme hypothetical case
of an organism that possesses only three codons, two of
which occur at the same frequency, the lariat method
would establish two bins: one bin (containing one codon)
for the unique codon frequency, and one bin (containing
two codons) for the other codon frequency. As such, the
lariat method can be easily extended from studies in humans to other biological organisms.
The linearity of the data becomes even more pronounced if the first-ranked codon is omitted. This phenomenon raises interesting questions regarding the biological utility of nonlinear scaling in the genetic code.
The data suggests that the degree of nonlinear, exponential scaling observed in a DNA codon distribution is
directly determined by the degree of recurrence of the
frequency of the codon usage within the codon lexicon,
for any species, not just H. sapiens. The evolutionary
implications of this are rather interesting, considering
that a divergence away from a linear rank-frequency DNA
codon distribution allows for less frequent, or less popular, codons (i.e., high rank) in the genome to occur with
close to the same frequency as more popular codons (i.e.,
low rank). Examining the potential evolutionary pressure that may have driven the genetic code to exhibit
this kind of scaling phenomena is a subject of future research.
We indeed see from the original dataset (Fig. 2) that
the frequency recurrence of the codon usage causes the
shape of the codon distribution to assume a nonlin-

10

15

20

−2

−4

−6

power law residuals
exponential residuals
linear residuals

Rank r

FIG. 5: Linear fit residuals are randomly distributed about
zero showing no systematic, non-random deviation of the original dataset (Fig. 2) from the linear fit model, in contrast to
the exponential and power law models.

ear form, where our linear regression approach has revealed strong linearity in rank-frequency dependency of
the DNA codons when their frequency recurrence is controlled for by the lariat method (Fig. 4). As such, a highranked codon does not occur with a much lower frequency
than a low-ranked codon, as would occur if the data (the

5
DNA codon rank-frequency distribution) was scaled linearly. Therefore, the exponential scaling seems to serve
a certain buffering capacity, the biological significance of
which is now an open question. We subsequently followed
up this result with an entropy calculation on the original
dataset (Fig. 2) to establish a quantitative measure evaluating the tendency of the 64 codons to roughly evenly
distribute across the genome with respect to frequency:

H(S) = −

N
X

p(si ) logN (p(si )) = 0.926

(8)

i=1

In a hypothetical scenario where all the DNA codons are
evenly distributed with respect to frequency regardless
of the rank (i.e., all the codons occur with the same frequency), the entropy, H(S), of such a system is unity.
A rigorous mathematical proof of this result is demonstrated in Appendix; however, this result can also be
arrived to using Lagrange multipliers. It is a well-known
fact that the unique probability distribution having maximum entropy is the uniform distribution; therefore, the
Appendix serves merely to demonstrate a new proof of a
famous information-theoretic result as applied to the human genome. As the hypothetical scenario of a uniformly
distributed codon frequency is clearly not applicable to
the human genome, an entropy value of this magnitude
(≈ 1) suggests the presence of biological mechanisms that
ensure that even though certain codons are more prevalent than others in quantity due to frequency recurrence,
the spatial distribution of the DNA codons in the genome
behaves as if to mask this effect.

CONCLUSION

We provide new information-theoretic analyses which
strongly suggest that the coding region of the human
genome does not behave according to Zipf’s law. We
prove that if the 64 DNA codons of the human genetic
code are not equiprobable, then the entropy, H(S), of the
genetic code is less than unity. We also discover that any
deviation away from linear rank-frequency DNA codon
scaling is a consequence of the recurrence of the frequency
of the codon usage. Hence, we show that the reason for
the existence of exponential scaling in the human genome
is the repetitiveness in the frequency of usage of different codons. We show that if frequency recurrence were
to be absent from the human genome, the frequency of
occurrence of codons would scale linearly with the codon
rank, instead of exponentially. This linear vs. exponential scaling dichotomy creates interesting open questions
regarding the potential evolutionary driving force which
has led to the rise of the preference of exponential scaling
in the DNA genetic code, as opposed to other kinds of
scaling (e.g., linear scaling or Zipfian scaling). We leave
the potential evolutionary implications of our findings as
a subject of future research to experts in the field.

ACKNOWLEDGMENTS

BBK wishes to acknowledge the support of the Department of Defense (DoD) through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program. BBK wishes to thank Wolfgang Nonner for useful
discussions and scientific guidance, and Claes Wahlestedt, Georges St. Laurent III, Seth J. Schwartz, and Hemant Ishwaran for critical review and helpful comments
on the manuscript.

6
Appendix A: Mathematical Proofs

Theorem 1. If the 64 DNA codons of the human genetic code are equiprobable, then the entropy, H(S), of the genetic
code is unity.
Proof. Let each of the 64 DNA codons that comprise the human genetic code to occur with the same frequency. Then
it follows from Eq.(4) that, for all codons si in the genetic code, p(si )=1/N where N = 64. From Eq.(8) it follows
that:
H(S) = −

N
X

p(si ) logN (p(si )) = −

N
X

p(si )

i=1

i=1

N
N
X
ln(p(si ))
1 ln(1/N )
1 X − ln(N )
=−
=−
=1
ln(N )
N ln(N )
N i=1 ln(N )
i=1

(9)

Theorem 2. If the 64 DNA codons of the human genetic code are not equiprobable, then the entropy, H(S), of the
genetic code is less than unity.
Proof. Suppose there exists one DNA codon in the human genetic code that does not occur with the same frequency
as the other 63 codons. Let ǫ > 0. Then for some given si , it follows from Theorem(1) that p(si )=1/N + ǫ and:
H(S) = −

N
X
i=1

p(si )

(1/N + ǫ) ln(1/N + ǫ) (1/N − ǫ) ln(1/N − ǫ) (1/N ) ln(1/N )
ln(p(si ))
=−
+
+
+ ···
ln(N )
ln(N )
ln(N )
ln(N )
!
(1/N ) ln(1/N )
+···+
ln(N )

(10)

where if p(si ) changes by +ǫ for one codon, it is necessarily true that p(si ) will change by −ǫ for some other codon.
Considering the polarity of the entropy definition, to prove this theorem it must be demonstrated from (10) that:

H(S) < −

64(1/N )(ln(1/N ))
ln(N )

(11)

To show (11) we proceed directly from (10):

H(S) = −

(1/N + ǫ)[ln(1/N ) + ln(1 + N ǫ)] (1/N − ǫ)[ln(1/N ) + ln(1 − N ǫ)] (1/N ) ln(1/N )
+
+
+ ···
ln(N )
ln(N )
ln(N )
!
(1/N ) ln(1/N ) ? 64(1/N )(ln(1/N ))
<−
+ ···+
ln(N )
ln(N )

(12)

Subtracting the p(si ) = 1/N terms from both sides:
(1/N + ǫ)[ln(1/N ) + ln(1 + N ǫ)] (1/N − ǫ)[ln(1/N ) + ln(1 − N ǫ)]
−
+
ln(N )
ln(N )

!

?

<−

2(1/N )(ln(1/N ))
ln(N )

(13)

Cancelling the polarities and the ln(N ) terms on both sides of the equation:


 ?
(1/N + ǫ)[ln(1/N ) + ln(1 + N ǫ)] + (1/N − ǫ)[ln(1/N ) + ln(1 − N ǫ)] < 2(1/N )(ln(1/N ))

(14)

Expanding out the equation, combining like terms, and cancelling on both sides leads to:
?
1
1
ln(1 + N ǫ) + ǫ ln(1 + N ǫ) + ln(1 − N ǫ) − ǫ ln(1 − N ǫ) < 0
N
N

Regrouping terms leads to:

(15)

7

?

ln(1 + N ǫ) + ln(1 − N ǫ) + N ǫ(ln(1 + N ǫ) − ln(1 − N ǫ)) < 0

(16)

Since ǫ > 0 is arbitrarily small, it follows that:
?

(17)

ln(1 + N ǫ) + ln(1 − N ǫ) < 0
Employing the series expansion definition of ln(1 + x):
1
1
1
ln(1 + x) = x − x2 + x3 − x4 + · · · f or − 1 < x < 1
2
3
4

(18)

Subsequent substitution and algebra yields:
1
1
1
(N ǫ)2k
ln(1 + N ǫ) + ln(1 − N ǫ) = −(N ǫ)2 − (N ǫ)4 − (N ǫ)6 − (N ǫ)8 − · · · −
2
3
4
k

(19)

Hence it has been proven that for all n ≥ k:

ln(1 + N ǫ) + ln(1 − N ǫ) = −

n
X
(N ǫ)2k
k=1

k

<0

(20)

Therefore we have proven (11) which proceeds directly from (10). This completes the proof.

Electronic address: b.khomtchouk@med.miami.edu
[1] C.E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 27, 1948.
[2] C.E. Shannon. The Mathematical Theory of Communication. University of Illinois Press, 1949.
[3] Information Theory and the Living System. Columbia
University Press, 1972.
[4] T.A. Reichert, D.N. Cohen, and A.K.C. Wong. An application of information theory to genetic mutations and the
matching of polypeptide chains. Journal of Theoretical
Biology, 1973.
[5] S. Guiasu.
Information Theory With Applications.
McGraw-Hill, 1977.
[6] R. Roman-Roldán, P. Bernaola-Galván, and J.L. Oliver.
Application of information theory to DNA sequence analysis: a review. Pattern Recognition, 29(7), 1996.
[7] S.F. Altschul. Amino acid substitution matrices from
an information theoretic perspective.
J Mol Biol,
219(3):555–65, Jun 1991.
[8] W. Li and K. Kaneko. Long-range correlation and partial 1/f α spectrum in a noncoding DNA sequence. Europhysics Letters, 17(7), 1992.
[9] C.K. Peng et. al. Long-range correlations in nucleotide
sequences. Nature, 356, 1992.
[10] R.F. Voss. Evolution of long-range fractal correlations
and 1/f noise in DNA base sequences. Physical Review
Letters, 68(25), 1992.
[11] I. Grosse et al. Species independence of mutual information in coding and noncoding dna. Physical Review E,
61(5), 2000.
∗

[12] I. Grosse et al. Average mutual information of coding and
noncoding dna. In R.B. Altman, A.K. Dunker, L. Hunter,
K. Lauderdale, and T.E. Klein, editors, Pacific Symposium on Biocomputing 2000, 2000.
[13] S. Vinga. Information theory applications for biological
sequence analysis. Brief Bioinform, Sep 2013.
[14] H.X. Dang and C.B. Lawrence. Allerdictor: fast allergen
prediction using text classification techniques. Bioinformatics, 2014.
[15] C.K. Peng et. al. Mosaic organization of dna nucleotides.
Physical Review E, 49(2), 1994.
[16] M. Borodovsky and S.M. Gusein-Zade. A general rule for
ranged series of codon frequencies in different genomes.
Journal of Biomolecular Structure and Dynamics, 6(5),
1989.
[17] S.V. Buldyrev et al. Long-range fractal correlations in
DNA. Physical Review Letters, 71, 1993.
[18] S.M Ossadnik et al. Correlation approach to identify
coding regions in DNA sequences. Biophysical Journal,
67, 1994.
[19] G.M. Viswanathan et al. Long-range correlation measures for quantifying patchiness: Deviations from uniform power-law scaling in genomic DNA. Physica A, 249,
1998.
[20] G.M. Viswanathan et al. Quantification of DNA patchiness using correlation measures. Biophysical Journal, 72,
1997.
[21] S.V. Buldyrev. Power Laws, Scale-Free Networks, and
Genome Biology. Springer Science+Business Media,
2006.
[22] S.V. Buldyrev et al. On long-range power law correlations
in DNA. Physical Review Letters, 71, 1993.

8
[23] S.V. Buldyrev et al. Long-range correlation properties of
coding and noncoding dna sequences: Genbank analysis.
Physical Review Letters E, 51(5), 1995.
[24] R.N. Mantegna et al. Linguistic features of noncoding
dna sequences. Physical Review Letters, 73(23), 1994.
[25] R.N. Mantegna et al. Systematic analysis of coding and
noncoding DNA sequences using methods of statistical
linguistics. Physical Review E, 52(3), 1995.
[26] B. Audit et al. Long-range correlations in genomic DNA:
A signature of the nucleosomal structure. Physical Review Letters, 86(11), 2001.
[27] R.K. Azad et al. Segmentation of genomic DNA through
entropic divergence: Power laws and scaling. Physical
Review Letters E, 65, 2002.
[28] H.E. Stanley et al. Scaling features of noncoding DNA.
Physica A, 273, 1999.
[29] A. Arneodo et al. Characterizing long-range correlations
in DNA sequences from wavelet analysis. Physical Review
Letters, 74(16), 1995.
[30] P. Allegrini et al. Dynamical model for DNA sequences.
Physical Review E, 52(5), 1995.
[31] H.E. Stanley et al. Scale Invariant Features of Coding
and Noncoding DNA Sequences. Fractal Geometry in Biological Systems: An Analytical Approach. CRC Press,
1996.
[32] M.Y. Azbel. Universality in a DNA statistical structure.
Physical Review Letters, 75(1), 1995.
[33] S. Havlin et al. Statistical and linguistic properties of
DNA sequences. Fractals, 3, 1995.
[34] P. Bernaola-Galván et al. Finding borders between coding and noncoding DNA regions by an entropic segmentation method. Physical Review Letters, 85(6), 2000.
[35] P. Bernaola-Galván, R. Roman-Roldán, and J.L. Oliver.
Compositional segmentation and long-range fractal cor-

[36]
[37]
[38]
[39]
[40]
[41]
[42]

[43]

[44]
[45]
[46]
[47]

relations in DNA sequences. Physical Review E, 53(5),
1996.
X. Lu et al. Characterizing self-similarity in bacteria
DNA sequences. Physical Review E, 58(3), 1998.
H. Herzel and I. Große. Correlations in DNA sequences:
The role of protein coding segments. Physical Review E,
55(1), 1997.
L. Frappat et al. Universality and Shannon entropy of
codon usage. Physical Review E, 68, 2003.
A. Som et al. Codon distributions in DNA. Physical
Review Letters E, 63, 2001.
G. Zipf. Human Behaviour and the Principle of Least
Effort: An Introduction to Human Ecology. AddisonWesley, 1949.
R. Ferrer i Cancho. Zipf’s law from a communicative
phase transition. European Physical Journal B, 47, 2005.
Y. Nakamura, T. Gojobori, and T. Ikemura. Codon usage
tabulated from international DNA sequence databases:
status for the year 2000. Nucleic Acids Research, 28(1),
2000.
M.H. Klapper. The independent distribution of amino
acid near neighbor pairs into polypeptides. Biochemical
and Biophysical Research Communications, 78(3), 1977.
R. Ferrer i Cancho. Least efforts and the origins of scaling
in human language. Proceedings of the National Academy
of Sciences, 100(3), 2003.
A. Czirók et al. Correlations in binary sequences and a
generalized Zipf analysis. Physical Review E, 52, 1995.
A.A. Tsonis, J.B. Elsner, and P.A. Tsonis. Is DNA a
language? Journal of Theoretical Biology, 184, 1997.
G. Martı́nez-Mekler et al. Universality of rank-ordering
distributions in the arts and sciences. PLoS One, 4(3),
2009.

