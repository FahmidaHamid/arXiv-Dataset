Analysis of the stabilized supralinear network‡
arXiv:1202.6670v6 [q-bio.NC] 1 Jul 2013

Yashar Ahmadian1,4,∗, Daniel B. Rubin1,2,∗ & Kenneth D. Miller1−4,†
July 19, 2013
1

Center for Theoretical Neuroscience, 1 Dept. of Neuroscience, 2 Doctoral Program in Neurobiology and Behavior, 3 Swartz Program in Theoretical Neuroscience, and 4 Kavli Institute
for Brain Science, College of Physicians and Surgeons, Columbia University, NY, NY 10032.
∗
: These authors contributed equally to this work.
†
: To whom correspondence should be addressed: ken@neurotheory.columbia.edu.
‡
: This preprint has been published as a peer-reviewed article:
Ahmadian, Y., Rubin, D. B., & Miller, K. D. (2013). Analysis of the stabilized supralinear
network. Neural Computation, 25, 1994–2037. (This preprint differs from the published article in
that here Figs. 1 and 4 are in color, and we have added “k = 0.04” to the caption of Fig. 2.)

Abstract
We study a rate-model neural network composed of excitatory and inhibitory
neurons in which neuronal input-output functions are power laws with a power
greater than 1, as observed in primary visual cortex. This supralinear inputoutput function leads to supralinear summation of network responses to multiple
inputs for weak inputs. We show that for stronger inputs, which would drive the
excitatory subnetwork to instability, the network will dynamically stabilize provided feedback inhibition is sufficiently strong. For a wide range of network and
stimulus parameters, this dynamic stabilization yields a transition from supralinear to sublinear summation of network responses to multiple inputs. We compare this to the dynamic stabilization in the “balanced network”, which yields
only linear behavior. We more exhaustively analyze the 2-dimensional case of 1
excitatory and 1 inhibitory population. We show that in this case dynamic stabilization will occur whenever the determinant of the weight matrix is positive
and the inhibitory time constant is sufficiently small, and analyze the conditions
for “supersaturation”, or decrease of firing rates with increasing stimulus contrast (which represents increasing input firing rates). In work to be presented
1

elsewhere, we have found that this transition from supralinear to sublinear summation can explain a wide variety of nonlinearities in cerebral cortical processing.

Acknowledgements: D.B.R. is supported by NIH training grant T32-GM007367 to the
M.D./Ph.D. training program at Columbia University. Y.A. is supported by a postdoctoral
fellowship from the Kavli Institute for Brain Science at Columbia University. K.D.M. is supported by R01 EY11001 from the NEI of the NIH and by the Gatsby Charitable Foundation
through the Gatsby Initiative in Brain Circuitry at Columbia University.

2

Contents
1 Introduction

4

2 Setup: Equations for the Supralinear Network

6

3 Scaling Argument
3.1 Scaling for small α . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Scaling for large α . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Comparison to the balanced network . . . . . . . . . . . . . . . . . . . . . .

8
9
10
12

4 Reduction to a 2-dimensional system
4.1 Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Conditions for Normalization in the 2-Dimensional System . . . . . . . . . .

15
15
20

5 Analyses of the 2-Dimensional Network
5.1 When Does the Network Dynamically Stabilize? . . . . . . . . . . .
5.1.1 The case of infinitely fast inhibition . . . . . . . . . . . . . .
5.1.2 More general requirements for stability . . . . . . . . . . . .
5.2 The case (−J−1 g)E < 0 and supersaturation . . . . . . . . . . . . .
5.2.1 When can rE or rI decrease with contrast? . . . . . . . . . .
5.2.2 The c at which rE becomes 0 . . . . . . . . . . . . . . . . .
5.2.3 Peak excitatory firing rate and corresponding contrast . . . .
5.3 Steady-state solutions for different parameter regimes . . . . . . . .
5.4 Different criteria for crossover to the sublinearly normalizing regime

21
21
21
22
25
26
27
27
29
36

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

6 Discussion

40

References

42

3

1

Introduction

In work to be presented elsewhere (presented as abstracts in Miller and Rubin (2010); Rubin
and Miller (2010); Miller and Rubin (2011); Rubin and Miller (2011)), we have found that
a large set of response properties of cells in primary visual cortex (V1) and other sensory
cortical areas can be understood from a very simple circuit motif. The response properties
have in common a change in integration with increasing input strength, so that responses
to weak inputs sum supralinearly while those to stronger inputs generically sum sublinearly.
In this paper, we mathematically analyze the model’s behavior.
One set of properties the model can address involves contextual modulation or “surround
suppression”. A visual sensory neuron has a classical receptive field (CRF), corresponding
to the region in which appropriate visual stimuli will drive the neuron’s responses. The size
of the CRF does not change with input strength (Song & Li, 2008). Stimuli outside the
CRF can modulate responses to CRF stimuli, although they cannot drive responses, and
typically are suppressive. However, the nature of the surround influence can vary with input
strength (Sengpiel, Blakemore, & Sen, 1997; Polat, Mizobe, Pettet, Kasamatsu, & Norcia,
1998). A size tuning curve is obtained by centering an effective stimulus on the CRF center
and studying response vs. stimulus radius. The summation field size is the stimulus size
evoking peak response. This summation field size shrinks with input strength, as represented
by stimulus contrast (Sceniak, Ringach, Hawken, & Shapley, 1999; Cavanaugh, Bair, &
Movshon, 2002; Anderson, Lampl, Gillespie, & Ferster, 2001; Shushruth, Ichida, Levitt, &
Angelucci, 2009; Tsui & Pack, 2011). This means that regions of the surround are changing
from facilitating to suppressing with increasing input strength.
Another set of properties involves sublinear summation of the responses to multiple stimuli: the response to two simultaneously presented stimuli can be closer to the average than
the sum of the responses to the stimuli presented individually. We refer to this property as
“normalization”, because it is the most prominent of a set of nonlinear response properties
that have been given that name (reviewed in Carandini and Heeger (2012)). In at least some
cases, this summation becomes supralinear when inputs are weak (Heuer & Britten, 2002;
Ohshiro, Angelaki, & DeAngelis, 2011). If one thinks of surround suppression as representing
the response to simultaneous presentation of a center stimulus that normally by itself evokes
a certain response and a surround stimulus that normally by itself evokes zero response, then
surround suppression can be thought of as an example of sublinear summation. Similarly,
facilitation by the near surround for weak inputs then represents supralinear summation.
As we will show elsewhere, these and other response properties can be understood in
some detail from a simple model. We consider a network of excitatory (E) and inhibitory
(I) neurons, extended across a 1-D or 2-D space. The strengths of each type of connection
– E ⇒ E, E ⇒ I, I ⇒ E, I ⇒ I – fall off as functions of cortical distance. We are guided
4

by previous results that showed that the inhibition received by cells is decreased when they
are being suppressed by a surround stimulus, relative to their response to a CRF stimulus
alone (Ozeki, Finn, Schaffer, Miller, & Ferster, 2009), and correspondingly that the firing of
inhibitory cells, like that of excitatory cells, is suppressed by surround stimuli (Song & Li,
2008). These results led to the conclusion that the E ⇒ E connections must be sufficiently
strong that, when the network is being driven by a CRF stimulus, they would render the
network unstable in the absence of feedback inhibition (Tsodyks, Skaggs, & Sejnowski, 1997;
Ozeki et al., 2009), a conclusion also supported by other work (London, Roth, Beeren,
Hausser, & Latham, 2010). We termed such a network an inhibition-stabilized network or
ISN.
We then add to this the fact that individual neurons have a supralinear, power-law inputoutput function. This is based on intracellular recordings in anesthetized cat primary visual
cortex (V1) that showed that a neuron’s instantaneous firing rate is well described as a
power law function of its instantaneous mean voltage relative to rest (rates and voltages
measured in 30 ms bins) with powers ranging from 2 to 5, and that this holds true over the
entire dynamic range of neuronal response to visual stimuli (N. Priebe, Mechler, Carandini,
& Ferster, 2004; N. J. Priebe & Ferster, 2005, 2006; Finn, Priebe, & Ferster, 2007).1 This
power law relationship is predicted on theoretical grounds when mean input is subthreshold
and spiking is driven by input fluctuations (Miller & Troyer, 2002; Hansel & van Vreeswijk,
2002), as appears to be the case in V1 (Anderson, Lampl, Gillespie, & Ferster, 2000).
Here we mathematically analyze the model. We focus particularly on exposing the origins
of the generic transition in model behavior from supralinear to sublinear summation as input
strength increases. This typically corresponds to a transition from a regime in which the
excitatory network is stable by itself to one in which the excitatory subnetwork by itself
is unstable, but is stabilized by feedback inhibition. Hence we refer to the network as the
stabilized supralinear network or SSN. We also conduct a more detailed analysis of the
2-dimensional case consisting of a single excitatory and a single inhibitory population.
1

We are assuming that mean voltage is linear in the input. Nonlinearities such as spike-rate-adaptation
currents could complicate this picture. We also are ignoring the fact that the power increases with contrast,
because the noise level decreases with contrast (Finn et al., 2007), which yields increasing powers (Miller
& Troyer, 2002; Hansel & van Vreeswijk, 2002). However the picture we describe in this paper primarily
concerns stabilization against the otherwise explosive nonlinearity of a supralinear input-output function.
Thus, the picture should hold so long as the input-output function is supralinear over the cell’s dynamic
range, as expected for fluctuation-driven spiking – the closer the cell is to threshold, the greater the increase
in spiking driven by a given increment of input.

5

2

Setup: Equations for the Supralinear Network



rE
We take r =
to be the N-dimensional vector of neuronal firing rates, ordered so
rI
that the top NE neurons, represented by rE , are all excitatory neurons, and the remaining
NI neurons, represented by rI are inhibitory neurons, NE + NI = N. (We refer to the units
in our model as “neurons”, but, as discussed below, the equations represent average firing
rates and so excitatory or inhibitory units may be better understood as local interconnected
groups of excitatory or inhibitory neurons, over
is taken.) The matrix
 which the average

WEE −WEI
of connections between the neurons is W =
where WXY is the matrix
WIE −WII
of connections from neurons of type Y (E or I) to neurons of type Xand has
 non-negative
hE
entries. The feedforward input to the neurons in the network is h =
.
hI
We study the simplest standard firing-rate-model equations (reviewed in Ermentrout
and Terman (2010), Chapter 11; Gerstner and Kistler (2002), Chapter 6; Dayan and Abbott
(2001) Chapter 7), in which a neuron’s firing rate approaches a nonlinear function of its
input with first-order dynamics:
τT

dr
= −r + f(Wr + h)
dt

(1)

Here T is a diagonal matrix of relative time constants, i.e. the time constant of the ith
neuron is τ Tii . f is a vector function of a vector argument that acts elementwise on its
argument, (f(v))i = fi (vi ), for some scalar functions of a scalar variable, fi , where vi is
the ith element of v. These rate model equations do not capture fast time scales that arise
in spiking networks, and cannot capture synchronization of spikes across neurons, but tend
to be reliable in describing steady states or slower aspects of dynamics when neurons spike
asynchronously. We will focus on the steady state and its stability.
We will study the case in which the fi are identical for all elements, fi ≡ f , and f is a
rectified power law with power n > 1:
f (x) = k([x]+ )n

(2)

where [x]+ = x, x > 0; = 0, otherwise. We will summarize this by saying
τT

dr
= −r + k(Wr + h).n
dt

(3)

where v.n is the vector with ith element ([vi ]+ )n (the period in the exponent .n, based on
Matlab notation, is to indicate that the operation is done element-by-element rather than
6

to the vector as a whole). A power-law relation between the mean input and mean response
arises in the case that spiking is driven by input fluctuations (Miller & Troyer, 2002; Hansel
& van Vreeswijk, 2002), and similarly it is observed in V1 as the relation between the trialaveraged mean voltage and mean response. Note that the fact, noted in the Introduction,
that the power law holds over the entire dynamic range of visual responses means that V1
neurons never reach firing rates at which intrinsic saturation of the input-output function
plays a role. Since V1 firing rates in response to optimal visual stimuli are typically among
the highest seen in cerebral cortex, the same conclusion is likely to apply to cortex more
generally. For this reason, we model the input/output function of the neuron simply as a
power law without considering saturating parts of the input/output function. Model results
will only apply when the model produces firing rates that remain within the non-saturating
regime.
We now change variables to dimensionless ones. This allows us to determine the dimensionless combinations of parameters on which model behavior depends and in which
expansions for small or large values may be undertaken. We let ψ = kWk where kWk is
some matrix norm or other measure of the size of W, and write W = ψJ with J dimensionless and kJk = 1. Similarly we let c = |h| and write h = cg with g dimensionless and |g| = 1
(again, |g| indicates some measure of the size of a vector, e.g. a vector norm). Note that
c and ψr have the same units, so that ψr/c is dimensionless, and that kcn and r have the
same units, so that kcn /(c/ψ) = kcn−1 ψ is dimensionless. We thus define the dimensionless
variable and parameter:
y = rψ/c
α = kcn−1 ψ
Then equation 3 becomes

(4)
(5)

dy
= −y + α(Jy + g).n
(6)
dt
Thus, given J, g, T, and n, the dynamics depends only on the single parameter α.
The fact that Eq. 6 has a single α for all neurons is quite general: if neuron i had
parameter αi , this could be replaced with α by multiplying all weights Jij and inputs gi to
1/n
neuron i by ααi
, leaving the form of the equation unchanged. However the fact that the
equation has a single n for all neurons is a real restriction. Consideration of n’s that vary
between neurons or between neuron types remains a question for future study.
= −r + f (I) where f (x) = x.n and the input I =
Note that we can rewrite Eq. 3 as τ T dr
dt
1
1
k n Wr + k n cg. By incorporating the factors of k into the input I, the steady-state rate f (I)
becomes of unit magnitude when the input I is of unit magnitude. This is a natural scaling
1
1
for defining the effective recurrent weights, k n W, and the effective input strength, k n c. Then
τT

7

 1
  1 n−1
α = kcn−1 ψ = k n kWk k n c
, that is, α = (recurrent weight)(feedforward strength)n−1 .
Note also that whether the input is dominated by feedforward input cg or recurrent input
Wr is determined by the size and structure of y for a given α (because Wr + cg = c(Jy + g),
so that the balance depends only on the relative sizes of Jy vs. g), and is not impacted at
all by the ratio c/ψ, which naively might be thought to determine the feedforward/recurrent
balance. For a given α, this ratio simply scales r (r = (c/ψ)y).
We will focus on the equation for the steady-state:
y = α(Jy + g).n

(7)

However, in considering stability of the steady state we will need to use the dynamical
equation 6.

3

Scaling Argument

In this section, we show that the supralinear network generically makes a transition between
responses that scale in two different ways with α. For weak inputs (α ≪ 1), the net input
(feedforward plus recurrent) to a neuron grows linearly with the feedforward strength. Because of the supralinear neuronal input/output function, this yields supralinear summation
of responses to multiple sets of feedforward inputs. For stronger inputs (α ≫ 1), the recurrent input largely cancels the feedforward input, leaving only a net input component that
grows sublinearly with the feedforward input strength. This yields sublinear response summation for a broad range of parameters. The transition between these two scaling regimes,
which we refer to as the supralinear and sublinear scaling regimes respectively, occurs for α
of order of magnitude 1, for which we use the standard notation α ∼ O(1). We then compare
this dynamic input cancellation to that in the balanced network model of Van Vreeswijk and
Sompolinsky ((1998)).
The transition from supralinear to sublinear scaling is generally marked by recurrent excitation becoming strong enough to yield instability and explosive growth of activity on its
own, along with dynamical stabilization of network activity by feedback inhibition. The effective connection strength between two neurons tells how much the steady-state postsynaptic
rate changes for a given change in steady-state presynaptic rate. This is given by the weight
between the neurons times the postsynaptic gain. The gain is the slope of the input-output
function (Eq. 2), which is monotonically increasing
cell’s firing rate.
P with the postsynaptic
.n
That is, given the steady-state equation ri = k( j Wij rj + cgi ) , the effective weight from
n−1
P
1
dri
n−1
n r n W , which monotonically
neuron j to neuron i is dr
=
nk(
W
r
+
cg
)
W
=
nk
ij
j
i
ij
ij
i
j
j
increases with ri . With increasing input, the network responses typically increase and hence
8

effective connections grow until the recurrent excitatory-to-excitatory connections become
strong enough to yield instability absent stabilization by feedback inhibition. The transition
from supralinear to sublinear scaling is generally closely associated with this transition to
potential excitatory instability, as we will see (Figs. 2–4 and Section 5.4).

3.1

Scaling for small α

For α ≪ 1 we expect the steady state to satisfy y ≈ αg.n , since then the Jy term is small
relative to the g term and so adds only a small correction to this solution. More generally,
we can write a formal expression for the steady state by starting with Eq. 7 and iteratively
substituting α(g + Jy).n for each instance of y, yielding:
y = α(g + αJ(g + αJ(g + αJ(. . .).n ).n ).n ).n

(8)

r = k(cg + kW(cg + kW(cg + kW(. . .).n ).n ).n ).n

(9)

or, in terms of r,

where the ellipses indicate infinite repetition of the pattern. Assuming quantities in the
parentheses are positive (which they will be for sufficiently small α as we assume all components of g are positive) so that we can ignore rectification, Eq. (8) can be converted into an
infinite series in increasing integer powers of α with dominating (lowest-order) term αg.n.2
The terms multiplying αp will involve factors of g interspersed with J’s, with the sum of the
powers on the g’s equal to p(n − 1) + 1. Similarly for r, one obtains a series involving an infinite set of powers of c, with lowest-order term k(cg).n and higher-order terms proportional
to k p cp(n−1)+1 and involving a set of g’s with summed power also equal to p(n − 1) + 1. If
this series converges, which it will for sufficiently small α, it will give a steady state solution.
Thus, for small α, feedforward inputs sum supralinearly to produce responses (i.e., the
response r depends supralinearly on the feedforward input cg). Intuitively: as previously
1

n−1

noted, the effective connection from neuron j to neuron i is nk n ri n Wij . For small α,
1

n−1

ri ≈ k(cgi )n so k n ri n Wij ≈ αgin−1Jij is small, that is, effective connections are small. In
this regime the network is essentially feedforward driven, with small modifications by the
weak effective recurrent connections. Since individual cells respond supralinearly to their
inputs, the network sums responses supralinearly.
2

This can be done by expanding each power in Eq. 8 as α(g+αJ(. . .).n ).n = α(g.n +ng.(n−1) .∗αJ(. . .).n +
∗ (αJ(. . .).n ).2 + . . ., where .∗ indicates element-by-element multiplication of two vectors to
create another vector, and then collecting together the terms of each given order in α.
n(n−1) .(n−2)
g
.
2

9

3.2

Scaling for large α

The scaling y ∼ α (or equivalently r ∼ cn ) cannot hold once α is sufficiently large. In particular, for sufficiently large α, the series in Eq. 8 will explode rather than converge. Physically,
this occurs because inputs are raised to the power n > 1 to produce responses which in turn
feed back in as inputs; once inputs are sufficiently large, this process is explosive, like a nuclear reaction going critical. We expect this to occur for α = O(1). Moreover, for α & 1, the
effective recurrent weights typically become strong enough that the excitatory subnetwork
by itself becomes unstable in the absence of dynamic feedback inhibition.
For Eq. 7 for the steady state to be self-consistent, the dependence of α(Jy + g)n on the
leading α must be cancelled, because otherwise y ∼ α, which enters into Jy and (dominating
over the feedforward input, g, for large enough α) is raised to the nth power to give3 y =
α(Jy + g)n ∼ αn+1 , which in turn enters into Jy, and so on – the infinite series in powers of
α results, which will blow up for sufficiently large α. Thus to cancel the leading α, it must
1
be the case that, to leading order in α, Jy + g ∼ α− n . This in turn requires that, to leading
order, y has the same α-dependence as g, y ∼ α0 , so that the leading order of y can cancel
1
the g term leaving only terms of order α− n .
Thus, if the steady-state Eq. (7) is stable, the recurrent input Jy must have dynamically
adjusted itself to approximately cancel the feedforward input g, leaving a remainder that
becomes smaller with growing α. We refer to this as dynamic stabilization, for two reasons.
First, mathematically, this dynamic cancellation is necessary for the existence of a stable
steady state for large α. Second, physically, the cancellation typically arises as the excitatory subnetwork becomes unstable by itself and the network is dynamically stabilized by
feedback inhibition, although as we will discuss later (Section 5.4) there are cases in which
the cancellation arises without excitatory subnetwork instability.
Let us define
1
(10)
β = α− n .
We write y = y0 + βy1 , with both y0 and y1 approximately O(β 0) so that βy1 scales
approximately as β. It follows from the requirement Jy + g ∼ β that
y0 = −J−1 g.

(11)

Substituting y = −J−1 g + βy1 in Eq. (7) then yields
−J−1 g + βy1 = (Jy1 ).n .

(12)

The latter equation shows that y1 itself has some further dependence on β. We will further
discuss below whether the assumption holds that βy1 scales approximately as β.
3

This could be avoided if Jy = 0 to leading order in α, but that requires fine tuning, i.e. it requires
Det J = 0.

10

These arguments can be translated in terms of r. Once c is sufficiently large, selfconsistency requires cancellation of the linear dependence of Wr+cg on c, because otherwise
2
r ∼ cn , which enters back into Wr and is raised to the n to yield cn dependence, and so on.
Cancellation requires that, to leading order, r ∼ c, which in turn requires that to leading
1
order Wr + cg ∼ c n , that is, the net input to a cell grows sublinearly with the feedforward
1
input strength. Writing r = cr0 + c n r1 , with r0 and r1 both O(1), we find that r0 = ψ1 y0 =
−W−1 g and r1 =

c β
1
ψ cn

y1 =

1

1
ψ(kψ) n

y1 , with r1 satisfying −W−1 g + c−

n−1
n

r1 = (Wr1 )n .

These solutions show that, if the network dynamically stabilizes, the net input to cells
grows sublinearly with the feedforward input strength and responses are given by a sum of
terms that are linear and sublinear in the feedforward inputs. In studies of 2-dimensional systems (one excitatory and one inhibitory population), we will find that, when the excitatoryneuron element of −J−1 g (equivalently of r0 ) is negative, the sublinear term becomes dominant (as it must: (y0 )E < 0, so one must have β(y1 )E > |(y0 )E | for yE > 0) and network
behavior becomes strongly sublinear. In this case, excitatory firing rates eventually peak
and then are ultimately pushed to zero with increasing c, i.e. with decreasing β, but there
is typically a large dynamic range of c beyond the supralinear-to-sublinear transition before
this peak occurs (see Figure 2). The behavior from c = 0 until somewhat beyond the peak
yields behavior much like that seen in biology, and so we guess that this dynamic range
represents the dynamic range of the feedforward input to cortex. This will be discussed in
Sections 5.2-5.3. In simulations, the dependence of y1 on β remains weak, so that βy1 decreases nearly as fast as β, as was assumed, over the range from the supralinear-to-sublinear
transition until rE has been pushed close to zero. By contrast, when the elements of −J−1 g
1
are positive, the subleading correction, c n r1 , can have either sign, and the total response
1
r = cr0 + c n r1 can be a supralinear or sublinear function of c,4 according to whether the
sign of r1 is negative or positive respectively. However, even in the case where the response
as a function of c remains supralinear for large c (small β), the response to the sum of two
non-aligned inputs, g1 and g2 , will typically sum sublinearly for large c, as we will see in
Section 4.2. Given positive elements of −J−1 g, Eq. 12 shows that y1 goes to an O(1) con1
stant as β decreases to zero (y1 → J−1 (−J−1 g). n ), and thus the term βy1 is guaranteed to
decay approximately as β for sufficiently small β.
A more systematic account of the large α (small β) case can be obtained by formulating
a solution like that of Eq. 8 for small α. When the elements of y are > 0 (and thus the
elements of Jy + g are > 0), we can rearrange Eq. 7 for the steady state as
1

y = −J−1 g + βJ−1 y. n

(13)

Then we can formally write a steady-state solution by starting with equation 13 and itera4

By saying that r(c) is a supralinear or sublinear function of c, we mean that

11

d2 r
dc2

is > 0 or < 0 respectively.

1

tively substituting −J−1 g + βJ−1 y. n for y to obtain
1

1

y = −J−1 g + βJ−1 (−J−1 g + βJ−1 (. . .). n ). n
or
r = −cW−1 g +

1
k 1/n

W−1(−cW−1 g +

1
k 1/n

(14)
1

1

W−1(. . .). n ). n

(15)

If quantities in parentheses are positive, a series solution in powers of β can be obtained
from Eq. 14 in the same manner as outlined for Eq. 8. When this series converges, which it
will for small enough β, it gives a steady-state solution. However, if elements of −J−1 g are
negative, then for small enough β the elements in parentheses will no longer be positive (and
correspondingly, as mentioned above, in the 2-D case yE is pushed to zero with decreasing
β at finite β, so that Eq. 13 fails at that point). We can instead regard Eq. 13 as an
1
iterative scheme, y[p + 1] = −J−1 g + βJ−1 y[p] n , beginning from some initial condition y[0]
(Eq. 8 can also be regarded in this way), which generates Eq. 14 as p → ∞. Writing this
as y[p + 1] = f (y[p]), if all of the eigenvalues of the Jacobian of f at the fixed point have
absolute values less than 1, then the iteration will converge to the fixed point within some
basin of attraction about the fixed point. Hence with suitable initial conditions, one can find
solutions through this iterative scheme, although not for β’s less than that at which some
elements of y are pushed to zero.
These scaling arguments provide key insights into the supralinear network (Eq. 3) that
is confirmed by other analysis and simulations: for small α, recurrence is weak and the
network supralinearly adds responses to different feedforward inputs; with increasing α,
there is a transition, for α = O(1), to a dynamic stabilization that causes the net input
neurons receive, and in many cases their responses, to add sublinearly. Note that, when
responses add sublinearly, individual neurons still supralinearly sum the net (feedforward
plus recurrent) inputs they receive, but the network “conspires” to deliver net input that
is so strongly sublinear that, even after the neuron raises its net input to the power n,
its responses add sublinearly. We have found in both high-dimensional and 2-dimensional
simulations, and we will show below for the 2-dimensional case, that stabilization will occur
provided feedback inhibition is sufficiently strong and the inhibitory time constant is not too
slow relative to the excitatory time constant. This transition from supralinear to sublinear
behavior in turn appears to underly a wide variety of nonlinearities in neocortical behavior.

3.3

Comparison to the balanced network

Van Vreeswijk and Sompolinsky (1996, 1998) introduced the “balanced network” model (see
also Renart et al. (2010)). They considered a circuit of randomly connected stochastic excitatory and inhibitory units that could have activity states 0 or 1, in the limit in which both
12

feedforward and recurrent inputs were very large. (The same model could also be studied
in other regimes, but we use “balanced network” to refer to the model in the large-input
regime that they studied and “balanced state” to refer to the solution they characterized in
that regime.) They studied the conditions in which the network would dynamically find its
way to a balanced state in which the mean input is subthreshold, yet firing rates are nonzero
(where firing rate is defined as the average activity), meaning firing is driven by fluctuations.
√
They assumed each unit received K inputs of strength √1K , or a net input of strength K,
for K large (e.g., thousands of inputs). The mean field equations for the average E and
I firing rates are the 2-dimensional version of the √
rate equation, Eq. 1, for one E and one
I population, where both W and cg are of order K,5 and the function f is a sigmoidal
function rising from 0 to 1 as the input moves from approximately −3 to 3, and saturating
at 0 or 1 for smaller or larger values respectively. To be in the balanced state, the mean
firing rate must be neither 0 nor saturated at 1, so the net input must be O(1) (i.e. between
−3 and 3).
Thus,
and cg
√ the condition for the balanced state is that Wr + cg ∼ O(1) where both W
1
√
are O( K). The solution, much as in our scaling argument, is to write r = r0 + K r1 + . . .,
where r0 and r1 are O(1) and the dots represent higher-order terms in √1K . The balance
√
condition is that the O( K) term in the input vanishes, that is, Wr0 = −cg, leaving as
input only the O(1) term √WK r1 and terms that are O( √1K ).
The dynamic cancellation condition for the balanced state, r0 = −cW−1 g, is of course
identical to the condition we have found for approximate dynamic cancellation in the SSN.6
Although the condition is formally identical, the meaning is different in crucial ways:
1. In the balanced network, the cancellation is required because inputs are large and must
cancel to leave something small, in order to avoid zero or saturated output. In the SSN,
partial cancellation can already arise when none of the inputs are large and is required
so that the supralinear input-output functions do not give rise to an inconsistent,
explosive scaling. The difference in the size of the inputs when cancellation occurs can
be seen by recalling that, in the SSN, α = (recurrent weight)(feedforward strength)n−1
(see paragraph above Eq. 7).7 In√the balanced network, the recurrent weights and the
n
feedforward weight are both O( K), so the SSN’s α would be O(K 2 ). In contrast,
dynamic cancellation arises in the SSN when α is O(1).
5

The input is expressed in units of a variance term which itself is dynamically determined, but this term
is O(1) and does not impact the points made here.
6
We wrote the leading term as cr0 rather than r0 , but the leading terms are identical.
7
Note that this expression for α holds when the input is scaled appropriately so that the steady state firing
rates, f (input), are O(1) when the input is O(1) (see paragraph above Eq. 7). This relationship between
input and steady state rates also holds for the balanced network, so we have a common scale for the input
on which to compare the size of the recurrent weights and of the feedforward strengths in the two models.

13

2. In the balanced network, the second-order term √1K r1 is negligibly small relative to
the first-order term r0 (because the stabilization is to cancel large things, leaving
something small). The first-order term is linear in the input, r0 = −cW−1 g, and so
in the balanced network responses are always linear in the input. In the SSN, the
first-order term in y, y0 = J −1 g, is O(1), and while the second-order term βy1 scales
approximately as β, it can have a large pre-factor such that it can be comparable to
or larger than the first-order term over a wide dynamic range, enabling a variety of
sublinear behavior (and r0 and r1 are just c/ψ times the corresponding y’s).
In particular, in the SSN, elements of r0 can be negative, meaning that for such an
element r1 > |r0 | over the relevant dynamic range of behavior (discussed in more detail
for the 2-D model in Section 5.2). In the balanced network, since all terms except r0
are negligibly small, the elements of r0 must be positive for activity to be nonzero.
In sum, in the balanced network, inputs are huge relative to the distance from rest to
threshold, and must dynamically cancel for the network to neither saturate nor have 0 activity
but instead be in the fluctuation-driven regime. The dynamic cancellation or stabilization
yields network responses that are always linear in the input. In the SSN, the supralinear
input-output function renders the network explosive – input is raised to a power greater
than 1 to produce responses, which feed back as input. Stabilization against this explosive
nonlinearity arises when inputs are relatively small, yielding a range of sublinear behavior.
Finally, for clarity we note that the balanced network and SSN are not necessarily different
models, but rather very different solutions with different behaviors in different regimes, which
may be found in the same model. For example, in the power-law model studied here, the
balanced-state solution can be found if α is large, the elements of y0 = −J−1 g are positive,
1
1
and the√feedforward input k n c and recurrent input k n ψ both have the same scaling, e.g. both
are O( K), so that (c/ψ) is O(1). In this case, as α → ∞, y → y0 and r → ψc y0 , so r is O(1);
 1/n
1
1
c
and the first-order correction is c n r1 = ψc βy1 = kψ
y , which scales as 11 , e.g. as √1K
ψ 1
knψ

(recall y1 is O(1) for large α). This is the balanced-state solution. Similarly, in the model
studied in the balanced network, the input/output function is initially supralinear before
saturating, so we expect that SSN-like solutions could be found for appropriate regimes of
input and recurrent strengths with α = O(1).8
8

The value of n in the definition of α could be derived from an approximate power-law fit to the supralinear
portion of the input/output function in that model.

14

4

Reduction to a 2-dimensional system

Most of our analysis hereafter will focus on a 2-dimensional system of one excitatory and one
inhibitory population, as it is difficult to say much in general in higher dimensions. A 2-D
system of one E and one I population can be derived as a mean field equation from higherdimensional models in which E and I neurons have random connectivity (e.g. van Vreeswijk
and Sompolinsky (1998), Renart et al. (2010)). In particular, if the high-dimensional model
involves integrate-and-fire neurons, their input-output functions in the fluctuation-driven
regime can be reasonably approximated by power-law functions (Hansel & van Vreeswijk,
2002).
Here we consider a higher-dimensional system with structured connectivity. We show
a heuristic derivation of a 2-D system that preserves a surprising amount of the behavior
of the higher-dimensional system. We then show how the conditions for “normalization” –
sublinear addition of responses to multiple stimuli – in the high-D system can be expressed
as simple conditions in the 2-D system on the growth of r with increasing c, or the growth of
y with increasing α. In particular, the transition to the high-α regime represents a transition
to sublinear addition in the corresponding high-D system, even when the dependence of r
on c remains supralinear in the 2-D system.

4.1

Reduction

We consider a topographic network, with pairs of excitatory (E) and inhibitory (I) units
arranged on a 1-D or 2-D grid with periodic boundary conditions. The grid dimensions
mirror stimulus parameters such as orientation or position on the retina, such that units at a
certain location in the grid prefer stimuli with the corresponding parameter(s) (other stimulus
preferences may also be incorporated, e.g., a 2-D retinotopic grid with a superimposed map of
preferred orientations). We will use as an illustration a 1-D ring of cells of similar retinotopic
position but varying preferred orientation, with preferred orientation represented by position
on the ring, but the reduction framework is more general. Stimuli are localized on the grid
(e.g. in the 1-D ring a single oriented grating stimulus evokes a localized Gaussian-shaped
bump of input, centered at the neurons that prefer the stimulus orientation), though there
may be more than one localized stimulus present (e.g. two superimposed oriented gratings
of different orientations). For simplicity, we assume a single time constant for all E cells
and one for all I cells. We let θ represent the position on the grid, and rE (θ) and rI (θ) the

15

excitatory and inhibitory firing rates at position θ. Thus, we can write Eq. 3 as
drE (θ)
= −rE (θ) + k (WEE ∗ rE (θ) − WEI ∗ rI (θ) + cgE (θ)).n
(16)
dt
drI (θ)
= −rI (θ) + k (WIE ∗ rE (θ) − WII ∗ rI (θ) + cgI (θ)).n
(17)
τI
dt
P
Here, WXY ∗ rY (θ) = θ′ WXY (θ, θ′ )rY (θ′ )∆θ, where ∆θ is the stimulus parameter volume
per grid point.
We will consider “normalization”, the sublinear addition of the responses to two stimuli
(?, ?). We let one stimulus be centered at θ = 0. We let W̃XY = WXY (0, 0), and we define
WXY (0,θ)
w
~ XY ≡ W
∆θ to be the vector of weights to position 0, normalized by W̃XY . Similarly,
XY (0,0)
we let r̂E , r̂I be the vectors of excitatory and inhibitory firing rates, respectively, normalized
to equal 1 at position 0, with elements r̂E (θ), r̂I (θ). Then the equations for the units at
position 0 are
τE


.n
drE (0)
= −rE (0) + k W̃EE rE (0) (w
~ EE · r̂E ) − W̃EI rI (0) (w
~ EI · r̂I ) + cgE (0) (18)
dt

.n
drI (0)
τI
= −rI (0) + k W̃IE rE (0) (w
~ IE · r̂E ) − W̃II rI (0) (w
~ II · r̂I ) + cgI (0)
(19)
dt

τE

Although we had previously incorporated changes in |g| into c, we now take addition of a
second stimulus to simply alter g with no change in c, so that in particular addition of a
second stimulus that gives no input to position 0 does not alter gE (0) or gI (0). We now
define
(w
~ XY · r̂Y )
(w
~ EE · r̂E )
Ψ ≡ w
~ EE · r̂E

JXY

≡ W̃XY

(20)
(21)

Note that in general the JXY depend on the normalized shapes of the responses, r̂E and in
some cases r̂I , which in turn may depend on c and/or g. Letting rE ≡ rE (0), gE ≡ gE (0),
etc., our equations become
drE
dt
drI
τI
dt

τE

= −rE + k (ΨJEE rE − ΨJEI rI + cgE ).n

(22)

= −rI + k (ΨJIE rE − ΨJII rI + cgI ).n

(23)

As written, Eqs. (22)–(23), together with the definitions (20) and (21), are simply a rewriting
of Eqs. (18)–(19) and hence exact. In particular, the seemingly closed system of equations
16

for the two variables rE = rE (0) and rI = rI (0) are in general parametrically dependent on
the values of r̂X (θ) at other θ’s through the dependence of JXY and Ψ on the normalized
shapes of the response curves.
We now adopt the ansatz that, as the stimulus changes, the four dot products w
~ XY · r̂Y
are all scaled by a common factor, whether the stimulus changes in strength (changing c) or
in shape (changing g, e.g. by adding a second stimulus). This means that we treat the JXY
as constants independent of c and g. On the other hand, Ψ is scaled by this common scaling
factor. We make the further ansatz that Ψ depends only on g and not on c. Since Ψ depends
only on the shape of the response, r̂E , and not its magnitude, this amounts to the ansatz
that stimulus strength c alters response magnitude without altering response shape. With
these two ansatze, Eq. 23 is simply the 2-dimensional version of Eq. 3, and is equivalent to
Eq. 6 for a 2-dimensional y if Ψ replaces ψ in the definitions of y and α (Eq. 5). (We are no
longer following our convention of kJk = 1, as the matrix J composed of these J’s need not
satisfy kJk = 1 for any standard matrix norm; one could, however, return to this convention
by dividing the JXY ’s by kJk and multiplying Ψ by the same factor.) If, furthermore, we also
assume that the shapes of the population responses roughly follow the shape of the input,
then, since the weights w
~ XY are non-negative, the effect of adding a second non-negative
stimulus is to increase Ψ. Hence “normalization” of the E or I population corresponds to
a decrease in firing rates rE (0) or rI (0), respectively, with increasing Ψ, which for a second
stimulus of infinitesimal strength corresponds to
drE (0)
< 0,
dΨ

drI (0)
< 0.
dΨ

(24)

The ansatze, of course, are not in general true, but they can be close enough to true to
give a good qualitative account of the higher-dimensional system (however, see the discussion at the end of Section 5.3 for a discussion of cases where the assumptions in our ansatze
fail considerably and therefore Eq. (24) no longer expresses correctly the condition for normalization). To illustrate this, we simulate the model on a one-dimensional ring, which we
think of as representing the preferred orientations of neurons representing a common position
in visual space.9 We consider 180 E/I pairs at grid positions separated by 1o in preferred
orientation, with 0o = 180o . All four connection types have the same width, following evidence that excitatory and inhibitory inputs received by cells in upper layers have similar
9

The paradigm we study here – suppression of response to one orientation by presentation of an orthogonal
orientation – is known as “cross-orientation suppression”. In V1, this appears to be primarily mediated by
sublinear addition of the feedforward inputs to V1 evoked by the two stimuli (Lauritzen, Krukowski, & Miller,
2001; N. J. Priebe & Ferster, 2006; B. Li, Thompson, Duong, Peterson, & Freeman, 2006). However we use
this paradigm to study how the model cortex sums responses to multiple stimuli, assuming the feedforward
inputs sum linearly.

17

orientation tuning (Ferster, 1986; Anderson, Carandini, & Ferster, 2000; Martinez, Alonso,
Reid, & Hirsch, 2002; Marino et al., 2005). The connectivity takes the form
− d(θ,θ2

WXY (θ, θ′ ) = JXY e

2σ

′ )2

ori

(25)

where d(θ, θ′ ) is the shortest distance around the circle between θ and θ′ . We consider
stimulation by either one oriented luminance grating or two orthogonal gratings of equal
contrast. Each grating is represented by a Gaussian-shaped curve of feedforward input with
width (standard deviation of the Gaussian) σstim and height c; a single grating is centered
at θ = 0o , a second added grating is centered at θ = 90o . For any given stimulus (1 or 2
stimuli, stimulus height c, given stimulus width σstim ) the equivalent 2-D model is found as
follows: we use the same τE,I , k, n, and JXY ’s as in the high-dimensional model and take
Ψ to be the value of the convolution, at θ = 0, of the connectivity Gaussian (Eq. 25 with
JXY = 1, i.e. the vector of weights to θ = 0 normalized to equal 1 at θ = 0) with g.n (for
our stimuli, g is 1 at position 0). We use g.n as a surrogate for the shape of the response,
with the knowledge that at least at low contrast it gives a good approximation to this shape
(Section 3.1).
The result is that the reduced 2-D model accurately reproduces the behavior of the full
model as shown in Fig. 1. (See also Figs. 2–4 of Section 5.3 for a more detailed comparison
in various parameter regimes; as explained at the end of Section 5.3, the bottom three
rows of Figs. 4 show directly the quality of the approximations involved in the ansatze
introduced after Eq. (23).) The firing rates of the cells at θ = 0 vs. stimulus strength closely
match the firing rates in the 2-D model (Fig. 1A). Both models show a similar transition
from supralinear summation of responses to the two gratings for weak stimuli to sublinear
summation or “normalization” for stronger stimuli (Fig. 1B). The network also shows a
form of surround suppression, in which the “summation field size” – the stimulus width
that yields maximal response for a given stimulus strength – shrinks monotonically with
increasing stimulus strength, as is well known in real space (rather than orientation space)
for V1 cells (Sceniak et al., 1999; Cavanaugh et al., 2002), and this behavior is extremely
similar in the full and reduced models (Fig. 1C).10 Thus, the 2D model can provide a good
basis for understanding more general models.
10

Note that this “summation field size” for orientation selectivity should not be confused with the orientation tuning width, which is the width of the orientation tuning curve obtained by studying response vs.
single orientations (more precisely: studying response vs. center orientation, using stimuli that evoke a fixed
curve of feedforward input vs. orientation that is symmetric about the center orientation). The orientation
tuning curve, representing the set of single orientations that can drive the cell, is analogous to the “minimal
response field” in real space, which represents the sum of the set of small regions in visual space in which
appropriate light stimuli can evoke spiking responses. The minimal response field in real space is invariant
with stimulus contrast (Song & Li, 2008), and so too is the shape of the orientation tuning curve (Skottun,
Bradley, Sclar, Ohzawa, & Freeman, 1987; Anderson, Lampl, et al., 2000; Ferster & Miller, 2000) (contrast

18

Firing Rate

Full Model

Firing Rate

B

150

150

100

100

50

50

0
0

40

60

80

100

150

100

100

50

50

0
0

20

40

60

80

100

0
0

1.2

1

1

0.8

0.8

0.6

0.6
20

40

60

80

100

0.4
0

60

80

100

2 Stim
20

40

60

80

100

20

40

60

80

100

40

60

80

100

100

∞

80

∞

60

60

40

40

20

20

0
0

40

1.4

1.2

80

20

1.6
I cells
E cells

1.4

100

1 Stim

→

Weight w

1.6

Preferred Stim Width

0
0

150

0.4
0

C

20

19

2−D Reduced Model

→

A

20

40

60

80

Stimulus Strength

100

0
0

20

Stimulus Strength

Figure 1: Two neuron approximation of the full ring model. (A) The reduced version of
the model (right) produces qualitatively similar curves of response vs. stimulus strength c as the
full model (left; for the full model, this is the response of the cells at θ = 0o ). The top plots show
the response curves for a stimulus composed of a single grating with orientation θ = 0o and the
bottom plots show the response for a two-grating stimulus composed of the grating at θ = 0o and a
grating at θ = 90o . In the 2-D reduced model, these two cases are represented by using Ψ = 0.774
for one grating and Ψ = 1.024 for two gratings (see Eq. (21) for the definition of Ψ and the text
after Eq. (25) for the method we used to calculate these values). (B) Full and reduced models
show a similar stimulus-strength-dependent transition from supralinear summation (weight > 1) to
sublinear summation (weight < 1) of the responses to two gratings, where the weight w is defined
as follows. For the full model, for either E or I cells, we let R1 (θ), R2 (θ), and R12 (θ) be the response
R12 (0)
,
to one grating, the other grating, or the superposition of the two, and we define w = R1 (0)+R
2 (0)
where θ = 0 is the orientation of the first grating. For the reduced model, we define the weight
12
, where R1 , and R12 are the responses to one or two gratings (modeled by the two
as w = R1R+R
2
values of Ψ given above) and we set R2 = 0 (by the way we defined the reduction, R1 , R2 and R12
should approximate the responses of the full model at θ = 0). (Continued on next page)

Figure 1: (Continued). (C) Full and reduced models have nearly identical stimulus-strengthdependent tuning for the width in orientation, σstim , of a feedforward stimulus (full model: width
of Gaussian stimulus centered at θ = 0 with given stimulus strength c that gives the strongest
response in cells at θ = 0; reduced model: Ψ is computed for each stimulus width as described
in the text after Eq. (25), and plot shows width whose Ψ gives maximal response). In all curves,
red shows E cells and blue shows I cells. All responses are steady-state responses. Full model
solutions found by simulating until convergence to steady state. Parameters: JEE = 2.5, JIE = 2.4,
JEI = 1.3, JII = 1.0, τE = 20 ms, τI = 10 ms, k = 0.04, n = 2.0, σori = 32o ; σstim = 30o in A,B.

4.2

Conditions for Normalization in the 2-Dimensional System

Here we show that, when our ansatze hold, the high-dimensional network exhibits normalization precisely when the 2-dimensional network shows sublinear scaling. We consider
X
to refer to the dependence of the steady
steady-state r or y and use expressions like ∂r
∂ψ
state on parameters. We have seen that rX (X ∈ {E, I}) exhibits normalization in response
X
to addition of an infinitesimal second stimulus if ∂r
< 0 in the 2-D model (more generally,
R Ψfinal ∂rX ∂Ψ
for a finite-strength second stimulus, if Ψinit ∂Ψ dΨ < 0). Since Eqs. 22-23 are equivalent
to Eqs. 5-6 with Ψ replacing ψ, we revert to the notation of Eqs. 3-6 and use ψ.
X
We work with the 2-D model and express the conditions ∂r
< 0 as a single vector

 ∂ψ
dy
dy
dy
∂r
dα
condition. We note first that ∂ψ
= c dy/ψ
= c ψ1 dψ
− ψy2 and dψ
= dy
= kcn−1 dα
.
dψ
dα dψ

n
∂r
= kcψ dy
− αy . Thus, the condition for normalization is
Putting these together we find ∂ψ
dα
dy
ln y
that y grow more slowly than linearly with increasing α: dα
< αy or dd ln
< 1 or, roughly,
α
that y ∼ αp for p < 1. As we have seen, p becomes less than 1 precisely when the transition
from the supralinear to the sublinear scaling regime occurs.
We can reexpress this in terms of r. Using algebra similar to the above, we find ∂r
=
∂c
is monotonically related to the firing rate of the inputs to cortex (e.g. Ohzawa, Sclar, and Freeman (1985)).
The fact that the summation field size in real space is larger than the minimal response field indicates that
stimuli in regions where light cannot directly drive spikes can facilitate responses to stimuli in the minimal
response field. Recall that the size of this facilitating area shrinks with contrast. The model suggests that
the same may be true in the orientation domain, in terms of cortical processing of feedforward input to cells
of different preferred orientations. However, attempts to test this idea will likely be compromised by two
facts: (1) simultaneous presentation of multiple orientations does not yield linear summation of the input to
cortex evoked by the individual orientations (Lauritzen et al., 2001; N. J. Priebe & Ferster, 2006; B. Li et al.,
2006) and (2) varying the feedforward orientation tuning by changing stimulus attributes – e.g. a sinusoidal
luminance grating of a given size provides drive to cortical cells with an orientation tuning that narrows with
increasing spatial frequency, and similarly a longer bar drives narrower orientation tuning than a shorter bar
– also changes other attributes to which the neurons are independently sensitive, such as spatial frequency
or bar length.

20

(n−1)α
ψ



dy
dα

+

y
(n−1)α



, from which we find that

dy
dα

<

y
α

is equivalent to

∂r
∂c

< n rc . Thus, the

r
<n
condition for normalization is that r grow more slowly than cn with increasing c: dd ln
ln c
p
or, roughly, that r ∼ c for p < n. Again, p becomes less than n precisely at the transition
from supralinear to sublinear scaling.
Finally, noting that the steady state condition is r = k(Wr + cg).n , without loss of
generality we write Wr = cf(c) for some vector function f of c, so that the steady state
condition r = k(Wr + cg).n becomes r = kcn (f(c) + g).n . Thus we see that a component
of r grows more slowly than cn precisely when the corresponding component of f(c) is a
decreasing function of c (that is, for corresponding components r and f , ∂r
< n rc precisely
∂c
′
when f (c) < 0). Thus, the condition for normalization can alternatively be expressed as
the requirement that the recurrent input Wr grow more slowly than linearly with c, i.e.
∂(Wr/c)
< 0 yields ∂Wr
< Wr
or ∂ ln(Wr)
< 1.
∂c
∂c
c
∂ ln c

5

Analyses of the 2-Dimensional Network

We will assume throughout this analysis that gE ≥ 0, gI ≥ 0. We will use the following
definitions:

ΩE ≡ Det J −J−1 g E = JII gE − JEI gI
(26)

ΩI ≡ Det J −J−1 g I = JIE gE − JEE gI
(27)

We also note that there are three possible conditions: (1) (−J−1 g)E > 0 and (−J−1 g)I > 0;
(2) (−J−1 g)E < 0 and (−J−1 g)I > 0; and (3) (−J−1 g)E < 0 and (−J−1 g)I < 0. The 4th
condition, (−J−1 g)E > 0 and (−J−1 g)I < 0, is not mathematically possible for gE ≥ 0 and
gI ≥ 0: ΩE > 0 and ΩI < 0 together imply Det J < 0, and similarly ΩE < 0 and ΩI > 0
together imply Det J > 0.

5.1
5.1.1

When Does the Network Dynamically Stabilize?
The case of infinitely fast inhibition

We first analyze the case of infinitely fast inhibition, τI /τE = 0, with constant feedforward
inputs. We show that in this case, if Det J > 0, the network is always driven to a stable
fixed point from arbitrary starting conditions. The condition Det J > 0 means that feedback
inhibition is sufficiently strong: JEI JIE > JEE JII . In addition, we show that if Det J < 0,
sufficiently large initial firing rates will cause the system to “blow up”, i.e. firing rates will
grow arbitrarily large.

21

With τI = 0, the value of yI is “slaved” to, or instantaneously set by the value of, yE
according to the dydtI part of Eq. 6 for y. Because of the nonlinearity, we cannot solve this
for yI as a function of yE , but we can instead solve for yE as a function of yI :

  1
yI n
1
+ JII yI − gI
(28)
yE =
JIE
α
Substituting this in the dydtE part of Eq. 6 yields, after a bit of algebra, an equation for
induced by the slaving of yI to the yE dynamics:

dyI
dt

n−1


.n 

 y  n1
 y  n1
α
I
I
−JII yI −
+ gI + n−1 −Det J yI + JEE
+ ΩI
α
α
JIE
(29)
α
For sufficiently large yI , if Det J > 0, the term inside the parentheses in the J n−1 (. . .).n
IE
term will be negative, and so will be set to zero after the thresholding involved in the ().n
operation. The dominant term will then be the −JII yI term, which is negative. So for
sufficiently large yI , dydtI < 0. On the other hand, if Det J < 0, then for sufficiently large yI ,
the (. . .).n will be positive and larger than the sum of the other terms, so that dydtI > 0 and,
since increasing yI will increase dydtI > 0, this derivative is ever-increasing.
For sufficiently small yI , dydtI > 0 if either gI or gE is nonzero, which can be seen as
follows. For sufficiently small yI , the source terms gI and ΩI , if nonzero, dominate the terms
involving yI . Both gI and the (. . .).n term containing ΩI are non-negative, so if either is
positive dydtI will be positive; if ΩI > 0, the (. . .).n term is positive; if ΩI ≤ 0, this implies
gI > 0 (given that at least one of gI and gE is nonzero, and that both are non-negative).
Thus, for Det J > 0, yI is driven to a stable fixed point, and yE is then determined from
Eq. 28, so the system will arrive at a stable fixed point. Note that the system could have
multiple fixed points with varying levels of yI . The topology of flow along the yI axis tells
us that there must be an odd number of fixed points, alternating from stable to unstable
to stable with increasing yI , with the outermost fixed points (those with lowest and highest
yI ) being stable. In the simplest case, there is a single stable fixed point. In addition, for
Det J < 0, the system will blow up for sufficiently large initial firing rates.
1

dyI
nα n yI n
τE
=
n−1
1
dt
1 + JII nα n yI n

5.1.2

More general requirements for stability

Changes in the time constants can alter the stability of the fixed points, but do not alter the
number or positions of the fixed points. The results of the previous section tells us that, for
Det J > 0, the system always has a fixed point that is stable for τI = 0. We consider such a
fixed point, and ask when it retains or loses stability for finite τI .
22


yE
, and assess stability by linearizing the dynamics about
We let the fixed point be
yI
this fixed point. We let q = τI /τE > 0. Setting τ = τE in Eq.
! 6, the matrix T is given by
n−1


1
0
yE n
1 0
. Writing the identity matrix
. Define the matrix Φ = nα n
T=
n−1
0 q
0
yI n
as 1, the Jacobian matrix of the 2-D system is:


JEE −JEI
= T−1 (ΦJ − 1)
(30)
J ≡
JIE −JII


A fixed-point of the dynamics will be stable if J has a negative trace and a positive determinant.
The negative trace condition is JEE < JII , which becomes
!
n−1
1
n−1
ny n J
1
nα
+
1
II
I
(31)
JEE = nα n yEn JEE − 1 ≤ 0 OR JEE > 0 AND q <
n−1
1
nα n yEn JEE − 1
The condition JEE ≤ 0 means that the excitatory subnetwork by itself is stable (or marginally
stable), which guarantees that the network will always be stable, since only unchecked recurrent self-excitation can destabilize the network. When the excitatory subnetwork is unstable,
we can further reduce the condition on q for n = 2:11
p
1 + 4αJII (gI + JIE yE )
√
q<
(n = 2, 2 αyE JEE > 1)
(32)
√
2JEE αyE − 1
The determinant condition, Det J > 0, is always true for any fixed point that is stable
at q = 0. To see this, note that the sign of the determinant does not depend on q for
q > 0 (because Det AB = Det A Det B for any matrices A, B, and Det T−1 = 1q ). So if we
prove that Det J > 0 for arbitrarily small q > 0, we will have shown that it holds for all
q > 0. For q = 0, the determinant, which is the product of the two eigenvalues, was infinite:
because the fixed point was stable, both eigenvalues had negative real part: one real part
was infinite, corresponding to the instantaneous flow onto the inhibitory nullcline (the line
in the yE /yI plane on which dydtI = 0); the other was finite, corresponding to the flow along
the nullcline converging onto the fixed point. (Since the two real parts were unequal, both
eigenvalues were real.) As q is moved infinitesimally from 0, the infinite eigenvalue becomes
√
√
√
yI = α (JIE yE − JII yI + gI ) as a quadratic equation for yI .
√
√
−1+ 1+4αJII (gI +JIE yE )
√
. Substituting this into Eq. 31
Discarding the negative solution, this yields yI =
2JII α
for n = 2 yields Eq. 32.
11

This condition is found by solving

23

a large but finite negative eigenvalue, while the finite eigenvalue is perturbed by arbitrarily
small amounts as q is made arbitrarily small. This means that there is a range of q > 0
for which the eigenvalues continue to have negative real parts, and therefore for which the
determinant condition holds. Therefore, the determinant condition holds for all q. Thus, for
a fixed point that is stable for q = 0, the fixed point remains stable so long as condition 31,
or condition 32 for n = 2, is satisfied.
We also note that, for the case n = 2 and for q ≤ 1, a sufficient condition to conclude
2
that there is only a single fixed point, which is stable, is Det J > 0 and JEE
< JIE JII , which
can be seen as follows. The determinant condition is Det (ΦJ − 1) > 0. We note that,
for an arbitrary 2-dimensional matrix M, Det (M − 1) = Det M − Tr M + 1. Thus, the
determinant condition is Det ΦJ > Tr ΦJ − 1. Since Det Φ > 0 (because firing rates and α
are > 0), this condition will be satisfied if Det J > 0 and Tr ΦJ < 1. The trace condition
for stability is Tr T−1 ΦJ < 1 + q. But, for q ≤ 1 and given the structures of Φ and J,
Tr T−1 ΦJ ≤ Tr ΦJ, so the condition Tr ΦJ < 1 ensures that the trace condition is also
satisfied. This condition is
n−1
n−1
1
JEE yEn − JII yI n <
(33)
1
nα n
√
For n = 2, we substitute the solution for yI as a function of yE (footnote 11) into Eq. 33 for
2
I JII
. Since the right side is positive, a sufficient condition
n = 2 to find JEE
− JIE JII < 1+4αg
4αyE
2
for this to be true is JEE < JIE JII . Recall that, if there is more than one fixed point, some
will be unstable at q = 0, and they must remain unstable for some region of small but finite
q. Since this condition guarantees that any fixed point is stable, we conclude that there can
only be one fixed point, which is stable, when this condition holds.
In summary, for q = τI /τE = 0, the network always flows to a stable fixed point if
Det J > 0. For q > 0, a fixed point that is stable at q = 0 remains stable when Eq. 31 or, for
n = 2, Eq. 32 is satisfied. Note that this condition does not ensure that the network always
flows to a stable fixed point; for nonzero q there may be initial conditions outside the basin
of attraction of the stable fixed point or points. A condition that ensures that any fixed
point is stable for q ≤ 1, and therefore that there is only one fixed point, is Det J > 0 and
2
JEE
< JIE JII . If there are no limit cycles (stable or unstable), this ensures that the network
will flow to the stable fixed point. Excepting Eqs. 31-32, these conditions involve feedback
J2
inhibition being sufficiently strong: JIE JEI > JEE JII , and JIE > JEE
.
II
In Fig. 2, bottom row, we will illustrate the range of q’s yielding stability for various
parameter choices with n = 2.

24

5.2

The case −J−1 g



E

< 0 and supersaturation

We consider Eq. 3 for r, but substituting ψJ for W. We restrict to the case Det J > 0, which
ensures a stable fixed point for at least some range of ττEI > 0. We note that for Det J > 0,
(−J−1 g)E < 0 and (−J−1 g)I < 0 are equivalent to ΩE < 0 and ΩI < 0, respectively.
We shall equate increasing or decreasing c with increasing or decreasing stimulus contrast. This is based on the fact that the contrast of a visual stimulus is monotonically (but
nonlinearly) related to the firing rate of the inputs to V1 from the lateral geniculate nucleus
(LGN) (e.g. Ohzawa et al. (1985)).
In simulations, we find that if ΩE < ΩI < 0 for gE = gI , then rE grows with c for a
range of c considerably beyond the transition from supralinear to sublinear behavior, but
ultimately peaks and is pushed back to 0 with increasing c (see Fig. 2A). The inputs to
cortex have limited dynamic range (e.g. stimulus contrast cannot increase beyond 100%),
and so we imagine that this circuit may model cortex but that the maximal input strength
seen biologically cannot drive excitatory responses too far beyond their peak. The decrease
in response with increasing contrast after a peak response is referred to as “supersaturation”,
and is seen in virtually all V1 cells for contrasts larger than about 75% (Ledgeway, Zhan,
Johnson, Song, & Baker, 2005; C. Y. Li & Creutzfeldt, 1984; Tyler & Apkarian, 1985; Peirce,
2007). This model behavior provides one possible explanation for supersaturation, although
supersaturation might also in part reflect a supersaturation of inputs, e.g. if feedforward
inhibition (Bruno, 2011) overtakes feedforward excitation with increasing contrast.
Here we analyze this behavior. We shall find that (1) if r is a stable fixed point, then
gE
gI
∂rE
I
and ∂r
are negative precisely when ΩE < −
and ΩI < −
, respectively
1 n−1
1 n−1
∂c
∂c
nψk n rI

n

nψk n rE n

(and so in particular can only be negative if ΩE < 0 or ΩI < 0, respectively); (2) if ΩE < 0,
then there is a stable fixed point with rE = 0 at a finite positive value of c, which we
g2
calculate; and, (3) for n = 2, if in addition ΩE < gE2 ΩI , then the set of fixed point excitatory
I

2

rates, rE (c), has a maximum (where ∂r∂cE = 0 and ∂∂cr2E < 0), which is typically the peak of
rE before it is pushed to zero, and we calculate the corresponding c and peak value of rE .
The condition ΩE < 0 states that the linear term in c in the high-c expansion for rE
(Eq. 15) is negative, driving rE to zero. A related criterion for supersaturation was noted
by Persi, Hansel, Nowak, Barone, and van Vreeswijk (2011), who studied a high-dimensional
ring network with power-law input/output functions much like the ring model studied here,
though with varying connectivity widths for the four connection types and different power
laws for E vs. I cells. They assumed a stimulus of orientation θ gave feedforward input
−

d(θ,θ ′ )
2σ 2
X,LGN

IX e
to cells of population X (E or I) and preferred orientation θ′ . For IE = II , their
criterion for supersaturation was JEI σI,LGN − JII σE,LGN > 0, which appears closely related
25

g2

to our criterion ΩE < 0, i.e. JEI gI − JII gE > 0. Our other condition, ΩE < gE2 ΩI , states that
I
the linear term in the expansion for rI is either positive, or not so negative as to disrupt the
ability of inhibition to drive rE to zero.
These results suggest, but do not prove, that rE will be driven to zero for arbitrary n
whenever ΩE < 0 (although there is a stable fixed point with rE = 0 at finite c, we have not
proven that it is the only stable fixed point). We note that rI can never be zero for finite c
if gI > 0 (since rE ≥ 0), so given gI > 0, rI can never be driven to zero with increasing c
even for ΩI < 0.
For ΩE < ΩI < 0, gE = gI , we find in simulations that rI only increases with increasing
f (n)

c (see Fig. 2A). We speculate that, for ΩE < 0 and ΩE <

gE

f (n)

gI
∂rI
∂c

ΩI , where f (n) may equal n

< 0, while rI always becomes
or may equal 2, rE can never become large enough to set
∂rE
large enough to set ∂c < 0 and so ultimately to drive rE to zero.
When ΩI < ΩE < 0 for gE = gI , we find unbiological behavior in simulations in which
both rE and rI jump to very high levels at very low c, after which rE monotonically decreases
and is ultimately pushed to 0 (see Fig. 2E). Numerical calculations suggest a discontinuity at
the jump, which may explain why our calculations do not find a zero of ∂r∂cE for real positive
c in this case. We have not tried to analyze this behavior.
When can rE or rI decrease with contrast?
!
n−1
n
1
0
rE
We define the matrix Φr = nk n
= ψ1 Φ. Then a simple calculation shows
n−1
n
0
rI
∂r
∂r
∂r
that ∂c = Φr (ψJ ∂c + g) or ∂c = (1 − ψΦr J)−1 Φr g, which gives
5.2.1

∂r
=
∂c



1
nk n 

n−1

rE n
n−1

rI n

 
n−1
1
ΩE nψk n rI n + gE

 
n−1
1
n
ΩI nψk n rE + gI



Det (1 − ψΦr J)


 
n−1
1
ny n
Ω
nα
+
g
E
E
1
I
1
 
nα n  n−1 
n−1
1
ψ
n
n
yI
ΩI nα n yE + gI


=

n−1

y En

Det (1 − ΦJ)

(34)

Stability requires that Det (1 − Φr W) > 0. Thus, this expression shows that, for a stable
fixed point, rE or rI decrease with contrast precisely when


gE
gE
∂rE
<0
(35)
ΩE < −
= − 1 n−1
1 n−1
∂c
nψk n rI n
nα n yI n


gI
gI
∂rI
ΩI < −
= − 1 n−1
<0
(36)
1 n−1
∂c
nψk n r n
nα n y n
E

E

26

5.2.2

The c at which rE becomes 0

The c > 0 at which rE first becomes 0 with increasing c can be determined as follows. First,
at this c, rI = cgE /ψJEI , because this is the value of rI that sets the input
to rE tozero

n

cgE
II
= k cgI − cgE JJEI
=
when rE = 0. The equation for the rI steady state then yields ψJ
EI
n

E
. The right side gives zero unless ΩE < 0, so a solution for c 6= 0 exists only
kcn −Ω
JEI
1

 n−1
gE
. This corresponds to
for ΩE < 0. In this case, one can solve to find c = JEI kψ(−Ω
n
E)
n−1
gE
JEI
(−ΩE )n

−ΩE
12
1 .
n−1
gE ) n
(JEI
Note that any fixed point yE = 0, yI > 0 is!stable for any q since the Jacobian matrix is
−1
1
 n−10
 , which has two negative eigenvalues (equal
n−1
J = nα n
1
1
n
n
y ψJIE − q yI ψJII + 1
q I
to the two diagonal entries of J ).
This shows that rE = 0, rI = cgE /ψJEI is a stable fixed point for this value of c, but
does not rule out the existence of other fixed points.

α=

5.2.3

or β =

Peak excitatory firing rate and corresponding contrast

For the case n = 2 and ΩE < 0, we can calculate explicitly the contrast at which the
steady-state excitatory firing rate reaches a local maximum, ∂r∂cE = 0, and the corresponding
excitatory firing rate. We will refer to this as the peak excitatory firing rate. We are
imagining that there is a continuous curve of stable fixed points vs. contrast stretching from
zero firing rate for c = 0 to the fixed point with rE = 0 at positive c found in the previous
section and that the dynamics converge to these fixed points, in which case we are finding
the peak steady-state excitatory firing rate. This has been the case in all the simulations we
have studied. However, for other parameters it is possible that other stable fixed points may
appear, that the maximum occurs at negative c and thus is biologically irrelevant (further
discussed below), and/or that the local maximum found here is an unstable fixed point.
To find the peak excitatory firing rate and the c at which it occurs for the case n = 2,
we have to solve the steady-state equations k(ψJr + cg).2 = r, together with the extremum
condition ∂r∂cE = 0, for the three variables rE , rI and c. The extremum condition is given by
p
Eq. (35) with equality, which for n = 2, yields 4kψ 2 rI = − ΩgEE (as noted above, this can
only happen when ΩE < 0, which we assume here). In order to simplify the equations, we
12

Once rE has been pushed to zero, for increasing
c, rI continues
to increase according to rI = k(cgI −
2
√
1+4cgI ψJII k2 −1
, and rE remains 0.
ψJII rI )n , which for n = 2 has the solution rI =
4kψ 2 J 2
II

27

.2

x
will change variables according to r = 4kψ
2 , where without loss of generality we require x to
be real and positive, ensuring the positivity of r. We also rewrite the steady-state equations
by taking their square roots as

Jx.2 − 2x = −4kψc g,

(37)

gE
. Multiplying Eq. (37) by J−1 ,
|ΩE |


ΩE
Ω
.2
−1
. We then multiply the E
we obtain x − 2J x = 4kψc Det J where the vector Ω =
ΩI
and I components of the latter equation by ΩI and ΩE , respectively (we assume ΩI 6= 0),
and subtract the results. After simplifying, this yields ΩI x2E − ΩE x2I + 2gI xE − 2gE xI = 0.
g2
Finally, substituting for xI using the extremum condition, we obtain ΩI x2E +2gI xE − |ΩEE | = 0,
which has one positive solution
"s
#
gE2 ΩI
gI
−1 ,
(38)
1+ 2
xE =
ΩI
gI |ΩE |

while the extremum condition now becomes xI = − ΩgEE =

yielding

x2E
(39)
4kψ 2
for the maximum firing rate. Note that for this to be positive, xE must be real, which means
an additional condition needs to be satisfied, namely gE2 ΩI > −gI2 |ΩE |.
The contrast, cmax , at which rE peaks can then be solved for using either component of
Eq. (37), yielding


1
gE2
max
2
c
=
JEI 2 + 2xE − JEE xE
(40)
4kψgE
ΩE


gE gI
gE
1
2
JEI 2 +
− JIE xE
(41)
=
4kψgI
ΩE
|ΩE |
rEmax =

with xE given by Eq. (38). Explicit calculation shows that this is always a maximum: the
2
2nd derivative ddcr2E < 0. cmax is not guaranteed to be positive – this is governed by rather
complicated conditions on the g’s and J’s – but in practice we have found it to be positive
for the simulation parameters we have used.13
13 max

c

and rmax correspond to α

x2E gE

JEI

g2
E
Ω2
E

−JEE x2E +2xE

.

=

1
4gE



g2
JEI ΩE2 − JEE x2E + 2xE and yE
E

=

ψ
max
cmax rE

=

However, note that this is not a maximum of the yE vs. α curve, but rather oc-

curs for α higher
than that
4.2 that

 peak, where the curve has a negative slope. We saw in section
1
(n−1)α dy
y
dy
y
− n−1
∂r
∂r
.
∂c =
ψ
dα + (n−1)α , so ∂c = 0 implies dα = − (n−1)α , i.e. y is locally evolving as α

28

When cmax is positive and rEmax is indeed the peak steady-state excitatory firing rate, then
Eqs. 39 and 41 show that both the maximum excitatory firing rate that can be achieved by
the network and the contrast at which this maximum is achieved decrease with increasing
ψ. In this case, when the 2-D reduced model, Eqs. 22-23, accurately captures the highdimensional model, Eqs. 18-19, then in the high-D model, if the stimulus is widened or a
second stimulus is added, the maximum excitatory firing rate will decrease and will occur at
a lower contrast.
In sum, for Det J > 0 and n = 2, the steady state solution for rE has a maximum value
2
as a function of c (i.e., a point with ∂r∂cE = 0 and ∂∂cr2E < 0), given by Eq. 39, precisely when
(1) ΩE < 0 and (2)

5.3

gI2
2 ΩE
gE

< ΩI .

Steady-state solutions for different parameter regimes

In Figs. 2-3 we illustrate model behavior, as a function of stimulus strength c, for 5 parameter
regimes, with Det J > 0, n = 2, and gE = gI in all cases. In Fig. 2 we illustrate behavior
across a large range of c, sufficient to see overall model behavior. To better illustrate the
region around the transition to normalizing behavior, in Fig. 3 we replot Fig. 2 but restricting
to the range c = 0 to 40. The 5 illustrated parameter regimes are: ΩE < 0 and ΩI < 0, with
either ΩE < ΩI (Figs. 2-3A) or ΩE > ΩI (Figs. 2-3E); ΩE < 0 and ΩI > 0 (Figs. 2-3B);
and ΩE > 0 and ΩI > 0, with either ΩE < ΩI (Figs. 2-3C) or ΩE > ΩI (Figs. 2-3D). We
chose parameters relatively arbitrarily, by starting with a set of parameters that had worked
well in simulations of the ring model (Fig. 1 and Fig. 2-3A) and changing small sets of
parameters to change the regime. However in small amounts of studies of other parameters
in the different regimes we have found behaviors to be similar to those illustrated, with one
exception. For ΩE < 0, the transition to sublinear scaling can occur and the excitatory firing
rate can peak and be driven to zero without the excitatory rate ever reaching a level at which
the excitatory subnetwork is unstable. This would be manifested in the figures as stability
for all possible values of q (fifth row, described below). In simulations we only encountered
this for ΩI > 0 with relatively weak JEE (note that ΩI > 0 includes the case JEE = 0, and
presumably sufficiently small JEE behaves similarly to that case), but it can occur for ΩI < 0
as well. The conditions in which the excitatory subnetwork does not become unstable are
discussed more generally in Section 5.4.
For each set of parameters, we first illustrate firing rates (top row), with red and blue
indicating rE and rI respectively. As expected, parameters with ΩE < 0 (columns A,B,E) all
show rE eventually pushed to zero with increasing c, while those with ΩE > 0 (columns C,D)
show rE moving toward linear growth with increasing c. The combination ΩI < ΩE < 0
(column E) leads, as mentioned previously, to unbiological behavior in which both E and I
rates abruptly jump (discontinuously, in numerical calculations with c discretized in 0.00001
29

steps) to high rates at low c, after which rE monotonically falls with increasing c.
If biology is represented by a case with ΩE < 0 and ΩE < ΩI (columns A,B), we imagine
the dynamic range of cortex, corresponding to the dynamic range of the firing rates of the
inputs to cortex, represents a smaller range extending up to and slightly beyond the point
at which rE peaks as a function of c, as discussed in Section 5.2. An example is the range
through c = 100 in Fig. 1A, reduced model, 1 stimulus, which uses essentially the same
parameters as Figs. 2-3A. Biologically, supersaturation begins at high contrasts, e.g. 75%
(C. Y. Li & Creutzfeldt, 1984), well beyond the contrasts (10%-20%) at which the transition
from sublinear to supralinear summation (Heuer & Britten, 2002; Ohshiro et al., 2011) or
from surround facilitation to surround suppression (Sengpiel et al., 1997; Polat et al., 1998)
occur. That is, while the dynamic range of cortex ends shortly after supersaturation is seen,
much of this dynamic range exhibits normalizing behavior. Similarly, the model shows a
broad dynamic range between the onset of normalization and of supersaturation for most
parameter choices we have explored, the only exception again being the case ΩE < 0 and
ΩI > 0 for small JEE .
We next illustrate normalization weights (second row), computed just as in Fig. 1B, right
column, so that weights > 1 (weights < 1) indicate supralinear (sublinear) summation of
responses to two orthogonal gratings of equal strength in the corresponding ring model. All
but the case ΩE > ΩI > 0 show a regime of supralinear summation for very low contrasts
(behavior in all cases is sublinear for c > 10), although the supralinear behavior is weak for
ΩI > 0.
The third and fourth rows of Fig. 3 illustrate the iterative solutions that stem from the
scaling solutions in the low- and high-contrast regimes (the high-contrast iterative solutions
are also illustrated in the third row of Fig. 2). The values of J used (listed in legend of
Fig. 2) are not normalized to have kJk = 1, so for these iterations we take Ĵ = J/kJk where
kJk is the 2-norm of J (the maximum singular value of J), and redefine α and y such that
yc
α = kcn−1 ψkJk and r = ψkJk
. We show the iteration results as rE vs. c. The reason for this
rescaling is that, as discussed in Section 3, with this definition of α the transitions to the
sublinearly normalizing regime happen at α ∼ 1, irrespective of kJk.
The small-α (low contrast) iterations are shown in the third row of Fig. 3. Here, we treat
the equation y = α(Ĵy + g).n as the recurrence relation
y[t] = α(Ĵy[t − 1] + g).n

(42)

Iteration of this recurrence relation generates higher- and higher-order approximations to
Eq. 8 for the steady state. We use the starting condition y[0] = 0. Results are shown
for numbers of iterations ranging from 1 to 19 (red through yellow colors). As few as 5
iterations gives a good approximation for small c, while increasing the number of iterations
to 19 adds little. The low-contrast iterations all fail before or very slightly after c = 10,
30

A

Firing Rate

C

B
Ω E < 0 < ΩI

ΩE < Ω I < 0

D
0 < ΩE < ΩI

E
0 < ΩI < ΩE

Ω I < ΩE < 0

40

10

1000

4000

200

20

5

500

2000

100

0
0

200

400

0
0

200

400

0
0

200

400

0
0

200

400

0
0

1000

1000

4000

2000

1000

500

500

2000

1000

500

0
0

200

Weight

1.5

400

E cells
I cells

1
0.5
0
0

200

400

400

1.5
1

0
0

200

400

8

30

0
0

200

400

4

10

2
200

400

10

1.4

1

1.2

0.95

1

0.9

0.8

0.85

0

200

400

400

0
0
1.5

200
40

400

20
0
0

1

10

20

0.5

200

400

0
0

200

400

200

400

200

400

200

400

2000

0
0

500

200

400

15

0
0

200

400

10

5
5

400

200

400

0

200

400

0
0

200

400

0

0
0

200

400

0
0

20

20

15

15

10

10

5

5

200

400

0
0

200

400

0

200

400

0
0

200

400

0

criteria

200

0
0

50

1000

10

q

0.8
0

200

100

1000

5

0
0

0
0

400

6

20

0
0

200

0.5

40

Firing Rate

0
0

200

0

Stimulus Strength

Stimulus Strength

Stimulus Strength

Stimulus Strength

Stimulus Strength

Figure 2: Behavior of the 2D Model in Different Parameter Regimes. Each column
corresponds to a different connectivity matrix J, corresponding to different conditions on ΩE and
ΩI as indicated at top. In all cases, Det J > 0, n = 2.0, k = 0.04, and gE = gI = 1. The first
column uses the same parameters as the 2-D reduced model in Fig. 1. In all figures the horizontal
axis is stimulus strength c; Fig. 3 shows all plots (and one additional set of plots) for the smaller
range c ∈ [0, 40]. Top row: E (red, top) and I (blue, bottom) firing rates, rE and rI , at fixed
point. For cases with ΩE < 0, dashed vertical lines indicate analytic calculations for c at which rE
goes to zero (Sec. 5.2.2) and, for ΩE < ΩI , at which rE peaks (Eq. 41). Second Row: Weights
reflecting supralinear (weight > 1) or sublinear (weight < 1) summation in an equivalent ring
model, computed as in Fig. 1B. Red and blue indicate E- and I-subnetworks, respectively. Inset in
column E shows supralinear responses at low values of c. Third Row: Iterative solutions for rE
in the high-contrast regime (Eq. 43). We plot rE [t] = yE [t]c/(ψkJk) vs. c, for t = 1, 5, 10, 14, 19
iterations (blue to cyan curves); black curves are exact solutions. Iterative solutions are shown only
over the range for which they are real. (Iterative solutions in the low-contrast regime are shown
in Fig. 3.) Fourth Row: Values of q = τI /τE separating regions in which fixed point is stable
(below red line) vs. unstable (above red line). Fifth Row: Horizontal lines showing the extent of
the sublinear regime according to the different definitions introduced in Sec. 5.4. Blue and red lines
(E component solid, I component dashed): definitions 1 (normalization in corresponding highdimensional ring model) and 5 (r a sublinear function of c), respectively. Green line: definition
2 (excitatory subnetwork unstable). The cyan lines show the range where the modulus of each
eigenvalue of the Jacobian is > 1; sublinear regime according to definition 3 (instability of lowcontrast iterative solution) or 4 (stability of high-contrast iterative solution) is the region in which
either (def. 3) or both (def. 4) lines are present. Parameters used: ψ = 0.774 or, for two-grating
case in 2nd row, ψ = 1.024 (the values of Ψ in Fig. 1); JEI = 1.3; JEE = 2.5, except 0.8 in (D);
JII = 1.0, except 2.2 in (C) and 5.0 in (D); JIE = 2.4, 4.7, 4.7, 3.6, 2.2 in (A) to (E), respectively.

32

A

Ω E < 0 < ΩI

Firing Rate

40

10

20

5

0
0

C

B
ΩE < Ω I < 0

10

20

30

40

0
0

D
0 < ΩE < ΩI

E
0 < Ω I < ΩE

50

ΩI < ΩE < 0

50

100
50

10

20

30

40

0
0

10

20

30

40

0
0

10

20

30

40

0
0

100

40

200

40

200

50

20

100

20

100

0
0

10

20

30

Weight

1.5

40

E cells
I cells

0
0

10

20

30

40

0
0

10

20

30

40

0
0

1.5

1.5

1

1

1

0.95

10

20

30

40

0
0

10

20

30

40

10

20

30

40

10

20

30

40

3
2

1

1
0.5
0

10

20

30

40

10

20

30

40

10

20

30

40

1

0
0

5

10

Firing Rate

0
0

5

10

5

10

40

30

30

20

4

20

20

10

2

10

10

30

40

10

0
0

4

2

0
0

6

20

10

20

30

40

15

0
0

10

20

30

40

0
0

10

100

5

50

5

10

30

40

10

20

30

40

0

q

3

4

5

10

20

30

40

0
0

10

20

30

40

10

20

30

40

10

20

30

40

20

10
5

10

20

30

40

0
0

10

20

30

40

0

10

20

30

40

0
0

10

20

30

40

0

10

20

30

40

0
0

10

20

30

40

0

criteria

20

2

50

5

10

1

15

5

0
0

0
0

100

10

0
0

0
0

1

0
0

50

10

40

2

40

0
0

30

3

50

30

20

5

8

40

10

10

4
5

0.9
0

6

2

10

0.5
0
8

3

15

Firing Rate

0.5
0

0

Stimulus Strength
0

1

2

3

α

Stimulus Strength
4

0

2

4

α

Stimulus Strength
6

0

2

4

α

6

Stimulus Strength
0

2

4

α

6

Stimulus Strength
0

1

2

3

4

α

Figure 3: Crossover of the 2D Model to the High Contrast Sublinear Regime for
Different Network Parameters. The plots in this figure are the same as the ones in Fig. 2,
except that (1) only the range of stimulus strengths, c, from 0 to 40 is shown, to highlight the
transition to the sublinear regime as c grows and (2) we also illustrate the low-contrast iterative
solutions for rE , which have been inserted as the third row (conventions as for high-contrast iterative
solutions, except here red to yellow curves represent 1 to 19 iterations). See the caption of Fig. 2 for
explanation of plots and parameters. The extra horizontal axes at the bottom translate the stimulus
strengths into values of α as defined in Eq. (5). In addition, vertical dashed lines in the first to fifth
rows indicate the transition points to the sublinear regime, according to the different definitions
introduced in Sec. 5.4 and illustrated in the bottom row, with definitions 1 to 5 corresponding to
colors blue, green, orange, cyan and red, respectively (for definitions 1 and 5 the line is drawn
at the point where the condition holds for both E and I components). The values of the α’s
at these transition lines, in the order mentioned, are (1.4, 0.7, 1.4, 1.4, 2.4), (1.0, 1.4, 1.4, 1.4, 16.3),
(2.4, 1.1, 3.1, 3.1, −), (0.2, 4.7, 1.0, 28.5, −), and (0.8, 0.6, 0.8, 0.8, 0.8) in columns A to E, respectively.
Notice that the transitions to the sublinear regime typically happen for α ∼ 1, as expected.

which corresponds to α in the range 1.4 to 2.4 across the parameters. That is, the failure
occurs for α ∼ 1, as expected.


1
For the high-α or small-β (high contrast) case, we treat the equation y = Ĵ−1 −g + βy. n
as the recurrence relation:


1
y[t] = Ĵ−1 −g + βy[t − 1]. n
(43)

Iterations generate approximations to Eq. 14 for the steady state. We use as starting conditions yE [0] = 0 with yI [0] = 0 for ΩE > 0, yI [0] = gI /JˆEI for ΩE < 0. For ΩE < 0, using
y[0] = 0 would give complex solutions. We instead use as a starting condition the value √
of y
when yE reaches zero with increasing c. Recall that β increases with decreasing c, β = 1/ α.
For small β (large contrasts) we expect a basin of attraction about the fixed point, such that
the iterations will converge to the fixed point for initial conditions in the basin of attraction.
As β increases (contrast decreases) the basin of attraction should disappear for β ∼ 1, so
that convergence will fail for any initial condition.
The third row of Fig. 2 (fourth row of Fig. 3) illustrates these high-contrast solutions,
with blue through cyan colors corresponding to 1 through 19 iterations. Again, 5 iterations
do about as well as larger numbers of iterations. The iterations give good approximations
for high c but, for ΩE < 0, fail for larger c as rE approaches zero. β is very small for these
large c’s, so this presumably represents the initial conditions no longer being in the basin
of attraction of the fixed point. For low c failure of convergence is expected for β ∼ 1,
although problems with the basin of attraction could also arise. None of the iterations work
for c below about 9 or 10, corresponding to β roughly above .65 to .85, with the exception
of column E. In that column, the largest number of iterations works down to the jump in
rE and rI , which occurs at about c = 5.435 for the given parameters, or β around 1.1. In
column D the iterations do not work below c about 190, which corresponds to β above about
0.15, a somewhat lower value than expected for the β at which iterations fail.
In the fourth row of Fig. 2 (fifth row of Fig. 3), we show the value of q = ττEI that
divides stability (values below curves) from instability (values above curves) of the fixed
point, according to Eq. 32. In all cases except ΩI < ΩE < 0, the fixed point remains
stable for q < 1 across the range of studied stimulus strengths, indicating that fine tuning
or unreasonably small values of q are not required.
Finally, the fifth row of Fig. 2 (sixth row of Fig. 3) shows the extent of the sublinearly
normalizing regime. Specifically, the solid and dashed blue horizontal lines indicate the range
E
I
of stimulus strengths for which ∂r
< 0 and ∂r
< 0, respectively. As discussed in Section 4.2,
∂ψ
∂ψ
these conditions are roughly equivalent (so long as the approximate ansatz for 2-D reduction
introduced in Section 4.1 is valid) to sublinear normalization of E and I subnetworks in the

33

full high-dimensional ring network considered in Section 4.1.14 The other horizontal lines
in the plots of this row show the extent of the sublinear regime according to other criteria
introduced in the next subsection, and are explained there.
In Fig. 4 we examine the quality of the approximate 2-dimensional reduction of the highdimensional ring model (Section 4.1) in the five different parameter regimes of Fig. 2. In
the top two rows, we have plotted the peak responses of the E and I subnetworks and their
respective normalization weights for the original high-dimensional ring network of Eqs. (16)–
(17) with the connectivity given by Eq. (25), which can be compared with those in the
top two rows of Fig. 2. We see that the 2-D model captures the behavior of the original
model very well. Recall that the approximation involved in the reduction to the 2-D model
·r̂Y
in Eq. (20) as constants, independent of parameters of
involved taking the ratios ww~~XY
EE ·r̂E
the input stimulus (e.g. its shape and strength), and absorbing all such dependencies into
Ψ=w
~ EE · r̂E . More generally, we could have defined ΨXY = w
~ XY · r̂Y (with X, Y ∈ {E, I}).
In the model of Eq. (25), the vectors w
~ XY are
independent
of X and Y by construction:


d(0,θ)2
w
~ XY ≡ w
~ where w
~ has elements w(θ)
~
≡ exp − 2σ2
∆θ. Therefore in this case we only
ori
have two independent ΨXY , which we relabel as ΨE ≡ w
~ · r̂E and ΨI ≡ w
~ · r̂I . Finally, we
also made the approximation that the response shape curves, r̂X , can be well approximated
by the shape of the input, g, raised to the power n, and we used the latter to calculate the
ψ used in the 2-D model, i.e. we took ψ = w
~ · ĝ.n . The third and fourth rows of Fig. 4 show
plots of ΨE (red), ΨI (blue), and ψ (green) as a function of stimulus strength, for the case of
one or two grating stimuli, respectively. The bottom row of Fig. 4 plots the ratio of the curves
in the fourth row (two gratings) to the corresponding ones in the third row (one grating). In
the discussion of Section 4.2 we assumed that ΨX should typically be larger for the case of two
gratings; we then concluded that sublinear normalization weights for the E (I) subnetwork
are hence roughly equivalent to rE (rI ) being a decreasing function of ψ in the 2-D reduced
model. We see from the bottom row of Fig. 4 that in some parameter regimes and for high
enough c this assumption can weakly fail for ΨE . Surprisingly, sometimes this failure is
E
accompanied by the condition ∂r
< 0 in the 2-D model, and yet the normalization weights
∂ψ
in the full model are sublinear (< 1). This is due to the failure of the other assumption in
the ansatz, i.e. it is due to the fact that ΨE 6= ΨI 6= ψ.
X
Note that the onset of the conditions ∂r
∂ψ < 0 (vertical dashed blue lines in 2nd row, Fig. 3, corresponding
to onset of blue lines in bottom row) occurs for slightly higher c than the onset of normalizing behavior (2nd
row, Fig. 3). This is because in the 2nd row we are assaying normalization in response to a finite-strength
(equal-contrast) 2nd stimulus, for which, as discussed in Section 4.2, the condition for normalization becomes
R Ψfinal
X
dΨ ∂r
∂Ψ < 0.
Ψinit

14

34

A
50

Firing Rate

C

B
Ω E < 0 < ΩI

ΩE < Ω I < 0
20

200

400

200

400

500
0
0

2

E cells
I cells

Weight

0
0

200

400

0
0
4000

200

400

200

400

2000
0
0

2

1.4

1.5

1.2

0
0
2000

100
200

400

200

400

1000
0
0

1

1

1

0.5

0.5

0.8

1.05

400

0
0

200

400

0.6
0

0
0

1.5

200

400

0.8
0

200

400

0
0

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.2

200

400

200

400

200

400

0
0

200

400

0
0

1

1

1

1

1

0.5

0.5

0.5

0.5

0.5

0
0

200

400

0
0

200

400

0
0

200

400

0
0

200

400

0
0

Ψ2 1.5
Ψ1

1.5

1.5

1.5

1.5

1

1

1

1

1

0.5

0.5

0.5

0.5

0.5

0
0

200

400

Stimulus Strength

0
0

200

400

Stimulus Strength

0
0

400

10
0
0

10

20

0.5

0.8

0
0

200
20

0.85

0.6

0
0

400

1

0.8

0
0

200

1

0.9

200

0
0
1000
500

0.95

0
0

Ψ2

400

ΩI < ΩE < 0
200

2000

500
200

500

1.5

Ψ1

0
0
1000

E
0 < ΩI < ΩE

4000

1000

10
0
0
1000

D
0 < ΩE < ΩI

200

400

Stimulus Strength

0
0

200

400

Stimulus Strength

0
0

200

400

200

400

200

400

200

400

Stimulus Strength

Figure 4: Behavior of the Full Ring Model in Different Parameter Regimes. Behavior of
the steady state of the ring network of Sec. 4.1, in the same parameter regimes as in Fig. 2. The
ring network’s connectivity matrix is given by Eq. (25), with different J’s in different columns equal
to those in the corresponding column of Fig. 2. The rest of the parameters are the same as in the
left column of Fig. 1 (in particular, all parameters of column A match those of Fig. 1, left column).
The signs and orderings of ΩE and ΩI are indicated on the top of each column. In all figures the
horizontal axis is stimulus strength c. Top row: E (red) and I (blue) firing rates, rE (θ = 0) and
rI (θ = 0), at fixed point. For cases with ΩE < 0, dashed vertical lines indicate analytic calculations
for c in the 2-D reduced model at which rE goes to zero (Section 5.2.2) and, for ΩE < ΩI , at
which rE peaks (Eq. 41). Second Row: Weights reflecting supralinear summation (weight > 1)
or sublinear summation (weight < 1) computed as in Fig. 1B. Again, red and blue indicate Eand I-subnetworks, respectively. Inset in column E shows supralinear responses at low values of c.
Third Row: The red and blue curves show ΨE ≡ w
~ · r̂E and ΨI ≡ w
~ · r̂I , which we approximated
.n
by ψ = w
~ · ĝ (green lines) in the 2-D reduction for the case of a one-grating stimulus (see the
discussion at the end of Sec. 5.3). Fourth Row: The same as the third row, but for two-grating
stimuli. Fifth Row: The red, blue and green curves show the ratios of the red, blue and green
curves in the fourth row (two gratings) to those in the third row (one grating), respectively.

5.4

Different criteria for crossover to the sublinearly normalizing
regime

As we saw in Section 4.2, the condition that the E and I responses in the high-dimensional
I
E
< 0 and ∂r
< 0 in the 2-D model,
network be normalizing is roughly equivalent to ∂r
∂ψ
∂ψ
respectively. Here, rE and rI refer to their values at a stable fixed point. More generally,
we have seen that for sufficiently low stimulus strengths the network is supralinear (with
normalization weights > 1), but switches to a sublinear regime (with normalization weights
< 1) as stimulus strength becomes sufficiently large. One can, however, come up with
different notions or criteria for the transition from the supralinear to the sublinear regime
as stimulus contrast grows. Furthermore, since this is typically a smooth crossover, and
not a sharp phase transition, such different criteria in general do not yield the exact same
value of contrast at the transition, although as we will see, they all yield the same order of
magnitude for the transition contrast. As this crossover is at the heart of the present study,
in this section we set out to examine more closely the different criteria for the transition to
the sublinear regime and their inter-relationships.
We note that the criteria we will examine all involve addition of stimuli without change
in the values of gE or gI (or, for normalization in the high-dimensional ring model, addition
of a second stimulus at a different location on the ring but with the same relative strengths of
inputs to E vs. I cells as the first stimulus). One can imagine a different kind of sublinearity
due to rectification, in which a stimulus 1 has a large enough ggEI that by itself it produces
rE > 0 and rI > 0, while a smaller stimulus 2 that has small ggEI by itself produces rE = 0
and rI > 0. Then addition of stimulus 2 to stimulus 1 would reduce rE and, if the network
is in the ISN regime in response to stimulus 1, reduce rI (Ozeki et al., 2009; Tsodyks et
al., 1997), relative to the response to stimulus 1 alone. This sublinearity due to rectification
would also occur in a linear threshold network and is a separate effect from effects due to
the sublinear regime of the SSN. More generally, the results of adding stimuli with different
values of ggEI are beyond the scope of what we consider in this paper.
Here we introduce the following five different criteria for the transition to the sublinearly
normalizing regime of the SSN, expressed in terms of the 2-D model, and study their interrelationships:
1. The direct definition of normalization in the high-dimensional ring model, as it follows
from the approximate 2-D reduction:
∂r
∂ ln r
dy
< 0 ⇐⇒
< n ⇐⇒ α
<y
∂ψ
∂ ln c
dα

(44)

where the equivalences (demonstrated in Section 4.2) hold component by component. Note that each component in Eq. (44) expresses a separate condition, i.e.
36

the normalizing property for the E and the I rates, respectively. We can obtain
dy
by looking at the variation of the fixed point equation Eq. (7),
an expression for dα
which yields (−1 + ΦĴ)δy + (Ĵy + g).n δα = 0. Here, Φ is the diagonal matrix
1
1
1
nα n diag(y1− n ) = nβ −1 diag(y1− n ). Using Eq. (7) again, we obtain
α
where we defined
K=



dy
= −(β −1 K − 1)−1 y
dα

KEE −KEI
KIE −KII

1



(45)

1

≡ n diag(y1− n )Ĵ

(46)

(note that given the positivity of diag(y1− n ), all KXY are positive as defined). Thus
Eq. (44) is equivalent (component by component) to
y > −β(K − β1)−1 y.

(47)

2. The instability of the excitatory subnetwork by itself (i.e. with rI frozen at its fixed
point value). As we saw in Section 5.1.2, this can be expressed as
JEE > 0

(48)

where the matrix J = T−1 (β −1K − 1), defined in Eq. (30), is the Jacobian of the 2-D
flow at the fixed point. Given that T is positive and diagonal, Eq. (48) is equivalent
to β −1 KEE > 1, or
1− 1
(49)
KEE = nJˆEE yE n > β.
1− 1

1

1

1− 1

This criterion can also be written nα n JˆEE yE n > 1 or nk n WEE rE n > 1. For ΩE > 0,
rE becomes arbitrarily large and this criterion will always be met for nonzero JˆEE . For
ΩE < 0, the latter form makes clear that the excitatory subnetwork becomes unstable
iff it is unstable at the maximum value of rE . For ΩE < 0 for n = 2, if we assume
a single curve of stable fixed points vs. contrast that peaks at the maximum value
of rE given in Eq. 39, we find that the condition for instability at the maximum of
rE , and thus for the excitatory network to become unstable, is JEE x
E > 1 (xE given

2
by Eq. (38)). This can be reduced to the condition gE2 JˆEE
> |ΩE | gI JˆEE + gE JˆIE
(along with the condition gI2 gE2 ΩE < ΩI required for real xE ). This illustrates that the
excitatory subnetwork will never become unstable for sufficiently small JEE . However,
when other conditions are also imposed (excitatory subnetwork never becomes unstable
AND Det J > 0, cmax > 0 (Eqs. 40-41), fixed point at cmax stable) the requirements on
the J’s and g’s for all of these conditions to be satisfied become far more complex.
37

3. Local instability of the low-contrast iteration scheme, Eq. (42), at its fixed point (a
sufficient condition for its divergence). By local instability, we mean the instability of
the linearization of Eq. (42) around the fixed point solution. It is seen from Eq. (42)
that the Jacobian of this linearization is exactly ΦĴ = β −1K where K is the matrix
defined above. The condition for stability of a linear recurrence equation is that the
modulus of all eigenvalues of the (Jacobian) matrix be smaller than 1. Thus the
iteration is linearly unstable around the fixed point if and only if at least one eigenvalue
of K has modulus larger than β:
|λ1 | > β

OR

|λ2 | > β

(50)

where λ1 and λ2 are the eigenvalues of K.
4. Stability of the high-contrast iteration scheme Eq. (43) at its fixed point (a necessary
condition for its convergence). Similarly to the previous criterion, by this condition we
mean the stability of the linearization of Eq. (43) around the fixed point. The Jacobian
1
of the right side of Eq. (43) is given by Ĵ−1 diag( βn y n −1 ) = Ĵ−1 Φ−1 = βK−1 . Thus the
linearization is stable if and only if both eigenvalues of βK−1 have modulus smaller
than 1. Since the eigenvalues of K−1 are the inverse of the eigenvalues of K, this is
equivalent to both eigenvalues of K having modulus larger than β:
|λ1 | > β

AND

|λ2 | > β.

(51)

In particular, this criterion clearly implies criterion 3.
Even though these four criteria do not define exactly the same transition point (i.e. the
smallest value of α, or the corresponding value of β, for which a criterion first holds), as we
will now argue, they typically occur for similar values of α (or β) that are O(1). First, let
us consider the parameter regimes for which ΩE ∝ (−Ĵ−1 g)E > 0.15 As we saw in Sec. 3.2,
in this case, for small β, y(β) asymptotically approaches the value −Ĵ−1 g. As we have
normalized the magnitude of Ĵ and g, this β-independent asymptotic value will be typically
O(1). Therefore, from Eq. (46), K also asymptotically approaches a β-independent limit
with entries, eigenvalues, and norm of modulus O(1). Hence, the left hand sides of the
inequalities in Eq. (47) and Eqs. (49)-(51) approach an O(1) positive constant value as β
decreases (equivalently, as α increases), while the right hand sides asymptotically decrease
linearly in magnitude with β to zero.16 Thus these criteria will always be satisfied for small
15

As we noted after Eq. (26), for positive inputs, gE ≥ 0 and gI ≥ 0, (−Ĵ−1 g)E > 0 implies (−Ĵ−1 g)I > 0
The linear decrease of the right hand sides in Eqs. (49)-(51) with β is obvious. The right hand side
of Eq. (47) asymptotically (as β → 0) behaves like −βK−1 y, and thus asymptotically also decreases in
magnitude linearly with β to zero.
16

38

enough β, and we expect that the transition (largest β for which the criterion holds) happens
for β ∼ 1.
The argument for parameter regimes for which ΩE ∝ (−Ĵ−1 g)E < 0 is less straightforward, as in this case −Ĵ−1 g = O(1) does not provide a valid asymptotic value for y. Instead,
as we saw in the discussion of supersaturation in Sec. 5.2.1, yE (or rE ) typically reaches a
maximum and then decreases with increasing α, vanishing at a finite value of α (or β).
However, as long as parameters (Ĵ and g) are such that (1) the maximal value of yE (or rE )
is O(1) or larger, so that the left-hand sides of the inequalities are positive and of magnitude
O(1) or larger; and (2) the contrast at which yE (or rE ) is maximized is large enough and
thus the corresponding β ≡ β max small enough, β max ≪ 1, so that the right-hand sides of the
inequalities are of magnitude O(β max ); then there will be a finite region of β’s around β max
for which the above criteria are satisfied. Thus, as long as supersaturation does not begin
too early, we expect that transitions according to all the above criteria typically happen for
β ∼ O(1) also in this case.17 As we noted in Sec. 5.3, biologically supersaturation begins at
high contrasts, e.g. 75% (C. Y. Li & Creutzfeldt, 1984), relative to the contrasts (10%-20%)
at which the transition from supralinear to sublinear behavior occur (Heuer & Britten, 2002;
Ohshiro et al., 2011; Sengpiel et al., 1997; Polat et al., 1998). Thus we expect that biologically relevant parameters should yield a relatively large cmax , and a correspondingly small
β max ≪ 1.
In addition to the above criteria, we also introduce a fifth criterion for sublinear response,
which is directly based on the sublinearity of the E and I contrast-response curves, such as
those plotted in the first rows of Figs. 2–3. However, as we will see this last criterion is not
always as strongly associated with the crossover that happens for α, β ∼ O(1).
5. Sublinear growth of rE and/or rI with stimulus strength c:
∂ ln r
dy
< 1 ⇐⇒
< 0.
∂ ln c
dα

(52)

Here, the inequality on the left is the mathematical expression of sublinear growth. As
for criterion 1, we have two separate conditions here, stating the sublinear growth of
the E and I rates, respectively. To see the equivalence with the right side in Eq. (52),
note that from the definitions (4)–(5) we have ln r = ln c − ln ψ + ln y + const., and
ln y
r
= 1 + (n − 1) dd ln
, from which (given that
ln α = (n − 1) ln c + ln ψ + const. Thus ∂∂ ln
ln c
α
d ln y
n > 1) the equivalence of the left side with d ln α < 0 follows. Given the positivity of
y and α the latter is equivalent to dy/dα < 0. Finally, it follows from Eq. (45) that
17

An exception to this rule was noted at the beginning of Sec. Eq. (5.3). In that example, JEE is atypically
small, while supersaturation starts at α ∼ 1 and yE is pushed to zero too fast, for a value of α that is not
large, such that criteria 2, i.e. the instability of the excitatory subnetwork, is never realized.

39

Eq. (52) is equivalent component-wise to
(K − β1)−1 y > 0.

(53)

The reasoning used for criteria 1-4 to argue that they should typically occur for β & 1,
cannot be used for this criteria, as the left hand side of Eq. (53) has an asymptotic
value K−1 y for small β, and this need not have a definite sign for either component.
However, as discussed in Sec. 5.2.1, when ΩE < 0, and assuming a single curve of fixed
r
points vs. c, rE will eventually decrease with c, i.e. eventually ∂∂ ln
becomes negative,
ln c
which means that for some lower value of c (or α) it must have become less than unity.
Thus the transition according to this criterion should always occur when ΩE < 0. On
the other hand, for parameter regimes for which ΩE > 0, criteria 2 may never be
realized (as in columns C and D of Fig. 2),18 so this criteria is the least suitable way
of characterizing this transition.
The range of stimulus strengths, c, corresponding to the sublinear regime according to
each of these criteria is demonstrated in the bottom row of Figs. 2 and 3, for the five choices
of the connectivity matrix J as explained in Section 5.3 (see the figure captions for further
explanation). Moreover, vertical lines in the plots of Fig. 3 indicate the transition points
from the supralinear (low c) to the sublinear (high c) regime. The values at the transitions of
α = kcn−1 ψkJk ( = kψkJkc for the value n = 2 used in the figures) according to these criteria
are also given in the caption of Fig. 3. As expected, the transitions occur for α = O(1).
In summary, even though the transitions according to different criteria happen at different
numerical values of α, the transitions according to criteria 1-4 typically happen for α ∼ 1,
as motivated on general grounds in Section 3 and discussed in more detail here (with the
exception that criterion 2 may not occur for some parameter regimes with weak E ⇒ E
connections). That is, all occur as part of the overall transition from supralinear behavior
in the weak-input regime (α ≪ 1) to sublinear behavior in the strong-input regime (α ≫ 1).
However, the transition according to criteria 5 may never be realized if ΩE > 0, and is less
suitable as a marker of the overall transition studied here.

6

Discussion

We have shown in studies of a 2-D system (and found in simulation studies of higherdimensional systems, to be presented elsewhere) that the supralinear network will dynami18

This is because in those parameter regimes the zeroth order solution Eq. (11) for the high contrast
regime yields positive values for both the E and I components, allowing for the subleading correction βy1
to be negative without making the total y = y0 + βy1 negative, as long as contrast is large enough (i.e. β
is small enough). Now according to Eq. (52) criteria 2 is equivalent to ∂y/∂α < 0, or ∂y/∂β > 0, while to
leading order ∂y/∂β = y1 . Hence if the allowed possibility y1 < 0 is realized, criteria 2 will never be.

40

cally stabilize with increasing input strength provided the I ⇒ E and E ⇒ I connections
mediating feedback inhibition are sufficiently strong and the inhibitory time constant is not
too slow. This dynamic stabilization results in a change from responses scaling supralinearly
to responses scaling sublinearly with the addition of a second input. The system can also
yield “supersaturation”, in which excitatory firing rates reach a peak with increasing input
strengths and then decrease (as observed biologically (Ledgeway et al., 2005; C. Y. Li &
Creutzfeldt, 1984; Tyler & Apkarian, 1985; Peirce, 2007), and as also noted theoretically by
Persi et al. (2011)), with rates ultimately decreasing to zero for large enough input strengths
(which presumably are beyond the dynamical range of biological inputs). The conditions for
this to occur were characterized in the 2-D system. The strongest sublinear behavior, and
hence behavior most likely to underly biological observations in cerebral cortex, occurs for
parameters that lead to supersaturation. As we will show in work to be presented elsewhere
(presented as Abstracts in Miller and Rubin (2010); Rubin and Miller (2010); Miller and
Rubin (2011); Rubin and Miller (2011)), this framework offers a unifying explanation for a
number of processes involving multi-input integration in sensory cortex, including normalization and surround suppression.
Many questions remain outstanding. As some examples: within the range of models
analyzed here, can more precise results, analogous to those obtained here for 2-dimensional
models, be obtained for higher-dimensional models, for which we only discussed general
scaling arguments? For any dimensionality, can useful results be obtained as to when the
network is globally stable? How will diversity of network parameters, including in particular
of the power n, alter behavior? Presumably an even slightly larger mean n for I vs. E cells will
enormously enhance the range of parameters that will stabilize; experiments suggest that I
cells have significantly higher powers (Haider et al. (2010), Supp. Fig. S3d). How will cell-tocell variability of n affect behavior? How will behavior be affected by taking into account the
decreased noise level, and thus increase in n (Miller & Troyer, 2002; Hansel & van Vreeswijk,
2002), that occurs with increasing stimulus contrast (Finn et al., 2007), i.e. with increasing
input firing rate? How will network behavior be modified by addition of short-term synaptic
facilitation and depression (e.g. Fioravante and Regehr (2011))? Can analysis be done of
more biophysically realistic models, such as networks of integrate-and-fire neurons, which
have an input/output function well approximated by a power law so long as they are firing
on input fluctuations rather than the mean input (Hansel & van Vreeswijk, 2002)? Note
that in these models, the noise level, which as just mentioned controls the power n, can
itself be determined dynamically and differ between E and I cells (e.g. van Vreeswijk and
Sompolinsky (1998); Renart et al. (2010)). What can we learn as we move beyond the steady
state to network dynamics, particularly using more realistic models that can better capture
faster dynamics and that incorporate synaptic delays? How will the network behave when
we incorporate multiple types of inhibitory neurons (e.g. Isaacson and Scanziani (2011)), or
41

of excitatory neurons, each with their own (largely still unknown) connectivity patterns and
biophysical properties?
Despite the many open questions, we believe the basic findings are likely to be quite
robust and to underly a wide range of cerebral cortical behavior: networks of units with
supralinear input/output functions can dynamically stabilize, resulting in a transition from
supralinear to sublinear input summation.

References
Anderson, J. S., Carandini, M., & Ferster, D. (2000). Orientation tuning of input conductance, excitation, and inhibition in cat primary visual cortex. J Neurophysiol , 84 (2),
909–926.
Anderson, J. S., Lampl, I., Gillespie, D., & Ferster, D. (2000). The contribution of noise to
contrast invariance of orientation tuning in cat visual cortex. Science, 290 , 1968–1972.
Anderson, J. S., Lampl, I., Gillespie, D. C., & Ferster, D. (2001). Membrane potential and
conductance changes underlying length tuning of cells in cat primary visual cortex. J
Neurosci , 21 (6), 2104–2112.
Bruno, R. M. (2011). Synchrony in sensation. Curr Opin Neurobiol , 21 (5), 701–708.
Carandini, M., & Heeger, D. J. (2012). Normalization as a canonical neural computation.
Nat Rev Neurosci , 13 (1), 51–62.
Cavanaugh, J. R., Bair, W., & Movshon, J. A. (2002). Nature and interaction of signals
from the receptive field center and surround in macaque v1 neurons. J Neurophysiol ,
88 (5), 2530–2546.
Dayan, P., & Abbott, L. (2001). Theoretical neuroscience. Cambridge: MIT Press.
Ermentrout, G. B., & Terman, D. H. (2010). Mathematical foundations of neuroscience.
New York: Springer.
Ferster, D. (1986). Orientation selectivity of synaptic potentials in neurons of cat primary
visual cortex. J Neurosci , 6 (5), 1284–1301.
Ferster, D., & Miller, K. D. (2000). Neural mechanisms of orientation selectivity in the
visual cortex. Annu Rev Neurosci , 23 , 441–471.
Finn, I. M., Priebe, N. J., & Ferster, D. (2007). The emergence of contrast-invariant
orientation tuning in simple cells of cat visual cortex. Neuron, 54 (1), 137–152.
Fioravante, D., & Regehr, W. G. (2011). Short-term forms of presynaptic plasticity. Curr
Opin Neurobiol , 21 (2), 269–274.
Gerstner, W., & Kistler, W. (2002). Spiking neuron models: Single neurons, populations,
plasticity. Cambridge, UK: Cambridge University Press.

42

Haider, B., Krause, M. R., Duque, A., Yu, Y., Touryan, J., Mazer, J. A., & McCormick,
D. A. (2010). Synaptic and network mechanisms of sparse and reliable visual cortical
activity during nonclassical receptive field stimulation. Neuron, 65 (1), 107–121.
Hansel, D., & van Vreeswijk, C. (2002). How noise contributes to contrast invariance of
orientation tuning in cat visual cortex. J Neurosci , 22 (12), 5118–5128.
Heuer, H. W., & Britten, K. H. (2002). Contrast dependence of response normalization in
area mt of the rhesus macaque. J Neurophysiol , 88 (6), 3398–3408.
Isaacson, J. S., & Scanziani, M. (2011). How inhibition shapes cortical activity. Neuron,
72 (2), 231–243.
Lauritzen, T. Z., Krukowski, A. E., & Miller, K. D. (2001). Local correlation-based circuitry
can account for responses to multi-grating stimuli in a model of cat v1. J Neurophysiol ,
86 (4), 1803–1815.
Ledgeway, T., Zhan, C., Johnson, A. P., Song, Y., & Baker, C. L. J. (2005). The directionselective contrast response of area 18 neurons is different for first- and second-order
motion. Vis Neurosci , 22 (1), 87–99.
Li, B., Thompson, J. K., Duong, T., Peterson, M. R., & Freeman, R. D. (2006). Origins of
cross-orientation suppression in the visual cortex. J Neurophysiol , 96 (4), 1755–1764.
Li, C. Y., & Creutzfeldt, O. (1984). The representation of contrast and other stimulus
parameters by single neurons in area 17 of the cat. Pflugers Arch, 401 (3), 304–314.
London, M., Roth, A., Beeren, L., Hausser, M., & Latham, P. E. (2010). Sensitivity to
perturbations in vivo implies high noise and suggests rate coding in cortex. Nature,
466 (7302), 123–127.
Marino, J., Schummers, J., Lyon, D. C., Schwabe, L., Beck, O., Wiesing, P., . . . Sur, M.
(2005). Invariant computations in local cortical networks with balanced excitation and
inhibition. Nat Neurosci , 8 (2), 194–201.
Martinez, L. M., Alonso, J.-M., Reid, R. C., & Hirsch, J. A. (2002). Laminar processing of
stimulus orientation in cat visual cortex. J Physiol , 540 (Pt 1), 321–333.
Miller, K. D., & Rubin, D. B. (2010). Contrast dependence of summation field size and
surround properties in a nonlinear circuit model of V1. Program No. 126.2. 2010
Neuroscience Meeting Planner. Washington, DC: Society for Neuroscience, Online.
Miller, K. D., & Rubin, D. B. (2011). Balanced amplification and normalization in a
simple circuit model of visual cortex explain multiple aspects of attentional modulation.
Program No. 428.09. 2011 Neuroscience Meeting Planner. Washington, DC: Society
for Neuroscience, Online.
Miller, K. D., & Troyer, T. W. (2002). Neural noise can explain expansive, power-law
nonlinearities in neural response functions. J. Neurophysiol., 87 , 653-659.
Ohshiro, T., Angelaki, D. E., & DeAngelis, G. C. (2011). A normalization model of multisensory integration. Nat. Neurosci., 14 , 775–782.
43

Ohzawa, I., Sclar, G., & Freeman, R. D. (1985). Contrast gain control in the cat’s visual
system. J. Neurophysiol., 54 , 651-667.
Ozeki, H., Finn, I. M., Schaffer, E. S., Miller, K. D., & Ferster, D. (2009). Inhibitory
stabilization of the cortical network underlies visual surround suppression. Neuron,
62 , 578–592.
Peirce, J. W. (2007). The potential importance of saturating and supersaturating contrast
response functions in visual cortex. J Vis, 7 , 13.
Persi, E., Hansel, D., Nowak, L., Barone, P., & van Vreeswijk, C. (2011). Power-law
input-output transfer functions explain the contrast-response and tuning properties of
neurons in visual cortex. PLoS Comput. Biol., 7 (2).
Polat, U., Mizobe, K., Pettet, M. W., Kasamatsu, T., & Norcia, A. M. (1998). Collinear
stimuli regulate visual responses depending on cell’s contrast threshold. Nature, 391 ,
580-584.
Priebe, N., Mechler, F., Carandini, M., & Ferster, D. (2004). The contribution of spike
threshold to the dichotomy of cortical simple and complex cells. Nat. Neurosci., 7 ,
1113-22.
Priebe, N. J., & Ferster, D. (2005). Direction selectivity of excitation and inhibition in
simple cells of the cat primary visual cortex. Neuron, 45 , 133-45.
Priebe, N. J., & Ferster, D. (2006). Mechanisms underlying cross-orientation supression in
cat visual cortex. Nature Neurosci., 9 , 552-561.
Renart, A., de la Rocha, J., Bartho, P., Hollender, L., Parga, N., Reyes, A., & Harris, K. D.
(2010). The asynchronous state in cortical circuits. Science, 327 , 587–590.
Rubin, D. B., & Miller, K. D. (2010). Normalization in a nonlinear circuit model of V1.
Program No. 126.1. 2010 Neuroscience Meeting Planner. Washington, DC: Society for
Neuroscience, Online.
Rubin, D. B., & Miller, K. D. (2011). Normalization in a simple circuit model of visual cortex
explains stimulus-induced reduction in shared variability. Program No. 428.10. 2011
Neuroscience Meeting Planner. Washington, DC: Society for Neuroscience, Online.
Sceniak, M., Ringach, D. L., Hawken, M., & Shapley, R. (1999). Contrast’s effect on spatial
summation by macaque v1 neurons. Nature Neurosci., 2 , 733-739.
Sengpiel, F., Blakemore, C., & Sen, A. (1997). Characteristics of surround inhibition in cat
area 17. Exp. Brain Res., 116 , 216-228.
Shushruth, S., Ichida, J. M., Levitt, J. B., & Angelucci, A. (2009). Comparison of spatial
summation properties of neurons in macaque V1 and V2. J. Neurophysiol., 102 , 2069–
2083.
Skottun, B. C., Bradley, A., Sclar, G., Ohzawa, I., & Freeman, R. D. (1987). The effects of
contrast on visual orientation and spatial frequency discrimination: A comparison of
single cells and behavior. J. Neurophysiol., 57 , 773-786.
44

Song, X. M., & Li, C. Y. (2008). Contrast-dependent and contrast-independent spatial
summation of primary visual cortical neurons of the cat. Cerebral Cortex , 18 , 331336.
Tsodyks, M. V., Skaggs, W. E., & Sejnowski, B. L., T. J.and McNaughton. (1997). Paradoxical effects of external modulation of inhibitory interneurons. J. Neurosci., 17 ,
4382-4388.
Tsui, J. M., & Pack, C. C. (2011). Contrast sensitivity of MT receptive field centers and
surrounds. J. Neurophysiol., 106 , 1888–1900.
Tyler, C. W., & Apkarian, P. A. (1985). Effects of contrast, orientation and binocularity in
the pattern evoked potential. Vision Res., 25 , 755–766.
van Vreeswijk, C., & Sompolinsky, H. (1996). Chaos in neuronal networks with balanced
excitatory and inhibitory activity. Science, 274 , 1724–1726.
van Vreeswijk, C., & Sompolinsky, H. (1998). Chaotic balanced state in a model of cortical
circuits. Neural Computation, 10 , 1321-1371.

45

