Dynamical criticality in the collective activity of a population of retinal neurons
Thierry Mora,1 Stéphane Deny,2 and Olivier Marre2
1

arXiv:1410.6769v2 [q-bio.NC] 31 Jan 2015

Laboratoire de physique statistique, École normale supérieure,
CNRS and UPMC, 24 rue Lhomond, 75005 Paris, France
2
Institut de la Vision, INSERM and UMPC, 17 rue Moreau, 75012 Paris, France
(Dated: February 3, 2015)
Recent experimental results based on multi-electrode and imaging techniques have reinvigorated
the idea that large neural networks operate near a critical point, between order and disorder [1, 2].
However, evidence for criticality has relied on the definition of arbitrary order parameters, or on
models that do not address the dynamical nature of network activity. Here we introduce a novel
approach to assess criticality that overcomes these limitations, while encompassing and generalizing
previous criteria. We find a simple model to describe the global activity of large populations of
ganglion cells in the rat retina, and show that their statistics are poised near a critical point. Taking
into account the temporal dynamics of the activity greatly enhances the evidence for criticality,
revealing it where previous methods would not. The approach is general and could be used in other
biological networks.

Complex brain functions usually involve large numbers of neurons interacting in diverse ways and spanning
a wide range of time and length scales. At first sight,
systems of inanimate matter seem to enjoy more regular
properties, but they may also display complex and heterogeneous behaviors when in a critical state, which corresponds to special points of the parameter space. Thinking about the brain as a system near a critical point has
been an attractive idea, which has gained attention after
the suggestion that such critical states could be achieved
in a self-organized manner, without fine-tuning [3], but
also the proposal that operating near a critical point
could be beneficial for computation [4].
Despite considerable work on the foundations of a theory of critical neural networks (see [5, 6] for recent examples), the validation of these ideas by experimental data
has proven difficult, largely because it requires to measure the detailed activity of large populations of neurons.
Recent progress has been made possible by the advance of
multi-electrode or imaging techniques, which have helped
detect signatures of criticality in a variety of neural contexts. Two lines of empirical evidence, rooted in different
approaches to critical systems, have been followed, albeit
with little intersection. In line with the original ideas
of self-organised criticality and branching processes, the
statistics of neural avalanches in cortical layers has been
shown to display power-law statistics [7–10]. This observation is indicative of the critical nature of the system’s dynamics, but it relies on arbitrary choices, such
as the number of units considered, the minimal silence
time to call the end of an avalanche, or the definition of
a neural event itself. The stability exponents of the neural dynamics, which become positive at the transition
to chaos, have also been used as signatures of criticality [11]. This criterion relies on a continuous description
of neural activity, which is inappropriate for codes relying on combinations of spikes and silences. Both these
approaches address the dynamical aspect of criticality.
They require the definition of an ad hoc order parameter
(avalanche size, firing rates), which may not be the most

relevant one for neural activity. A second line of enquiry,
which focuses on the thermodynamic aspect of criticality, has been to study the frequency of combinations of
spikes and silences in a neural population as a statistical mechanics problem, and explore its properties in the
thermodynamic limit [12, 13], using non-parametric signatures such as the divergence of the specific heat to
demonstrate critical behaviour [14, 15]. These analyses
have however been restricted to the simultaneous distribution of neural activity, with no regard to its dynamical properties, which may be strongly out of equilibrium
and may contain important clues about critical behavior.
Because of their respective limitations, neither of these
approaches gives us a coherent picture for assessing and
understanding all aspects of criticality.
In this paper we overcome these limitations by introducing a framework for analysing the critical dynamics of
neural networks. We apply a thermodynamic approach
to the population’s spiking activity over long periods,
treating time as an extra dimension. We propose a generalized, time-normalized specific heat of spike trains as an
indicator of critical dynamics. The approach accounts for
the combinatorial nature of the code, and does not rely
on the choice of an order parameter. It reduces to the
usual notion of dynamical criticality through the stability exponents of the dynamics when the number of spikes
can be approximated as a continuous variable. It is also
equivalent to the thermodynamic criticality of [13, 15]
when time correlations are ignored. We apply our criterion to a dense population of ganglion cells recorded in
the rat retina. We will show that the dynamics of this
population are close to a critical point, where the specific
heat diverges. This divergence appears to be much more
pronounced once the temporal dynamics are taken into
account.
To describe the discrete spiking activity of a population
of N neurons, we divide time into small windows of length
∆t, and assign a binary variable σi;t = 1 if neuron i has
spiked at least once within window t, and 0 otherwise. ∆t
must be small enough so that two spikes are unlikely to

2
d. 100
0.004

−2

Probability

C3 , predicted

a.

0.002
0

10

−4

10

−6

10

−8

10

−10

−0.002
−0.002

0

10

0.002 0.004

C3 , observed

40

c. 10−1

−2

10

Probability

Probability

20

Number of spikes K

b. 10−1

−3

10

observed
v=0
v=1
v=2
v=3
v=4
v=5

−4

10

−5

10

Gaussian model
Observed

0

0

10

1

10

−2

10

−3

10

observed
v=0
v=1
v=2
v=3
v=4
v=5

−4

10

−5

2

10

10

3

10

1

10

Avalanche size (number of spikes)

2

10

3

10

Avalanche duration (ms)

FIG. 1: The model captures the global dynamics
of the network.
(a) Predicted versus observed connected correlation functions C3 = P (Kt , Kt+1 , Kt+2 ) −
P (Kt+2 )P (Kt+1 )P (Kt ) between the total number of spiking neurons in three consecutive time windows of length
∆t = 10ms, for a subnetwork of N = 61 neurons. (b) and
(c) Model prediction for the size and duration of avalanches,
with different temporal ranges v, for the same subnetwork of
N = 61 neurons. An avalanche is a series of non-silent 10 ms
windows, ended by a silent window. While a model of independent spikewords (v = 0) is a poor predictor of avalanche
statistics, including time correlations over a few time windows
greatly improves the prediction. (d) The distribution of the
number of spiking neurons in a window ∆t = 10ms (black
curve) is exactly fitted by the model, by construction. By
contrast, it is not well predicted by a Gaussian model (red
curve).

occur in the same window. In the following we will take
∆t = 10ms. The probability of a given multi-neuron
spike train between t = 1 and t = L, or generalized
“codeword” {σi,t }, can formally be written in a Boltzman
form:
Pβ ({σi,t }) =

1 −βE({σi,t })
e
,
Z(β)

(1)

where Z(β) is a normalization constant. By analogy to
equilibrium statistical mechanics, E is interpreted as the
energy of the spike train. In information-theoretic terms,
the surprise of the spike train is related to its energy
through − log P = βE + log Z(β). β is an adjustable
control parameter equivalent to an inverse temperature,
set to 1 by convention to describe the observed spike
statistics. Its function is to study the parameter space of
models in the vicinity of the actual system at β = 1, and
thus assess its proximity to a critical state.
One possible indicator for detecting a critical point is
the specific heat [2, 16], defined in our formalism as:
c(β) =

1
∂
∂ log Z(β)
β2
β β2
=
hδE 2 iβ ,
N T ∂β ∂β
β
NL

(2)

where δE = E − hEi denotes fluctuations from the mean
energy, and h·iβ denotes averages taken under probability
law Pβ (see Appendix B). The specific heat has a clear
biological interpretation in terms of the spike train statistics: it is the normalized variance of the surprise of neural
spike trains, Var(log P )/N L. It quantifies the breadth
of codeword utilization: c(β) = 0 means that all utilized codewords have uniform usage probability, whereas
a large c(β) means that the code is balanced between
a few frequent codewords and many more rare codewords [2]. We included the normalization N L because
the variance of the surprise is expected to be an extensive quantity scaling linearly with the system size, taken
both across neurons and time. Thus, in the limit where
spiking events σi,t are independent or weakly correlated,
c(β) should converge to a finite value as N and L → ∞.
For example, if all spiking events were independent with
the same spiking probabibility p in each time window, we
would have c(β) = β 2 (pq)β (log p − log q)2 /(pβ + q β ) with
q = 1 − p, for all N and L (see Appendix B). However,
if the system is strongly interacting (between neurons,
across time, or both) the specific heat may diverge for a
certain critical value of the control parameters. Treating
time windows and neurons on equal footing allows us to
address both the many-body nature of the problem and
its critical dynamics with a single criterion. Since this
criterion is based on the surprise, which follows directly
from the probabilistic nature of the process, it does not
require us to choose an order parameter (spike rates, size
of avalanche, etc.).
Using the divergence of this specific heat as a diagnostic tool for criticality generalizes previous approaches.
Firstly, in the limit L = 1, where codewords are simultaneous combinations of neurons and silences, with no
regards to the dynamics, we recover the static thermodynamic approach of [15]. Secondly, the method is consistent with the notion of dynamical criticality based on
stability exponents. Let us assume that the dynamics is
well described by a single projection of the spikes onto a
PN
continuous variable, e.g. Kt = i=1 σi,t , and linearized
to a Gaussian, Markovian dynamics
Kt+1 = aKt + b + Gaussian noise,

(3)

where the stability exponent of the dynamics is log(a) <
0. This system is critical for a ∼ 1; above the transition,
the linearized dynamics breaks down as the system becomes chaotic. The specific heat of this model at β = 1,

c(β = 1) ∼

log

N − hKi
hKi

2

Var(K) 1 + a
,
N
1−a

(4)

diverges at the critical point a = 1 (see Appendix
C). Lastly, the approach can detect criticality in simple models of neural avalanches. Consider the spiking model proposed in [7], where a neuron i spikes
at time t + 1 in response to a pre-synaptic neuron j
spiking at time t with probability pij (Appendix D).

3

E=−

X
t

h(Kt ) −

v
XX
t

Ju (Kt , Kt+u ),

(5)

u=1

where v is the model’s temporal range – the larger the v,
the more accurate the model. Applying the maximum entropy principle to trajectories rather than instantaneous
states is sometimes also refered to as the maximum caliber method [21]. The model is learned by fitting the
parameters h(K) and Ju (K, K 0 ) to the data using the
technique of transfer matrices (see Appendices G and H).
We find that a temporal range of v = 4 suffices to account
for the temporal correlations of K (see Appendix I and
Figs. S2 and S3).
The obtained model reproduces key dynamical features
of the data. Fig. 1a compares data and model for the
joint distribution of the numbers of spikes in three consecutive time windows, showing excellent agreement despite this observable not being fitted by the model. More
importantly, the model predicts well the distributions of
size and duration of neural avalanches, defined as continuous epochs of K > 0, as shown in Fig. 1b and c for
a subset of N = 61 neurons. The agreement extends
over seconds, way beyond the model’s temporal range of

a.
v
v
v
v
v

Specific heat c(β)

20

15

=0
=1
=2
=3
=4

10

5

0
0.9

0.95

1

1.05

1.1

Temperature 1/β
b.
20

Specific heat c(β)

This model is P
parametrized by the branching parameter ω = (1/N ) ij pij , which controls the spread of neural avalanches. At the critical point ω = 1 the system
exhibits avalanches with power-law statistics. We estimated the specific heat c(β = 1) of that model numerically, and found it to diverge with the system size precisely at the critical value of the branching parameter
ω = 1 (Fig. S1). In sum, the specific heat, when defined on the temporal statistics of spike trains, allows
us to detect dynamical critical transitions, without having to know the order parameter or the definition of an
avalanche.
Our goal is to apply our criterion to the spiking activity
of a dense population of N = 185 retinal ganglion cells
in the rat retina [17], stimulated by films of randomly
moving bars and binned with ∆t = 10ms (see Methods).
However, to carry out our analysis we first need to learn
a probabilistic law P ({σi;t }) from the spike trains, which
in itself can be a daunting task. We do so by employing
the principle of maximum entropy [12, 18, 19], which consists in finding the least constrainedPdistribution of spike
trains (i.e. of maximum entropy − P log P ) consistent
with a few selected observables of the data (see Appendix
E). In [13] the global network activity of the salamander retina was modeled by constraining the distribution
P (K) of the total number of spikes in the population (see
also [20]). The inferred model was shown to be near a
critical point. However that choice of constraints did not
address the dynamical nature of the spike trains. To do
that while making as few additionnal assumptions as possible, we also constrain a dynamical quantity – the joint
distribution of Kt at two different times Pu (Kt , Kt+u ).
This leads to a family of time translation invariant models of the form in Eq. 1 with:

15

N
N
N
N
N
N
N
N
N

=5
= 10
= 20
= 30
= 40
= 50
= 61
= 97
= 185

10

5

0
0.9

0.95

1

1.05

1.1

Temperature 1/β

FIG. 2: Divergence of the specific heat of spike trains.
(a) Specific heat c(β) of spike trains of the entire population
(N = 185), as a function of the temperature 1/β, for an
increasing temporal range v. Temperature β = 1 corresponds
to the observed statistics of spike trains. The curve with
v = 4, which fully accounts for the dynamics of the spike
trains, shows a markedly higher peak than that obtained from
the statistics of instantaneous codewords (v = 0). (b) Specific
heat of spike trains for subnetworks of increasing sizes N , for
v = 4. Each point is averaged over 100 random subnetworks
for N ≤ 50, and shows one representative network for N = 61
and 97. The error bars show standard deviations. The peak
increases with network size, indicating a divergence in the
thermodynamic limit.

v × 10ms = 50ms. Although we will not use avalanche
statistics to discuss criticality in this paper, as is often
done [7–10], the success of our model in predicting them
demonstrates its ability to capture complex, collective
dynamical behaviour.
Simplifying the model further does not capture important statistics of the data. We could make a continuous
approximation for Kt and constrain only the first two
moments of the distributions. This approximation would
yield an autoregressive model generalizing Eq. 3 (see Appendix F). However, the statistics of such models would
all be Gaussian, in plain contradition with the observed
distribution of spikes P (K), see Fig. 1d. Since the tail
of that distribution is related to the collective properties
of the population [13], it is important to account for it
fully, and our model is the simplest one that does that.

4
1.2

v
v
v
v
v
v

1.15
1.1

=0
=1
=2
=3
=4
=5

1.05
1
0

100

200

Var(surprise)/N L

b.

a.

Peak temperature

Confident that our model gives a precise account of the
temporal dynamics of the global network activity, we can
use it to estimate its specific heat. Fig. 2A represents
the specific heat of the learned models (Eq. 5) for all
N = 185 neurons as a function of the temperature 1/β,
for different choices of the temporal range v. The special
case v = 0, in which time correlations are ignored, shows
only a moderate peak in specific heat, and far from β = 1.
By contrast, including time correlations (v > 0) greatly
enhances the peak, which rapidly approaches β = 1 as
the temporal range v is increased. Fig. 2B shows how
the peak in specific heat behaves for random subgroups
of neurons of increasing size, for v = 4. Similarly, the
peak becomes larger, sharper and closer to β = 1 as the
network size grows. These are striking results, if we recall
that all these curves would fall on top of each other for
independent (or weakly correlated) spiking events. The
unusal scalings of Fig. 2 suggest that the system is indeed
close to a critical point. But they also show that both the
collective behaviour of the population and the temporal
correlations play a crucial role in revealing the critical
properties of the network. In fact, the convergence of the
peak of the specific heat towards β = 1 is only apparent
when time correlations are taken into account (v > 0),
as illustrated by Fig. 3A. This is in contrast with the
results of [13], which found signatures of criticality even
for v = 0, although this apparent disagreement may be
attributed to differences between species (the rat having
much higher average firing rates in their retinal ganglion
cells than the salamander).
Although the peak of the specific heat is a somewhat abstract quantity, the fact that it increases and
approaches β ∼ 1 implies that the normalized variance
of the surprise c(β = 1) = Var(log P )/N L also increases
with the system size, as shown in Fig. 3B. These variances
are extremely high compared to that we would obtain if
all spiking events were independent, cinde (β = 1) = 0.38,
indicating a very wide spectrum of codeword usage.
For the sake of simplicity and tractability, we have here
only modelled the global network activity of the retina.
Although these models capture important features of the
dynamics (Fig. 1), more detailed models accounting for
the full temporal cross-correlations between individual
neurons [22, 23] could provide us with a more precise
description of the spiking dynamics, and better approximations to the specific heat curves. In principle our
inference procedure may also depend on the choice of
window size ∆t. We repeated the analysis for windows
of 5ms, and found the same results, with an excellent
agreement between models that have a different ∆t but
the same temporal range v ×∆t expressed in seconds (see
Appendix J and Figs. S4 and S5).
We have introduced a framework for studying the collective dynamics of a population of neurons. This formalism provides us with a non-parametric criterion for
detecting the proximity to a critical state, whether this
criticality stems from strong collective effects in the population, from critical dynamics at the edge of chaos, or

20
15

v
v
v
v
v
v

=0
=1
=2
=3
=4
=5

10
5
0
0

100

200

Network size N

Network size N

FIG. 3: Finite-size scaling. (a) Position of the peak 1/β in
specific heat (see Fig. 2) as a function of network size N , for
increasing time ranges v. Accounting for the dynanics of spike
trains (v > 0) gives peaks that are much closer to the temperature of real spike trains (β = 1) than for instantaneous
spikewords (v = 0). (b) The specific heat of real spike trains,
c(β = 1), is equal to the variance of the surprise per neuron
and per unit time, Var(log P )/N L. This variance increases
with the system size N and with the temporal range v. Note
that the v = 5 curves are very close to the v = 4 ones up to
N = 61 (above which they are not calculated).

from both, thus generalizing previous approaches. When
we apply our approach to large-scale recordings in the
retina, we find that the population dynamics are very
close to a critical state. Compared to the static thermodynamic approach of [13, 15], which focused on the statistics of instantaneous codewords, the peak in specific heat
that we find is 10 times larger, and much closer to the system’s actual temperature of 1. Our results suggest that
although simultaneous correlations between neurons are
an important marker of near-critical behavior, accounting for their dynamical component greatly enhances our
confidence and understanding of it.
The idea that biological systems may operate near a
critical point is not restricted to the case of neurons
[2], with evidence in systems as diverse as the cochlea
[24], immune repertoires [25], natural images [26], animal flocks [27–29] or the regulation of genes in early fly
developpment [30], to name but just a few, and we expect our approach to be useful when both the collective
behaviour and the dynamics play an important role.
We thank William Bialek and Gasper Tkacik for helpful comments on the manuscript. This work was supported by the DEFI-SENS 2013 program from the Centre
National de la Recherche Scientifique, by the Agence Nationale pour la Recherche (ANR: OPTIMA), and by the
French State program “Investissements d’Avenir” managed by the Agence Nationale de la Recherche [LIFESENSES: ANR-10-LABX-65]. SD was supported by a
PhD fellowship from the region Ile-de-France.

Methods

Retinal recordings. Recordings were performed on the
Long-Evans adult rat. In brief, animals were euthanized
according to institutional animal care standards. The

5
retina was isolated from the eye under dim illumination
and transferred as quickly as possible into oxygenated
AMES medium. The retina was then lowered with the
ganglion cell side against a multi-electrode array whose
electrodes were spaced by 60 microns, as previously described [17]. Raw voltage traces were digitized and
stored for off-line analysis using a 252-channel preamplifier (MultiChannel Systems, Germany). The recordings
were sorted using custom spike sorting software developed specifically for these arrays [17]. We extracted the
activity from 185 neurons with satisfying standard tests

[1] Chialvo DR (2010) Emergent complex neural dynamics.
Nature Physics 6:744–750.
[2] Mora T, Bialek W (2011) Are biological systems poised
at criticality? J Stat Phys 144:268–302.
[3] Bak P, Tang C, Wiesenfeld K (1988) Self-organized criticality. Phys Rev, A 38:364–374.
[4] Bertschinger N, Natschläger T (2004) Real-time computation at the edge of chaos in recurrent neural networks.
Neural Comput 16:1413–36.
[5] Levina A, Herrmann JM, Geisel T (2007) Dynamical
synapses causing self-organized criticality in neural networks. Nat Phys 3:857–860.
[6] Magnasco MO, Piro O, Cecchi GA (2009) Self-tuned
critical anti-hebbian networks. Phys Rev Lett 102:258102.
[7] Beggs JM, Plenz D (2003) Neuronal avalanches in neocortical circuits. J Neurosci 23:11167–77.
[8] Petermann T, et al. (2009) Spontaneous cortical activity
in awake monkeys composed of neuronal avalanches. Proc
Natl Acad Sci USA 106:15921–6.
[9] Friedman N, et al. (2012) Universal critical dynamics in
high resolution neuronal avalanche data. Phys Rev Lett
108:208102.
[10] Tagliazucchi E, Balenzuela P, Fraiman D, Chialvo DR
(2012) Criticality in large-scale brain fmri dynamics unveiled by a novel point process analysis. Front. Physio.
3:15.
[11] Solovey G, Miller KJ, Ojemann JG, Magnasco MO, Cecchi GA (2012) Self-regulated dynamical criticality in human ecog. Front. Integr. Neurosci. 6:1–9.
[12] Schneidman E, Berry MJ, Segev R, Bialek W (2006)
Weak pairwise correlations imply strongly correlated network states in a neural population. Nature 440:1007–12.
[13] Tkačik G, et al. (2013) The simplest maximum entropy
model for collective behavior in a neural network. J. Stat.
Mech. 2013:P03011.
[14] Tkacik G, Schneidman E, Berry MJ, Bialek W (2006)
Ising models for networks of real neurons. arXiv :q–
bio/0611072.
[15] Tkacik G, et al. (2014) Thermodynamics for a network
of neurons: Signatures of criticality. arXiv :1407.5946.
[16] Chaikin PM, Lubensky TC (1995) Principles of Condensed Matter Physics (Cambridge University Press,
Cambridge).
[17] Marre O, et al. (2012) Mapping a complete neural population in the retina. Journal of Neuroscience 32:14859–
14873.
[18] Jaynes ET (1957) Information theory and statistical mechanics. Physical Review 106:620.

of stability and limited number of refractory period violations.
Visual stimulation. Our stimulus was composed of
several black bars moving randomly on a gray background. The trajectory was a random walk with a restoring force to keep the bar close to the array (see Appendix
A for details). The stimulus was displayed using a Digital Mirror Device and focused on the photoreceptor plane
using standard optics. The analysed data corresponds to
one hour (L = 360, 000 with ∆t = 10ms) of recordings.

[19] Jaynes ET (1957) Information theory and statistical mechanics. ii. Physical Review 108:171.
[20] Okun M, et al. (2012) Population rate dynamics and
multineuron firing patterns in sensory cortex. J Neurosci
32:17108–19.
[21] Pressé S, Ghosh K, Lee J, Dill KA (2013) Principles
of maximum entropy and maximum caliber in statistical
physics. Reviews of Modern Physics 85:1115–1141.
[22] Marre O, Boustani SE, Frégnac Y, Destexhe A (2009)
Prediction of spatiotemporal patterns of neural activity
from pairwise correlations. Phys Rev Lett 102:138101.
[23] Vasquez JC, Marre O, Palacios AG, Ii MJB, Cessac B
(2012) Gibbs distribution analysis of temporal correlations structure in retina ganglion cells. Journal of Physiology - Paris 106:120–127.
[24] Eguı́luz VM, Ospeck M, Choe Y, Hudspeth AJ, Magnasco MO (2000) Essential nonlinearities in hearing.
Phys Rev Lett 84:5232–5.
[25] Mora T, Walczak AM, Bialek W, Callan CG (2010) Maximum entropy models for antibody diversity. Proc Natl
Acad Sci USA 107:5405–10.
[26] Stephens GJ, Mora T, Tkačik G, Bialek W (2013) Statistical thermodynamics of natural images. Phys Rev Lett
110:018701.
[27] Bialek W, et al. (2014) Social interactions dominate
speed control in poising natural flocks near criticality.
Proc Natl Acad Sci USA.
[28] Attanasi A, et al. (2013) Wild swarms of midges linger
at the edge of an ordering phase transition. arXiv condmat.stat-mech.
[29] Cavagna A, et al. (2014) Dynamical maximum entropy
approach to flocking. Phys. Rev. E 89:042707.
[30] Krotov D, Dubuis JO, Gregor T, Bialek W (2014)
Morphogenesis at criticality. Proc Natl Acad Sci USA
111:3683–8.
[31] Weigt M, White RA, Szurmant H, Hoch JA, Hwa T
(2009) Identification of direct residue contacts in proteinprotein interaction by message passing. Proc Natl Acad
Sci USA 106:67–72.

Appendix A: Visual stimulation

Our stimulus was composed of two black bars moving
randomly on a gray background. Each bar was animated
by a brownian motion, with additional feedback force to
stay above the array, and repulsive forces so that they

6
do not overlap. The bars stay within an area that covers
the whole recording array. The amplitude of the bar
trajectories allowed them to sweep the whole recording
zone. The trajectories of the bars x1 and x2 are described
by the following equations:

6
v1
R
dv1
= − + sign(x1 − x2 )
dt
τ
|x1 − x2 |
dv2
dt

−ω02 (x1 − µ1 ) + σ W1 (t)

6
v2
R
= − + sign(x2 − x1 )
τ
|x2 − x1 |
−ω02 (x2 − µ2 ) + σ W2 (t)

can be calculated as S(β) = ∂F/∂β = β −1 (hEiβ −F (β)),
where h·iβ denotes an average taken over spike trains with
probability law Pβ . This last relation is better known in
the form F = E − T S, with T = β −1 is temperature.
The heat capacity is defined as:
C(β) = T

(A1)

(A2)

where W1 (t) and W2 (t) are two Gaussian white noises of
unit amplitude, µ2 − µ1 = 600µm is the shift between
the means, ω0 = 1.04 Hz and τ = 16.7 ms. The width of
one bar is 100µm. The stimulus was displayed using a
Digital Mirror Device and focused on the photoreceptor
plane using standard optics.

∂S
∂S
= −β
= β 2 (hE 2 iβ − hEi2β ).
∂T
∂β

In statistical physics it is an extensive quantity, meaning
that it scales with the system size N L. The specific heat
c(β) = C(β)/N L is the heat capacity normalized by the
system size.
Let us consider a simple example, where each neuron
spikes with probabily pi = ri ∆t in each time window
(where ri is its spike rate), independently of the other
neurons and of its own spiking history. In the limit ∆t →
0 these are just Poisson neurons. The probability of a
given spike train factorizes over neurons and over time,
and reads:
Pβ ({σi,t }) =

Appendix B: Thermodynamics of spike trains

Let us start with the probability distribution for entire
spike trains {σi,t }, i = 1, . . . , N , t = 1, . . . , L. By analogy
with Boltzmann law we can write this probability as:
P ({σi,t }) =

1 −E({σi,t })
e
,
Z

(B1)

where E({σi,t }) and − log Z are defined up to a common constant. The surprise − log P ({σi,t }) is equal to
E({σi,t }) + log Z. Note that considering the statistics
of entire spike trains over time allows for a well-defined
∆t → 0 limit, with the concomitant scaling L ∼ 1/∆t, by
contrast to the static thermodynamic approach (L = 1)
where this limit tends to the all-silent state with probability one.
The probability distribution in Eq. B1 will produce
typical spike trains with the same statistics as the experiment. To explore this model across a line in parameter
space, we can generalize Eq. B1 to an arbitrary fictious
temperature:
Pβ ({σi,t }) =

1 −βE({σi,t })
e
.
Z(β)

(B2)

While Pβ=1 describes “typical” spike trains with the
same statistics as the experiment, this generalized distribution allows us to explore atypical spike trains of low
or high energy (accessed by high and low β), or equivalently of high and low surprises.
The free energy is defined as F (β) = −β −1 log Z(β).
The Shannon entropy of Pβ ,
S(β) = −

X
{σi,t }

Pβ ({σi,t }) log Pβ ({σi,t }),

(B3)

(B4)

N
L Y
Y

1 βσi,t β(1−σi,t )
pi
qi
,
z
(β)
t=1 i=1 i

(B5)

where qi = 1 − pi and zi (β) = pβi + qiβ . The specific heat
can be calculated from Eq. B4:
c(β) =

N
1 X 2
β (pi qi )β (log pi − log qi )2 /(pβi + qiβ ). (B6)
N i=1

This expression has no divergence as a function of β. For
small uniform spiking probability pi = p  1, the specific
heat at the natural temperature is also small: c(β =
1) ∼ p(log p)2 . In that same limit, the peak in specific
heat is reached at high temperatures, βc ∼ −α/ log p,
where α ≈ 2.2 is solution of the irrational equation α =
2(1 + e−α ); the value of the peak does not depend on p,
and is c(βc ) ∼ α(α − 2) ≈ 0.48.
Appendix C: Thermodynamics of a simple
auto-regressive model

We consider a simple case where we assume that the
neural population is well described by a continuous parameter describing the total number of spiking neurons in
PN
a time window. Let us call Kt = i=1 σi,t that number.
Its mean is hKi = rN , where r is the average spike rate
of each cell per time window. We denote Kt = rN + xt .
Assuming that K and N are large, we can treat xt as
a continuous variable, and model it by a simple Markov
dynamics, or auto-regressive model:
xt+1 = axt + t ,

(C1)

with t a Gaussian noise of mean zero and variance σ 2 .
xt is of mean zero, and its auto-correlation function of xt
reads:
hxt xt0 i =

0
0
0
σ2
a|t−t | = hx2t ia|t−t | ≡ f rN a|t−t | , (C2)
2
1−a

7
where f = Var(K)/hKi is the Fano factor of the number
of spiking neurons. f = 1 when the distribution of K
is Poisson. When a → 1, the system becomes critical in
the traditional dynamical sense, with a diverging correlation time −1/ log(a). This is the “stability parameter”
obtained from an auto-regressive model [11].
For each K, the probability of a given spiking pattern
is uniform:
P
δ( i σi , K)
P (σ1 , . . . , σN |K) =
.
(C3)

N
K

Assuming that the system is stationary at t = 1, the
probability of a whole spike train of duration L is thus
given by:
log P ({σi,t }) = −

L
x21
1
log(2πσ 2 ) −
− log(2πf rN )
2
2f rN
2

− EK − Eσ ,
(C4)
with
EK = −

Eσ =

L
1 X
(xt+1 − axt )2
2σ 2 t=1

L
X

(C5)

[log Γ(N + 1) − log Γ(rN + xt + 1)

t=1

− log Γ((1 − r)N − xt + 1)] .
(C6)


N
where we have replaced K
= rNN+xt by its expression in terms of Gamma functions Γ(x). The term −EK ,
combined with the first term on the right-hand side of
Eq. C4, corresponds to the Gaussian distribution of t
with replacement using Eq. C1. The term −Eσ corresponds the conditional distribution in Eq. C3. The second and third terms on the right-hand side correspond to
the Gaussian distribution of x1 , of zero mean and variance f rN = Var(K).
We expand Eσ by assuming that x  N , using Stirling’s formula, and obtain at leading order:

 X

X
N r + xt
1−r
N H(r) + log
Eσ ≈
NH
≈
xt ,
N
r
t
t
(C7)
where H(x) = −x log(x) − (1 − x) log(1 − x) is the binary
entropy.
If we neglect terms containing the initial condition x1 ,
the total surprise is, up to a constant, equal to EK + Eσ .
Its variance, also called heat capacity by analogy with
statistical mechanics, is given by
2
C(β = 1) = (hEK
i − hEK i2 ) + (hEσ2 i − hEσ i2 ),

(C8)

as the cross-correlation term involves the third moments
of xt and thus is zero. A calculation using Gaussian
integration rules gives, at leading order in the limit L →
∞:
L
2
hEK
i − hEK i2 = .
(C9)
2

On the other hand we obtain:

2
1−r
1+a
hEσ2 i − hEσ i2 = N L log
fr
.
r
1−a

(C10)

Both variances scale linearly with L. This is consistent
with the extensivity of the heat capacity: the average
surprise scales linearly with L, and its variance does as
well. But only the second part of the variance scales
linearly with N . Thus in the limit N , L → ∞,
2

C(β = 1)
1−r
1+a
c(β = 1) =
= log
fr
. (C11)
NL
r
1−a
The variance of the surprise diverges as a → 1, i.e. as
the system becomes critical in the usual dynamical sense.
When the Fano factor f = Var(K)/hKi diverges with
N , the specific heat c(β = 1) diverges as well. This is
the case when fluctuations of K are of the same order
of magnitude as K itself, e.g. Var(K) ∼ K 2 and thus
f ∼ K ∼ rN , as was observed in the salamander retina
[13].
Appendix D: Thermodynamics of a model of neural
avalanches

We now study a simple model of spiking dynamics that
is known to display critical avalanche statistics [7]. We
will show that applying our specific heat criterion allows
us to detect the critical point. In this model, neuron i
spikes at time t if it receives signal from at least one other
neuron j, which happens with probability pij , provided
that that neuron has spiked at time t−1. The probability
for a spike train can be written as:
P ({σi,t }) =

N
YY

pi (t)σi,t [1 − pi (t)]1−σi,t

(D1)

t i=1

Q
where pi (t) = 1 − j (1 − pij )σi,t−1 is the probability that
neuron i spikes at time t. The energy ofP
this process can
be easily calculated as E = − log(P ) = t t , with
X
t = −
σi,t log pi (t) − (1 − σi,t ) log[1 − pi (t)] (D2)
i

P
The parameter ω = (1/N ) ij pij quantifies the probability that a spike generates another spike at the next
time step. When ω < 1, the spiking activity goes
extinct, while when ω > 1, it explodes exponentially.
Around ω ∼ 1, the system is critical and exhibits neural
avalanches with power-law statistics [7]. Since the allsilent state is absorbing, in the simulation we further assume that when the system goes into the all-silent state,
one random neuron (out of N ) is made to spike to restart
the activity.
Taking the L → ∞ limit, the specific heat is just estimated numerically from simulations as
2 X
1
hδt δt+u i,
(D3)
c(β = 1) = hδ2t i +
N
N
u≥1

8

Specific heat c(β = 1)

1600
N
N
N
N
N
N
N
N
N
N

1400
1200
1000
800

=5
= 10
= 15
= 30
= 60
= 125
= 250
= 500
= 750
= 1000

600
400
200
0
0.9

1

1.1

1.2

1.3

Branching parameter ω

where λa are Lagrange multipliers that must be adjusted
to satisfy Eq. E2, and Z is a normalization constant.
There are many ways to choose the set of observables Oa , and just as many resulting models. Here for
simplicity we assume that the system is in a stationary state, so that the statistics of spike trains is timeinvariant. This implies that the observables will be time
averaged. Our choice of observables are the joint distributions of the number of spiking neurons at different
times, Pu (Kt , Kt+u ), for u = 1, . . . , v, defined as:
Pu (K, K 0 ) =

L−u
1 X X
δK,Kt δK 0 ,Kt+u P ({σi,t }),
L − u t=1
{σi,t }

FIG. S1: Specific heat of a simple model of neural
avalanches. The specific heat c(β = 1), or variance of the
surprise Var(log P )/N L, is plotted as a function of the branching parameter ω in a simple model of neural avalanches, for
increasing network sizes N . The specific heat gets increasingly peaked as the network size grows, and the peak gets
closer to the critical value branching parameter ω = 1.

where δt = t − ht i. Fig. S1 shows the specific heat
as function of the branching parameter ω, for increasing
network sizes N . The specific heat peaks close to ω = 1.
The peak diverges and gets closer to 1 as the system
size is increased. This demonstrates that our criterion
for criticality based on the specific heat can help detect
a critical transition in this simple model. Note that, in
doing so, we have not had to define what an avalanche
is. Instead, we have solely relied on the thermodynamic
properties of the spike train statistics.
Appendix E: Maximum entropy modeling

We want to infer a model for the probability of a
entire multi-neuron spike train {σi,t }, i = 1, . . . , N ,
t = 1, . . . , L. The principle of maximum entropy allows
us to infer an approximation of that probability from
measurable observables. We look for a model distribution P ({σi,t }) that has maximum entropy:
X
−
P ({σi,t }) log P ({σi,t })
(E1)
{σi,t }

under the constraint that it agrees with the expected value of a few chosen observables O1 ({σi,t }),
O2 ({σi,t }), . . ., estimated from the data:
X
hOa idata =
Oa ({σi,t })P ({σi,t }), for all a. (E2)
{σi,t }

The technique Lagrange multipliers gives us the form of
such a distribution:
"
#
X
1
λa Oa ({σi,t }) ,
(E3)
P ({σi,t }) = exp
Z
a

(E4)
where δa,b = 1 if a = b and 0 otherwise. The corresponding model of maximum entropy is:
"
#
v
X
XX
1
P ({σi,t }) = exp
h(Kt ) +
Ju (Kt , Kt+u )
Z
t
t u=1
(E5)
where h(K) and Ju (K, K 0 ) are the Lagrange multipliers
λa associated with the constraints on Pu (K, K 0 ). Introducing h(K) is not necessary, because Ju (K, K 0 ) suffices
to enforce the constraints on the marginals, but doing
so allows us to formally separate first-order from secondorder terms, at the cost of redundancy. As a result, the
definition of the model in Eq. E5 allows for some freedom
in the definition of the parameters. Indeed the distribution is unchanged upon the transformations:
Ju (K, K 0 ) → Ju (K, K 0 ) + (K),

h(K) → h(K) − (K),
(E6)
and likewise for the second argument of Ju . This degeneracy can be lifted by imposing the following relations:
X
P (K)h(K) = 0,
(E7)
K

X

P (K)Ju (K, K 0 ) = 0 for all K 0 ,

(E8)

P (K 0 )Ju (K, K 0 ) = 0 for all K.

(E9)

K

X
K0

Note that this choice of parametrization does not affect
the model distribution itself. It is merely a choice of
convention, which ensures that the energy terms h and
J are balanced around 0.
In practice it is enough to study the model for
(K1 , . . . , KL ), the distribution of which is:
"
 
X
N
1
h(Kt ) + log
P (K1 , . . . , KL ) = exp
Z
K
t
t
#
v
XX
+
Ju (Kt , Kt+u ) ,
t

u=1

(E10)

N

where the binomial factors Kt counts the spiking patterns (σ1,t , . . . , σN,t ) having Kt spiking cells among N .

9
Appendix F: Gaussian approximation
MI(Kt , Kt+u ) (bits/sec)

50

It is possible to further simplify the maximum entropy
model by treating K as a continuous variable and constraint only its first and second moments hKt i, hKt Kt+u i.
Using Eq. E3, these constraints lead to a Gaussian distribution for the number of spiking neurons:
"
#
v
1
1 XX
P (K1 , . . . , KL ) = exp −
xt Au xt+u , (F1)
Z
2 t u=0

v
X

γu xt−u + t ,

(F2)

u=1

with t a Gaussian variable of zero mean and covariance
ht t0 i = σ 2 δtt0 and the correspondance:
!
v
X
1
2
γu
(F3)
A0 = 2 1 +
σ
u=1


X
2
γu0 γu00  .
(F4)
Au = − 2 γu −
σ
0
00

40

30

20

10

0
0

where xt = Kt −hKi as before. This process is equivalent
to a generalized auto-regressive model:
xt =

observed
v=1
v=2
v=3
v=4
direct info

50

100

150

200

Time (ms)

FIG. S2: Temporal correlations. Mutual information between Kt and Kt+u as a function of u × ∆t (∆t = 10 ms), for
all N = 185 neurons. The mutual information quantifies the
correlation between two quantities. The model prediction for
different v is compared to the data. The agreement is good
for v = 3 and 4. The gray curve shows the direct information
between different times [31], which quantifies the strengh of
interaction between t and t + u, within the v = 4 model.

u=1

u=2
30

|u −u |=u

10

This class of models generalizes Eq. C1. They predict a
Gaussian distribution for the number of spiking neurons,
in contradiction with experimental observations.

20

20

30
10

20

20
10

30

10
30

40
50
20

Appendix G: Model solution

0

40

−10

50

40

0
−10
20

u=3

40

u=4
30

The fitting problem of the maximum entropy distribution reduces to finding the parameters h(K), Ju (K, K 0 )
so that the distribution in Eq. E10 agrees with the experiments on the values of the marginal probabilities
Pu (K, K 0 ) (for all K, K 0 ). Data estimates are simply
obtained from the frequency of (Kt , Kt+u ) pairs in the
recordings. The model prediction, defined by Eq. E4, requires to sum over all possible trajectories of Kt , which, if
done with brute force, would be prohibitively long. However, it is possible to perform these sums using the technique of transfer matrices, which requires much less computational power. This technique is commonly used to
solve one-dimensional problems in statistical mechanics.
It is also known in computer science as an instance of
dynamic programming.
We start by assuming that the trajectory (K1 , . . . , KL )
is an v th order Markov process (this assumption will
be verified later). We define the super variable Xt =
(Kt , Kt+1 , . . . , Kt+v−1 ), and rewrite Eq. E10 as:
"
#
X
X
1
P ({Xt }) = exp
H(Xt ) +
W (Xt , Xt+1 )
Z
t
t
×

Y v−1
Y
t u=1

δX (u) ,X (u+1)
t

10

20

20

30
10

20

20
10

30

10
30

40
50
20

40

0

40

−10

50

0
−10
20

40

FIG. S3:
Value of the coupling parameters
Ju (Kt , Kt+u ). The x and y axes represent Kt and Kt+u ,
respectively. The model was fittted with v = 4 and all
N = 185 neurons.

(u)

where Xt
with

is the uth component of Xt , i.e. Kt+u−1 , and



v 
N
1X
h(Kt+u−1 ) + log
H(Xt ) =
v u=1
Kt+u−1
+

v
X
u0 <u=1

1
Ju−u0 (Kt+u−1 , Kt+u0 −1 ).
v − (u − u0 )
(G2)

and

t−1

(G1)

W (Xt , Xt+1 ) = Jv (Kt , Kt+v ).

(G3)

10
If Kt is v th -order Markovian, then the super-variable Xt
is Markovian:
P ({Xt }) = P (X1 )

L
Y

P (Xt |Xt−1 ).

(G4)

t=2

The conditional distribution can be written in the form:
t
t−1
1 H(Xt )+g→
(Xt )−g→
(Xt−1 )+W (Xt−1 ,Xt )
e
t
z→
v−1
Y
×
δX (u) ,X (u+1) ,

simply gives the eigenvalue z. This procedure takes a
computational time of order (Kmax +1)v+1 , which is large
but manageable for small enough v.
The same reasoning can be repeated by writing the
Markov dynamics of Xt backward in time:
1 H(Xt )+g← (Xt )−g← (Xt+1 )+W (Xt ,Xt+1 )
e
z
v−1
Y
×
δX (u+1) ,X (u) ,

P (Xt |Xt+1 ) =

P (Xt |Xt−1 ) =

t

u=1

t−1

(G5)
t
where g→
is a function that will be specified by normalization (see below). This identification can be verified
by replacing Eq. G5 into Eq. G4 and comparing with
L
Eq. G1, with g→
(XL ) = 0,

P (X1 ) =

1
1 H(X1 )+g→
(X1 )
e
,
z1

(G6)

QL t
and Z = t=1 z→
. Thus, Xt is indeed Markovian, and
Kt is v th order Markovian.
The parameter to be learned is the function g→ (X). In
general that function depends on t, but here we assume
that it is constant because of stationarity. This assumption is only valid in the bulk (i.e. for t far away from
both 1 and L). The normalization condition
X
P (Xt |Xt−1 ) = 1,
(G7)

u=1

t

t+1

(G9)
which gives the self-consistent equation:
"
#
v−1
0
1 X H(X 0 )+W (X 0 ,X) Y
e
=
e
δX 0(u+1) ,X (u) eg← (X ) .
z 0
u=1
X
(G10)
The only difference with Eq. G8 is the exchange of X
and X 0 . Thus, g→ and g← may be different for general
time-irreversible processes. The eigenvalue z remains unchanged, however, because the right and left eigenvalues
of a matrix are the same.
Armed with g→ and g← , we can now calculate all
marginals. Using the Markovian nature of the sequence:
g← (X)

P ({Xt0 }) =P (Xt )P (X1 , . . . , Xt−1 |Xt )P (Xt+1 , . . . , XL |Xt )
=P (Xt )

t
Y
t0 =2

P (Xt0 −1 |Xt0 )

L−1
Y

P (Xt0 +1 |Xt0 ),

t0 =t

(G11)

Xt

which must hold for all Xt−1 , gives the following selfconsistent equation for g→ (X):
"
#
v−1
0
1 X H(X 0 )+W (X,X 0 ) Y
g→ (X)
e
=
e
δX 0(u) ,X (u+1) eg→ (X ) ,
z 0
u=1
X
(G8)
where we have replaced Xt−1 by X and Xt by X 0 to
ease notations (but also because these are dummy variables). We can view this equation as an eigenvalue problem: eg→ (X) is the eigenvector of the matrix defined in
the bracket (called the transfer matrix), associated with
its largest eigenvalue z. This equation can be solved by
g→ (X)
simply iterating Eq. G8, and normalizing
after
P e
each iteration (by e.g. maxX g→ (X) or X g→ (X)). After convergence, that normalization constant at each step

P (Kt , Kt+u+1 , . . . , Kt+u+v ) =

X

and replacing with Eqs. G5,G9 and G1, we get:
P (Xt ) =

1 g→ (Xt )+g← (Xt )+H(Xt )
e
zt

(G12)

and
P (Xt , Xt+1 ) = P (Kt , . . . , Kt+v )
1
eg→ (Xt+1 )+g← (Xt )+H(Xt )+H(Xt+1 )+W (Xt ,Xt+1 ) .
=
zt zt+1
(G13)
We can also calculate pairwise marginals between Kt at
arbitrary time differences by using the following recursion, for u > v:

P (Kt , Kt+u , . . . , Kt+u+v−1 )P (Kt+u+v |Kt+u , . . . , Kt+u+v−1 ).

(G14)

Kt+u

starting with u = 0:
P (Kt , Kt+1 , . . . , Kt+v ) = P (Xt , Xt+1 ).

This whole procedure can be performed at an arbitrary
(G15)

11
inverse temperature β. The energy of a given spike train
is, according to Eq. E5:
E=−

X

h(Kt ) −

v
XX

t

t

Ju (Kt , Kt+u ),

(G16)

u=1

and thus at temperature 1/β the distribution of spike
trains reads:

hEiβ
1 X
=−
h(Kt )Pβ (Kt )
NL
N
Kt

v
1 X
−
N u=1

X

Ju (Kt , Kt+u )Pβ;u (Kt , Kt+u )

Kt ,Kt+u

(G17)

where Z(β) enforces normalization. The distribution
Pβ (K1 , . . . , Kt ) is given by Eq. E10 with the substitutions:
h(K) → βh(K),
Ju (K, K 0 ) → βJu (K, K 0 ).

(G18)
(G19)

All the results of the procedure, z(β), g→ (X; β) and
g← (X; β) thus depend on β. The free energy F (β) =
−β −1 log Z(β) can be calculated per unit time through
f (β) ≡ F (β)/N L = −β −1 log z(β)/N . The average en-

X

(β) ≡

(G20)

1 −βE({Kt })
e
,
Pβ ({σi,t }) =
Z(β)

Q` (Kt+` , . . . , Kt+`+v−1 ) =

ergy (Eq. G16) per unit time is given by:

and the entropy per unit time by s(β) ≡ S(β)/N L =
β(β) + log z(β)/N . The specific heat c(β) = −β∂s/∂β
is obtained by numerical derivation.
The technique of transfer matrices can also be extended to calculate the statistics of avalanches. Two distributions can be calculated: that of the duration of the
avalanche, and that of the number of spikes in it. An
avalanche starts at t if Kt−1 = 0 and Kt > 0. It ends
after ` steps if Kt+` = 0, and Kt0 > 0 for all t0 such that
t ≤ t0 < t + `. The probability Q` for an avalanche to
last at least ` steps, and have Kt+` , . . . , Kt+`+v−1 spiking
neurons at the v subsequent step is given recursively by:

Q` (Kt+`−1 , . . . , Kt+`+v−2 )P (Kt+` |Kt+`−1 , . . . , Kt+`+v−2 )

(G21)

Kt+`−1 >0

with initialization ` = 0:
Q` (Kt , . . . , Kt+v−1 ) =

P (Kt−1 = 0, Kt , . . . , Kt+v−1 )
P (Xt−1 , Xt )
=
.
P (Kt−1 = 0)
P (Kt−1 = 0)

Then the probability that the avalanche lasts ` steps is calculated through:
X
P` =
Q` (Kt+` = 0, Kt+`+1 , . . . , Kt+`+v−1 ).

(G22)

(G23)

Kt+`+1 ,...,Kt+`+v−1

Restricting to non-zero avalanches, the distribution is given by P` /(1 − P`=0 ).
The distribution of the number of spiking events in the avalanche can be calculated in a similar way, although at
a higher computational cost. We define R` (Kt+` , . . . , Kt+`+v−1 ; n) as the probability that an avalanche has lasted at
least ` steps, has accumulated n spiking events during these steps, and has (Kt+` , . . . , Kt+`+v−1 ) spiking cells in the
v time windows following the `th step. Then the following recursion holds:
X
R` (Kt+` , . . . , Kt+`+v−1 ; n) =
R` (Kt+`−1 , . . . , Kt+`+v−2 ; n − Kt+`−1 )P (Kt+` |Kt+`−1 , . . . , Kt+`+v−2 ). (G24)
Kt+`−1 >0

The initialization at ` = 0 simply reads:
R` (Kt , . . . , Kt+v−1 , n) =

P (Kt−1 = 0, Kt , . . . , Kt+v−1 )
δn,0
P (Kt−1 = 0)

(G25)

As before the joint distribution P`,n for the size and duration of avalanches is obtained by summing over
Kt+`+1 , . . . , Kt+`+v−1 as in Eq. G23, and restricting to non-zero avalanches (` > 0).

Appendix H: Model learning

The procedure described in the previous section allows
us to calculate the marginals and thermodynamic quanti-

ties for a given set of parameters h(K) and Ju (K, K). We

12
c.

b.

a.

12

16
14

v × ∆t =
v × ∆t =
v × ∆t =
v × ∆t =

10ms,
10ms,
20ms,
20ms,

∆t = 5ms
∆t = 10ms
∆t = 5ms
∆t = 10ms

15

12

10

c(β)

Specific heat c(β)

14

=0
=1
=2
=3
=4

8

10

c(β)

v
v
v
v
v

16

8

6

6

4

4

2

2

10

N
N
N
N
N
N
N
N
N

=5
= 10
= 20
= 30
= 40
= 50
= 61
= 97
= 185

5

0
0.9

0.95

1

1.05

0
0.95

1.1

1

1.05

0
0.9

0.95

1

1.05

1.1

1/β

1/β

Temperature 1/β

FIG. S4: Effect of window size on the specific heat. Same as figure 2 of the main text, for a window size ∆t = 5 ms.
(a) Specific heat c(β) of spike trains of the entire population (N = 185), as a function of temperature 1/β, for an increasing
temporal range v. (b) Comparison with the curves obtained for ∆t = 10 ms, with the same temporal range v × ∆t expressed
in seconds (=10 ms for cyan curves, 20 ms for the red curves). Solid lines are for ∆t = 5 ms, and dashed line for ∆t = 10 ms.
(c) Specific heat of spike trains of subnetworks of increasing sizes N , for v = 4. Each point is averaged over 100 random
subnetworks for N ≤ 50, and shows one representative network for N = 61 and 97. The error bars show standard deviations.

b.
1.2

v
v
v
v
v
v

1.15
1.1

Var(surprise)/N L

Peak temperature

a.
=0
=1
=2
=3
=4
=5

1.05
1
0

100

200

10

v
v
v
v
v
v

want to solve the inverse problem, which is to find these
parameters for a given set of marginals Pu (Kt , Kt+u ). To
do this we implement the following iteration:

=0
=1
=2
=3
=4
=5

5

0
0

100

200

Network size N

Network size N

FIG. S5: Effect of window size on finite-size scaling.
Same as figure 3 of the main text, for a window size ∆ = 5
ms. (a) Position of the peak 1/β in specific heat as a function
of network size N , for an increasing temporal range v. (b)
Normalized variance of the surprise as a function of N , for an
increasing temporal range v.

h(K) ← h(K) +  [Pdata (K) − Pmodel (K)]
Ju (Kt , Kt+u ) ← Ju (Kt , Kt+u ) +  [Pdata (Kt , Kt+u ) − Pmodel (Kt , Kt+u )] ,
after which we enforce our constraints (Eqs. E7, E8, E9) by:
"
#
v
X
X
X
0
0
0
0
h(K) ← h(K) +
P (K )Ju (K, K ) +
P (K )Ju (K , K)
K0

u=1

h(K) ← h(K) −

X

(H1)
(H2)

(H3)

K0

P (K 0 )h(K 0 )

(H4)

K0

Ju (K, K 0 ) ← Ju (K, K 0 ) −

X
K 00

P (K 00 )Ju (K 00 , K 0 ) −

X
K 00

Note that only the first step H1,H2 actually modifies the
model. At each step, Pmodel must be re-calculated from
the new set of parameters (h, Ju ).

P (K 00 )Ju (K, K 00 ) +

X

P (K 00 )P (K 000 )Ju (K 00 , K 000(H5)
)

K 00 ,K 000

We initialize the algorithm by setting Ju = 0 for u > 1.
This corresponds to the case v = 1, for which h(K) and
J1 (K, K 0 ) can be deduced directly from P (Xt |Xt−1 ).

13
This procedure is equivalent to a gradient descent algorithm on the log-likelihood [2], and therefore is guaranteed to converge to the solution provided that  is small
enough.

Appendix I: Inferred parameters, and choice of v

To assess the performance of the model, we can ask
how well it predicts the correlations of K at different
times. The mutual information, defined as:
MI(Kt , Kt+u ) =

X

Pu (K, K 0 ) log

K,K 0

Pu (K, K 0 )
, (I1)
P (K)P (K 0 )

is a non-parametric measure of these correlations. Fig. S2
shows this mutual information estimated from the data,
as well as its prediction for models with different v. Note
that by construction, the agreement is perfect for u ≤ v.
The v = 3 and v = 4 model predictions are fairly good
even for larger u, indicating that a larger v would not
improve the model prediction much.
The inferred Ju (Kt , Kt+u ) are represented in Fig. S3
for v = 4 and N = 185. They become smaller as u increases, indicating that the effective interactions between
time windows decay with the time difference. This can
be quantified using the Direct Information, which measures the strength of interaction between two variables in
a complex interaction network [31]. The direct pairwise
distribution is defined as:
Pudir (K, K 0 ) = eJu (K,K

0

)+φ(K)+φ0 (K 0 )

,

(I2)

where
φ(K) and φ(K 0 ) are Pchosen so that
P
dir
0
0
dir
0
K Pu (K, K ) = P (K ), and
K 0 Pu (K, K ) =
P (K). This distribution corresponds to the effect that
Kt and Kt+u would have on each other if they were
not interacting with Kt0 at other times t0 . The direct
information is then defined as the mutual information in
this pairwise distribution:

DI(Kt , Kt+u ) =

X
K,K 0

Pudir (K, K 0 ) log

Pudir (K, K 0 )
. (I3)
P (K)P (K 0 )

This quantity is represented in gray in Fig. S2, and
shows a sharp decay as a function of u, a further indication that v = 4 is sufficient.

Appendix J: Effect of the window size

Both the thermodynamic approach and the model used
to describe spike trains depend on the window size ∆t.
We repeated the analysis with a shorter window size of
∆t = 5 ms. The results are shown in Figs. S4 and S5.
In the limit of small window sizes, we expect that models with different ∆t, but with the same temporal range in
seconds, v×∆t, should yield similar predictions. Fig. S4b
shows that this is indeed the case. This indicates that the
results of our analysis do not depend much on the choice
of window size.

