Math. Model. Nat. Phenom.
Vol. 5, No. 3, 2010, pp. 146-184

arXiv:0905.0149v3 [q-bio.NC] 18 Jun 2010

Observers for Canonic Models of Neural Oscillators
D. Fairhursta , I. Tyukin a,c,d1 , H. Nijmeijerb , and C. van Leeuwenc
a

Department of Mathematics, University of Leicester, University Road, LE1 7RH, UK
b
Department of Mechanical Engineering, Eindhoven University of Technology,
P.O. Box 513 5600 MB, Eindhoven, The Netherlands
c
RIKEN (Institute for Physical and Chemical Research) Brain Science Institute,
2-1, Hirosawa, Wako-shi, Saitama, 351-0198, Japan
d
Deptartment of Automation and Control Processes, St-Petersburg State University
of Electrical Engineering, Prof. Popova str. 5, 197376, Russia

Abstract. We consider the problem of state and parameter estimation for a class of nonlinear
oscillators defined as a system of coupled nonlinear ordinary differential equations. Observable
variables are limited to a few components of state vector and an input signal. This class of systems
describes a set of canonic models governing the dynamics of evoked potential in neural membranes, including Hodgkin-Huxley, Hindmarsh-Rose, FitzHugh-Nagumo, and Morris-Lecar models. We consider the problem of state and parameter reconstruction for these models within the
classical framework of observer design. This framework offers computationally-efficient solutions
to the problem of state and parameter reconstruction of a system of nonlinear differential equations, provided that these equations are in the so-called adaptive observer canonic form. We show
that despite typical neural oscillators being locally observable they are not in the adaptive canonic
observer form. Furthermore, we show that no parameter-independent diffeomorphism exists such
that the original equations of these models can be transformed into the adaptive canonic observer
form. We demonstrate, however, that for the class of Hindmarsh-Rose and FitzHugh-Nagumo
models, parameter-dependent coordinate transformations can be used to render these systems into
the adaptive observer canonical form. This allows reconstruction, at least partially and up to a
(bi)linear transformation, of unknown state and parameter values with exponential rate of convergence. In order to avoid the problem of only partial reconstruction and at the same time to be able
to deal with more general nonlinear models in which the unknown parameters enter the system
nonlinearly, we present a new method for state and parameter reconstruction for these systems.
The method combines advantages of standard Lyapunov-based design with more flexible design
and analysis techniques based on the notions of positive invariance and small-gain theorems. We
1

Corresponding author. E-mail: I.Tyukin@le.ac.uk

1

D. Fairhurst et al.

Observers for Canonic Models

show that this flexibility allows to overcome ill-conditioning and non-uniqueness issues arising in
this problem. Effectiveness of our method is illustrated with simple numerical examples.
Key words: Parameter estimation, adaptive observers, nonlinear parametrization, convergence,
nonlinear systems, neural oscillators
AMS subject classification: 93B30, 93B10, 93B07, 93A30, 92B05

Notations and Nomenclature
The following notational conventions are used throughout the paper:
• R is the field of real numbers.
• R>0 = {x ∈ R | x > 0}.
• Z denotes the set of integers, and N stands for the set of positive integers.
• The Euclidian norm of x ∈ Rn is denoted by kxk.
• C r denotes the space of continuous functions that are at least r times differentiable.
• Let h : Rn → R be a differentiable function and f : Rn → Rn . Then Lf h(x), or simply
Lf h, is the Lie derivative of h with respect to f :
Lf h(x) =

∂h
f (x)
∂x

• Let f, g : Rn → Rn be differentiable vector-fields. Then the symbol [f, g] stands for the Lie
bracket:
∂f
∂g
[f, g] =
g−
f
∂x
∂x
The adjoint representation of the Lie bracket is defined as
ad0f g = g, adkf g = [f, adfk−1g]
• Let A be a subset of Rn , then for all x ∈ Rn , we define dist(A, x) = inf q∈A kx − qk.
• O(·) denotes a function such that lims→0 O(s)/s = d, d ∈ R, d 6= 0.
• Finally, let ǫ ∈ R>0 , then kxkǫ stands for the following:

kxk − ǫ, kxk > ǫ,
kxkǫ =
0,
kxk ≤ ǫ.

2

D. Fairhurst et al.

Observers for Canonic Models

1. Introduction
Mathematical modelling of brain processes and function is recognized as an important tool of modern neuroscience [14]. It allows us to predict, analyze and understand intricate processes of neural
computations, without invoking technically involving and costly experiments. Successful examples include but are not limited to modelling the memory function [4], [15] and the mechanisms of
phase-resetting in olivo-cerebellar networks [38]. Availability of quantitatively accurate models of
individual neural cells is an important prerequisite of such studies.
The majority of available models of individual biological neurons are the systems of ordinary
differential equations describing the cell’s response to stimulation; their parameters characterize
variables such as time constants, conductances, and response thresholds, important for relating
the model responses to behavior of biological cells. Typically two general classes of models
co-exist: phenomenological and mathematical ones. Models of the first class, such as e.g. the
Hodgkin-Huxley equations, claim biological plausibility, whereas models of the second class are
more abstract mathematical reductions without explicit relation of all of their variables to physical
quantities such as conductances and ionic currents (see Table 1).
Despite these differences, these models admit a common general description which will be
referred to as canonic. In particular, the dynamics of a typical neuron are governed by the following
Table 1: Examples of typical mathematical models of spiking single neurons. Biologically plausible equations of membrane potential generation in a giant axon of a squid (left panel), and a
reduction of these equations to an oscillator with polynomial right-hand side (right panel)
Hodgkin-Huxley Model [10]

Hindmarsh-Rose Model [9]



v̇ = I(t) − θ1 m3 h(v + θ2 ) + θ3 n4 (v + θ4 ) + θ5 v + θ6


v
v + θ7
ṁ = (1 − m)ϕ
− m θ9 exp
θ
θ10
 8

v + θ12
v
ṅ = (1 − n)θ11 ϕ
− n θ14 exp
(1.1)
θ13
θ15


v + θ18
v
− h/ 1 + exp
ḣ = (1 − h)θ16 exp
θ17
θ19
ϕ(x) = x/(exp x − 1), θ1 , ..., θ19 – parameters, I : R → R –
input current

3

v̇ = θ1 (θ2 r − f (v)) + I(t)
ṙ = θ3 (g(v) − θ4 r) (1.2)
f (v) and g(v) are polynomials:
f (v) = θ5 + θ6 v +
= θ7 v 2 + θ8 v 3 ;
g(v) = θ9 + θ10 v + θ11 v 2
I : R → R is the external input, θ1 , ..., θ11 – parameters

D. Fairhurst et al.

Observers for Canonic Models

set of equations
v̇ =

X

j

ϕj (v, t)pj (r)θj + I(t);

ṙi = −ai (v, θ, t)ri + bi (v, θ, t);
θ = (θ1 , θ2 , . . . ).

(1.3)

in which the variable v is the membrane potential, and ri are the gating variables of which the
values are not available for direct observation. Functions ϕj (·, ·), pj (·) ∈ C 1 are assumed to
be known; they model components of specific ionic conductances. Functions ai (·), bi (·) ∈ C 1
are also known, yet they depend on the unknown parameter vector θ. System (1.3) is a typical
conductance-based description of the evoked potential generation in neural membranes [13]. It is
also an obvious generalization of many purely mathematical models of spike generation such as the
FitzHugh-Nagumo [7] or the Hindmarsh-Rose equations [9]. In this sense systems (1.3) represent
typical building blocks in the modelling literature.
In order to be able to model the behavior of large numbers of individual cells of which the
input-output responses are described by (1.3), computational tools for automated fitting of models
of neurons to data are needed. These tools are the algorithms for state and parameter reconstruction
of (1.3) from the available measurements of v(t) and I(t) over time.
Fitting parameters of nonlinear ordinary differential equations to data is recognized as a hard
computational problem [5] that “has not been yet treated in full generality” [20]. Within the field of
neuroscience, conventional methods for fitting parameters of model neurons to measured data are
often restricted to hand-tuning or exhaustive trial-and-error search in the space of model parameters
[31]. Even though these strategies allow careful and detailed exploration in the space of parameters
they suffer from the same problem – the curse of dimensionality.
Available alternatives, recognizing obvious nonlinearity of the original problem, propose to
reformulate the original estimation problem as that of searching for the parameters of a system
of difference equations approximating solutions of (1.3) [1]; or predominantly offer search-based
optimization heuristics (see [37] for a detailed review) as the main tool for automated fitting of
neural models. Straightforward exhaustive-search approaches however are limited to varying only
few model parameters over sparse grids, e.g. as in [31] where 8 parameters were split into 6
bands. Coarseness of this parametrization leads to non-uniqueness of signal representation, leaving
room for uncertainty and inability to distinguish between subtle changes in the cell. More finegrained search algorithms are currently infeasible, technically speaking. Other heuristics, such
as evolutionary algorithms, are examined in [2]. According to [2], replacing exhaustive search
with evolutionary algorithms allows to increase the number of varying parameters to 24. Yet,
computational complexity of the problem still delimits the search to sparse grids (6 bands per single
parameter) and requires days of simulation by a cluster of 10 Apple 2.3 GHz nodes. Furthermore,
because all these strategies are heuristic, accuracy of final results is not guaranteed.
The main aim of this article is to present a feasible substitute to these heuristic strategies for
automatic reconstruction of state and parameters of canonic neural models (1.3). To develop computationally efficient procedures for state and parameter reconstruction of (1.3) we propose to
exploit the wealth of system-identification and estimation approaches from the domain of control
4

D. Fairhurst et al.

Observers for Canonic Models

theory. These approaches are based on the system-theoretic concepts of observability and identifiability [30], [12],[19] from control theory, and the notions of Lyapunov stability [22] and weakly
attracting sets [26]. The advantage of using these approaches is that there is an abundance of algorithms (observers) already developed within the domain of control. These algorithms guarantee
asymptotic and stable reconstruction of unmeasured quantities from the available observations,
provided that the system equations are in an adaptive observer canonical form. Moreover, this
reconstruction can be made exponentially fast without the need of substantial computational recourses. We study if system (1.3) is at all observable with respect to the output v, that is if its
state and parameters can be reconstructed from observations of v. We present and analyze typical algorithms (adaptive observers) that are available in the literature. We show that for a large
class of mathematical models of neural oscillators at least a part of the model parameters can be
reconstructed exponentially fast.
In order to deal with more general classes of models and also to recover the rest of the model
parameters we introduce a novel observer scheme. This scheme benefits from 1) the efficiency of
uniformly converging estimation procedures (stable observers), 2) success of explorative search
strategies in global optimization by allowing unstable convergence along dense trajectories, and 3)
the power of qualitative analysis of dynamical systems. We present a general description of this
observer and list its asymptotic properties. The theory of this new class of algorithms is based
on the results of our previous studies in the domain of unstable convergence [35], [36]. We will
present examples to demonstrate the performance of these algorithms.
The paper is organized as follows. In Section 2 we provide the basic notions of observability
from the domain of mathematical control, test if typical canonical neural oscillators are observable,
and present two major classes of systems (canonical forms) for which computationally efficient
reconstruction procedures are available. In Section 3 we analyze the applicability of standard
observers to the problem of reconstructing all unmeasured variables and parameters of typical
models of neurons. We present two special cases in which such reconstruction is possible. In
Section 4 we provide a description and asymptotic properties of our algorithm that applies to the
most general subset of models (1.3). Section 5. contains examples of application of the considered
observers, and Section 5 concludes the paper. Proofs of the main technical statements are presented
in the Appendix.

2. Observer-based approaches to the problem of state and parameter estimation
Let us consider the following class of dynamical systems
ẋ = f (x, θ) + g(x, θ)u(t),
y = h(x),

x ∈ Rn ,

x(t0 ) ∈ Ωx ⊂ Rn

θ ∈ Rd ,

5

y∈R

(2.1)

D. Fairhurst et al.

Observers for Canonic Models

where f, g : Rn × Rm → Rn , h : Rn → R are smooth functions2 , and u : R → R. Variable x
stands for the state vector, u ∈ U ⊂ C 1 [t0 , ∞) is the known input, θ ∈ Rm is the vector of unknown
parameters, and y is the output of (2.1). System (2.1) includes equations (1.3) as a subclass and
in this respect can be considered as plausible generalizations. Obviously, conclusions about (2.1)
should be valid for systems (1.3) as well.
Given that the right-hand side of (2.1) is differentiable, for any x′ ∈ Ωx , u ∈ C 1 [t0 , ∞) there
exists a time interval T = [t0 , t1 ], t1 > t0 such that a solution x(t, x′ ) of (2.1) passing through x′ at
t0 exists for all t ∈ T . Hence y(t) = h(x(t)) is defined for all t ∈ T . For the sake of convenience
we will assume that the interval T of the solutions is large enough or even coincides with [t0 , ∞)
when necessary.
We are interested in finding an answer to the following question: suppose that we are able
to measure the values of y(t) and u(t) precisely; wether and how the values of x′ and parameter
vector θ can be recovered from the observations of y(t) and u(t) over a finite subinterval of T ? A
natural framework to answer to these questions is offered by the concept of observability [30].
Definition 1 (Observability). Two states x1 , x2 ∈ Rn are said to be indistinguishable (denoted
by x1 Ix2 ) for (2.1) if for every admissible input function u the output function t → y(t, 0, x1, u),
t ≥ 0 of the system for initial state x(0) = x1 , and the output function t → y(t, 0, x2, u), t ≥ 0
of the system for initial state x(0) = x2 , are identical on their common domain of definition. The
system is called observable if x1 Ix2 implies x1 = x2 .
According to Definition 1, observability of a dynamical system implies that the values of its
state, x(t), t ∈ [t1 , t2 ] are completely determined by inputs and outputs u(t), y(t) over [t1 , t2 ].
Although this definition does not account for any unknown parameter vectors, one can easily see
that the very same definition can be used for parameterized systems as well. Indeed, extending
original equations (2.1) by including parameter vector θ as a component of the extended state
vector x̃ = (x, θ)T results in
ẋ = f (x, θ) + g(x, θ)u(t),
θ̇ = 0
y = h(x),

(2.2)
x(t0 ) ∈ Ωx ⊂ R

n

or, similarly, in
x̃˙ = f˜(x̃) + g̃(x̃)u(t)
y = h̃(x̃),

(2.3)

x̃(t0 ) = (x(t0 ), θ)T ∈ Ωx̃ ⊂ Rn+d

where f˜(x̃) = (f (x, θ), 0)T , g̃(x̃) = (g(x, θ), 0)T , and h̃(x̃) = (h(x), 0). All uncertainties in (2.1),
(2.2), including the parameter vector θ, are now combined into the state vector of (2.3). Hence
the problem of state and parameter reconstruction of (2.1) can be viewed as that of recovering the
values of state for (2.3).
2

Let us recall that a function is smooth in G if for every x ∈ G and n ∈ N the function

6

dn
dxn f (x)

is always defined.

D. Fairhurst et al.

Observers for Canonic Models

Definition 1 characterizes observability as a global property of a dynamical system. Sometimes, however, global observability of a system in Rn is not necessarily needed. Instead of asking
if every point in the system’s state space is distinguishable from any other point it may be sufficient to know if the system’s states are distinguishable in some neighborhood of a given point.
This necessitates the notion of local observability [30].
Let V be an open subset of Rn . Two states x1 , x2 ∈ V are said to be indistinguishable (denoted
by x1 I V x2 ) on V for (2.1) if for every admissible input function u : [0, T ] → R with the property
that the solutions x(t, 0, x1 , u), and x(t, 0, x2 , u) both remain in V for t ≤ T the output function
t → y(t, 0, x1 , u), t ≥ 0 of the system for initial state x(0) = x1 , and the output function t →
y(t, 0, x2, u), t ≥ 0 of the system for initial state x(0) = x2 , are identical for 0 ≤ t ≤ T on their
common domain of definition.
Definition 2 (Local observability [30]). The system is called locally observable at x0 if there exists
a neighborhood W ⊂ Rn of x0 such that for every neighborhood V ⊂ W of x0 the relation x0 I V x1
implies x0 = x1 . The system is locally observable if it is observable at each x0 .
A number of observability tests are available that, given the functions h, f in the right-hand side
of (2.3), indicate if a given system is observable. Particular formulations of these tests may vary
depending on whether e.g. the functions f, g are analytic or time-invariant (inputs are constants).
In this article we will restrict our attention to those systems (2.2) in which the inputs u(t) are
constants. In this case we can replace the function u(t) with an unknown parameter, and system
(2.2) can be viewed as a system (2.3) yet without inputs. One of the most common observability
tests for this class of autonomous systems is given below (see also [30], Theorem 3.32):
Proposition 3 (Observability test (Corollary 3.33, [30])). System (2.3) is locally observable at a
point xo ∈ U ⊂ Rn+d if
rank

∂ 
h̃(x̃) Lf˜h̃(x̃) L2f˜h̃(x̃)
∂ x̃

Lfn+d−1
h̃(x̃)
˜

...

T

= n + d,

∀ x̃ ∈ U

(2.4)

In what follows we shall use the test above to determine if the models of neural dynamics are
at all observable.

2.1. Local observability of neural oscillators
We start our observability analysis by applying the local observability test (2.4) to the HindmarshRose model (1.2). In order to do so we shall extend the system state space so that unknown
parameters are the components of the extended state vector. In the case of the Hindmarsh-Rose

7

D. Fairhurst et al.

Observers for Canonic Models

1.0

x2

K

1.0

1.0

x2

0.5

K

0

0.5

0.5

x1

K

1.0

K

1.0

1.0

x2

0.5

K

0

0.5

0.5

x1

1.0

K

1.0

K

K

1.0

(a) δ = 10

20

(b) δ = 10

1.0

x1

0.5

K

1.0

0.5

K

0.5

K

0

0.5

K

0.5

0.5

1.0

15

(c) δ = 105

Figure 1: Observability tests for Hindmrsh-Rose model neuron (1.2)
model this procedure leads to the following extended system of equations:

 

ẋ1
θ13 x31 + θ12 x21 + θ11 x1 + θ10 + x2
 ẋ2  

−λx2 + θ22 x21 + θ21 x1



 θ̇13  


0

 

 θ̇  

0
 12  


 

0
 θ̇11  = 


 

0
 θ̇10  


 

0
 θ̇22  


 

0
 θ̇21 
0
λ̇

(2.5)

To test if there are points of local observability of system (2.5) it is sufficient to find a point in
the state space of (2.5) at which the rank condition (2.4) holds. Here we computed the determinant:
T
∂
h(x̃) Lf h(x̃) L2f h(x̃) ... Lfn−1 h(x̃)
∂ x̃
x̃ = (x1 , x2 , θ13 , θ12 , θ̇11 , θ10 , θ22 , θ21 , λ)T

D(x̃) =

on a sparse grid (of 101 × 101 pixels) and plotted those regions for which the determinant is less
than a certain value, δ. The neuron parameters were set to L = −2, θ13 = −10, θ12 = −4, θ11 =
6, θ10 = 1, θ22 = −32, θ21 = −32. Figure 1 shows results (obtained using Maple) for various
values of δ. The shaded regions correspond to the domains where D(x̃) < δ. According to these
results, when the value of delta is made sufficiently small, condition D(x̃) > δ holds for almost
all points in the grid. This suggests that there are domains in which model (1.2) is indeed at least
locally observable.
Let us now consider a more realistic, with respect to biological plausibility, set of equations.

8

D. Fairhurst et al.

K

1.0

Observers for Canonic Models

1.0

1.0

1.0

0.5

0.5

0.5

K

0

0.5

0.5

1.0

K

1.0

K

0

0.5

0.5

1.0

K

1.0

K

0

0.5

K

K

K

K

K

K

0.5

0.5

1.0

1.0

(a) δ = 10000

(b) δ = 1000

0.5

1.0

0.5

1.0

(c) δ = 100

Figure 2: Observability tests for Morris-Lecar model neuron
One of the simplest models of this type is the Morris-Lecar system [28]:




v(t)+1
1
1
v̇(t) = −gCa 2 + 2 tanh E4
(v(t) − E1 ) − gK w(t)(v(t) − E2 ) − gm (v(t) − E3 )


 


v(t)
(2.6)
−
w(t)
cosh
ẇ(t) = 15 12 + 21 tanh v(t)+1
E5
E6
E1 = 100, E2 = −70, E3 = −50, E4 = 15, E5 = 30, E6 = 60, gCa = 1.1, gK = 2.0, gm = 0.5
As in the previous example we extend the system state space by considering unknown parameter
as components of the extended state vector. This extension procedure results in the following set
of equations:


 


v̇(t)
−gCa 21 + 21 tanh v+1
(v
−
100)
−
g
w(v
+
70)
−
g
(v
+
50)
k
m
15



v
1 1
 ẇ(t)  

+ 12 tanh v+1
− w cosh 60
5 2
30
 


 ġCa (t)  

0
=


 ġK (t)  

0
 


 ġm (t)  

0
0
λ̇(t)

For this extended set of equations we estimated the regions where value of D(x) exceeds some
given δ > 0. These regions for different values of δ are presented in figure 2 These results demonstrate that the Morris-Lecar system (2.6) is also locally observable.
As we have seen above, a fairly wide class of canonical mathematical and conductance-based
models of evoked responses in neural membranes satisfy local observability conditions. We may
thus expect to be able to solve the reconstruction problem for these models. In fact, as we show
below in Sections 3, 4, the reconstruction problem can indeed be resolved efficiently at least for a
part of unmeasured variables of the system. However, before we proceed with detailed description
of these reconstruction algorithms, let us first review classes of systems for which solutions to the
problem of exponentially fast reconstruction of all components of state and parameter vectors are
already available in the literature.
9

D. Fairhurst et al.

Observers for Canonic Models

2.2. Bastin-Gevers canonical form
We start with a class of systems comprising of a linear time-invariant part of which the equations
are known and an additive time-varying component with linear parametrization. Parameters of this
time-varying component are assumed to be uncertain. This class of systems was presented by G.
Bastin and M. Gevers in 1989, [3], and its general form is as follows:
ẋ = Rx + Ω(t)θ + g(t)




Ω1 (t)
0 kT
Ω(t) =
R=
0 F
Ω(t)

(2.7)

y(t) = x1 (t)

In (2.7), x ∈ Rn is the state vector with y = x1 assigned to be the output. θ ∈ Rp = (θ1 , · · ·, θp )T
is the vector of unknown parameters. R is a known matrix of constants where k T = (k2 , · · ·, kn )
and F has dimension (n − 1) × (n − 1) with eigenvalues in the open left half plane. Ω(t) ∈ Rn×p
is an n × p matrix of known functions of t; the first row is designated Ω1 and the remaining n − 1
rows Ω. The vector function g(t) : R → Rn is known.
Equations (2.7) are often referred to as an adaptive observer canonical form. This is because,
subject to some mild non-degeneracy conditions, it is always possible to reconstruct the vector of
unknown parameters θ and state x from observations of y over time. Moreover, the reconstruction
can be made exponentially fast. Shown below is the adaptive observer presented in [3]. The
system to be observed, state estimator, parameter adaption, auxiliary filter and regressor are given
in equations (2.7), (2.8), (2.9), (2.10), (2.11) respectively
!
c1 ỹ
˙
(2.8)
x̂(t)
= Rx̂(t) + Ω(t)θ̂ + g(t) +
˙
V (t)θ̂
˙
θ̂(t) = Γϕ(t)ỹ(t)
(2.9)
V̇ (t) = F V (t) + Ω(t), V (0) = 0
(2.10)
T
T
ϕ(t) = V (t)k + Ω1 (t)
(2.11)
The output is y = x1 , its estimate is ŷ = x̂1 and its error is ỹ = y − ŷ. This observer contains some
parameters of its own which are at the design’s disposal. Γ = ΓT is an arbitrary positive definite
matrix, normally chosen as Γ = diag(γ1 , γ2, ..., γp ), γi > 0. c1 > 0. The auxiliary filter V (t) is an
(n − 1) × p matrix and ϕ(t) is a p vector.
Using the transformation (2.12), the error system (2.13) is obtained.


0
∗
x̃ = x̃ −
, x̃ = x − x̂
(2.12)
V θ̃
˙∗

x̃ =




 T 
−c1 k T
ϕ θ̃
∗
x̃ +
0
F
0
˙
θ̃ = −Γϕx̃∗1
10

(2.13)

D. Fairhurst et al.

Observers for Canonic Models

It is shown in [3], for constant unknown parameters, that the solution x(t) = x̂(t), θ = θ̂(t)
of the extended system (2.8), (2.9), (2.10), (2.11) is globally exponentially stable provided certain
conditions on the regressor vector, ϕ(t), are met. These conditions are:
• the regressor vector ϕ(t) is bounded for all t ≥ 0
• ϕ̇(t) is bounded for all t ≥ 0 except possibly at a countable number of points {ti } such that
min|ti − tj | ≥ △ > 0 for some arbitrary fixed △.
• ϕ(t) is persistently exciting: that is, there exists positive constant α, T such that for all t0 ≥ 0
Z

t0 +T

t0

ϕ(t)ϕT (t) dt ≥ αI > 0

(2.14)

Formally, asymptotic properties of observer (2.8), (2.9) are specified in the theorem below [3]3

Theorem 4. Suppose that
1) c1 > 0 and F is a Hurwitz matrix, that is its eigenvalues belong to the left half of the complex
plane;
2) the function ϕ(t) is globally bounded in t, and its time derivative exists and is globally
bounded for all t ≥ 0;
3) the function ϕ(t) is persistently exciting.
Then the origin of (2.13) is globally exponentially asymptotically stable.
Adaptive observer canonical form (2.7) applies to systems in which the regressor Ω(t)θ does
not depend explicitly on the unmeasured components of the state vector. The question, however,
is when a rather general nonlinear system can be transformed into the proposed canonical form.
This question was addressed in [24] in which a modified adaptive observer canonical form was
proposed together with necessary and sufficient conditions describing when a given system can be
transformed into such form via a diffeomorphic coordinate transformation. This canonical form is
described in the next subsection.
3

Here we provide a slightly reduced formulation of the main statement of [3] corresponding to the case in which
the values of θ do not change over time.

11

D. Fairhurst et al.

Observers for Canonic Models

2.3. Marino-Tomei canonical form
The canonical form presented in [24] is now shown here. The system to be observed (2.15), state
estimator (2.16) and parameter adaption (2.17) are given below
P
ẋ(t) = A1 x(t) + φ0 (y(t), u(t)) + b pi=1 βi (y(t), u(t))θi
y(t) = 
C1 x(t)

0 1 0 ... 0
 0 0 1 ... 0 



.
.
.
...
0
(2.15)
A1 = 


 0 0 0 ... 1 
0 0 0 ... 0
1 0 0 ... 0
C1 =
x(t) ∈ Rn , y(t) ∈ R, βi (·, ·) : R × R → R

In (2.15) x(t) ∈ Rn is the state vector with x1 (t) assigned to be the output y. Matrices A1 , C1 are
in canonical observer form. θ ∈ Rp = (θ1 , · · ·, θp )T is the vector of unknown parameters. The
functions βi are known, bounded and piecewise continuous functions of y(t), u(t). The column
vector b ∈ Rn is assumed to be Hurwitz4 with b1 6= 0.
Shown below is the adaptive observer presented in [24]
˙
x̂(t)
= (A1 − KC1 )x̂(t) + φ0 (y(t), u(t)) + b

p
X

βi (y(t), u(t))θ̂i + Ky(t)

(2.16)

i=1

˙
θ̂ = Γβ(t)(y − C1 x̂)sign(b1 )
(2.17)
1
K =
(A1 b + λb) = (k1 , · · ·, kn )T
(2.18)
bn
with Γ an arbitrary symmetric positive definite matrix and λ an arbitrary positive real. The n × 1
vector, b, is Hurwitz with bn 6= 0.
For the more general case where the vector b is an arbitrary vector, an observer is presented in
[25].
Theorem 5. [23] There exists a local change of coordinates, z = Φ(x), transforming
ẋ = f (x) +

p
X

θi (t)qi (x),

y = x1

i=1

x ∈ Rn , y ∈ R, θi ∈ R, qi : Rn → Rn , n ≥ 2

(2.19)

with h(xo ) = 0 and (f, h) an observable pair, into the system
ż = A1 z + ψ(y) +

p
X

θi (t)ψi (y),

y = C1 z

i=1

z ∈ Rn , ψi : R → Rn

(2.20)

We say that a vector b = (b1 , . . . , bn )T ∈ Rn is Hurwitz if all roots of the corresponding polynomial b1 pn−1 +
· · · + bn−1 p + bn have negative real part.
4

12

D. Fairhurst et al.

Observers for Canonic Models

with (A1 , C1 ) in canonical observer form (2.15), if and only if
(i) [adif g, adjf g] = 0,
(ii) [qi , adjf g] = 0,

0 ≤ i, j ≤ n − 1

0 ≤ j ≤ n − 2,

1≤i≤p

where the vector field, g(x), is uniquely defined by



h(x)
*
+


∂ 
 Lf h(x)  , g(x) = 



...
∂x
n−1
Lf h(x)


0
0 

... 
1

(2.21)

The proof of this result is made along the following lines. Suppose we use the change of
coordinates: z = Φ(x), then we have
∂Φ
ẋ
∂x


p
∂Φ X
∂Φ
f (x)
+
θi qi (x)
=
∂x
∂x i=1
x=Φ−1 (z)

ż =

(2.22)
(2.23)

It is shown in [23] that providing we meet the constraint:
[adif g, adjf g] = 0,

∀x ∈ U, 0 ≤ i, j ≤ n − 1

(2.24)

then we can cast the system into the adaptive observer canonical form
ż = A1 z + ψ(y) +

p
X
∂Φ
i=1

∂x

θi qi (x),

y = C1 z

(2.25)

with A1 , C1 in canonical observer form (2.15). Furthermore, it is shown in [23] that providing we
meet the constraint
[qi , adjf g] = 0,

∀x ∈ U, 0 ≤ j ≤ n − 2

(2.26)

then we can put the system into
ż = A1 z + ψ(y) +

p
X

θi ψi (y),

y = C1 z

(2.27)

i=1

This representation is linear in the unknown variables, z1 (t), z2 (t), ..., zn (t), and θ1 (t), θ2 (t), ..., θp (t),
while it is nonlinear only in the output, y(t), which is available for measurement.

13

D. Fairhurst et al.

Observers for Canonic Models

3. Feasibility of conventional adaptive observer canonical forms
In this section we consider technical difficulties preventing straightforward application of conventional adaptive observers for solving the state and parameter reconstruction problems for typical
neural oscillators. We start with the most simple polynomial systems such as the Hindmarsh-Rose
equations. We show that even for this relatively simple class of linearly parameterized models the
problem of reconstructing all parameters of the system is a difficult theoretical challenge. Whether
complete reconstruction is possible depends substantially on what part of the system’s right-hand
side is corrupted with uncertainties. Despite in the most general case reconstruction of all components of the parameter vector by using standard techniques may not be possible, in some special
yet relevant cases estimation of a part of the model parameters is still achievable in principle.
Let us consider, for example, the problem of fitting parameters of the conventional HindmarshRose oscillator to measured data. In particular we wish to be able to model a single spike from
the measured train of spikes evoked by a constant current injection. Classical two-dimensional
Hindmarsh-Rose model is defined by the following system:
ẋ = −ax3 + bx2 + y + I
ẏ = c − dx2 − y, a = 1, b = 3, c = 1, d = 5,

(3.1)

in which I ∈ R stands for the stimulation current. Trajectories x(t) of this model are known to
be able to reproduce a wide range of typical responses of actual neurons qualitatively. Quantitative modelling, however, requires the availability of a linear transformation of (x(t), y(t)) so the
amplitude and the frequency of oscillations x(t) can be made consistent with data.
In what follows we will consider (3.1) subject to the following class of transformations:

  
 

px
x
k1 0
x1
, k2 < k3
(3.2)
+
=
py
y
k2 k3
x2
where ki > 0 and px , py ∈ R are unknown. Transformations (3.2) include stretching and translations as a special case. In addition to (3.2) we will also allow that the time constants in the
right-hand side of (3.1) be slowly time-varying. This will allow us to adjust scaling of the system
trajectories with respect to time.
Taking these considerations into account we obtain the following re-parameterized description
of model (3.1):
ẋ1 =

3
X

xi1 θ1,i + x2

i=0

ẋ2 = −λx2 +

3
X

(3.3)
xi1 θ2,i

i=1

Alternatively, in vector-matrix notation we obtain:




x1
ẋ1
+ ϕ(x1 )θ
= A(λ)
x2
ẋ2
14

(3.4)

D. Fairhurst et al.

Observers for Canonic Models

where


 3 2

0 1
x1 x1 x1 1 0 0 0
A(λ) =
, ϕ(x1 ) =
0 −λ
0 0 0 0 x31 x21 x1
(3.5)
θ = (θ1,3 , θ1,2 , θ1,1 , θ1,0 , θ2,3 , θ2,2 , θ2,1 )
One of the main obstacles is that the original equations of neural dynamics are not written in
any of the canonical forms for which the reconstruction algorithms are available. The question,
therefore, is if there exists an invertible coordinate transformation such that the model equations
can be rendered canonic. Below we demonstrate that this is generally not the case if the transformation is parameter-independent. This is formally stated in Section 3.1. However, if we allow our
transformation to be both parameter and time-dependent, a relevant class of models with polynomial right-hand sides can be transformed into one of the canonic forms. This is demonstrated in
Section 3.2.


3.1. Parameter-independent time-invariant transformations
Let us consider a class of systems that can be described by (3.4). Clearly this system is not in a
canonical adaptive observer form because A(λ) depends on the unknown parameter λ explicitly.
The question, however, is if there exists a differentiable coordinate transformation
z = Φ(x), z1 = x1
such that in the new coordinates the equations of system (3.4) satisfy one of the canonic descriptions. We show that the answer to this question is negative, and it follows from the following
slightly more general statement
Theorem 6. The system
ẋ = f (x) +

p
X

θi qi (x),

y = h(x)

i=1

x ∈ Rn , y ∈ R, qi : Rn → Rn , n ≥ 2

(3.6)

with


0
 0
f (x) = 
 .
0

1
0
.
0

1
0
.
0

...
...
...
...


1
0 
 x,
. 
0

h(x) =

1 0 0 ... 0



x

(3.7)

cannot be transformed by diffeomorphic change of coordinates, z = φ(x), into
ż = A1 z + ψ0 (y) +

p
X

θi ψi (y),

y = C1 z

i=1

z ∈ Rn , y ∈ R, ψi : R → Rn

(3.8)

with A1 , C1 in canonical observer form (2.15), if either (i) n > 2 or (ii) there exists i ∈
{1, ..., p}, j ∈ {2, ..., n} such that ∂qi /∂xj 6= 0.
The proof of Theorem 6 and other results are provided in the Appendix.
15

D. Fairhurst et al.

Observers for Canonic Models

3.2. Parameter-dependent and time-varying transformations
Let us now consider the case in which the transformation z = Φ(x, λ, θ, t) is allowed to depend
on unknown parameters and time. As we show below, this class of transformations is much more
flexible. In principle it allows us to solve the problem of partial state and parameter reconstruction for an important class of oscillators with polynomial right-hand side and time-invariant time
constants.
We start by searching for a transformation Φ:
Φ : q = T (λ)x, |T (λ)| =
6 0
such that
T (λ)A(λ)T

−1

(λ) =



⋆ 1
⋆ 0



(3.9)

where the matrix A(λ) is defined as in (3.5). It is easy to see that the transformation satisfying this
constraint exists, and it is determined by


1 0
T (λ) =
.
(3.10)
λ 1
According to (3.9), (3.10) and (3.5) equations of (3.4) in the coordinates q can be written as
q̇ = A1 q + ψ(q1 )η(θ, λ),

(3.11)

where
A1 =



0 1
0 0



, ψ(q1 ) =



q13 q12 q1 1 0 0 0 0
0 0 0 0 q13 q12 q1 1



,

η(θ, λ) = (θ1,3 , θ1,2 , θ1,1 − λ, θ1,0 , λθ1,3 + θ2,3 , λθ1,2 + θ2,2 , λθ1,1 + θ2,1 , λθ1,0 )

(3.12)
T

Remark 7. Notice that
• availability of the parameter vector η in (3.11), (3.12), expressed as a function of θ, λ,
implies the availability of θ, λ if θ1,0 6= 0. Indeed, in this case the value of λ = η8 /η4 and
the values of all θi,j are uniquely defined by ηi ;
• condition θ1,0 6= 0 is sufficient for reconstructing the values of x2 provided that q and η are
available; indeed in this case x = T −1 (λ)q
As follows from Remark 7 the problem of state and parameter reconstruction of (3.4) from measured data x1 (t) amounts to solving the problem of state and parameter reconstruction of (3.11).
In order to solve this problem we shall employ yet another coordinate transform:
z1 = q1
z2 = q2 + ζ T (t)η
16

(3.13)

D. Fairhurst et al.

Observers for Canonic Models

in which the functions ζ(t) are some differentiable functions of time. Coordinate transformation
(3.13) is clearly time-dependent. The role of this additional transformation is to transform the
equations of system (3.11) into the form for which a solution already exists.
Definitions of these functions, specific estimation algorithms and their convergence properties
are discussed in detail in the next section.

3.3. Observers for transformed equations
3.3.1. Bastin-Gevers Adaptive Observer
Proceeding from (3.11), (3.12) and applying a second change of coordinates given by





1 0
z1
q1
=
,
f
1
z2
q2
k
k
where f ∈ R<0 and k ∈ R are some design parameters, we obtain the canonical form (2.7)
presented in [3]




ż1
z1
=R
+ g(t) + Ω(y)η(θ, λ)
ż2
z2 

0 k
R=
0 f 
f
(3.14)
g(t) = −y f 2
k
 3

y
y2 y 1 0
0
0 0
Ω(y) =
f 3 f 2 f
y k y k y fk k1 y 3 k1 y 2 k1 y k1
k
η(θ, λ) = (θ13 , θ12 , θ11 − λ, θ10 , θ23 + λθ13 , θ22 + λθ12 , θ21 + λθ11 , λθ10 )T
System (3.14) now is in the Bastin-Gevers adaptive observer canonical form. Notice that the
parameter vector η(λ, θ) remains unchanged and recall Remark 7. Let us proceed to the observer
construction following the steps described in (2.8) – (2.13).
We start by introducing an auxiliary filter of which the general form is given by (2.10). According to (3.14) the auxiliary filter is defined as follows:
v̇1
v̇2
v̇3
v̇4
v̇5
v̇6
v̇7
v̇8

=
=
=
=
=
=
=
=

f v1 +
f v2 +
f v3 +
f v4 +
f v5 +
f v6 +
f v7 +
f v8 +

17

f 3
y
k
f 2
y
k
f
y
k
f
k
1 3
y
k
1 2
y
k
1
y
k
1
k

(3.15)

D. Fairhurst et al.

Observers for Canonic Models

Hence in accordance with (2.11) the regressor vector ϕ(t) is written as
ϕ1
ϕ2
ϕ3
ϕ4
ϕ5
ϕ6
ϕ7
ϕ8

=
=
=
=
=
=
=
=

kv1 + y 3
kv2 + y 2
kv3 + y
kv4 + 1
kv1
kv2
kv3
kv4

and the observer equations are as follows:






0
−c
k
c
1
1
x̂˙ =
x̂ +
x̂1 + g(t) + Ω(x1 )η̂ +
0 f
0
V η̂˙
η̂˙ = Γϕ(x1 − x̂1 ),

(3.16)

(3.17)

V = (v1 , v2 , v3 , v4 , v5 , v6 , v7 , v8 )

Taking (3.15) – (3.17), and (2.13) into account we obtain the following equations governing
the dynamics of the estimation error, (x̃∗ , η̃)T


 T 
˙x̃∗ = −c1 k x̃∗ + ϕ η̃
0 f
0
(3.18)
η̃˙ = −Γϕx̃∗1
The auxiliary filter (3.15) acts here as an inherent component of a time-varying coordinate transformation rendering the error dynamics into (2.13). This coordinate transformation is similar to
that defined by (3.13), provided that z, η in (3.13) are replaced by estimation errors x̃∗ , η̃.
Let us now explore asymptotic properties of the observer. First we notice that v4 (t), v8 (t) both
converge to constant values exponentially fast as t → ∞. In fact,
1
1
lim v4 (t) = − , lim v8 (t) = − .
t→∞
k t→∞
fk
Thus accordingly ϕ4 (t), ϕ8 (t) both tend to constant values as t → ∞:
1
lim ϕ4 (t) = 0, lim ϕ8 (t) = − .
t→∞
f

t→∞

The latter fact implies that the persistency of excitation requirement is necessarily violated for
regressor (3.16). Indeed, condition (2.14) does not hold if one of the components of ϕ(t) is exponentially converging to zero. The question therefore, is if this approach can be used at all to
construct asymptotically converging estimators of state and parameters of (3.14). The answer to
this question is provided in the corollary below
18

D. Fairhurst et al.

Observers for Canonic Models

Corollary 8. Consider the function
ϕ̄(t) = (ϕ1 (t), ϕ2 (t), ϕ3 (t), ϕ5 (t), ϕ6 (t), ϕ7 (t), ϕ8 (t))T
If it is globally bounded and persistently exciting, (2.14), then the following holds along the solutions of (3.15) – (3.17):
lim x̂(t) − x(t) = 0, lim η̂i (t) − ηi , i 6= 4,

t→∞

t→∞

and the convergence is exponential.
Remark 9. Corollary 8 demonstrates that despite the original result of [3], i.e. Theorem 4, does
not apply to system (3.14) directly one can still construct a reduced order observer for this system.
This reduced observer guarantees partial reconstruction of unmeasured parameters, and this reconstruction is exponentially fast. To recover the true values of unknown parameters one needs to
solve the following system
η1
η2
η3
η5
η6
η7
η8

= θ13
= θ12
= θ11 − λ
= θ23 + λθ13
= θ22 + λθ12
= θ21 + λθ11
= λθ10

for θi , λ taking the values of η̂i as the estimates of ηi . Solution to this system may not be unique,
hence the reconstruction is generally possible only up to a certain scaling factor.
Simulation results for this observer are presented in Section 5.
3.3.2. Marino-Tomei Observer
Let us define the vector-function ζ(t) in (3.13) as follows:
ζ̇i = −kζi + kψ1,i (q1 ) − ψ2,i (q1 ), k ∈ R>0
In this case we have
ż2 =

8
X

ψ2,i (q1 )ηi + k −

i=1
8
X

=k

8
X

ζi ηi + ψ1,i (q1 )ηi

i=1

(−ζi + ψ1,i (q1 ))ηi

i=1

19

!

−

8
X
i=1

ψ2,i (q1 )ηi

D. Fairhurst et al.

Observers for Canonic Models

Hence, taking equality (3.13) into account and expressing q2 as q2 = z2 − ζ T (t)η(θ, λ) we obtain
ż1 = z2 +

8
X

(−ζi + ψ1,i (q1 ))ηi

i=1

ż2 = k

8
X

(3.19)

(−ζi + ψ1,i (q1 ))ηi

i=1

Notice that ψ1,4 (q1 ) = ψ2,8 (q1 ) = 1, hence −ζ4 + ψ1,4 (q1 ) and −ζ8 + ψ1,8 (q1 ) converge to some
constants in R exponentially fast as t → ∞. Moreover, the sum −ζ4 + ψ1,4 (q1 ) is converging to
zero, and the sum −ζ8 + ψ1,8 (q1 ) is converging to −1/k as t → ∞. Taking these facts into account
we can conclude that system (3.19) can be rewritten in the following (reduced) form
ż = A1 z + bφT (z1 , t)υ(θ, λ) + bε(t),

 

1
0 1
, b=
A1 =
k
0 0


−ζ1 + ψ1,1 (z1 )
 −ζ2 + ψ1,2 (z1 ) 


 −ζ3 + ψ1,3 (z1 ) 



−ζ
+
ψ
(z
)
φ(z1 , t) = 
8
1,8
1


 −ζ5 + ψ1,5 (z1 ) 


 −ζ6 + ψ1,6 (z1 ) 
−ζ7 + ψ1,7 (z1 )

(3.20)

υ(θ, λ) = (θ1,3 , θ1,2 , θ1,1 , θ̄1,0 , λθ1,3 + θ2,3 , λθ1,2 + θ2,2 , λθ1,1 + θ2,1 )T ,

where ε(t) is an exponentially decaying term.
System (3.20) is clearly in the adaptive canonic observer form. Hence it admits the following
adaptive observer
ẑ˙ = A1 ẑ + L(ẑ − z) + bφT (z1 , t)υ̂


−l1 0
L=
, l1 = k + 1, l2 = k
−l2 0
υ̂˙ i = −γ(ẑ1 − z1 )φi (z1 , t), γ ∈ R>0

(3.21)

of which the asymptotic properties are specified in the following Theorem
Theorem 10. Let us suppose that system (3.20) be given and its solutions are defined for all t.
Then, for all initial conditions, solutions of the combined system (3.20), (3.21) exist for all t and
lim ẑ(t) − z(t) = 0

t→∞

Furthermore, if the function φ(z1 , t) is persistently exciting and z(t) is bounded then
lim υ̂(t) − υ(θ, λ) = 0,

t→∞

and the dynamics of ẑ − z, υ̂ − υ are exponentially stable in the sense of Lyapunov.
20

D. Fairhurst et al.

Observers for Canonic Models

The proof of Theorem 10 is provided in the Appendix.
Remark 11. Similar to Corollary 8 for Bastin-Gevers observer, Theorem 10 provides us with a
computational scheme that, subject to that φ(z1 , t) is persistently exciting, can be used to estimate
the values of the modified vector of uncertain parameters υ(θ, λ). The question, however, is that if
the values of θ, λ can always be restored from υ(θ, λ). In general, the answer to this question is
negative. Indeed, according to (3.11) we have
θ1,3 = υ1
θ1,2 = υ2
θ1,1 − λ = υ3
θ̄1,0 = λθ1,0 = υ4
λθ1,3 + θ2,3 = υ5
λθ1,2 + θ2,2 = υ6
λθ1,1 + θ2,1 = υ7

(3.22)

As follows from (3.22) one can easily recover the values of θ1,3 , θ1,2 , and θ1,1 . However, recovering
the values of remaining parameters explicitly from the estimates of υ(θ, λ) is possible only up to
a certain scaling parameter. Indeed, if the number of unknowns in (3.22) exceeds the number of
equations by one.
Remark 12. Notice that in the relevant special cases, when the value of either θ2,3 , θ2,2 , or θ2,1 is
zero, such reconstruction is obviously possible. Let us suppose that θ2,3 = 0. Hence the value of λ
can be expressed from (3.22) as
υ5
λ= ,
(3.23)
υ1
and thus the rest of parameters can be reconstructed as well. Due to the presence of division in
(3.23), this scheme may be sensitive to persistent perturbations when υ5 = λθ1,3 is small.
So far we considered special cases of (1.3) in which the time constants of unmeasured variables were unknown yet constant and parametrization of the right-hand side was linear. As we
mentioned in Remark 12, even for this simpler class of systems solving the problem of parameter
reconstruction may not be a straightforward operation. For example, if there are cubic, quadratic
and linear terms in the second equation of (3.4) then recovering all parameters of (3.4) by observer
(3.21) may not be possible. Nonlinear parametrization, time-varying time constants and nonlinear coupling between equations in the right-hand side of (1.3) make the reconstruction problem
even more complicated. Even though there are results that partially address the issue of nonlinear
parametrization, see e.g. [34], [33], [6], [32], [18], the estimation problem for systems with general
nonlinear parametrization is still an open issue.
In the next section we show that for a large subclass of (1.3) there always exists an observer
that solves the problem of state and parameter reconstruction from the measurements of V . Moreover the structure of this observer does not depend significantly on specific equations describing
21

D. Fairhurst et al.

Observers for Canonic Models

dynamics of the observed system. For this reason, and similarly to [11], we refer to this class of
observers as universal adaptive observers.

4. Universal adaptive observers for conductance-based models
The ideas of universal adaptive observers for systems with nonlinearly parameterized uncertainty
was introduced in a series of works [35], [36] devoted to the study of convergence to unstable
invariant sets. Here we provide a review of these results and discuss how they can be applied to
the problem of state and parameter reconstruction of (1.3).
The following class of models is considered in [36]:
P

ẋ0 = θ0T φ0 (x0 , p0 , t) + ni=1 ci (x0 , qi , t)xi + c0 (x0 , q0 , t) + ξ0 (t) + u(t)




ẋ1 = −β1 (x0 , τ1 , t) x1 + θ1T φ1 (x0 , p1 , t) + ξ1 (t),



..

.
(4.1)
ẋi = −βi (x0 , τi , t) xi + θiT φi (x0 , pi , t) + ξi (t),




..


.


ẋn = −βn (x0 , τn , t) xn + θnT φi (x0 , pn , t) + ξn (t),
y = (1, 0, . . . , 0)x = x0 , xi (t0 ) = xi,0 ∈ R,
x = col(x0 , x1 , . . . , xn ), θi = col(θi,1 , . . . , θi,di ),

where
φi :R × Rmi × R≥0 → Rdi , φi ∈ C 0 , di , mi ∈ N, i = {0, . . . , n}
βi :R × R × R≥0 → R>0 , βi ∈ C 0 , i = {1, . . . , n}
ci :R × Rri × R≥0 → R, ci ∈ C 0 , ri ∈ N, i = {0, . . . , n}
are continuous and known functions, u : R≥0 → R, u ∈ C 0 is a known function of time modelling
the control input, and ξi : R≥0 → R, ξi ∈ C 0 are functions that are unknown, yet bounded. The
functions ξi (t) represent unmodeled dynamics, external perturbations, residuals due to the coarsegraining procedures at the stage of reduction [8], etc.
Variable y in system (4.1) is the output, and the variables xi , i ≥ 1 are the components of state
x, that are not available for direct observation. Vectors θi ∈ Rdi consist of linear parameters of
uncertainties in the right-hand side of the i-th equation in (4.1). Parameters τi ∈ R, i = {1, . . . , n}
are the unknown parameters of time-varying relaxation rates, βi (x0 , τi , t), of the state variables xi ,
and vectors pi ∈ Rmi , qi ∈ Rri , consist of the nonlinear parameters of the uncertainties. The
functions ci (x0 , qi , t) are supposed to be bounded.
Notice that system (4.1) is almost as general as (1.3). The only difference is that variables xi ,
i ≥ 1 enter the first equation of (4.1) as
n
X

ci (x0 , qi , t)xi

i=1

22

D. Fairhurst et al.

Observers for Canonic Models

whereas the corresponding variables ri in system (1.3) enter the first equation in a slightly more
general way
X
ϕj (v, t)pj (r)θj .
j

This difference, however, is not critical for the observers presented in [36] can be adjusted to deal
with this more general case as well.
For notational convenience we denote:
θ = col(θ0 , θ1 , · · · , θn ),
λ = col(p0 , q0 , τ1 , p1 , q1 . . . , τn , pn , qn ),
n
X
s = dim (λ) = n +
(mi + ri ).
i=0

Symbols Ωθ and Ωλ , respectively, denote domains of admissible values for θ and λ.
The system state x = col(x0 , x1 , · · · , xn ) is not measured; only the values of the input u(t)
and the output y(t) = x0 (t), t ≥ t0 in (4.1) are accessible over any time interval [t0 , t] that belongs
to the history of the system. The actual values of parameters θ, λ are assumed to be unknown
a-priori. We assume however, that they belong to a set, e.g. a hypercube, with known bounds:
θi,j ∈ [θi,min , θi,max ], λi ∈ [λi,min , λi,max].
Instead of imposing the traditional requirement of asymptotic estimation of the unknown parameters with arbitrarily small error we relax our demands to estimating the values of state and
parameters of (4.1) up to a certain tolerance. This is because we allow unmodeled dynamics, ξi (t),
in the right-hand side of (4.1). As a result of such a practically important addition there may exist a
set of systems of which the solutions are relatively close to the measured data yet their parameters
could be different. Instead of just one value of unknown parameter vectors θ, λ we therefore have
to deal with a set of θ, λ corresponding to the solutions of (4.1) that over time are sufficiently
close. This set of model parameters is referred to as an equivalence class of (4.1).
Similarly to canonical observer schemes [23], [3], [25] the method presented in [36] relies on
the ability to evaluate the integrals
Z t R
t
µi (t, τi , pi ) ,
e− τ βi (x0 (χ),τi ,χ)dχ φi (x0 (τ ), pi , τ )dτ
(4.2)
t0

at a given time t and for the given values of τi , pi within a given accuracy. In classical adaptive
observer schemes, the values of βi (x0 , τi , t) are constant. This allows us to transform the original
equations by a (possibly parameter-dependent) non-singular linear coordinate transformation, Φ :
x 7→ z, x1 = z1 , into an equivalent form in which the values of all time constants are known. In
the new coordinates the variables z2 , . . . , zn can be estimated by integrals (4.2) in which the values
of βi (x0 , τi , t) are constant and known. This is usually done by using auxiliary linear filters. In our
case, the values of βi (x0 , τi , t) are not constant and are unknown due to the presence of τi . Yet if

23

D. Fairhurst et al.

Observers for Canonic Models

the values of τi would be known we could still estimate the values of integrals (4.2) as follows
Z t R
t
e− τ βi (x0 (χ),τi ,χ)dχ φi (x0 (τ ), pi , τ )dτ ≃
(4.3)
t0
Z t
Rt
e− τ βi (x0 (χ),τi ,χ)dχ φi (x0 (τ ), pi , τ )dτ , µ̄i (t, τi , pi ),
t−T

where T ∈ R>0 is sufficiently large and t ≥ T + t0 .
Alternatively, if φi (x0 (t), pi , t), βi (x0 (t), τi , t) are periodic with rationally - dependent periods
and satisfy the Dini condition in t, integrals (4.2) can be estimated invoking a Fourier expansion.
Notice that for continuous and Lipschitz in pi functions µi (t, τi , pi ) the coefficients of their Fourier
expansion remain continuous and Lipschitz with respect to pi .
In the next sections we present the general structure of the observer for (4.1) and provide a list
of its asymptotic properties.

4.1. Observer definition and assumptions
Consider the following function ϕ(x0 , λ, t) : R × Rs × R≥0 → Rd , d =

Pn

i=0

ϕ(x0 , λ, t) = (φ0 (x0 , p0 , t), c1 (x0 , q1 , t)µ1 (t, τ1 , p1 ), . . .
. . . , cn (x0 , qn , t)µn (t, τn , pn ))T

di :
(4.4)

The function ϕ(x0 , λ, t) is a concatenation of φ0 (·) and integrals (4.2). We assume that the values
of ϕ(x0 , λ, t) can be efficiently estimated for all x0 , λ, t ≥ 0 up to a small mismatch. In other
words, we suppose that there exists a function ϕ̄(x0 , λ, t) such that the following property holds:
kϕ̄(x0 , λ, t) − ϕ(x0 , λ, t)k ≤ ∆ϕ , ∆ϕ ∈ R>0 ,

(4.5)

where values of ϕ̄(x0 , λ, t) are efficiently computable for all x0 , λ, t (see e.g. (4.3) for an example
of such approximations), and ∆ϕ is sufficiently small.
If parameters τi , pi , and qi in the right-hand side of (4.1) would be known and ci (x0 , qi , t) = 1,
βi (x0 , τi , t) = τi , then the function ϕ(x0 , λ, t) could be estimated by (φ0 (x0 , t), η1 , . . . , ηn ) where
ηi are the solutions of the following auxiliary system (filter)
η̇i = −τi ηi + φi (x0 , pi , t)

(4.6)

with zero initial conditions. Systems like (4.6) are inherent components of standard adaptive observers [17], [3], [23]. In our case we suppose that the values of τi , qi , pi are not know a-priori and
that ci (x0 , qi , t), βi (x0 , τi , t) are not constant. Therefore, we replace ηi with their approximations,
e.g. as in (4.3):
ϕ̄(x0 , λ, t) = (φ0 (x0 , p0 , t), c1 (x0 , q1 , t)µ̄1 (t, τ1 , p1 ), . . .
. . . , cn (x0 , qn , t)µ̄n (t, τn , pn ))T .
24

D. Fairhurst et al.

Observers for Canonic Models

For periodic φi (x0 (t), pi , t), βi (x0 (t), τi , t) a Fourier expansion can be employed to define ϕ̄(x0 , λ, t).
The value of ∆ϕ in (4.5) stands for the accuracy of approximation, and as a rule of thumb the more
computational resources are devoted to approximate ϕ(x0 , λ, t) the smaller is the value of ∆ϕ .
With regard to the functions ξi (t) in (4.1) we suppose that an upper bound, ∆ξ , of the following
sum is available:
n
X
1
kξi (τ )k∞,[t0 ,∞] + kξ0 (τ )k∞,[t0 ,∞] ≤ ∆ξ , ∆ξ ∈ R≥0 .
τ
i=1 i

(4.7)

Denoting c0 (x0 , q0 , t) = c0 (x0 , λ, t), for notational convenience, we can now define the observer as

 x̂˙ = −α(x̂ − x ) + θ̂ T ϕ̄(x , λ̂, t) + c (x , λ̂, t) + u(t)
0
0
0
0
0 0
(4.8)
 θ̂˙ = −γ (x̂ − x )ϕ̄(x , λ̂, t), γ , α ∈ R
θ
0
0
0
θ
>0
x̂˙ i = −βi (x0 , τ̂i , t)x̂i + θ̂iT φi (x0 , p̂i , t), i = {1, . . . , n},

(4.9)

where
θ̂ = col(θ̂0 , θ̂1 , · · · , θ̂n )
is the vector of estimates of θ. The components of vector λ̂ = col(p̂0 , q̂0 , τ̂1 , p̂1 , q̂1 , . . . , τ̂n , p̂n , q̂n ) =
col(λ̂1 , . . . , λ̂s ), with s = dim (λ), evolve according to the following equations

 ˙
x̂1,j
= γ · ωj · e · x̂1,j − x̂2,j − x̂1,j x̂21,j + x̂22,j 


 ˙
x̂2,j
= γ · ωj · e · x̂1,j + x̂2,j − x̂2,j x̂21,j + x̂22,j
(4.10)
λj,max −λj,min

(x̂1,j + 1),

2
 λ̂j (x̂1,j ) = λj,min +
e
= σ(kx0 − x̂0 kε ),
j = {1, . . . , s}, x̂21,j (t0 ) + x̂22,j (t0 ) = 1,

(4.11)

where σ(·) : R → R≥0 is a bounded continuous function, i.e. σ(υ) ≤ S ∈ R>0 , and |σ(υ)| ≤ |υ|
for all υ ∈ R. We set ωj ∈ R>0 and let ωj be rationally-independent:
X
ωj kj 6= 0, ∀ kj ∈ Z.
(4.12)
In order to proceed further we will need the notions of λ-uniform persistency of excitation [21]
and nonlinear persistency of excitation [6]:

Definition 13 (λ-uniform persistency of excitation). Let ϕ : R≥0 × D → Rn×m , D ⊂ Rs be a
continuous function. We say that ϕ(t, λ) is λ-uniformly persistently exciting (λ-uPE) if there exist
µ ∈ R>0 , L ∈ R>0 such that for each λ ∈ D
Z t+L
ϕ(t, λ)ϕ(t, λ)T dτ ≥ µI ∀t ≥ t0 .
(4.13)
t

25

D. Fairhurst et al.

Observers for Canonic Models

In contrast
definitions, the present notion requires that the lower bound for the
R t+L to conventional
T
integral t ϕ(t, λ)ϕ(t, λ) dτ in (4.13) does not vanish for all λ ∈ D, and is separated away
from zero. We need this property in order to determine the linear parts, θi , of the parametric
uncertainties in model (4.1).
To reconstruct the nonlinear part of the uncertainties, λ, we will require that ϕ̄(x0 , λ, t) is nonlinearly persistently exciting in λ. Here we adopt the definition of nonlinear persistent excitation
from [6] with a minor modification. The modification is needed to account for a possibility that
ϕ̄(x0 , λ, t) = ϕ̄(x0 , λ′ , t), λ 6= λ′ , t ∈ R,
which is the case, for example if ϕ̄(x0 , λ, t) is periodic in λ. The modified notion is presented in
Definition 14 below.
Definition 14 (Nonlinear persistency of excitation). The function
ϕ̄(x0 , λ, t) is nonlinearly persistently exciting if there exist L, β ∈ R>0 such that for all λ, λ′ ∈ Ωλ
and t ∈ R there exists t′ ∈ [t − L, t] ensuring that the following inequality holds
kϕ̄(x0 , λ, t) − ϕ̄(x0 , λ′ , t′ )k ≥ β · dist(E(λ), λ′ ),

(4.14)

E(λ) = {λ′ ∈ Ωλ | ϕ̄(x0 , λ′ , t) = ϕ̄(x0 , λ, t) ∀ t ∈ R}

(4.15)

The symbol E(λ) denotes the equivalence class for λ, and dist(E(λ), λ′ ) in (4.14) substitutes
the Euclidian norm in the [6] original definition. The nonlinear persistency of excitation condition
(4.14) is very similar to its linear counterpart (4.13). In fact (4.13) can be written in the form of
inequality (4.14), cf. [27]. For further discussion of these notions, see [6], [21].

4.2. Asymptotic properties of the observer
The main results of this section are provided in Theorems 15 and 17. Theorem 15 establishes
conditions for state boundedness of the observer, and states its general asymptotic properties. Theorem 17 specifies a set of conditions for the possibility of asymptotic reconstruction of θi , τi , and
pi , up to their equivalence classes and small mismatch due to errors.
Proofs of Theorems 15, 17 and other auxiliary results can be found in [36].
Theorem 15 (Boundedness). Let system (4.1), (4.8) – (4.10) be given. Assume that function
ϕ̄(x0 (t), λ, t) is λ-uniformly persistently exciting, and the functions ϕ̄(x0 (t), λ, t), c0 (x0 (t), λ, t)
are Lipschitz in λ:
kϕ̄(x0 (t), λ, t) − ϕ̄(x0 (t), λ′ , t)k ≤ Dkλ − λ′ k,
kc0 (x0 (t), λ, t) − c0 (x0 (t), λ′ , t)k ≤ Dc kλ − λ′ k.
Then there exist numbers ε > 0, γ ∗ > 0 such that for all γ ∈ (0, γ ∗ ]:
26

(4.16)

D. Fairhurst et al.

Observers for Canonic Models

1) trajectories of the closed loop system (4.8) – (4.10) are bounded and
lim kx̂0 (t) − x0 (t)kε = 0;

t→∞

(4.17)

2) there exists λ∗ ∈ Ωλ , κ ∈ R>0 such that
lim λ̂(t) = λ∗

t→∞

lim sup kθ̂(t) − θk < κ((Dkθk + Dc )kλ∗ − λk + 2∆).

(4.18)

t→∞

∆ = kθk∆ϕ + ∆ξ

(4.19)

Remark 16. Theorem 15 assures that the estimates θ̂(t), λ̂(t) asymptotically converge to a neighborhood of the actual values θ, λ. It does not specify, however, how close these estimates are to
the true values of θ, λ.
The next result states that if the values of ∆ϕ and ∆ξ :
kϕ(x0 , λ, t) − ϕ̄(x0 , λ, t)k ≤ ∆ϕ
n
X
1
kξi (τ )k∞,[t0 ,∞] + kξ0 (τ )k∞,[t0 ,∞] ≤ ∆ξ
τ
i=1 i
in (4.5), (4.7) are small, e.g. ϕ̄(x0 (t), λ, t) approximates ϕ(x0 (t), λ, t) with sufficiently high
accuracy and the unmodeled dynamic is negligible, the estimates θ̂(t), λ̂(t) will converge to small
neighborhoods of the equivalence classes of θ, λ. The sizes of these neighborhoods are shown to
be bounded from above by monotone functions ∆:
∆ = kθk∆ϕ + ∆ξ
vanishing at zero. Formally this result is stated in Theorem 17 below
Theorem 17 (Convergence). Let the assumptions of Theorem 15 hold, assume that ϕ̄0 (x0 , λ, t) ∈
C 1 , the derivative ∂ ϕ̄0 (x0 (t), λ, t)/∂t is globally bounded, and ∆ = kθk∆ϕ + ∆ξ is small. Then
there exist numbers ε > 0, γ ∗ > 0 such that for all γi ∈ (0, γ ∗ )
1)

√
lim sup kθ̂(t) − θk = O( ∆) + O(∆)
t→∞

2) in case θ T ϕ̄(x0 (t), λ, t) + c0 (x0 (t), λ, t) is nonlinearly persistently exciting with respect to
λ, then the estimates λ̂(t) converge into a small vicinity of E(λ):
√
lim sup dist(λ̂(t), E(λ)) = O( ∆) + O(∆)
(4.20)
t→∞

27

D. Fairhurst et al.

Observers for Canonic Models

5. Examples
5.1. Parameter estimation of the 2D Hindmarsh-Rose model with BastinGevers observer
The canonical form (3.14) and the observer presented in Section 3.3 were built in MATLAB and
using the differential equation solver ode45, numerical results were obtained. Figure 3 shows the
parameter convergence of each η̂i (t), i 6= 4. The various parameter values were set as follows: for
the neuron
λ = 2.027, θ13 = −10.4, θ12 = −4.35, θ11 = 6.65, θ10 = 0.9125, θ22 = −32.45, θ11 = −32.15
and for the observer dynamics
k = 1, c1 = 1, Γ = diag(1, 1, 1, 1, 1, 1, 1, 1), F = −1

5.2. Parameter estimation of the 2D Hindmarsh-Rose model with MarioTomei observer
In addition to simulating the Bastin-Gever observer for the Hindmarsh-Rose model we also simulated observer (3.21) derived within the framework of the approach presented in [23]. In this case
we set
θ1,3 = −1, θ1,2 = 3, θ1,3 = 0, θ1,0 = 1.5, θ2,3 = 0, θ2,2 = −5, θ2,1 = 0, λ = −1
There is no particular reasoning behind our choice of parameters in both this and previous case
apart from that these parameters must induce persistent oscillatory dynamics of the solutions of
(3.3). Parameters of the observer were chosen as follows:
l1 = l2 = 1, γ = 1, k = 1.
Simulation results for this system are shown in Figure 4.

5.3. Parameter estimation of the Morris-Lecar model
Let us now turn to a more realistic class of equations, i.e. conductance-based models. In particular,
we consider the Morris-Lecar model, [28]:
1
(−ḡCa m∞ (V )(V − ECa ) − ḡK w(V − EK ) − ḡL (V − V0 )) + I
C
1
w∞ (V )
ẇ = −
w+
τ (V )
τ (V )

V̇ =

28

(5.1)

D. Fairhurst et al.

Observers for Canonic Models

2

8

0

6

10

8

−2

4

−4

6

2
−6
0

4

−8
−2

−10

2
−4

−12
−14
0

1

2

3

4

5

−6
0

1

2

3

4

5

5

0
0

1

x 10

3

4

5
5

x 10

(a) η̂1 (t) v. t

x 10

(b) η̂2 (t) v. t

(c) η̂3 (t) v. t

10

10

5

5

0

0

−10

−5

−20

−10

−30

−15

−40

−20

0

2

5

−5
−10
−15
−20
−25
0

1

2

3

4

5

−50
0

1

2

3

4

5

5

−25
0

1

2

3

5

x 10

(d) η̂5 (t) v. t

(e) η̂6 (t) v. t

4

5
5

x 10

x 10

(f) η̂7 (t) v. t

2
1
0
−1
−2
−3
−4
−5
−6
0

1

2

3

4

5
5

x 10

(g) η̂8 (t) v. t

Figure 3: Simulation results for Bastin-Gevers observer. Each η̂i (t), i 6= 4 are shown with the true
values indicated with a broken line. The periodic time of the spikes is circa 10 seconds while the
total time simulated is 5 × 105 seconds, thus the figure spans thousands of spike cycles. The visual
effect of these extreme time scales is seen in the figure; hunting oscillations within the observer
are seen as thickening of the graphs of certain estimated parameters.

29

D. Fairhurst et al.

Observers for Canonic Models

5

3
0

-5

0
-2

-15

-3
0

2

4

6

8

x 10

3

-4

0

2

(a) υ̂1 (t) v. t

4

6

8

x 10

3

0

2

(b) υ̂2 (t) v. t

4

6

8

x 10

8

x 10

3

(c) υ̂3 (t) v. t

8
1
4

2
-2

0

-5

-4

-4
0

2

4

6

8

x 10

3

0

2

(d) υ̂4 (t) v. t

4

6

8

x 10

3

(e) υ̂5 (t) v. t

0

2

4

6

3

(f) υ̂6 (t) v. t

0

-3

-6
0

2

4

6

8

x 10

3

(g) υ̂7 (t) v. t

Figure 4: Simulation results for Marino-Tomei observer. Each υ̂i (t) are shown with the true values
indicated with a broken line. The periodic time of the spikes is circa 10 seconds while the total
time simulated is 10 × 103 seconds, thus the figure spans thousands of spike cycles. The visual
effect of these extreme time scales is seen in the figure; hunting oscillations within the observer
are seen as thickening of the graphs of certain estimated parameters.

30

D. Fairhurst et al.

Observers for Canonic Models

where



V − V1
m∞ (V ) = 0.5 1 + tanh
V2



V − V3
w∞ (V ) = 0.5 1 + tanh
V4
1


τ (V ) = T0
−V3
cosh V2V
4

System (5.1) is a reduction of the standard 4-dimensional Hodgkin-Huxley equations, and is one
of the simplest models describing the dynamics of evoked membrane potential and, at the same
time, claiming biological plausibility.
Parameters ḡCa , ḡK , and ḡL stand for the maximal conductances of the calcium, potassium and
leakage currents respectively; C is the membrane capacitance; V1 , V2 , V3 , V4 are the parameters of
the gating variables; T0 is the parameter regulating the time scale of ionic currents; ECa and EK are
the Nernst potentials of the calcium and potassium currents, and EL is the rest potential. Variable
I models an external stimulation current. In this example the value of I was set to I = 10.
The total number of parameters in system (5.1) is 12, excluding the stimulation current I. Some
of these parameters, however, are already available or can be considered typical. For example the
values of the Nernst potentials for calcium and potassium channels, ECa , EK , are known and
usually are set as follows ECa = 100, EK = −70. The value of the rest potential, V0 , can be
estimated from the cell explicitly. Here we set V0 = −50. Parameters V1 , V2 characterize the
steady-state response curve of the activation gates corresponding to the calcium channels, and V3 ,
V4 are the parameters of the potassium channels. In the simulations we set these parameters to
standard values as e.g. in [16]: V1 = −1, V2 = 15, V3 = 10, and V4 = 29.
The values of parameters, ḡCa , ḡK , ḡL , and T0 , however, may vary substantially from one cell
to another. For example, the values of ḡCa , ḡK , ḡL depend on the density of ion channels in a
patch of the membrane; the value of T0 is dependent on temperature. Hence, in order to model the
dynamics of individual cells, we need to be able to recover these values from data.
As before, we suppose that the values of V over time are available for direct observation, and
the values of w are not measured. System (5.1) has no linear time-invariant part, and the dynamics
of w are governed by a nonlinear differential equation with the time-varying relaxation factor,
1/τ (V ). Therefore, observers presented in Section 3 may not be applied explicitly to this system.
This does not imply, however, that parameters ḡCa , ḡK , ḡL , and T0 cannot be recovered from the
measurements of V . In fact, as we show below, one can successfully reconstruct these parameters
by using observers defined in Section 4.
For the sake of notational consistency we denote x0 = V , x1 = w, and without loss of generality suppose that C = 1. Hence system (5.1) can now be rewritten as follows:
ẋ0 = θ0,1 m∞ (x0 )(x0 − ECa ) + θ0,1 x1 (x0 − EK ) + θ0,3 (x0 − V0 ) + I
1
ẋ1 = −β1 (x0 , λ)x1 + φ1 (x0 )
λ
31

(5.2)

D. Fairhurst et al.

Observers for Canonic Models

where


1
x0 − V3
β1 (x0 , λ) = cosh
, λ = T0
λ
2V4


x0 − V3
w∞ (x0 )
φ1 (x0 ) = cosh
2V4
Noticing that β1 (x0 , λ) is separated away from zero for all bounded x0 and positive λ we substitute
variable x1 in (5.2) with its estimation


Z t


3 ds
1 − λ1 Rτt cosh x0 (s)−V
x0 (τ ) − V3
2V4
χ(λ, t) =
cosh
w∞ (x0 (τ ))dτ
e
2V4
t−T λ
The larger the value of T the higher the accuracy of estimation for large t. After this substitution
system (5.2) reduces to just only one equation
ẋ0 = θ0,1 φ0,1 (x0 ) + θ0,2 φ0,2 (x0 , λ, t) + θ0,3 φ0,3 (x0 ) + I + ξ0 (t)

(5.3)

where
φ0,1 (x0 ) = m∞ (x0 )(x0 − ECa )
φ0,2 (x0 , λ, t) = (x0 − EK )χ(λ, t)
φ0,3 (x0 ) = (x0 − V0 )
and term ξ0 (t) is bounded.
Equation (5.3) is a special case of (4.1), and hence we can apply the results of Section 4 to
construct an observer for asymptotic estimation of the values of θ0,1 , θ0,2 , θ0,3 , and λ. In accordance
with (4.8) – (4.10) we obtain the following observer equations
x̂˙ 0
˙
θ̂1
˙
θ̂2
˙
θ̂3

= −α(x̂0 − x0 ) + θ̂1 φ0,1 (x0 ) + θ̂2 φ0,2 (x0 , λ̂, t) + θ̂3 φ0,3 (x0 ) + I
= −γθ (x̂0 − x0 )φ0,1 (x0 )

(5.4)

= −γθ (x̂0 − x0 )φ0,2 (x0 , λ̂, t)
= −γθ (x̂0 − x0 )φ0,3 (x0 )
λ̂ = 3 + x̂1,1

x̂˙ 1,1 = γe x̂1,1 − x̂2,1 − x̂1,1 x̂21,1 + x̂22,1

x̂˙ 2,1 = γe x̂1,1 + x̂2,1 − x̂2,1 x̂21,1 + x̂22,1
e = σ(kx0 − x̂0 kε )

(5.5)

Parameters of the observer were set as follows: α = 1, ε = 0.001, γ = 0.01, γθ = 0.05.
According to Theorems 15, 17 observer (5.4), (5.5) should ensure successful reconstruction
of the model parameters provided that the regressor is persistently exciting. This requirement is
32

D. Fairhurst et al.

Observers for Canonic Models

2

4

1

<

3.6

0

<

q3

l

3.2
-1
2.8
-2

<
<

-3

q1

2.4

q2

0

2

4

6

x 10 4 ,

0

t

2

4

6

x 10 4 ,

t

Figure 5: Trajectories of the estimates of parameters θ1 , θ2 , θ3 and λ as functions of time. The
periodic time of the spikes is circa 10 seconds while the total time simulated is 8 × 104 seconds,
thus the figure spans thousands of spike cycles. The visual effect of these extreme time scales is
seen in the figure; hunting oscillations within the observer are seen as thickening of the graphs of
certain estimated parameters.
satisfied for model (5.2) generating periodic solutions. We simulated system (5.2), (5.4), (5.5) over
a wide range of initial conditions. Figure 5 shows an example of typical behavior of the observer
over time. As we can see from this figure all estimates converge to small neighborhoods of true
values of the parameters.

6. Conclusion
In this article we have reviewed and explored observer-based approaches to the problem of state
and parameter reconstruction for classes of typical models of neural oscillators. The estimation
procedure in this approach is defined as a system of ordinary differential equations of which the
right-hand side does not depend explicitly on the unmeasured variables. The solution of this system
(or functions of the solutions) should asymptotically converge to small neighbourhoods of the
actual values of the variables to be estimated. Until recently, due to nonlinear dependence of
the vector-fields of the models on unknown parameters and also due to uncertainties in the time
scales of hidden variables, observer-based approach to solving the problem of state and parameter
estimation of neural oscillators was a relatively unexplored territory. Here we demonstrate that
despite these obvious difficulties the approach can be successfully applied to a wide range of
models.
Two different strategies to observer design have been studied in the paper. The first strategy is
based on the availability of canonical representations of the original system. Success of this strategy is obviously determined by wether one can find a suitable coordinate transformation such that
33

D. Fairhurst et al.

Observers for Canonic Models

the equations of the original model can be transformed into the canonical adaptive observer form.
Because a coordinate transformation is required, different classes of models are likely to lead to
different observers. The second strategy is based on the ideas and approaches of universal adaptive
regulation [11], non-uniform convergence and non-uniform small-gain theorems [35], [36]. The
structure of observers obtained as a result of this design strategy does not change much from one
model to another. The main difference between these design strategies is in the convergence rates:
exponential for the first and asymptotical for the second. As long as mere overall convergence time
is accounted for there is no big difference whether the convergence itself is exponential or not. Yet,
the fact that it can be made exponential with known rates of convergence allows us to derive the
a-priori estimates of the amount of time needed to achieve a certain given accuracy of estimation.
We have shown that for linearly parameterized models such as the FitzHugh-Nagumo and
Hindmarsh-Rose oscillators one can develop an observer for state and parameter estimation of
which the convergence rate is exponential. For the nonlinearly parameterized and more realistic
models such as the Morris-Lecar and Hodgkin-Huxley equations we presented an observer of
which the convergence is asymptotic. In both cases the rate of convergence depends on the degree
of excitation in the measured data. In the case of linearly parameterized systems this excitation
can be measured by the minimal eigenvalue of a certain matrix constructed explicitly from the data
and the model. For the nonlinearly parameterized systems the degree of excitation is defined by a
more complex expression, (4.14). In principle, one can ensure arbitrarily fast convergence of the
estimator provided that the excitation is sufficiently high. This property motivates the development
of measurements protocols that are most consistent with a range of models that will be fitted to the
collected data. In fact, in order to achieve higher computational effectiveness, one shall aim to
produce data of which the excitation is higher for the given range of models.
One question remains unexplored though – the actual amount of elementary computational
operations required to realize these two observer schemes. This number depends substantially
on the required accuracy of estimation. We aim to answer this important question in future case
studies.

References
[1] H.D.I. Abarbanel, D.R. Crevling, R. Farsian and M. Kostuk. Dynamical State and Parameter
Estimation. SIAM J. Applied Dynamical Systems, 8(4):1341–1381, 2009.
[2] P. Achard and E. Schutter. Complex parameter landscape for a comples neuron model. PLOS
Computational Biology, 2(7):794–804, 2006.
[3] G. Bastin and M. Gevers. Stable adaptive observers for nonlinear time-varying systems. IEEE
Trans. on Automatic Control, 33(7):650–658, 1988.
[4] R. Borisyuk and Y. Kazanovich. Oscillations and waves in the models of interactive neural
populations. Biosystems, 86(1–3):53–62, 2006.

34

D. Fairhurst et al.

Observers for Canonic Models

[5] D. Brewer, M. Barenco, R. Callard, M. Hubank, and J. Stark. Fitting ordinary differential
equations to short time course data. Philosophical Transactions of The Royal Society A,
366(1865):519–544, 2008.
[6] C. Cao, A.M. Annaswamy, and A. Kojic. Parameter convergence in nonlinearly parametrized
systems. IEEE Trans. on Automatic Control, 48(3):397–411, 2003.
[7] R. FitzHugh. Impulses and physiological states in theoretical models of nerve membrane.
Biophysical Journal, 1:445–466, 1961.
[8] A.N. Gorban. Basic types of coarse-graining. In A.N. Gorban, N. Kazantzis, I.G. Kevrekidis,
H.C. Ottinger, and C. Theodoropoulos, editors, Model Reduction and Coarse–Graining Approaches for Multiscale Phenomena, pages 117–176. Springer, 2006.
[9] J.L. Hindmarsh and R.M. Rose. A model of neuronal bursting using three coupled first order
differential equations. Proc. R. Soc. Lond., B 221(1222):87–102, 1984.
[10] A.L. Hodgkin and A.F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. J. Physiol., 117:500–544, 1952.
[11] A. Ilchman. Universal adaptive stabilization of nonlinear systems. Dyn. and Contr., (7):199–
213, 1997.
[12] A. Isidori. Nonlinear control systems II. Springer–Verlag, second edition, 1999.
[13] E. M. Izhikevich. Dynamical Systems in Neuroscience: the Geometry of Excitability and
Bursting. MIT Press, 2007.
[14] E. M. Izhikevich and G. M. Edelman. Large-scale model of mammalian thalamocortical
systems. Proc. of Nat. Acad. Sci., 105:3593–3598, 2008.
[15] Y. Kazanovich and R. Borisyuk. An oscillatory neural model of multiple object tracking.
Neural Computation, 18(6):1413–1440, 2006.
[16] C. Koch. Biophysics of Computation. Information Processing in Signle Neurons. Oxford
University Press, 2002.
[17] G. Kreisselmeier. Adaptive obsevers with exponential rate of convergence. IEEE Trans.
Automatic Control, AC-22:2–8, 1977.
[18] W. Lin and C. Qian. Adaptive control of nonlinearly parameterized systems: The smooth
feedback case. IEEE Trans. Automatic Control, 47(8):1249–1266, 2002.
[19] L. Ljung. System Identification: Theory for the User. Prentice-Hall, 1999.
[20] L. Ljung. Perspectives in system identification. In Proceedings of the 17-th IFAC World
Congress on Automatic Control, pages 7172–7184. 2008.
35

D. Fairhurst et al.

Observers for Canonic Models

[21] A. Loria and E. Panteley. Uniform exponential stability of linear time-varying systems: revisited. Systems and Control Letters, 47(1):13–24, 2003.
[22] A.M. Lyapunov. The general problem of the stability of motion. Int. Journal of Control,
55(3), 1992.
[23] R. Marino. Adaptive observers for single output nonlinear systems. IEEE Trans. Automatic
Control, 35(9):1054–1058, 1990.
[24] R. Marino and P. Tomei. Global adaptive observers for nonlinear systems via filtered transformations. IEEE Trans. Automatic Control, 37(8):1239–1245, 1992.
[25] R. Marino and P. Tomei. Adaptive observers with arbitrary exponential rate of convergence
for nonlinear systems. IEEE Trans. Automatic Control, 40(7):1300–1304, 1995.
[26] J. Milnor. On the concept of attractor. Commun. Math. Phys., 99:177–195, 1985.
[27] A. P. Morgan and K. S. Narendra. On the stability of nonautonomous differential equations
ẋ = [A + B(t)]x with skew symmetric matrix B(t). SIAM J. Control and Optimization,
37(9):1343–1354, 1977.
[28] C. Morris and H. Lecar. Voltage oscillatins in the barnacle giant muscle fiber. Biophysics J.,
35:193–213, 1981.
[29] K. S. Narendra and A. M. Annaswamy. Stable Adaptive systems. Prentice–Hall, 1989.
[30] H. Nijmeijer and A. van der Schaft. Nonlinear Dynamical Control Systems. Springer–Verlag,
1990.
[31] A. Prinz, C.P. Billimoria, and E. Marder. Alternative to hand-tuning conductance-based models: Contruction and analysis of databases of model neurons. Journal of Neorophysiology,
90:3998–4015, 2003.
[32] I. Yu. Tyukin, D.V. Prokhorov, and C. van Leeuwen. Adaptive algorithms in finite form for
nonconvex parameterized systems with low-triangular structure. In Proceedings of the 8-th
IFAC Workshop on Adaptation and Learning in Control and Signal Processing (ALCOSP
2004), pages 261–266. 2004.
[33] I.Yu. Tyukin, D. V. Prokhorov, and C. van Leeuwen. Adaptation and parameter estimation in
systems with unstable target dynamics and nonlinear parametrization. IEEE Transactions on
Automatic Control, 52(9):1543 – 1559, 2007.
[34] I.Yu. Tyukin, D.V. Prokhorov, and V.A. Terekhov. Adaptive control with nonconvex parameterization. IEEE Trans. on Automatic Control, 48(4):554–567, 2003.
[35] I.Yu. Tyukin, E. Steur, H. Nijmeijer, and C. van Leeuwen. Non-uniform small-gain theorems for systems with unstable invariant sets. SIAM Journal on Control and Optimization,
47(2):849–882, 2008.
36

D. Fairhurst et al.

Observers for Canonic Models

[36] I.Yu. Tyukin, E. Steur, H. Nijmeijer, and C. van Leeuwen. Adaptive observers and parametric identification for systems in non-canonical adaptive observer form. Submitted, preprint
available at http://arxiv.org/abs/0903.2361, 2009.
[37] W. van Geit, E. de Shutter, and P. Achard. Automated neuron model optimization techniques:
a review. Biol. Cybern, 99:241–251, 2008.
[38] Kazantsev V.B., Nekorkin V.I., Makarenko V.I., and Llinas R. Self-referential phase reset
based on inferior olive oscillator dynamics. Proceedings of National Academy of Science,
101(52):18183–18188, 2004.

7. Appendix
Proof of Theorem 6
Proof. The proof is straightforward. Indeed, for the observability test we have


h(x)


 Lf h(x) 
x1


 L2f h(x)   x2 + x3 + ... + xn 

 


=

.
0

 





.
.




.
0
n−1
Lf h(x)

(7.1)

and






∂ 

∂x 




h(x)
Lf h(x)
L2f h(x)
.
.
.
n−1
Lf h(x)






 
 
=
 
 



1
0
0
.
0

0
1
0
.
0

0
1
0
.
0

0
1
0
.
0

0
1
0
.
0

...
...
...
...
...

0
1
0
.
0








(7.2)

It follows that observability is lost for n > 2. This proves condition (i). In order to demonstrate
condition (ii) we use theorem 5 as follows. From equation (2.21) we have
  
 

0
g1
1 0
(7.3)
=
,
1
g2
0 1
giving
g=



0
1
37



(7.4)

D. Fairhurst et al.

Observers for Canonic Models

Theorem 5 condition (i) will be satisfied since the system is linear and from theorem 5 part (ii) we
have
[q1 , ad0f g] = [q1 , g]
∂g
∂q1
=
q1 −
g
∂x  ∂x
∂q1 0
= −
1
∂x
∂q1
= −
∂x2

(7.5)
(7.6)
(7.7)
(7.8)

we satisfy theorem 5 part (ii) if and only if ∂q1 /∂x2 = 0 for all x ∈ U.
Proof of Corollary 8
Proof. The proof of the corollary is straightforward. Consider the error system given by (3.18).
Given that the function ϕ(t) is bounded one can easily see that η̂, x̂ are bounded as well (consider
e.g. the following Lyapunov candidate: V = kx̃∗ k2 + η̃Γ−1 η̃). This implies that component
ϕ4 (t)η̃4 (t) is converging to zero exponentially.
Let us denote
η̄ = (η̃1 , η̃2 , η̃3 , η̃5 , η̃6 , η̃7 , η̃8 )T
and consider the following reduced error dynamics


 T 
˙x̃∗ = −c1 k x̃∗ + ϕ̄ η̄ + pε(t)
0 f
0
˙η̄ = −Γϕ̄x̃∗1

(7.9)

in which ε(t) stands for the term ϕ4 (t)η̃4 (t), and p = (1, 0)T . System (7.9) is a linear time-varying
system of which the homogenous part is exponentially stable provided that ϕ̄(t) is persistently
exciting (this follows explicitly from Theorem 4). Hence, taking into account that ε(t) is an exponentially converging to zero term, we can conclude that x̃∗ , η̄ converge to the origin too and that
such convergence is exponential.
Proof of Theorem 10
Proof. The proof of the theorem is standard and can be constructed from many other more general
results (see for example [23], [29]). Here we present just a sketch of the argument for consistency.
According to our assumptions, matrix A1 + LC1 is Hurwitz. Moreover, the transfer function
H(s) = C1 (s − (A1 + LC1 ))−1 b =

s2

s+k
s+k
=
+ (k + 1)s + k
(s + k)(s + 1)

is strictly positive real. Hence, using the Kalman-Yakubovich-Popov lemma, we can conclude that
there exists a symmetric and positive definite matrix H such that
H(A1 + LC1 ) + (A1 + LC1 )T H < −Q, Hb = (1, 0)T ,
38

(7.10)

D. Fairhurst et al.

Observers for Canonic Models

where Q is a positive definite matrix. Let us now consider the following function
Z ∞
1
1
T
2 −1
V (z, υ̂, t) = (z − ẑ) H(z − ẑ) + kυ − υ̂k γ + D
ε2 (τ )dτ
2
2
t
where the value of D is to be specified later. Clearly, the function V is well-defined for the term
ε(t) is continuous and exponentially decaying to zero as t → ∞. Thus the boundedness of V
implies that kẑ − zk and kυ̂ − υk are bounded.
Consider the time-derivative of V :
V̇ = (z − ẑ)T (H(A1 + LC1 ) + (A1 + LC1 )T H)(z − ẑ) + (z − ẑ)T HbφT (z1 , t)(υ − υ̂)
+ (z − ẑ)T Hbε − (υ − υ̂)(1, 0)(z − ẑ)φ(z1 , t) − Dε2

Taking (7.10) into account we obtain:
V̇ ≤ −(z − ẑ)T Q(z − ẑ) + kz − ẑk|ε(t)|M − Dε2 ≤ −αkz − ẑk2 +
kz − ẑk|ε(t)|M − Dε2,

(7.11)

where α > 0 is the minimal eigenvalue of Q, M is fixed positive number, and D is a parameter of
V which can be chosen arbitrarily and independently of M. Choosing D such that
r
αD
= M,
4
we ensure that

α
V̇ ≤ − kz − ẑk2
2
Thus the function V is bounded from above, and given that the solution z(t) exists for all t > t0 so
does the solution of the combined system. The rest of the proof follows directly from Barbalatt’s
lemma and the asymptotic stability theorem for the class of skew-symmetric time-varying systems
presented in [27].

39

