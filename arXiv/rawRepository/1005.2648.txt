Analytic methods for modeling stochastic regulatory networks
Aleksandra M. Walczak,1, ∗ Andrew Mugler,2, † and Chris H. Wiggins3
1

arXiv:1005.2648v1 [q-bio.MN] 15 May 2010

Princeton Center for Theoretical Science, Princeton University, Princeton, NJ 08544
2
Department of Physics, Columbia University, New York, NY 10027
3
Department of Applied Physics and Applied Mathematics, Columbia University, New York, NY 10027
(Dated: May 18, 2010)
The past decade has seen a revived interest in the unavoidable or intrinsic noise in biochemical
and genetic networks arising from the finite copy number of the participating species. That is,
rather than modeling regulatory networks in terms of the deterministic dynamics of concentrations,
we model the dynamics of the probability of a given copy number of the reactants in single cells.
Most of the modeling activity of the last decade has centered on stochastic simulation of individual
realizations, i.e., Monte-Carlo methods for generating stochastic time series. Here we review the
mathematical description in terms of probability distributions, introducing the relevant derivations
and illustrating several cases for which analytic progress can be made either instead of or before
turning to numerical computation.

Contents

I. Introduction
II. Simple birth–death process
A. Deterministic description: the kinetic rate equation
B. Introducing noise: the master equation
1. Steady state solution
2. Generating function representation
3. Operator representation
C. Fokker-Planck approximation
1. Large protein number
2. Small noise
D. Langevin approximation
E. Comparison of the descriptions

2
3
3
3
4
5
7
9
10
10
11
13

III. Autoregulation
A. Deterministic model
B. The master equation
C. Bistability and noise

13
13
13
15

IV. Bursts
A. Transcriptional bursts
1. The master equation
2. The two-state gene
3. The spectral solution
4. Bursting and auto-regulation
B. Translational bursts

15
16
16
18
19
20
21

V. Two genes
A. Deterministic model
B. The spectral method
VI. Summary

∗ Electronic
† Electronic

address: awalczak@princeton.edu
address: ajm2121@columbia.edu

22
22
23
24

2
A. Deriving the master equation

25

B. Summation of the birth–death master equation

26

C. Statistics of the birth–death distribution

27

D. Asymptotic distributions for large protein number

28

E. Orthonormality and the inner product

29

F. Properties of the raising and lowering operators

30

G. Eigenvalues and eigenfunctions in the operator representation

31

H. Overlaps between protein number states and eigenstates

32

I. The equivalence of the Fokker–Planck and Langevin descriptions

33

J. Derivation of the Hill function

34

K. Limiting case of the two-state gene

35

References

36

I.

INTRODUCTION

Cellular processes rely on biochemical signaling, i.e. chemical interactions among individual molecules. Theoretical
descriptions of biochemical processes rely on considering the concentration changes of the molecules involved in the
reactions, and specifying the types of regulatory interactions between them. The theoretical tools used to describe
many types of biochemical reactions share many similarities. In this chapter we give an overview of the analytical
approaches to study biochemical kinetics using the example of small gene regulatory networks.
The regulation of genes by transcription factor proteins is an intrinsically stochastic process, owing to the small
numbers of copies of molecules involved. With the development of imaging techniques in molecular biology, we are
able to observe directly the fluctuations in the concentrations of proteins and mRNAs and, by measuring the intensity
profiles of fluorescence markers, measure full probability distributions [1–4]. Experiments over the last decade have
shown that in fact gene regulation is a noisy process, and noise can propagate in gene networks.
Many methods for solving the resulting stochastic equations rely on computer simulations. The efficiency of these
methods has been greatly advanced in the last several years [5–16] . However numerical simulations are naturally
limited to a specific choice of parameters, and changing the parameters requires a completely new calculation. Furthermore they suffer from the curse of dimensionality: the computational runtime grows prohibitively as the number
of species increases These problems can be bypassed by developing analytical approaches, which often require certain
approximations.
This chapter is intended as a tutorial on theoretical descriptions of biochemical kinetics. For clarity of exposition,
we start by introducing the simplified kinetic description of the production of proteins. At the level of molecular
kinetics, this is the simplest type of process that can occur. We first consider the deterministic description of the
system and then introduce noise. After familiarizing the reader with different levels of description, we discuss models
of regulation. We also present a wide spectrum of analytic tools used to find the steady state probability distributions
of protein copy numbers in the cell. We point out that while for concreteness we focus on the case of gene regulation,
the methods presented in this chapter are very general.
The results presented in this review are not new; some can be found in textbooks [17–19], while others have been
derived more recently in the context of gene regulation [4, 20–38]. The goal of this review is to give the reader who is
not familiar with analytical methods for modeling stochastic gene regulation an overview of the mathematical tools
used in the field. Naturally, we are not able to cover all the developments in the field, but we hope to give the
reader a useful starting point. For concreteness we also limit our discussion to the case of small gene networks and
do not discuss approximations used to describe larger networks, which is currently an active area of research in many
communities [10, 11, 15, 16, 28, 29].

3
II.

SIMPLE BIRTH–DEATH PROCESS

In this section we describe a simple birth death process for one species, absent of regulation. We first remind the
reader of the deterministic description given by chemical kinetics. Next we introduce the full probabilistic description
(given by the master equation), and, restricting our discussion to the simplest case, show how the deterministic
equations arise as the dynamics of the mean. After calculating the variance and, we introduce the generating function
formalism, useful in solving the master equation. This formalism may also be described using raising and lowering
operators familiar to physicists from quantum mechanics. Finally we relate this description to the Fokker-Plank
equation, the analogous master equation for continuous state variables (e.g., real-valued coordinates rather than the
integer-valued copy number). These results will be used in later sections when we introduce autoregulation and
regulation among different species.

A.

Deterministic description: the kinetic rate equation

In the simplest case, the number of copies n of a protein species X can change either due to the production of a
protein, which occurs at a constant rate g̃, or due to degradation of a protein, which occurs at a constant rate r:
g̃

*
∅−
)
− X.
r

(1)

Here we condense the complicated molecular machinery of transcription, translation, and protein modification into a
single constant production rate. Similarly we do not specify the molecular processes which have led to degradation;
for simplicity we imagine either dilution due to cell division or active degradation with a constant rate.
When n is large, the dynamics for the mean hni are well approximated by the continuous, deterministic description
provided by the kinetic rate equation for the concentration c ≡ hni/V in a volume V :
g̃
dc
=
− rc.
dt
V

(2)

The solution of Eqn. 2 is
c(t) =



g̃
g̃
+ e−rt c(0) −
,
Vr
Vr

(3)

where c(0) is the concentration of proteins at initial time t = 0. In steady state the mean number of proteins is simply
the ratio of the production and degradation rates, hni = cV = g̃/r, which is easily seen by taking either dc/dt = 0 in
Eqn. 2 or t → ∞ in Eqn. 3.

B.

Introducing noise: the master equation

The general probabilistic description of chemical reactions, respecting the finite copy number of the reactants, is the
master equation, which specifies the rate of change of pn , the probability that there are n = (n1 , n2 , . . . , nL ) copies of
the L reactants. The macroscopic, deterministic description in terms of chemical kinetics is recovered by considering
the dynamics for the mean concentration of these reactants, i.e. the vector hni/V = (hn1 i/V, hn2 i/V, . . . hnL i/V ) =
(c1 , c2 , . . . cL ). Such a description in terms of a summary statistic necessarily ignores a great amount of information,
including all information about fluctuations about these mean values.
The general form of the master equation (c.f. Appendix A), set by conservation of total probability, is
X
ṗn =
[wnn0 (t)pn0 − wnn0 (t)pn ] ,
(4)
n0

For the case of only one species, we need specify only the dynamics of n = n. Restricting further to the case of a
simple birth-death process, there are only two nonzero contributions from the transition matrices wnn0 , given by
wn+1,n (t) = g̃,
wn−1,n (t) = rn,

(5)
(6)

4
all other values of wnn0 being 0. Under this restriction Eqn. 4 reduces to the familiar
ṗn = −g̃pn − rnpn + g̃pn−1 + r(n + 1)pn+1 .

(7)

Qualitatively, the four terms on the right hand side represent how the probability of having n proteins can either
(i) decrease in time, if there are n initially and one is either produced (first term) or degraded (second term), or
(ii) increase in time, if there are either n − 1 initially and one is produced (third term) or n + 1 initially and one is
degraded (fourth term).
P∞
The dynamics of the mean number of proteins hni = n=0 npn are readily obtained (see Appendix B), as
dhni
= g̃ − rhni.
dt

(8)

Eqn. 8 shows that the dynamics of the mean of the protein distribution reproduces the kinetic rate equation, Eqn. 2.
From here on, we will re-scale time t by the degradation rate r, such that rt → t, and define g = g̃/r, making the
master equation
ṗn = −gpn − npn + gpn−1 + (n + 1)pn+1 .
1.

(9)

Steady state solution

The master equation can be rewritten in terms of shift operators E + and E − which increase and decrease the
number of proteins by one, respectively [17], i.e.
E + fn = fn+1 ,
E − fn = fn−1 ,

(10)
(11)

for any function fn . We begin by writing Eqn. 9 in terms of only E + to make clear that, as is the case for any
one-dimensional master equation, its steady state can be found iteratively. In terms of E + ,
ṗn = (E + − 1)(npn − gpn−1 ).

(12)

Setting ṗn = 0 for steady state requires that the second term in parentheses vanish, giving for n > 0 the recursive
relation
g
(13)
pn = pn−1 .
n
Starting with n = 1 and computing the first few terms reveals the pattern
pn =

gn
p0
n!

(14)

∞
X
gn
= p0 eg .
n!
n=0

(15)

where p0 = e−g is set by normalization:
1=

∞
X

pn = p0

n=0

Thus the steady-state probability of having n proteins is the Poisson distribution,
pn =

g n −g
e ,
n!

(16)

with parameter g, the ratio of production to degradation rates.
We remind the reader that the variance σ 2 = hn2 i − hni2 of the Poisson distribution is equal to its mean hni
(Appendix C). Therefore the standard deviation σ over the mean falls off like
p
hni
σ
1
=
=p ,
(17)
hni
hni
hni

5
demonstrating that the relative effect of fluctuations diminishes for large protein number. In Appendix D we show
that in the limit of large protein number the steady state asymptotes to a Gaussian distribution with mean and
variance equal to g.
For comparison with the other representations of the master equation described below, we now write Eqn. 9 in
terms of both shift operators E + and E − :
ṗn = (E + − 1)(n − gE − )pn .

(18)

Eqn. 12 can be rewritten slightly by inserting between the parenthetic terms the unit operator 1 = E − E + and
distributing the E − to the left and the E + to the right, giving
ṗn = −(E − − 1)[(n + 1)E + − g]pn .

(19)

where the negative sign has been factored out of the first parenthetic term.

2.

Generating function representation

Not all master equations are solvable by straightforward iteration. A more generalizable way to solve a master
equation is by the introduction of a generating function [17]. Here we demonstrate the generating function approach
on the birth–death master equation, Eqn. 9.
The generating function G is defined as
G(x, t) ≡

∞
X

pn (t)xn ,

(20)

n=0

a power-series expansion in a continuous variable x whose coefficients are the probabilities pn . Since x is a variable
we introduce, we note that defining x = eiκ makes clear that the generating function is equivalent to the Fourier
transform of pn in protein number (this point is further developed in Appendix E). The probability distribution may
be recovered via the inverse transform
pn (t) =

1 n
∂ G(x, t)|x=0 .
n! x

(21)

Additionally we note that the `th moment may be generated by
hn` i = ∂xn G|x=1 ,

(22)

(which is the reason for the generating function’s name).
The utility of the generating function for solving a master equation is that it turns an infinite set of ordinary
differential equations (e.g. Eqn. 9) into a single partial differential equation. To see this here we multiply Eqn. 9 by
xn and sum over n to obtain (see Appendix B for the detailed derivation):
Ġ = −(x − 1)(∂x − g)G.

(23)

We see immediately from comparison of Eqns. 19 and 23 that the representations of operators E − and (n + 1)E + in
x space are x and ∂x , respectively.
In steady state (Ġ = 0) Eqn. 23 must satisfy (∂x − g)G = 0, which is solved by
G(x) = G(0)egx ,

(24)

where G(0) = e−g is set by normalization:
G(0)eg = G(1) =

∞
X

pn (1)n =

n=0

∞
X

pn = 1.

(25)

n=0

The steady state distribution is recovered via inverse transform:
pn =

1 n h g(x−1) i
gn
∂x e
= e−g ,
n!
n!
x=0

(26)

6
a Poisson distribution with parameter g, as before (Eqn. 16).
The time-dependent solution to Eqn. 23 may be obtained using the method of characteristics, in which one looks
for characteristic curves along which the partial differential equation becomes an ordinary differential equation. The
curves are parameterized by s, such that G(x, t) = G(x(s), t(s)) = G(s), and we look for the solution to the ordinary
differential equation for G(s), having eliminated x and t from the problem. First Eqn. 23 can be rewritten more
explicitly as
(x − 1)gG =

∂G ∂G
+
(x − 1).
∂t
∂x

(27)

Using the chain rule on G(s) gives
dG
∂G dt
∂G dx
=
+
.
ds
∂t ds
∂x ds

(28)

Consistency of Eqns. 27 and 28 requires
dt
= 1,
ds
dx
= x − 1,
ds
dG
= (x − 1)gG.
ds

(29)
(30)
(31)

Eqn. 29 implies s = t (we set t0 = 0 without loss of generality). Therefore, defining y ≡ x − 1, Eqn. 30 implies
y = y0 et .

(32)

Finally, straightforward integration of Eqn. 31 yields


Z t




0
G = G0 exp gy0
dt0 et = G0 exp gy0 (et − 1) = F (y0 ) exp gy0 et .

(33)

0

where we define F (y0 ) ≡ G0 e−gy0 . We may expand F (y0 ) as a function of its argument, i.e. F (y0 ) =
some coefficients Aj , such that
X


G=
Aj y0j exp gy0 et .

P∞

j=0

Aj y0j for
(34)

j

Inserting y0 = ye−t = (x − 1)e−t (Eqn. 32), we obtain
X
G(x, t) =
Aj e−jt (x − 1)j eg(x−1) .

(35)

j

For t → ∞ only the j = 0 term survives and we recover the steady state generating function (Eqn. 24),
G(x) = c0 eg(x−1)

(36)

where c0 = 1 by normalization. Eqn. 35 constitutes the full time-dependent solution to the birth death process;
the time-dependent distribution can be retrieved using the inverse transform, Eqn. 21, and the coefficients Aj are
computable from the initial distribution pn (0), which will be made explicit in the next section.
Since Eqn. 23 is linear in G, one may arrive at its solution in a second, elegant way by expanding in the eigenfunctions
of the x-dependent operator. That is, writing Eqn. 23 as Ġ = −LG, where
L = (x − 1)(∂x − g),
we expand G in eigenfunctions φj (x) with time-dependent expansion coefficients Gj (t),
X
G(x, t) =
Gj (t)φj (x),
j

(37)

(38)

7
where the φj (x) satisfy
Lφj = λj φj
(39)
P
P
for eigenvalues λj . Substituting Eqn. 38 into Eqn. 23 gives j Ġj φj = − j λj Gj φj , which, by orthogonality of the
φj , yields the set of ordinary differential equations Ġj = −λj Gj , solved by
Gj (t) = Aj e−λj t

(40)

for some constants Aj . Comparing the result,
G(x, t) =

X

Aj e−λj t φj (x),

(41)

j

with Eqn. 35 reveals the forms of the eigenvalues
λj = j ∈ {0, 1, 2, . . . }

(42)

φj (x) = (x − 1)j eg(x−1) ,

(43)

and the eigenfunctions

facts that we will confirm in the next section using operator methods.
3.

Operator representation

We now introduce an a representation of the master equation in terms of raising and lowering operators. This
representation makes the solution more elegant, yielding a simple algebra which allows calculation of projections
between the spaces of protein numbers and eigenfunctions without explicit computation of overlap integrals; it also
lays the formal groundwork for solving models of multidimensional regulatory networks. The formalism was first used
for diffusion by Doi [39] and Zel’dovich [40] and later developed by Peliti [41].
As before the generating function is defined as an expansion in a complete basis indexed by protein number n in
which the expansion coefficients are the probabilities pn . Within the operator formalism, this basis is represented as
the set of states |ni, i.e.
|G(t)i =

∞
X

pn (t)|ni.

(44)

n=0

Here we have adopted state notation commonly used in quantum mechanics [42]; the previous representation (Eqn.
20) is recovered by projecting onto this equation the conjugate states hx| with the provisions
hx|G(t)i ≡ G(x, t),
hx|ni ≡ xn .

(45)
(46)

In Appendix E we show how the orthonormality of the |ni states,
hn|n0 i = δnn0 ,

(47)

1
.
xn+1

(48)

dictates the form of the conjugate state:
hn|xi =

Projecting hn| onto Eqn. 45 and using Eqn. 47 gives the inverse transform:
hn|G(t)i = pn (t).

(49)

The equation of motion in the operator representation is obtained by summing the master equation (Eqn. 9) over
n against |ni (see Appendix B), giving
|Ġi = −L̂|Gi,

(50)

8
where
L̂ ≡ (â+ − 1)(â− − g).

(51)

Just as in the operator treatment of the quantum harmonic oscillator [42], the operators â+ and â− here raise and
lower the protein number by 1 respectively, i.e.
â+ |ni = |n + 1i,
â− |ni = n|n − 1i

(52)
(53)

(note, √
however, that
√ the prefactors here, 1 and n, are different than those conventionally used for the harmonic oscillator, n + 1 and n, respectively). Comparison of Eqns. 19, 23, and 51 makes clear the following correspondences
among the master equation, generating function, and operator representations respectively:
E − ↔ x ↔ â+ ,
−

+

(n + 1)E ↔ ∂x ↔ â .

(54)
(55)

While it might seem strange that the down-shift operator E − corresponds to the raising operator â+ (and the up-shift
operator to the lowering operator), in Appendix F we show that, just as with the quantum harmonic oscillator, â+
and â− lower and raise protein number respectively when operating to the left, i.e.
hn|â+ = hn − 1|,
hn|â− = (n + 1)hn + 1|,

(56)
(57)

which makes the correspondence more directly apparent. Finally, again as with the quantum harmonic oscillator, the
raising and lowering operators enjoy the commutation relation (see Appendix F)
 − +
â , â = 1,
(58)
and â+ â− acts as a number operator, i.e. â+ â− |ni = n|ni.
Eqn. 51 shows that the full operator L̂ factorizes, suggesting the definition of the shifted operators
b̂+ ≡ â+ − 1,
b̂

−

−

≡ â − g.

(59)
(60)

Since b̂+ and b̂− differ from â+ and â− , respectively, by scalars, they obey the same commutation relation, i.e.
h
i
b̂− , b̂+ = 1.
(61)
Since the equation of motion (Eqn. 50) is linear, it will benefit from expansion of |Gi in the eigenfunctions |λj i of
the operator L̂, where
L̂|λj i = λj |λj i

(62)

for eigenvalues λj . In Appendix G we show that the commutation relation (Eqn. 61) and the steady state solution
hx|Gi = eg(x−1) , for which L̂|Gi = 0, completely define the eigenfunctions and eigenvalues of L̂; we summarize the
results of Appendix G here. The eigenvalues are shown to be nonnegative integers j,
λj = j ∈ {0, 1, 2, . . . },

(63)

as in Eqn. 42. The eigenvalue equation now reads
L̂|ji = b̂+ b̂− |ji = j|ji,

(64)

which is consistent with interpretation of b̂+ b̂− as a number operator for the eigenstates |ji. The eigenfunctions are
shown to be (in x space)
hx|ji = (x − 1)j eg(x−1) ,

(65)

9
as in Eqn. 43 with φj (x) ≡ hx|ji. The conjugate eigenfunctions are shown to be
hj|xi =

e−g(x−1)
.
(x − 1)j+1

(66)

The operators b̂+ and b̂− are shown to raise and lower the eigenstates |ji, respectively, as â+ and â− do the |ni states,
i.e.
b̂+ |ji = |j + 1i,

(67)

−

b̂ |ji = j|j − 1i,

(68)

+

= hj − 1|,

(69)

−

= (j + 1)hj + 1|.

(70)

hj|b̂
hj|b̂

Now employing the expansion of |Gi in the eigenstates |ji,
X
|G(t)i =
Gj (t)|ji,

(71)

j

the equation of motion (Eqn. 50) gives a trivial equation for the expansion coefficients
Ġj = −jGj ,

(72)

Gj (t) = Aj e−jt

(73)

which is solved by

for some constants Aj , as in Eqn. 40. The inverse of Eqn. 71 (obtained by projecting hj| and using hj|j 0 i = δjj 0 ) is
hj|G(t)i = Gj (t).

(74)

The probability distribution is retrieved by inverse transform (Eqn. 49),
X
X
pn (t) = hn|G(t)i = hn|
Gj (t)|ji =
Aj e−jt hn|ji,
j

(75)

j

where the coefficients Aj are computed from the initial distribution pn (0):
X
Aj = Gj (0) = hj|G(0)i =
pn (0)hj|ni.

(76)

n

Eqns. 75 and 76 give the full time-dependent solution to the birth–death process as expansions in the eigenmodes
hn|ji and conjugate eigenmodes hj|ni. Because we have decomposed the problem using the eigenbasis, or spectrum,
of the underlying operator, we will refer to this as the spectral solution.
The quantities hn|ji and hj|ni are overlaps between the protein number basis |ni and the eigenbasis |ji. Both hn|ji
and hj|ni are readily computed by contour integration or more efficiently by recursive updating; these techniques are
presented in Appendix H. Notable special cases are
hn|0i = e−g

gn
,
n!

h0|ni = 1,

(77)
(78)

which confirm that Eqn. 75 describes a Poisson distribution in steady state (j = 0).

C.

Fokker-Planck approximation

The previous sections discuss several methods for calculating the steady state and time dependent solutions of the
master equation for the birth–death process. For many larger systems with regulation, full solution of the master
equation is not possible. In some of these cases one can make progress by deriving an approximate equation which is

10
valid when protein numbers are large. In the limit of large protein numbers the master equation can be expanded to
second order to yield the Fokker–Planck equation. In this section we derive the Fokker–Planck equation for a general
one-dimensional master equation with arbitrary production and degradation rates; the method is easily generalizable
to models with regulation.
For arbitrary production rate gn and degradation rate rn , the master equation reads
∂t pn = − (gn + rn ) pn + gn−1 pn−1 + rn+1 pn+1 ;

(79)

setting gn = g and rn = n recovers the simple birth–death process with time rescaled by the degradation rate, as in
Eqn. 9.
1.

Large protein number

The Fokker–Plank equation is derived under the assumption that the typical protein number is large (n  1), such
that n can be approximated as a continuous variable, and a change of 1 protein can be treated as a small change.
We will use parentheses when treating n as continuous and a subscript when treating n as discrete. Under this
approximation, the function f (n ± 1), where f (n) ∈ {g(n)p(n), r(n)p(n)}, can be expanded to second order as
1
f (n ± 1) = f (n) ± ∂n f (n) + ∂n2 f (n).
2

(80)

Inserting the results of the expansion into Eqn. 79, we obtain
1
∂t p(n) = −∂n [v(n)p(n)] + ∂n2 [D(n)p(n)] ,
2

(81)

where v(n) ≡ g(n) − r(n), an effective drift velocity, recovers the right-hand side of the deterministic equation, and
D(n) ≡ g(n) + r(n), an effective diffusion constant, sets the scale of the fluctuations in protein number. The drift
term plays the role of an effective force. The diffusion term plays the role of an effective temperature: the larger it
is, the larger the excursions a single trajectory (of particle number versus time) takes from the mean. Eqn. 81 is the
Fokker–Planck equation.
The steady state solution of Eqn. 81 is easily obtained by noticing that the Fokker–Planck equation is a continuity
equation of the form
∂t p = −∂n j,

(82)

1
2 ∂n

where j(n) ≡ v(n)p(n) −
[D(n)p(n)] is the current of the probability. In steady state (∂t p = 0) the current is
constant, and since p(n) → 0 and ∂n p(n) → 0 as n → ∞ (typically more quickly than v(n) or D(n) diverges), the
current vanishes for all n. The steady state distribution is then found by direct integration, i.e.
 Z n

0
N
0 v(n )
p(n) =
exp 2
dn
,
(83)
D(n)
D(n0 )
0
R∞
where N is a normalization constant ensuring 0 dn p(n) = 1.
In the simple birth–death process, for which v(n) = g − n and D(n) = g + n, Eqn. 83 evaluates to

4g−1
n
N
1+
e−2n .
(84)
p(n) =
g
g
In Appendix D, we show that the exact steady state (the Poisson distribution, Eqn. 16) and Eqn. 84 have the same
asymptotic limit for large protein number: a Gaussian distribution with mean and variance equal to g, the ratio of
production to degradation rates.
2.

Small noise

In addition to the approximation that the typical protein number is large, one may make the further approximation
that the noise is small. This is often referred to as the “linear noise approximation” [17, 37, 38] or “small noise
approximation” [32, 33]. Specifically, one assumes that the fluctuations η in n are small around the mean n̄, i.e.
n = n̄ + η,

(85)

11
P
where n̄ ≡ hni = n npn (for brevity we use bar notation in this and several subsequent sections). Since dn/dη = 1
we have p(n) = p(η), and the master equation (Eqn. 81) becomes an equation in η:
1
∂t p(η) = −∂η [v(n̄ + η)p(η)] + ∂η2 [D(n̄ + η)p(η)] ,
2

(86)

We use the approximation that η is small to expand the drift and diffusion terms to first nonzero order in η:
v(n̄ + η) = v(n̄) + ηv 0 (n̄) + · · · ≈ ηv 0 (n̄)
D(n̄ + η) = D(n̄) + ηD0 (n̄) + · · · ≈ D(n̄)

(87)
(88)

where prime denotes differentiation with respect to n, and the last step in Eqn. 87 recalls the fact that v(n̄) =
g(n̄) − r(n̄) = 0 in steady state, as given by the kinetic rate equation (e.g. Eqn. 8). Eqn. 86 is now
1
∂t p(η) = −v 0 (n̄)∂η [ηp(η)] + D(n̄)∂η2 [p(η)] .
2
As with Eqn. 83, the steady state of Eqn. 89 is found by direct integration:

 0
Z
2
2
1
N
2v (n̄) η 0 0
dη η = √
e−η /2σ ,
p(η) =
exp
2
D(n̄)
D(n̄) 0
2πσ

(89)

(90)

where
σ2 ≡

D(n̄)
−2v 0 (n̄)

(91)

(note that v 0 (n̄) is negative for stable fixed points). Eqn. 90 is a Gaussian distribution, meaning that under the linear
noise approximation, the steady state probability distribution is a Gaussian centered at the exact mean n̄ and with
width determined by mean birth and death rates according to Eqn. 91. We note that Eqn. 90 can be equivalently
derived by expanding the integrand in Eqn. 83 about its maximum (or “saddle point”) n̄.
The linear noise approximation is stricter than the large-protein number approximation made in the previous section
(Sec. II C 1). While the previous approximation makes no assumption about the form of the probability distribution,
the linear noise approximation assumes that the distribution is unimodal and sharply peaked around its mean. In
practice, n̄ and σ 2 are obtained by finding the steady state(s) (i.e. the stable fixed point(s)) of the corresponding
deterministic rate equation. However, it is easily possible (for processes more complicated than the simple birth–death
process) for the deterministic equation to have more than one stable fixed point (see Fig. 2). Although one may make
a Gaussian expansion around each fixed point in turn, the linear noise approximation does not describe how these
Gaussians might be weighted in a multi-modal distribution. In these cases it is most accurate to use (if solvable)
either the large-protein number approximation (Eqn. 83) or the original master equation (Eqn. 79).
In the simple birth–death process, for which v(n) = g − n, D(n) = g + n, and n̄ = g, Eqn. 91 gives σ 2 =
2g/[−2(−1)] = g, and therefore Eqn. 90 reproduces the asymptotic behavior derived in Appendix D.

D.

Langevin approximation

We now consider a second stochastic approximation: the Langevin equation. The advantage of the Langevin
approach is that a large amount can be learned about the process without finding the full distribution, but instead
by considering the correlation functions of the concentration, which are readily computed.
Starting from the Fokker–
P∞
Planck equation we can calculate the equation for the mean of the distribution hni = n=0 npn . We arrive at the
kinetic rate equation given in Eqn. 2.
We can think about the change in concentration in time as a trajectory in n space. Each realization of a birth-death
process, will be described by a certain n(t), and averaging over many experiments, we obtain hn(t)i, as given by Eqn.
2. If we consider the change of concentrations on time scales longer than the characteristic timescales of the particular
reactions, we can assume that there are many birth and death processes in each interval and that the fluctuations of
each realization of n(t) around the mean are Gaussian distributed:
P [η] ∼ e−

R

dn

R

dt

η 2 (n,t)
4D(n)

(92)

12
We can include fluctuations around the the mean values by considering an additional noise term η(t), such that the
equation for the change of the concentration of proteins n becomes:
dn
= v(n) + η(t) = g − n + η(t).
dt

(93)

hη(t)i = 0,
hη(t0 )η(t)i = δ(t − t0 )D(n) = δ(t − t0 )(g + n)

(94)
(95)

We require of the noise that:

where D(n) is in general the diffusion term in the Fokker–Planck equation. In Appendix I we show the equivalence
of the Fokker–Planck and Langevin descriptions.
In the general case when there are many types of proteins in the system, we can define a time dependent correlation
function
Cij (t) = hδNi (0)δNj (t)i,

(96)

where the average implies a time average and δNi (t) = Ni (t) − hδNi (t)i are the variances of each type of protein. In
one dimension δn2 is the variance σ 2 (in subsequent sections we use δn2 and σ 2 interchangeably). We note that in
steady state the time average can be replaced by an ensemble average. For the case of the single species birth–death
process we have already computed the means and variances in Sec. II B 1. In case of the birth–death process, we
calculate the time dependent correlation function to obey the equation:
dC(t)
= −C(t).
dt

(97)

In steady state the solution must reduce to the previously calculated variance (Appendix C): δn2
h(n − hni)((n − hni))i = hn2 i − hni2 = g. Therefore the solution of Eqn. 97 for a simple birth–death process is
C(t) = ge−t .
We can also consider the correlation functions in Fourier space
Z ∞
Cij (ω) =
dte−iωt Cij (t).

=
(98)

(99)

0

Often it is easier to calculate the Fourier transform of the correlation function directly from the Fourier transform of
the Langevin equation, and then invert the transform back. In case of the simple birth–death process we can vary the
Langevin equation around its mean values n(t) = hni + δn(t) (that is simply linearize the equation around its mean)
and consider the resulting equation in Fourier space to obtain
−iωδñ(ω) = −δñ(ω) + η̃(ω),
where δñ(ω) =

R∞
0

(100)

dte−iωt n(t) and
Z
η̃(ω) =

∞

dte−iωt η(t)

(101)

0

hη̃(ω)η̃(ω 0 )i = 2πδ(ω − ω 0 )(g + hni).

(102)

The correlation function is then:
hδñ∗ (ω)δñ(ω 0 )i = 2πδ(ω − ω 0 )

2g
g + hni
= 2π 2
.
2
ω +1
ω +1

Inverting the Fourier transform reproduces the real time correlation function.
Z ∞
2g
2g
C(t) =
dωeiωt 2
= 2πie−t
= ge−t ,
ω
+
1
2i
0

(103)

(104)

where we reproduce the result of Eqn.99. The real part of the auto correlation function in Fourier space is also called
the power spectrum
Z ∞
N (ω) = hδñ∗ (ω)δñ(ω 0 )i =
dte−iωt Cii (t),
(105)
0

because it tells us which frequency modes contribute most to the form of the noise. We note that the power spectrum
N (ω) may be written as integral of the correlation function in real time.

13
E.

Comparison of the descriptions

For the birth–death process, the steady state of the kinetic rate equation agrees with the mean of the steady state
probability distribution (under both the exact and approximate stochastic descriptions): hni = g. In the limit of large
protein number, the Poisson distribution is well approximated by a Gaussian, and the Fokker–Planck (and therefore
the Langevin) approximation is a good description. To investigate the validity of the Fokker–Planck approximation
at small and intermediate protein number, in Fig. 1 we compare for a range of protein numbers the probability
distribution obtained directly from the master equation (i.e. the Poisson distribution, Eqn. 16) and from the Fokker–
Planck approximation (Eqn. 83). We quantify the disagreement between two distributions using the Kullback–Leibler
divergence,
X
pn
DKL =
pn log ,
(106)
p̃n
n
where pn corresponds to the master equation and p̃n corresponds to the Fokker-Planck approximation. The Kullback–
Leibler divergence is not symmetric and is therefore appropriate for a comparison between a “true” distribution (pn
here) and its approximation (p̃n here). As the mean protein number hni = g increases, the accuracy of the approximation increases, and the divergence DKL decreases. We plot explicitly three sample pairs of exact and approximate
distributions, at small, intermediate, and large protein numbers. As expected the Fokker-Planck distribution deviates
from the Poisson distribution at small protein number, and agrees well at large protein number.
III.

AUTOREGULATION

We now begin to turn our attention from the simple one-dimensional birth–death process to more realistic models
gene regulation (see [43, 44] for a discussion of regulation functions of gene expression). We start with the description
of auto-regulation of one gene, in which its protein production rate is an arbitrary function of its own protein number.
A.

Deterministic model

As before the mean dynamics are captured by the kinetic rate equation. The kinetic rate equation for a birth–death
process in which the production rate is an arbitrary function g(n̄) of mean protein number n̄ is
dn̄
= g(n̄) − n̄.
(107)
dt
The problem becomes potentially much harder, since the auto-regulation function g(n̄) can be nonlinear.
The form of the auto-regulation function depends specifically on the molecular model of regulation which is being
considered. For example, the Hill function,
g(n) =

g− K h + g+ nh
,
K h + nh

(108)

can be derived by considering a gene with two production rates g+ and g− (corresponding to the states in which a
protein is bound and unbound to the DNA, respectively) for which the rate of switching to the bound state depends
on the protein number (see Sec. IV and Appendix J). The derivation assumes that the rates of binding and unbinding
are faster than the protein degradation rate (which sets the time-scale for changes in protein number), such that
equilibrium is reached, with equilibrium binding constant K.
The parameter h describes the cooperativity of protein binding. When g+ > g− , the case h > 0 corresponds to
activation and the case h < 0 corresponds to repression; the special case h = 0 reproduces the simple birth–death
process. For |h| ≥ 2, Eqn. 107 has two stable fixed points for certain parameter regimes; for |h| ≥ 2 fixed points must
be found numerically.
Although the Hill equation is often used in models of gene regulation, we note that other functional forms are
derivable from other biochemical processes. Many of the following results are valid for arbitrary g(n).
B.

The master equation

The full stochastic description corresponding to the deterministic Eqn. 107 is given by the birth–death master
equation in which the production rate is described by the arbitrary auto-regulation function gn (recall that we replace

14

0.7
0.6

DKL [bits]

0.4

*

0.5
0

FP
master

0.4
0.2

0

5
n

0

10

0

5
n

0.08
pn

0.2

10

g=20

0.1

0.3

x

0.06
0.04
0.02

0.1
0
0

o

0.6
pn

1

g=0.9

0.8

pn

0.5

g=0.1

1.5

0

5

10

<n>=g

0

20

15

n

40

20

60

25

FIG. 1: Comparison of the distributions obtained from the master equation (Eqn. 16) and from the Fokker–Planck approximation (Eqn. 83), for the simple birth–death process. The Kullback–Leibler divergence DKL (Eqn. 106) between the two
distributions is plotted as a function of mean protein number hni = g. Insets show distributions obtained from the master
equation (solid) and from the Fokker–Planck equation (dashed) for g = 0.1 (star), g = 0.9 (circle), and g = 20 (cross), where
the symbols correspond to points on the DKL curve.

parentheses with subscript when treating n as discrete):
ṗn = −gn pn − npn + gn−1 pn−1 + (n + 1)pn+1 ,

(109)

We may easily generalize the solution in Sec. II B 1 to find the steady state probability distribution,
pn =

n−1
p0 Y
gn0 ,
n! 0

(110)

n =0

with p0 set by normalization. Except for special cases of the regulation function we cannot find a closed form solution
for the distribution, but the product is easily evaluated.
The results in Sec. II for the simple birth–death process can be generalized in the case of auto-regulation. We will
pay particular attention to the generalization of the eigenfunction expansion in operator notation for arbitrary gn ,
when discussing two genes in Sec. V.

15
C.

Bistability and noise

Auto-regulation can affect the statistics of the steady state distribution. Even before specifying the form of g(n),
we may obtain a general statistical result in the limit of large protein number by using the linear noise approximation
(Sec. II C 2). Eqn. 91 describes the variance σ 2 of fluctuations around the steady state mean n̄. Here we compute the
ratio σ 2 /n̄ for auto-regulation in order to compare with the Poisson distribution, for which σ 2 /n̄ = 1.
For auto-regulation the diffusion term becomes D(n̄) = g(n̄)+ n̄ = 2n̄, where we have used the fact that 0 = g(n̄)− n̄
at steady state. The derivative of the drift term evaluated at the mean is v 0 (n̄) = g 0 (n̄) − 1, where the prime denotes
differentiation with respect to n. Thus Eqn. 91 becomes
σ2
1
=
.
n̄
1 − g 0 (n̄)

(111)

This expression shows that self-activation (g 0 (n̄) > 0) leads to super-Poissonian noise (σ 2 /n̄ > 1), and self-repression
(g 0 (n̄) < 0) leads to sub-Poissonian noise (σ 2 /n̄ < 1); when there is no regulation we have g 0 (n̄) = 0, and we recover
the Poisson result, σ 2 /n̄ = 1. Fig. 3 demonstrates this behavior by computing the exact distribution (Eqn. 110) for
Hill function auto-regulation (Eqn. 108) with various values of the cooperativity parameter h.
Eqn. 111 diverges when g 0 (n̄) = 1. This corresponds to a bifurcation point for the self-activating gene, where there
are two possible solutions to the steady state equation. In this case the ratio of the variance to the mean is not very
informative about the distribution because the distribution is bimodal. The self-repressing gene, in contrast, cannot
be bistable, and the ratio of the variance to the mean is always a good description of noise. In Fig. 2 we demonstrate
that the exact distribution becomes bimodal when the deterministic equation crosses a bifurcation point.
Eqn. 111 can be be equivalently obtained by computing the power spectrum from the linearized Langevin equation.
Linearizing the Langevin equation and going to Fourier space we obtain
−iωδñ(ω) = g 0 (n̄)ñ(ω) − ñ(ω) + η̃(ω).

(112)

The power spectrum becomes
N =

2n̄
ω 2 + [1 − g 0 (n̄)]2

(113)

The steady state auto–correlation function is therefore
n̄
,
|1 − g 0 (n̄)|

(114)

δn2
1
=
.
n̄
|1 − g 0 (n̄)|

(115)

δn2 =
and the ratio of the variance to the mean is

We point the reader to the paper of Warren, Tanase-Nicola and ten Wolde [35] for a pedagogical discussion of power
spectra in gene expression.

IV.

BURSTS

The previous section assumes that the processes of transcriptional and translational regulation can be described
by one deterministic regulation function. Recent experiments have observed a feature of gene regulation that is not
captured by such a model: protein production often occurs in bursts [4, 45, 46]. Bursts can result from the gene
transitioning among two or more states with different rates of protein production, for example when a transcription
factor is bound or unbound, or, in higher eukaryotes, when different production states are introduced by chromatin
remodeling [45]. Bursts can also arise during translation, in which a single mRNA transcript produces many copies of
the protein in a short amount of time. Here we present simple models of the first type, which we call transcriptional
bursts, and the second type, which we call translational bursts. We then discuss a model which combines bursting
with auto-regulation. We refer the reader to the literature for discussion of other burst models including those which
capture both transcriptional and translational bursts [25, 47].

16

10

steady state

8

<n1>
<n2>

6
4
2

pn

pn

poisson

0
0

1

0.4

0.4

0.2

0.2

0.2

0.1

0

0

10

20

g-

2

0

0

10

20

0

0

10
n

3

20

4

5

0.2

0.2

0.1

0.1

0

0

10

20

0

0

10

20

FIG. 2: The steady state solution(s) for a self-activating gene as a function of the basal rate g− . The figure shows regimes
which are mono-stable (high and low g− ) and bistable (intermediate g− ). The probability distributions below (solid black lines)
correspond consecutively to the points above marked by stars. In the mono-stable regime the distributions are compared to
Poisson distributions (solid gray lines). The auto-regulation function is a Hill function (Eqn. 108) with h = 4, g+ = 10, and
K = 6.

A.

Transcriptional bursts

In this section we consider a gene that can exist in multiple states, each with its own protein production rate.
We first specialize to the case of two states, which is commonly used as a model for bursting [4, 34]. We solve this
case conventionally using a generating function. The conventional solution is limited to two states; for an arbitrary
number of states we present the spectral solution using operator methods. Finally we present a solution to a simple
model with both bursting and auto-regulation.

1.

The master equation

We begin by considering the general process in which a gene has Z different states. In each state z the protein
production is described by a simple birth-death process (Sec. II) with production rate gz . Transitions between the

17

1.8
1.6
1.4

δ n2/<n>

1.2

0.2

0.1

pn

0.15

0.08

poisson

0.06

0.1
0.05
0

0

20

0.04

*

x

0.02

40

0

60

0

20

40

60

1
0.8

0.2
0.15
pn

0.6
0.4

0.05

0.2
0
−5

0.1

0

0

0
cooperativity coefficient, h

20

o

n

40

60

5

FIG. 3: The ratio of the variance δn2 to the mean hni as a function of the cooperativity coefficient h, for a gene undergoing
Hill function (Eqn. 108) autoregulation. For h < 0 the gene represses its own production; for h > 0 the gene activates its own
production. The case h = 0 recovers the simple birth-death process result δn2 /hni = 1. Points are plotted for integer values of
h; the line shows Eqn. 108 for continuous h. Insets show probability distributions (black solid lines) from the master equation,
compared with Poisson distributions with the same mean (gray solid lines), at h = −1 (star), h = 0 (circle) and h = 1 (cross),
where symbols correspond to points on the δn2 /hni curve. Parameters are g− = 2, g+ = 20, and K = 4, which restrict solutions
to the mono-stable regime, yielding unimodal distributions.

states occur at rates given by the transition matrix Ωzz0 , such that
Z
X

Ωzz0 = 0,

(116)

z=1

which follows from conservation of probability. The rates are constant here; in Sec. IV A 4 we extend the model such
that the rates depend on protein number.
The system is described by a Z-state probability vector with elements given by pzn . The particular states evolve
according to the coupled master equation
X
0
ṗzn = −gz pzn − npzn + gz pzn−1 + (n + 1)pzn+1 +
Ωzz0 pzn .
(117)
z0

where all but the last term describes the birth–death process for state z, and the last term describes transitions
among
P
states. The probabilities of being in state z regardless of the number of proteins are given by πz = n pzn ; summing

18
the master equation over n in steady state gives
X

Ωzz0 πz0 = 0,

(118)

z0

and normalization requires
X

πz = 1.

(119)

z

2.

The two-state gene

We first consider the case in which the gene has only two states (Z = 2), an active state (z = +) and an inactive
state (z = −):
 +
pn
z
pn =
.
(120)
p−
n
The transition matrix takes the form
Ω

zz 0



−ω− ω+
=
.
ω− −ω+

(121)

where ω+ and ω− are the transition rates to the active and to thePinactive states, respectively.
n
As in Sec. II B 2, we define the generating function G± (x) = n p±
n x and sum the master equation (Eqn. 117)
n
over x , yielding at steady state
0 = −y(∂y − g± )G± ± ω+ G− ∓ ω− G+ ,

(122)

where y ≡ x − 1. Writing G± (y) = eg± y H± (y) yields a simpler equation for H± (y); writing out the + and − cases
explicitly,
0 = −yey∆ ∂y H+ + ω+ H− − ω− ey∆ H+ ,
−y∆

0 = −ye

∂y H− + ω− H+ − ω+ e

−y∆

H− ,

(123)
(124)

where ∆ ≡ g+ − g− . Eqns. 123-124 are a coupled pair of first-order ordinary differential equations; combining them
yields a second-order differential equation. Specifically, solving Eqn. 123 (124) for H− (H+ ) and substituting it into
Eqn. 124 (123), one obtains
0 = u∂u2 H± + (β − u)∂u H± − αH± ,

(125)

where u ≡ ∓y∆, α ≡ ω∓ , and β ≡ ω+ + ω− + 1. Eqn. 125 is the canonical equation for the confluent hypergeometric
function. Only one of its two solutions satisfies pn → 0 as n → ∞; it is H± (x) = N± Φ[α, β; u], where
Φ[α, β; u] =

∞
X
Γ(` + α)
`=0

Γ(α)

Γ(β) u`
Γ(` + β) `!

(126)

is the confluent
hypergeometric function of the first kind, and the scalars N± are found by normalization: N± =
P
n
G± (1) = n p±
(1)
= π ± = ω± /(ω+ + ω− ) (the last step uses Eqns. 118-119). Thus,
n
ω±
eg± (x−1) Φ[ω∓ , ω+ + ω− + 1; ∓(g+ − g− )(x − 1)].
(127)
ω+ + ω−
P
As before, the probability distribution can be recovered from G(x) = ± G± (x) by differentiation (Eqn. 21).
As shown in Appendix K, in the limit g− = 0, Eqn. 127 reduces to the slightly simpler expression
G± (x) =

G(x) = Φ[ω+ , ω+ + ω− ; g+ (x − 1)],

(128)

for which the probability distribution has the explicit form
pn =

n
g+
Γ(n + ω+ ) Γ(ω+ + ω− )
Φ[ω+ + n, ω+ + ω− + n; −g+ ],
n! Γ(ω+ ) Γ(n + ω+ + ω− )

in agreement with the results of Iyer-Biswas et al. [34] and Raj et al. [4].

(129)

19
3.

The spectral solution

It is clear that the method presented in the previous section results in a Zth order ordinary differential equation,
and therefore its utility is generally limited to Z = 2 states. In contrast, here we show that the spectral solution, in
which we expand in eigenfunctions of the birth–death
P operator, can be used for an arbitrary number of states.
We begin as in Sec. II B 3 by defining |Gz i = n pzn |ni and writing the master equation in terms of raising and
lowering operators,
X
Ωzz0 |Gz0 i,
|Ġz i = −b̂+ b̂−
(130)
z |Gz i +
z0
−
where now b̂−
z = â − gz is z-dependent. We expand the generating function in the z-dependent eigenfunctions of the
birth–death process as
X
|Gz i =
Gzj |jz i,
(131)
j

where the eigenvalue relation reads
b̂+ b̂−
z |jz i = j|jz i.

(132)

This gives the following equation for the expansion coefficients:
X
X 0
Ġzj = −jGzj +
Ωzz0
Gzj 0 hjz |jz0 0 i.
z0

(133)

j0

Here the transition matrix Ωzz0 couples the otherwise independent birth–death processes, and
0

hjz |jz0 0 i =

(−∆zz0 )j−j
θ(j − j 0 ),
(j − j 0 )!

(134)

where ∆zz0 = gz − gz0 , and the convention θ(0) = 1 is used for the Heaviside function. Eqn. 134 is derived by
evaluating the inner product using the x space representations of the eigenfunctions,
hx|jz i = (x − 1)j egz (x−1) ,

(135)

−gz (x−1)

hjz |xi =

e
,
(x − 1)j+1

(136)

and Cauchy’s theorem, as in Appendix E.
The Heaviside function makes Eqn. 133 lower-triangular. This is explicitly clear, for example, in steady state,
0

jGzj −

X
z0

0

Ωzz0 Gzj =

X
z 0 6=z

Ωzz0

X
j 0 <j

0

Gzj 0

(−∆zz0 )j−j
,
(j − j 0 )!

(137)

where one observes that the jth component is a function only of components j 0 < j. The lower-triangular structure
allows Gzj to be computed iteratively, which makes the spectral solution numerically efficient. The lower-triangular
structure is a consequence of rotating to the eigenbasis of the birth-death operator; this structure was not present in
the original master equation. Other spectral decompositions are possible by exploiting other eigenbases; for example
by expanding in a single eigenbasis parameterized by a constant (not z-dependent) production rate, one obtains an
equation that is sub-diagonal in j [29].
The probability distribution is recovered via inverse transform,
X
pzn =
Gzj hn|jz i,
(138)
j

as in Sec. II B 3. The spectral solution is valid for any number of states Z, while direct solution of the differential equation is generally limited to Z = 2. As expected, for Z = 2 the spectral solution recovers the confluent hypergeometric
function solution (Eqn. 127), as shown in Appendix K.
Fig. 4 demonstrates the spectral solution for Z = 3 states when the transition rate among states is (a) equal to, and
(b) much less than, the degradation rate. In (a) the bursts give rise to a long tail in the distribution (compared to
the Poisson distribution); in (b) the system spends long times in each production state before transitioning, resulting
in a trimodal distribution.

20

switching rate = degradation rate

(a)

0.2

0.1

0

0

5
10
15
20
number of proteins, n

(b)

0.3
probability, pn

0.3
probability, pn

switching rate << degradation rate

0.2

0.1

25

0

0

5
10
15
20
number of proteins, n

25

FIG. 4: Steady state probability distribution for transcriptional bursting with Z = 3 states, when the transition rate among
states is (a) equal to the degradation rate (ω = 1), and (b) much less than the degradation rate (ω = 0.01). Production rates
are gz = (0, 3, 15) and the rate of transition ω between any two states is equal (i.e. all off-diagonal elements of Ωzz0 are ω and
all three diagonal elements are −2ω).

4.

Bursting and auto-regulation

Here we consider the simplest model that combines bursting with auto-regulation [27]. The solution follows that of
Sec. IV A 2. The gene has two states, an active state in which the regulatory protein is bound (p+
n ) and an inactive
state in which the regulatory protein is unbound (p−
n ). The regulation is incorporated by making the rate at which
the gene switches to the active state proportional to the protein number:


−ω− ω+ n
Ωzz0 =
.
(139)
ω− −ω+ n
In this case the analog of Eqn. 122 for the generating function is
0 = −(x − 1)(∂x − g± )G± ± ω+ x∂x G− ∓ ω− G+ ,

(140)

As in Sec. IV A 2, the + and − equations can be combined to give a second-order differential equation, e.g. for G− (x),
0 = ∂x2 G− + C1 (x)∂x G− + C0 (x)G− ,

(141)

with
g− + g+ + ω− + ω+ + 1 − x[g+ (1 + ω+ ) + g− ]
,
(1 + ω+ )x − 1
g− g+ x − g− (g+ + ω− + 1)
C2 (x) ≡
,
(1 + ω+ )x − 1
C1 (x) ≡

(142)
(143)

whose solution G− (x) = N− eg+ (x−1) Φ[α, β; u] is proportional to the confluent hypergeometric function of the first
kind (Eqn. 125) with


ω−
ω+ g−
α = 1+
1+
,
(144)
1 + ω+
g− − (1 + ω+ )g+
ω−
ω+ g−
β = 1+
+
,
(145)
1 + ω+
(1 + ω+ )2
[g+ (1 + ω+ ) − g− ][(1 + ω+ )x − 1]
u = −
.
(146)
(1 + ω+ )2

21
Again N− is set by normalization; G+ (x) is computed from G− (x) using Eqn. 140 (bottom signs).
In Appendix J we consider an extension of this model, in which the transition rate to the active state is proportional
to nh , where h is the cooperativity of the binding. We show that in the limit of fast transitions this model yields an
effective auto-regulation function of the Hill form (Eqn. 108). Full solution of this model using the method presented
here for h = 0 and h = 1 is of limited utility for h > 1, because the high-order differential equation is not solvable.

B.

Translational bursts

We now turn our attention to translational bursts, in which a single mRNA transcript produces many proteins
in a short amount of time. In this case the model includes just one protein production rate, but each production
event results in an instantaneous burst of multiple proteins instead of one. This approach does not explicitly consider
mRNAs; instead it models the translational step as an effective burst of proteins.
With production occurring in bursts of size N , the birth–death master equation becomes
ṗn = −gpn − npn + gpn−N + (n + 1)pn+1 .

(147)

This equation is easily solved by spectral decomposition. In terms of raising and lowering operators (Sec. II B 3), Eqn.
147 reads
h
i
˙ = −g − â+ â− + g â+ N + â− |Gi
(148)
|Gi


= −b̂+ b̂− + Γ |Gi,
(149)
N

where b̂+ = â+ − 1 and b̂− = â− − g as before, and Γ ≡ (â+ ) − â+ . Just as in Sec. II B 3, Eqn. 149 benefits from a
spectral expansion in the eigenfunctions |ji of the simple birth–death process, i.e.
X
|Gi =
Gj |ji,
(150)
j

where
b̂+ b̂− |ji = j|ji.

(151)

Eqn. 149 then becomes an equation for the expansion coefficients:
X
Ġj = −jGj +
hj|Γ|j 0 iGj 0 .

(152)

j0

The simplification
hj|Γ|j 0 i = hj|


"

= hj|

b̂+ + 1

N

N  
X
N
`=0

`



− b̂+ + 1 |j 0 i
b̂

+

"
+

= hj| (N − 1)b̂ +

`



− b̂ + 1

N  
X
N
`=2

= (N − 1)δj,j 0 −1 +

+

`

N  
X
N
`=2

`



+

b̂

(153)
#

`

|j 0 i

(154)

#
|j 0 i

δj,j 0 +` .

(155)

(156)

gives
Ġj = −jGj + (N − 1)Gj−1 +

N  
X
N
`=2

`

Gj−` ,

(157)

22
The equation is lower-triangular, which makes a recursive solution simple. Note that setting N = 1 recovers Eqn. 72
for the simple birth–death process. The probability distribution is retrieved by inverse transform,
X
pn =
Gj hn|ji.
(158)
j

with hn|ji computed as in Appendix H.
Translational bursts increase noise relative to the simple birth–death process. For example, we can easily compute
the variance and mean of the steady state distribution for translational bursts (see Appendix C); their ratio is
σ2
1+N
=
,
n̄
2

(159)

which for N > 1 is (potentially much) greater than the birth–death result σ 2 /n̄ = 1.
We note that within the Langevin approach we can model translational bursts as:
dn
= N g − n + η(t)
dt
0
hη(t)η(t )i = δ(t − t0 ) (N g + n) .

(160)
(161)

We obtain a power spectrum of the form:
N =

N +1
,
ω2 + 1

(162)

which again is a signature of a more noisy process.

V.

TWO GENES

At this point we have discussed models of a single gene, including the simple birth–death process, auto-regulation,
and bursts. Here we extend the discussion to include multiple genes with regulation. We focus on the case of two
genes, and we describe how the method may be extended to larger regulation networks. We refer the reader to the
literature for more detailed discussion of the application of the spectral method to cascades [28] and to systems with
both regulation and bursts [29].
Here we consider two genes, the first with protein number n, and the second with protein number m. The first gene
regulates its own expression via arbitrary function gn , and it regulates the expression of the second gene via arbitrary
function qn . This system can be thought of as the simplest possible regulatory network; it is also the minimum system
necessary for completely describing a regulatory cascade of arbitrary length [28]. Because the model is general, the
two degrees of freedom could correspond to two species of proteins, or to a protein and an mRNA species.
The master equation for this system is
ṗnm = −gn pnm − npnm + gn−1 pn−1,m + (n + 1)pn+1,m
+ρ [−qn pnm − mpnm + qn pn,m−1 + (m + 1)pn,m+1 ] ,

(163)

where time is rescaled by the degradation rate of the first gene and ρ is the ratio of the degradation rate of the first
gene to that of the second.

A.

Deterministic model

As always, the kinetic rate equations can be obtained by deriving the mean dynamics from the master equation.
Summing the master equation over both indices against either n or m (and performing index shifts as in Appendix
B) gives
dn̄
= g(n̄) − n̄,
dt
dm̄
= q(n̄) − m̄,
dt

(164)
(165)

23
P
P
respectively. Here we have assumed that n gn pn ≈ g(n̄) and n qn pn ≈ q(n̄) by taking n = n̄ + η and keeping the
first terms of the Taylor expansions g(n) = g(n̄) + ηg 0 (n̄) + . . . and q(n) = q(n̄) + ηq 0 (n̄) + . . . , respectively.
Solving for the steady state of the deterministic system requires finding the roots of two coupled nonlinear equations,
which can only be done numerically. Solving the deterministic system is essential for finding the power spectrum and
calculating the correlation functions in the Langevin approach. However, as we show in the next section, the full
stochastic description can be solved quite efficiently using the spectral method, i.e. by expanding in the eigenfunctions
of the simple birth–death process.
B.

The spectral method

Although it is usually not possible to find an exact solution at either the master and Fokker–Planck equation levels,
here we show that efficient numerical computation of the full steady state distribution is possible by expanding in the
eigenfunctions of the simple birth–death process (Sec. II B 3). This approach is general and does not depend on the
algebraic forms of the regulation functions.
P
As in Sec. II B 3 we define the generating function |Gi = nm pnm |n, mi, which makes the master equation
˙ = −L̂|Gi,
|Gi

(166)

−
+ −
L̂ = b̂+
n b̂n (n) + ρb̂m b̂m (n)

(167)

where the full linear operator

is written in terms of raising and lowering operators defined analogously to Eqns. 59-60:
+
b̂+
n = ân − 1,

(168)

b̂+
m

=

(169)

b̂−
n (n)
−
b̂m (n)

=
=

â+
m − 1,
â−
n − ĝ(n),
â−
m − q̂(n).

(170)
(171)

The difference here is that both lowering operators are n-dependent because of the regulation functions. This suggests
that we separate the full operator as L̂ = L̂0 + L̂1 (n), where we have defined a constant part,
−
+ −
L̂0 ≡ b̂+
n b̄n + ρb̂m b̄m ,

(172)

−
−
−
with b̄−
n ≡ ân − ḡ and b̄m ≡ âm − q̄, and an n-dependent part
+ ˆ
L̂1 (n) ≡ b̂+
n Γ̂(n) + ρb̂m ∆(n),

(173)

ˆ
with Γ̂(n) ≡ ḡ − ĝ(n) and ∆(n)
≡ q̄ − q̂(n). The operator L̂0 describes two independent birth-death processes with
constant production rates ḡ and q̄ respectively, and the operator L̂1 captures how the regulated processes differ from
the constant rate processes. The constants ḡ and q̄ act as gauges: they can be chosen freely, and the final solution is
independent of this choice.
We have already solved for the eigenfunctions |j, ki of the constant rate birth–death processes in Sec. II B 3. The
spectral method exploits this fact by expanding the solution of the full problem in these eigenfunctions:
X
|Gi =
Gjk |j, ki.
(174)
jk

Eqn. 166 then gives an algebraic relation among the expansion coefficients:
X
X
Ġjk = −(j + ρk)Gjk −
Γj−1,j 0 Gj 0 k − ρ
∆jj 0 Gj 0 ,k−1 ,
j0

(175)

j0

where
Γjj 0 = hj|Γ̂(n)|j 0 i =

X
hj|ni(ḡ − gn )hn|j 0 i,

(176)

n
0
ˆ
∆jj 0 = hj|∆(n)|j
i=

X
n

hj|ni(q̄ − qn )hn|j 0 i.

(177)

24

(a)

0.015

(b)

0.015

pnm

0.01

pnm

0.01
0.005

0.005

0

0
20
m

20
0 0

10
n

30

20

m

0 0

10
n

20

30

FIG. 5: Demonstration of the spectral method with two genes. Regulation is of Hill form, qn = (q− K h + q+ nh )/(K h + nh ),
with q− = 1, q+ = 12, K = 4, and h = 4. In (a) the first gene undergoes a simple birth–death process with gn = 7; in (b) the
first gene regulates itself with gn = qn . Degradation rates are equal: ρ = 1.

Eqn. 175 enjoys the property that it is sub-diagonal in the second gene’s eigenvalue k. This feature comes from the
fact that both regulation functions are dependent on the first gene’s protein number n (and not m). The sub-diagonal
structure allows the matrix Gjk to be computed iteratively by columns in k, which makes the method numerically
efficient. The initial condition
X
X
Gj0 = hj, k = 0|Gi =
pnm hj|nih0|mi =
pn hj|ni
(178)
nm

n

requires the marginal probability distribution of the first gene pn , which can be found recursively (Eqn. 110) as
described in Sec. III B on one-dimensional auto regulation. The joint probability distribution is retrieved by inverse
transform:
X
pnm =
Gjk hn|jihm|ki.
(179)
jk

To demonstrate the capability of the spectral method, Fig. 5 plots the steady state joint probability distribution
for a particular choice of regulation function, for cases (a) without and (b) with auto-regulation. To demonstrate
the accuracy of the spectral method, Fig. 6 compares steady state marginal distributions obtained by the spectral
method with those obtained both by direct iterative solution of Eqn. 163 and by standard Gillespie simulation [48].
The spectral method attains excellent agreement with both methods in orders of magnitude less runtime than either
other method; full details on the efficiency and accuracy of the spectral method are available in [28].
The spectral decomposition presented here constitutes an expansion in only one of many possible eigenbases: that
with constant production rates. Other spectral expansions are possible, as discussed in detail in [29]; we do not repeat
the discussion here.
VI.

SUMMARY

In this pedagogical chapter, we have attempted to introduce the reader to the phenomenon of intrinsic noise and
to the simplest possible mathematical tools for modeling. We have avoided detailed discussion of a vast literature
in Monte Carlo simulation of such phenomena in favor of simple calculations which can be performed analytically
or which are useful in numerical solutions of the master equation itself (rather than random generation of sample
trajectories).
The discussion also hopefully illustrates how different problem settings motivate different analytic methods, many of
which have close parallels in the physics canon. These include approximation of differences with derivatives, yielding
a description very similar to that used in Fokker-Planck descriptions; as well as the algebraic relations among the

25

0.15

0.15

(a)

(b)

spectral
iterative
simulation
0.1

pn

pm

0.1

spectral
iterative
simulation

0.05

0
0

0.05

10

20

0
0

30

n

10

20

30

m

FIG. 6: Marginal distributions for the (a) first and (b) second gene in a two-gene network, summed from the joint distribution in
Fig 5(b). Agreement is demonstrated among distributions obtained by the spectral method (solid), by direct iterative solution
of the master equation (Eqn. 163; plus signs), and by standard Gillespie simulation [48] (circles).

different eigenfunctions of the birth-death process itself, yielding an algebra of “ladder operators” similar to that used
in quantum mechanics.
While the literature in simulation of intrinsic noise has grown immensely since the introduction of time-varying
Monte Carlo methods over thirty years ago [48], it is our hope that for model systems these calculations will help
point out how the linearity of master equations may be exploited to yield either solutions, numerical methods, or
analytical approximations which give more direct insight into the deign principles of stochastic regulatory networks.

Appendix A: Deriving the master equation

Here we derive the master equation in one dimension from the laws of probability. The probability of having n
proteins at a time t + τ (where τ is small) is equal to the probability of having had n0 proteins at time t multiplied
by the (possibly time-dependent) probability of transitioning from n0 to n in time τ , summed over all n0 :
X
pn (t + τ ) =
pn0 (t)pn|n0 (t, τ ).
(A1)
n0

The transition probability pn|n0 (t, τ ) has two contributions: (i) the probability of transitioning from state n0 6= n
to state n, equal to a transition rate wnn0 (t) times the transition time τ , and (ii) the probability of starting and
remaining in state n0 = n, which we call πn (t):
pn|n0 (t, τ ) = (1 − δnn0 )wnn0 (t)τ + δnn0 πn (t)
P
Applying the normalization condition n pn|n0 (t, τ ) = 1 to Eqn. A2, we obtain
πn (t) = 1 − τ

X
n0 6=n

wn0 n (t),

(A2)

(A3)

26
making Eqn. A1

X

pn (t + τ ) =





pn0 (t) (1 − δnn0 )wnn0 (t)τ + δnn0 1 − τ

n0

= τ

X

wn00 n (t)

(A4)

X

(A5)

n00 6=n

X

pn0 (t)wnn0 (t) − τ pn (t)wnn + pn (t) − τ pn (t)

n0

wn00 n (t)

n00 6=n

= pn (t) + τ

"
X

#
pn0 (t)wnn0 (t) − pn (t)

n0

X

wn0 n (t) .

(A6)

n0

Taking the limit τ → 0, we obtain
X
X
dpn
pn0 wnn0 (t) − pn
=
wn0 n (t),
dt
0
0
n

(A7)

n

as in Eqn. 4.

Appendix B: Summation of the birth–death master equation

Here we sum the one-dimensional birth–death master equation over n against (i) n, to derive the mean dynamics,
(ii) xn , to derive the equation of motion in generating function space, and (iii) |ni, to find the equation of motion in
operator notation.
P
First we sum the master equation (Eqn. 9) over n against n to derive the dynamics of the mean hni = n npn :
∂t

X

npn = −g

X

n

npn −

n

X

n2 pn + g

X

n

npn−1 +

n

X

n(n + 1)pn+1 .

(B1)

n

The left-hand side (LHS) is the time derivative of the mean. On the right-hand side (RHS), recalling that n runs
from 0 to ∞, we may define for the third term n0 ≡ n − 1 and for the fourth term n00 ≡ n + 1, giving
∂t hni = −g

X

npn −

n

X

n2 pn + g

∞
X

(n0 + 1)pn0 +

n0 =−1

n

∞
X

(n00 − 1)n00 pn00 .

(B2)

n00 =1

The lower limits on all sums can be set to 0 once more without changing the expression (in the third term we impose
p−1 ≡ 0 since protein number cannot be negative); simplifying gives
X
X
X
X
X
X
∂t hni = −g
npn −
n2 pn + g
n0 pn0 + g
p n0 +
n002 pn00 −
n00 pn00
(B3)
n

= g

X

pn0 −

n0

n0

n

X

n0

n00

n00

n00 pn00

(B4)

n00

= g − hni,

(B5)

as in Eqn. 8.
Next we sum thePmaster equation (Eqn. 9) over n against xn to derive the equation of motion of the generating
function G(x, t) = n pn (t)xn :
∂t

X
n

pn xn = −g

X

p n xn −

n

X

npn xn + g

n

X

pn−1 xn +

n

X

(n + 1)pn+1 xn .

(B6)

n

The LHS and the first term on the RHS reduce directly to functions of G. The third and fourth terms on the RHS
benefit from the same index shifts applied to obtain Eqn. B2, giving
X
X
X
0
00
∂t G = −gG −
pn nxn + g
pn0 xn +1 +
n00 pn00 xn −1 .
(B7)
n

n0

n00

27
We may now eliminate bare appearances of n in the second and fourth terms on the RHS using nxn = x∂x xn and
nxn−1 = ∂x xn respectively, giving
X
X
X
0
00
∂t G = −gG −
pn x∂x xn + g
pn0 xn +1 +
pn00 ∂x xn
(B8)
n0

n

n00

= −gG − x∂x G + gxG + ∂x G
= −(x − 1)(∂x − g)G

(B9)
(B10)

as in Eqn. 23.
Finally we sum the master equation
P (Eqn. 9) over n against |ni to derive the equation of motion of the generating
function in operator notation |Gi = n pn |ni:
∂t

X

pn |ni = −g

n

X

pn |ni −

X

n

npn |ni + g

X

n

pn−1 |ni +

n

X

(n + 1)pn+1 |ni.

(B11)

n

Again we may reduce the LHS and the first term on the RHS directly, and we may apply index shifts to the third
and fourth terms on the RHS:
X
X
X
pn0 |n0 + 1i +
n00 pn00 |n00 − 1i.
∂t |Gi = −g|Gi −
npn |ni + g
(B12)
n0

n

n00

Now defining operators â+ and â− that raise and lower the protein number by 1 respectively, i.e.
â+ |ni ≡ |n + 1i,
â− |ni ≡ n|n − 1i,

(B13)
(B14)

Eqn. B11 can be written
X

∂t |Gi = −g|Gi −

pn â+ â− |ni + g

X

pn0 â+ |n0 i +

n0

n
+ −

X

pn00 â− |n00 i

(B15)

n00

= −g|Gi − â â |Gi + gâ+ |Gi + â− |Gi
= −(â+ − 1)(â− − g)|Gi

(B16)
(B17)

as in Eqn. 50.

Appendix C: Statistics of the birth–death distribution

In this appendix we calculate the means of a birth–death distribution. In the simplest case, starting directly from
the solution in Eqn. 16 we obtain:
hni =

X

npn =

n

X gn
X gn
= g.
n e−g = e−g g∂g
n!
n!
n
n

(C1)

Similarly we can calculate
hn(n − 1)i =

X
n

n(n − 1)

X gn
g n −g
e = e−g g 2 ∂g2
= g2 ,
n!
n!
n

(C2)

which gives us the variance
hn2 i − hni2 = g 2 + g − g 2 = g.

(C3)

These results can also be obtained in the operator formalism, and the fact that for steady state |Gi = |j = 0i. For
example:
X
X
X
hni = hx|â+ â− |j = 0i|x=1 = hx|â+ â− |Gi|x=1 = hx|â+ â−
pn |ni|x=1 =
npn xn |x=1 =
npn .
(C4)
n

n

n

28
To use the number operator on the |j = 0i state we need to rewrite it in terms of â+ = b̂+ + 1 and â− = b̂− + g.
Acting to the left we obtain:


(C5)
hni = hx| b̂+ b̂− + g + b̂− + g b̂+ |0ix=1 = hx|g|0ix=1 + hx|g|1ix=1 = g.
We can also calculate the mean of the bursty process. Multiplying Eqn. 147 by n and summing over n, we use,
∞
X

npn−N =

n=0

∞
X

(n + N )pn = hni + N,

(C6)

n=0

to obtain:
g(hni + N ) − ghni + hn2 i − hni − hn2 i = 0,

(C7)

hni = gN.

(C8)

which is solved by:

The second moment hn2 i =

P

n2 pn is calculated the same way:
hn2 i =


1
gN 2 + gN
2ghniN + hni + gN 2 = (gN )2 +
,
2
2

(C9)

which yields the variance as
δn2 = hn2 i − hni2 =

gN
(1 + N ).
2

(C10)

Appendix D: Asymptotic distributions for large protein number

Here we show that in the limit of large protein number n, the steady state of the birth–death process (the Poisson
distribution, Eqn. 16) approaches a Gaussian distribution with mean and variance equal to g, the ratio of production
to degradation rates. We then show that in the same limit, the steady state solution to the birth–death Fokker–Planck
equation (Eqn. 84) also approaches a Gaussian distribution with mean and variance equal to g.
The large-n limit of the Poisson distribution is most conveniently evaluated
by first taking the log, which allows
√
one to make use of Stirling’s approximation, log n! ≈ n log n − n + log 2πn for large n:
√
log p(n) ≈ −g + n log g − n log n + n − log 2πn.
(D1)
The derivative
∂n log p(n) = log

g
+ O(1/n)
n

(D2)

vanishes at the maximum
n=g

(D3)

about which we Taylor expand to second order:
1
log p(n) ≈ log p(g) + (n − g)2 ∂n2 [log p(n)]g
2
p
(n − g)2
= − log 2πg −
.
2g

(D4)
(D5)

The result is a Gaussian distribution with mean and variance equal to g:
p(n) = √

2
1
e−(n−g) /(2g) .
2πg

(D6)

29
The large-n limit of the Fokker–Planck distribution is similarly evaluated: the log and it’s derivatives are


N
n
log p(n) = log
+ (4g − 1) log 1 +
,
g
g
4g − 1
− 2,
∂n log p(n) =
n+g
1 − 4g
.
∂n2 log p(n) =
(n + g)2

(D7)
(D8)
(D9)

The first derivative vanishes at n = g − 1/2 ≈ g for large g, at which the second derivative evaluates to ∂n2 [log p(n)]g =
−1/g + 1/(4g 2 ) ≈ −1/g. Taylor expanding to second order and exponentiating then give
p(n) = √

2
1
e−(n−g) /(2g) ,
2πg

(D10)

where N is eliminated by normalization.
Appendix E: Orthonormality and the inner product

Here we show that interpreting the generating function as a Fourier transform motivates a particular choice of inner
product and conjugate state in the protein number basis.
P
n
We have defined the generating function in terms of a continuous variable x as G(x) =
n pn x . We could
iκ
equivalently write x ≡ e and define
X
G(κ) =
pn einκ ,
(E1)
n

which makes clear that the generating function is simply the Fourier transform of the probability distribution
P in protein
number n. In the state notation commonly used in quantum mechanics [42], Eqn. E1 is written hκ|Gi = n pn hκ|ni,
where hκ|Gi ≡ G(κ) and
hκ|ni ≡ einκ .

(E2)

Eqn. E2 is the representation of |ni in κ space. In order to compute projections of |ni onto other states or itself
using this representation, we must define a conjugate state hn|κi and a consistent inner product. With complex
exponentials, it is common to make the conjugate state the complex conjugate,
hn|κi ≡ hκ|ni∗ = e−inκ ,

(E3)

and the inner product the integral
Z
hf |hi ≡
0

2π

dκ
hf |κihκ|hi,
2π

(E4)

for any f and h. Given the definition of conjugate state, this choice of inner product ensures the orthonormality of
the |ni states:
Z 2π
Z 2π
dκ i(n0 −n)κ
dκ
hn|κihκ|n0 i =
e
= δnn0 .
(E5)
hn|n0 i =
2π
2π
0
0
We may now reinterpret these definitions in terms of our original variable x. Since x = eiκ we have dx = ieiκ dκ =
ixdκ, and in the orthonormality condition (Eqn. E5) the integration from 0 to 2π along the real k line becomes contour
integration along the unit circle in the complex x plane:
Z 2π
I
I
0
dk −ink in0 k
dx −n n0
dx 1
hn|n0 i =
e
e
=
x x =
xn = δnn0 .
(E6)
n+1
2π
2πix
2πi
x
0
Since we have defined
hx|ni = xn ,

(E7)

30
Eqn. E6 suggests the definition of conjugate state
hn|xi =

1
xn+1

(E8)

and inner product
I
hf |hi =

dx
hf |xihx|hi
2πi

in x space.
With these definitions, we will find Cauchy’s theorem,
I
1
f (x)
dx
= ∂xn [f (x)]x=a θ(n),
2πi (x − a)n+1
n!

(E9)

(E10)

(with the convention θ(0) = 1 for the Heaviside function) very useful in evaluating projections. For example, we may
immediately use it to confirm the orthonormality condition,
I
1 n h n0 i
dx 1
n0
x
=
θ(n) = δnn0 ,
∂ x
(E11)
hn|n0 i =
2πi xn+1
n! x
x=0
and in Appendix H we use it to compute analytic expressions for the projections between protein number states |ni
and birth–death eigenstates |ji.

Appendix F: Properties of the raising and lowering operators

Just as in the operator treatment of the quantum harmonic oscillator [42], in this paper we have defined operators
â+ and â− that act on |ni states by raising and lowering the protein number n by 1 respectively, i.e.
â+ |ni = |n + 1i,
â− |ni = n|n − 1i

(F1)
(F2)

(note, however,
that the
√
√ prefactors here, 1 and n, are different than those conventionally used for the harmonic
oscillator, n + 1 and n, respectively). In this appendix we derive the actions of these operators when acting to
the left, as well as their commutation relation.
The actions of â+ and â− to the left can be found by projecting onto Eqns. F1 and F2 the conjugate state hn0 |:
hn0 |â+ |ni = hn0 |n + 1i = δn0 ,n+1 = δn0 −1,n = hn0 − 1|ni,
hn0 |â− |ni = hn0 |n|n − 1i = nδn0 ,n−1 = (n0 + 1)δn0 +1,n = (n0 + 1)hn0 + 1|ni.

(F3)
(F4)

The first and last expressions in each case imply
hn0 |â+ = hn0 − 1|,
hn0 |â− = (n0 + 1)hn0 + 1|,

(F5)
(F6)

as in Eqns. 56-57.
The commutation relation between â− and â+ is defined as [â− , â+ ] ≡ â− â+ − â+ â− . It is evaluated by considering
its action on a state |ni:
 − +
â , â |ni = â− â+ |ni − â+ â− |ni = â− |n + 1i − â+ n|n − 1i = (n + 1)|ni − n|ni = |ni.
(F7)
The first and last expressions imply
 − +
â , â = 1,
just as with the quantum harmonic oscillator.

(F8)

31
Appendix G: Eigenvalues and eigenfunctions in the operator representation

Here we derive the eigenfunctions |λj i (also called eigenstates in the operator representation) and eigenvalues λj of
the operator L̂ = b̂+ b̂− , for which
L̂|λj i = λj |λj i,

(G1)

as well as the actions of the individual operators b̂+ and b̂− on the eigenstates. We will find that the commutation
relation
i
h
(G2)
b̂− , b̂+ = 1
and the existence of the steady state solution hx|Gi = eq(x−1) , for which
L̂|Gi = 0,

(G3)

are all that are necessary to completely define the eigenvalues and eigenstates of L̂. The treatment here finds many
parallels with the derivation of the eigenvalue spectrum of the quantum harmonic oscillator [42].
First it is useful to compute the commutation relation of L̂ with each of its components b̂+ and b̂− :
h
i
h
i
h
i h
i
b̂+ , L̂ = b̂+ , b̂+ b̂− = b̂+ b̂+ , b̂− + b̂+ , b̂+ b̂− = −b̂+ ,
(G4)
i h
i
h
i
h
i
h
b̂− , L̂ = b̂− , b̂+ b̂− = b̂+ b̂− , b̂− + b̂− , b̂+ b̂− = b̂−
(G5)
(here we have used Eqn. G2 and the properties [f, gh] = g[f, h] + [f, g]h and [f, f ] = 0 for any f , g, and h). We now
consider a particular eigenvalue λ, and evaluate the action of L̂b̂+ on |λi:
h
i
L̂b̂+ |λi = b̂+ L̂|λi − b̂+ , L̂ |λi = b̂+ λ|λi + b̂+ |λi = (λ + 1)b̂+ |λi.
(G6)
The equality of the first and last expressions,




L̂ b̂+ |λi = (λ + 1) b̂+ |λi

(G7)

reveals two results: (i) the existence of a state with eigenvalue λ implies the existence of a state with eigenvalue
λ + 1, and (ii) the state with eigenvalue λ + 1 is proportional to b̂+ |λi. By induction the first result means that the
eigenvalues are spaced by 1; moreover, the existence of a state with eigenvalue zero (the steady state solution; Eqn.
G3) anchors the eigenvalues to the integers, i.e.
λj = j

(G8)

for integer j. The second result can now be written explicitly
b̂+ |ji = |j + 1i,

(G9)

where we are free by normalization to set the proportionality constant to 1. Eqn. G9 demonstrates that b̂+ is a raising
operator for the eigenstates |ji.
Evaluating the action of L̂b̂− on eigenstate |ji similarly reveals that b̂− |ji is proportional to |j − 1i (cf. Eqn. G6);
consistency with the eigenvalue equation (Eqn. G1) requires that the proportionality constant be j, giving
b̂− |ji = j|j − 1i.

(G10)

Eqn. G10 demonstrates that b̂− is a lowering operator for the eigenstates |ji. The operators b̂+ and b̂− raise and
lower |ji as â+ and â− do |ni; therefore they act to the left as (cf. Eqns. F3-F4)
hj|b̂+ = hj − 1|,
hj|b̂

−

= (j + 1)hj + 1|.

(G11)
(G12)

32
Eqn. G10 imposes a floor on the eigenvalue spectrum, since b̂− |0i = 0, and no lower eigenstates can be generated.
Therefore the eigenvalues are limited to nonnegative integers:
j ∈ {0, 1, 2, 3, . . . }.
Eqn. G9 describes how any state can be obtained from the j = 0 state:
 j
|ji = b̂+ |0i.

(G13)

(G14)

Recalling that b̂+ = â+ −1 and that |0i is the steady state solution, Eqn. G14 can be used to derive the representation
of the eigenfunctions in x space, hx|ji. Projecting hx| onto Eqn. G14 and recalling that â+ corresponds to x (Eqn.
54) and that hx|0i = eg(x−1) (Eqn. 24), we obtain
hx|ji = (x − 1)j eg(x−1) .

(G15)

We may now define the conjugate state hj|xi such that the |ji are orthonormal under the inner product defined in
Eqn. E9:
I
I
I
dx
dy j 0
dx
0
j 0 g(x−1)
0
hj|xihx|j i =
hj|xi(x − 1) e
=
y fj (y),
(G16)
δjj 0 = hj|j i =
2πi
2πi
2πi
where y ≡ x−1 and fj (x−1) ≡ eg(x−1) hj|xi. Eqn. G16 is equivalent to Eqn. E6, from which we identify fj (y) = 1/y j+1
and thus
hj|xi =

e−g(x−1)
.
(x − 1)j+1

(G17)

We have shown that the operator L̂ = b̂+ b̂− has nonnegative integer eigenvalues j and eigenfunctions (in x space)
given by Eqns. G15 and G17.
Appendix H: Overlaps between protein number states and eigenstates

Here we describe two methods for computing the overlaps hn|ji and hj|ni between the protein number states |ni
and the eigenstates |ji: by contour integration and by recursive updating.
The first method evaluates the overlaps using the inner product defined in Eqn. E9 and Cauchy’s theorem (Eqn.
E10). Recalling the representations in x space of the protein number states and eigenstates (Eqns. 46, 48, and 65-66),
the first overlap becomes
I
I
h
i
dx
dx eg(x−1) (x − 1)j
1
hn|ji =
hn|xihx|ji =
= ∂xn eg(x−1) (x − 1)j
.
(H1)
n+1
2πi
2πi
x
n!
x=0
Repeated derivatives of a product follow a binomial expansion, i.e.
n
h
i


1 X
n!
∂xn−` eg(x−1)
∂x` (x − 1)j x=0
n!
`!(n − `)!
x=0
`=0


n
X
 n−` −g 
j!
1
g
e
(−1)j−` θ(j − `)
=
`!(n − `)!
(j − `)!

hn|ji =

(H2)
(H3)

`=0

= (−1)j e−g g n j!ξnj ,

(H4)

where we define
min(n,j)

ξnj ≡

X
`=0

1
.
`!(n − `)!(j − `)!(−g)`

(H5)

Following similar steps, the conjugate overlap evaluates to
hj|ni = n!(−g)j ξnj ,

(H6)

33
with ξnj as in Eqn. H5. For the special case j = 0, Eqns. H4 and H6 reduce to Eqns. 77-78.
It is more computationally efficient to compute the overlaps recursively using rules that can be derived from the
raising and lowering operations (Eqns. 52-53, 56-57, and 67-70). For example, using the raising operators,
hn|j + 1i = hn|b̂+ |ji = hn|(â+ − 1)|ji = hn − 1|ji − hn|ji,

(H7)

which can be initialized using hn|0i = e−g g n /n! (Eqn. H4) and updated recursively in j. Eqn. H7 makes clear that in
n space the (j + 1)th mode is simply the (negative of the) discrete derivative of the jth mode. The lowering operators
give an alternative update rule,
(n + 1)hn + 1|ji = hn|â− |ji = hn|(b̂− + g)|ji = jhn|j − 1i + ghn|ji,

(H8)

which can be initialized using h0|ji = (−1)j e−g (Eqn. H4) and updated recursively in n. One may similarly derive
recursion relations for hj|ni, i.e.
hj|n + 1i = hj − 1|ni + hj|ni,
(j + 1)hj + 1|ni = nhj|n − 1i − ghj|ni,

(H9)
(H10)

initialized with hj|0i = (−g)j /j! or h0|ni = 1 respectively (Eqn. H6) and updated recursively in n or j respectively.
Two-term recursion relations can be similarly derived from the full operator b̂+ b̂− [29].
Appendix I: The equivalence of the Fokker–Planck and Langevin descriptions

In this appendix we start from the Langevin equation defined in Eqns. 93 and 94 and derive the Fokker–Planck
equation in Eqn. 81. As described in Sec. II D the observed trajectory is one realization rη (t) of the random stochastic
process with Gaussian noise as presented in Eqn. 92:
Z
pn (t) = Dηδ (n − rη (t)) P [η] := hδ (n − rη (t))i.
(I1)
Consider the evolution of the probability distribution:
pn (t + ∆t) − pn (t) = hδ (n − rη (t + ∆t)) − δ (n − rη (t))i,

(I2)

and expand the increment in the trajectory as r(t + ∆t) = r(t) + ∆r(t). We can now Taylor expand the difference in
delta functions:
1
pn (t + ∆t) − pn (t) = h(−∆r(t))δ 0 (n − rη (t)) + (−∆r(t))2 δ 00 (n − rη (t))i,
2
1
= −∂n h∆r(t)δ (n − rη (t))i + ∂n2 h(∆r(t))2 δ (n − rη (t))i,
2

(I3)
(I4)

where the primes denote derivatives in n. Using the Langevin equation to calculate the increments in the trajectories:
∆r(t) = v [r(t)] ∆t + η(t)∆t

(I5)

we obtain:
h∆r(t)δ (n − rη (t))i = hv [r(t)] ∆tδ (n − rη (t))i + hη(t)∆tδ (n − rη (t))i
= v [r(t)] ∆thδ (n − rη (t))i = v (n])∆tpn (t)

(I6)
(I7)

h(∆r(t))2 δ (n − rη (t))i = (∆t)2 h(v [r(t)])2 δ (n − rη (t))i + 2hv [r(t)] η(t)δ (n − rη (t))i + hη 2 (t)δ (n − rη (t))i (I8)
2

= (∆t)2 (v(n)) pn (t) + hη 2 (t)ipn (t),

(I9)

where we have used (assuming a discretized process):
hη(t)δ (n − rη (t))i = hη(t)ihδ (n − rη (t))i = 0.

(I10)

We identify
D(n) =

∆t 2
hη (t)i.
2

(I11)

34
Putting together all elements and keeping only leading terms in ∆t:
pn (t + ∆t) − pn (t)
= −∂n [v (n])∆tpn (t)] + ∂n2 [D(n)pn (t)] .
∆t

(I12)

Taking the limit of ∆t → 0 we recover the Fokker–Planck equation:
∂t pn (t) = −∂n [v (n])∆tpn (t)] + ∂n2 [D(n)pn (t)] .

(I13)

Appendix J: Derivation of the Hill function

The Hill function can be derived as an effective production rate for an autoregulating gene with two production
states (Sec. IV A 4). The gene is found either in an inactive state (−), in which the production rate is a constant g− ,
or in an active state (+), in which the production rate ω+ nh depends on the number of proteins n, which incorporates
the autoregulation; h describes the cooperativity, with h > 0 corresponding to activation and h < 0 corresponding to
repression. The probability p±
n of the gene being in a given state + or − and there being n proteins evolves in time
according to the master equation in Eqn. 117 with Ωzz0 as in Eqn. 139; in steady state:
h −
+
0 = −L± p±
n ± ω+ n pn ∓ ω− pn ,

(J1)

where L± describe simple birth–death terms with constant production rates g± in each of the two states. We define
moments of the master equation as
X
π± ≡
p±
(J2)
n,
n

π ± µ±
`

≡

X

`
p±
nn

for ` ≥ 1.

(J3)

n

with
π+ + π− = 1

(J4)

by normalization.
Summing Eqn. J1 (top signs) over n (and recalling that the birth–death terms sum to zero) gives
X
X
0 = ω+
nh p−
p+
n − ω−
n
=

n
ω+ π − µ−
h

(J5)

n

− ω− π +

(J6)

which, with Eqn. J4, becomes an expression for π + , the probability of being in the active state:
π+ =

µ−
h
.
µ−
h + ω+ /ω−

(J7)

We solve for µ−
h using two approximations. The first is that higher moments can be be decoupled, i.e.
− −
µ−
h+1 ≈ µh µ1

for h ≥ 1,

(J8)

which implies that
− h
µ−
h ≈ (µ1 ) .

(J9)

This approximation allows one to simplify the mean equation in the + state (obtained by summing the Eqn. J1, top
signs, against n over n):
X
X
X
0 = π + g+ −
np+
nh+1 p−
np+
(J10)
n + ω+
n − ω−
n
n

n

− −
+ +
= π + g+ − π + µ+
1 + ω+ π µh+1 − ω− π µ1
+

≈ π g+ −
+

= π g+ −

π + µ+
1
π + µ+
1

+
+

−
+ +
ω+ π − µ−
h µ1 − ω− π µ1

+
ω− π + µ−
1 − µ1 ,

n

(J11)
(J12)
(J13)

35
where the last step uses Eqn. J6. The second approximation is that transitions between states are fast, i.e. ω+ ∼
ω−  1. This approximation allows us to neglect the first two terms of Eqn. J13 compared to the third term, which
+
implies that µ−
1 ≈ µ1 . Summing Eqn. J3 over ± for ` = 1 then gives
X
 −
−
+
−
π + µ+
µ1 = µ−
pn n = n̄,
(J14)
1 + π− µ1 ≈ π + π
1 =
n

which shows that µ−
1 approximates the mean of the distribution. Therefore with Eqn. J9 the probability that the
gene is active (Eqn. J7) can be written
π+ =

n̄h
,
n̄h + K

(J15)

with equilibrium constant K ≡ ω+ /ω− . The effective production rate is the sum of the production rates in each state
times the corresponding probabilities of being in each state:
g(n̄) = g− π − + g+ π +


n̄h
n̄h
+ g+ h
= g− 1 − h
n̄ + K
n̄ + K
=

(J16)
(J17)

g− K + g+ n̄h
,
n̄h + K

(J18)

which is the Hill function (Eqn. 108).

Appendix K: Limiting case of the two-state gene

Here we show that the spectral solution to transcriptional bursting (Eqn. 137) reduces to the hypergeometric form
(Eqn. 127) in the limit of Z = 2 production states. We also derive the slightly simpler expression for the special case
of zero production in the inactive state.
In the case of two states (Eqns. 120-121; z = ±), Eqn. 137 becomes
0

±
∓
jG±
j + ω∓ Gj − ω± Gj = ω±

X
j 0 <j

G∓
j0

(∓∆)j−j
,
(j − j 0 )!

(K1)

where ∆ = ∆+− = −∆−+ . Initializing with G±
0 = ω± /(ω+ + ω− ) and computing the first few terms reveals the
pattern
G±
j

Qj−1 0
ω±
(∓∆)j
j 0 =0 (j + ω∓ )
=
Qj−1 00
ω+ + ω− j!
j 00 =0 (j + ω+ + ω− + 1)
=

ω±
(∓∆)j Γ(j + ω∓ ) Γ(ω+ + ω− + 1)
,
ω+ + ω− j!
Γ(ω∓ ) Γ(j + ω+ + ω− + 1)

(K2)
(K3)

where in the second
line the products are written in terms of the Gamma function. Writing the total generating
P
function |Gi = ± |G± i in position space recovers the hypergeometric form (Eqn. 127):
G(x) =

X
hx|G± i

(K4)

±

=

XX
hx|j± ihj± |G± i
±

=

XX
(x − 1)j eg± (x−1) G±
j
±

=

X
±

(K5)

j

(K6)

j

ω±
eg± (x−1) Φ[ω∓ , ω+ + ω− + 1; ∓∆(x − 1)],
ω+ + ω−

(K7)

36
where
Φ[α, β; u] =

∞
X
Γ(j + α)
j=0

Γ(α)

Γ(β) uj
Γ(j + β) j!

(K8)

is the confluent hypergeometric function of the first kind.
In the limit g− = 0, Eqn. K7 reads
G(x) =

ω+
ω−
eu Φ[ω− , ω+ + ω− + 1; −u] +
Φ[ω+ , ω+ + ω− + 1; u],
ω+ + ω−
ω+ + ω−

(K9)

where u ≡ g+ (x − 1). Using the fact that [49]
eu Φ[α, β; −u] = Φ[β − α, β; u],

(K10)

Eqn. K9 can be written
G(x) =

ω+
ω−
Φ[ω+ + 1, ω+ + ω− + 1; u] +
Φ[ω+ , ω+ + ω− + 1; u],
ω+ + ω−
ω+ + ω−

or, noting Eqn. K8 and the fact that Γ(s + 1) = sΓ(s) for any s,

X  ω+
ω−
Γ(ω+ + ω− + 1) uj
Γ(j + ω+ + 1)
Γ(j + ω+ )
G(x) =
+
,
ω+ + ω− Γ(ω+ + 1)
ω+ + ω− Γ(ω+ )
Γ(j + ω+ + ω− + 1) j!
j

X  ω+
(j + ω+ )Γ(j + ω+ )
ω−
Γ(j + ω+ )
(ω+ + ω− )Γ(ω+ + ω− )
uj
=
+
,
ω+ + ω−
ω+ Γ(ω+ )
ω+ + ω− Γ(ω+ )
(j + ω+ + ω− )Γ(j + ω+ + ω− ) j!
j
=

X Γ(j + ω+ )
j

Γ(ω+ )

Γ(ω+ + ω− ) uj
Γ(j + ω+ + ω− ) j!

(K11)

(K12)
(K13)
(K14)

= Φ[ω+ , ω+ + ω− ; u],

(K15)

as in Eqn. 128. The marginal pn is obtained by pn = ∂xn [G(x)]0 /n!; using Eqn. K15 and the derivative of the confluent
hypergeometric function,
∂un Φ[α, β; u] =

Γ(n + α) Γ(β)
Φ[α + n, β + n; u],
Γ(α) Γ(n + β)

(K16)

one obtains
pn =

n
g+
Γ(n + ω+ ) Γ(ω+ + ω− )
Φ[ω+ + n, ω+ + ω− + n; −g+ ],
n! Γ(ω+ ) Γ(n + ω+ + ω− )

(K17)

as in Eqn. 129.

[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]

M. B. Elowitz, A. J. Levine, E. D. Siggia, and P. S. Swain, Science 297, 1183 (2002).
E. M. Ozbudak, M. Thattai, I. Kurtser, A. D. Grossman, and A. van Oudenaarden, Nat Genet 31, 69 (2002).
J. M. Pedraza and A. van Oudenaarden, Science 307, 1965 (2005).
A. Raj, C. S. Peskin, D. Tranchina, D. Y. Vargas, and S. Tyagi, Plos Biol 4, e309 (2006).
J. S. van Zon and P. R. ten Wolde, Physical Review Letters 94, 128103 1 (2005).
J. S. van Zon and P. R. ten Wolde, Journal of Chemical Physics 123, 234910 1 (2005).
R. J. Allen, D. Frenkel, and P. R. ten Wolde, Journal of Chemical Physics 124, 194111 1 (2006).
C. Valeriani, R. Allen, M. Morelli, D. Frenkel, and P. R. ten Wolde, Journal of Chemical Physics 127, 114109 (2007).
M. J. Morelli, R. J. Allen, S. Tanase-Nicola, and P. R. ten Wolde, Journal of Chemical Physics 128, 045105 1 (2008).
B. Munsky and M. Khammash, Journal of Chemical Physics 124, 044104 (2006).
B. Munsky and M. Khammash, Journal of Computational Physics 226, 818 (2007).
H. El Samad, M. Khammash, L. Petzold, and D. Gillespie, Int. J. Robust and Nonlinear Control 15, 691 (2005).
S. Lampoudi, D. T. Gillespie, and L. R. Petzold, J. Chem. Phys. 130, 094104 (2009).
D. Gillespie, M. Roh, and L. Petzold, J. Chem. Phys. 130, 174103 (2009).

37
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]
[26]
[27]
[28]
[29]
[30]
[31]
[32]
[33]
[34]
[35]
[36]
[37]
[38]
[39]
[40]
[41]
[42]
[43]
[44]
[45]
[46]
[47]
[48]
[49]

M. Chevalier and H. El-Samad, Journal of Chemical Physics 131, 054102 (2009).
F. Didier, T. A. Henzinger, M. Mateescu, and V. Wolf, HiBi09 - High Performance Computational Systems Biology (2009).
N. G. van Kampen, Stochastic processes in physics and chemistry (Amsterdam: North-Holland, 1992).
C. Gardiner, Handbook of Stochastic Methods: for Physics, Chemistry and the Natural Sciences (Springer; 3rd edition,
2004).
R. Zwanzig, Nonequilibrium Statistical Mechanics (Oxford University Press, USA; 1 edition, 2001).
T. B. Kepler and T. C. Elston, Biophys. J. 81, 3116 (2001).
P. S. Swain, M. B. Elowitz, and E. D. Siggia, Proc Natl Acad Sci USA 99, 12795 (2002).
J. Hasty, F. Issacs, M. Dolnik, D. McMillen, and J. J. Collins., Chaos 11, 207 (2001).
J. Hasty, J. Pradines, M. Dolnik, and J. J. Collins, Proc. Nat. Acad. Sci. USA 97, 2075 (2000).
W. Bialek, Advances in Neural Information Processing 13, TK Leen, TG Dietterich and V Tresp 13, 103 (2001).
P. Mehta, R. Mukhopadhyay, and N. S. Wingreen, Physical Biology 5, 026005 (2008).
M. Thattai and A. van Oudenaarden, Proc Natl Acad Sci USA 98, 8614 (2001).
J. E. M. Hornos, D. Schultz, G. C. P. Innocentini, J. Wang, A. M. Walczak, J. N. Onuchic, and P. G. Wolynes, Phys. Rev.
E 72, 051907 (2005).
A. M. Walczak, A. Mugler, and C. H. Wiggins, Proc Natl Acad Sci USA 106, 6529 (2009).
A. Mugler, A. M. Walczak, and C. H. Wiggins, Phys. Rev. E 80, 041921 (2009).
A. Mugler, E. Ziv, I. Nemenman, and C. H. Wiggins, arXiv q-bio.MN (2008), 0811.2834v1.
W. Bialek and S. Setayeshgar, Proc Natl Acad Sci USA 102, 10040 (2005).
G. Tkacik, A. M. Walczak, and W. Bialek, Phys Rev E 80, 031920 (2009).
A. M. Walczak, G. Tkacik, and W. Bialek, Phys Rev E p. 041905 (2010).
S. Iyer-Biswas, F. Hayot, and C. Jayaprakash, Phys. Rev. E 79, 31911 (2009).
P. B. Warren, S. Tanase-Nicola, and P. R. ten Wolde, Journal of Chemical Physics 125, 1449041 (2006).
S. Tănase-Nicola, P. B. Warren, and P. R. ten Wolde, Phys. Rev. Lett. 97, 068102 (2006).
J. Paulsson, Nature 427, 415 (2004).
J. Elf and M. Ehrenberg, Genome Res. 13, 2475 (2003).
M. Doi, Journal of Physics A: Mathematical and General 9, 1465 (1976).
Y. B. Zel’Dovich and A. A. Ovchinnikov, Soviet Journal of Experimental and Theoretical Physics 47, 829 (1978).
L. Peliti, Journal of Physics A: Mathematical and General 19, L365 (1986).
J. Sakurai, Modern quantum mechanics (Pearson Education India, 1985).
L. Bintu, N. E. Buchler, H. G. Garcia, and et al, Current Opinion in Genetics and Development 15, 116 (2005).
L. Bintu, N. E. Buchler, H. G. Garcia, and et al., Current Opinion in Genetics and Development 15, 125 (2005).
A. Raj and A. van Oudenaarden, Cell 135, 216 (2008).
I. Golding, J. Paulsson, S. M. Zawilski, and E. C. Cox, Cell 123, 1025 (2005).
A. M. Walczak, M. Sasai, and P. Wolynes, Biophys. J. 88, 828 (2005).
D. T. Gillespie, J Phys Chem 81, 2340 (1977).
W. Koepf, Hypergeometric summation: an algorithmic approach to summation and special function identities (Braunschweig, Germany: Vieweg, 1998).

