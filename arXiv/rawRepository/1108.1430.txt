Bayesian hidden Markov model analysis of single-molecule force spectroscopy:
Characterizing kinetics under measurement uncertainty
John D. Chodera,1, âˆ— Phillip Elms,1, 2, 3 Frank NoeÌ,4 Bettina Keller,4 Christian M. Kaiser,1, 5 Aaron
Ewall-Wice,6 Susan Marqusee,1, 7, 3 Carlos Bustamante,1, 7, 3, 5, 8, 9 and Nina Singhal Hinrichs10

arXiv:1108.1430v1 [cond-mat.stat-mech] 6 Aug 2011

1

California Institute of Quantitative Biosciences (QB3), University of California, Berkeley, CA 94720, USA
2
Biophysics Graduate Group, University of California, Berkeley, CA 94720, USA
3
Jason L. Choy Laboratory of Single Molecule Biophysics,
Institute for Quantitative Biosciences, University of California, Berkeley, CA 94720, USA
4
DFG Research Center Matheon, FU Berlin, Arnimallee 6, 14195 Berlin, Germany
5
Department of Physics, University of California, Berkeley, CA 94720, USA
6
University of Chicago, IL 60637, USA
7
Department of Molecular & Cell Biology, University of California, Berkeley, CA 94720, USA
8
Department of Chemistry, University of California, Berkeley, CA 94720, USA
9
Howard Hughes Medical Institute, University of California, Berkeley, CA 94720, USA
10
Departments of Statistics and Computer Science, University of Chicago, IL 60637, USA
(Dated: March 14, 2015)

Single-molecule force spectroscopy has proven to be a powerful tool for studying the kinetic behavior of biomolecules. Through application of an external force, conformational states with small
or transient populations can be stabilized, allowing them to be characterized and the statistics of individual trajectories studied to provide insight into biomolecular folding and function. Because the
observed quantity (force or extension) is not necessarily an ideal reaction coordinate, individual observations cannot be uniquely associated with kinetically distinct conformations. While maximumlikelihood schemes such as hidden Markov models have solved this problem for other classes of
single-molecule experiments by using temporal information to aid in the inference of a sequence of
distinct conformational states, these methods do not give a clear picture of how precisely the model
parameters are determined by the data due to instrument noise and finite-sample statistics, both significant problems in force spectroscopy. We solve this problem through a Bayesian extension that
allows the experimental uncertainties to be directly quantified, and build in detailed balance to further reduce uncertainty through physical constraints. We illustrate the utility of this approach in
characterizing the three-state kinetic behavior of an RNA hairpin in a stationary optical trap.

I.

INTRODUCTION

Recent advances in biophysical measurement have led to
an unprecedented ability to monitor the dynamics of single biological macromolecules, such as proteins and nucleic
acids [1]. As a new approach to probing the behavior of biological macromolecules, these experiments promise to change
the way we study folding, dynamics, catalysis, association,
transcription, translation, and motility, providing otherwiseinaccessible information about microscopic kinetics, energetics, mechanism, and the stochastic heterogeneity inherent in
these processes. Advances in instrumentation for optical force
spectroscopy in particular have produced instruments of extraordinary stability, precision, and temporal resolution [2, 3]
that have already demonstrated great utility in the study
of biomolecules in the presence of externally perturbative
forces [4â€“6]. Under external force, it becomes possible to stabilize and characterize short-lived conformational states, such
as protein folding and unfolding intermediates [7â€“9].
In a typical single-molecule optical trapping experiment, a
protein or nucleic acid is tethered to two polystyrene beads
by dsDNA handles that prevent the molecule under study
from interacting with the beads (see Figure 1). The handlebiomolecule-handle assemblyâ€”referred to as a fiberâ€”is associated with the beads through tight noncovalent interactions,

âˆ—

Corresponding author

with one bead held in an optical trap and the other either suctioned to a micropipette or held in a second optical trap. During an experiment, the position of the bead within the laser
trap is monitored, and either the relative displacement from
the trap center or the total force on the bead is recorded, resulting in a timeseries such as the one depicted in Figure 2.
The instrument can generally be operated in several modes: a
force ramp mode, in which the trap is translated rapidly enough
to potentially carry the system out of equilibrium; an equilibrium passive mode, in which the trap is held fixed; and a constant force-feedback mode, in which the trap is continually repositioned to maintain a set constant force on the fiber. Here, we
concern ourselves with the latter two classes of experiment,
though nonequilibrium experiments remain an exciting topic
of active research [10].
Often, the dynamics observed in these experiments appears to be dominated by stochastic transitions between two
or more strongly metastable conformational states [11, 12]â€”
regions of conformation space in which the system remains
for long times before making a transition to another conformational state. These transitions are generally well-described
by first-order kinetics [13]. While visual inspection of the dynamics may suggest the clear presence of multiple metastable
states, quantitative characterization of these states is often difficult. First, the observed force or extension is unlikely to correspond to a true reaction coordinate easily able to separate
all metastable states [14â€“17], and second, measurement noise
may further broaden the force or extension signatures of individual states, increasing their overlap. Attempting to separate

2
Optical Trap

DNA

Folded
Pipette
Tip

k1

k2

k-1

k-2

RNA

Partially
Folded

Unfolded

= Strepavadin
= Anti-Digoxigenin Antibody
= Biotin
= Digoxigenin
= Polystyrene Bead

FIG. 1. Single-molecule optical trapping configuration. The
biomolecule of interestâ€”here, the p5ab RNA hairpinâ€”is tethered to two polystyrene beads by dsDNA handles. The fluctuating force on one bead held in an optical trap is monitored,
while the other bead is held suctioned to a micropipette tip.
Conformational transitions of the hairpinâ€”such as transitions
among the three kinetically metastable states illustrated hereâ€”
are observed indirectly through motion of the bead in the trap.

these states by simply dividing the observed force or extension
range into regions, following current practice [18, 19], can often lead to a high degree of state mis-assignment that results in
the estimated rate constants and state distributions containing
a significant amount of error [20] (see Supplementary Material:
Comparison with threshold model).
Hidden Markov models (HMMs) [21], which use temporal information in addition to the instantaneous value of
the observable (force or extension) to determine which conformational states the system has visited during the experiment, have provided an effective solution to the hidden
state problem in many other classes of single-molecule experiments, such as ion channel currents [22â€“25], single-molecule
FRET [26â€“30], and the stepping of motor proteins [31â€“33]. In
applying hidden Markov modeling to the analysis of singlemolecule force spectroscopy data, the observed force or extension trace is assumed to come from a realization of an
underlying Markov chain, where the system makes historyindependent transitions among a set of discrete conformational states with probabilities governed by a transition or rate
matrix. Data, in the form of force or bead-to-bead extension
measurements, is sampled at an interval that ensures that sequential observations satisfy the Markov property of historyindependence, though the appropriate interval depends on
the properties of the experimental configuration. Under a
given set of external force conditions, each state has a distribution of forces or extensions associated with it. Given observed timeseries data for forces or extensions, the maximum
likelihood estimate (MLE) of the model parameters (transition
rates and state force or extension distributions) and sequence
of hidden states corresponding to the observed data can be
determined by standard methods [34, 35], as demonstrated in
recent work [36].
Unfortunately, this approach has a number of significant
drawbacks. Due to technical limitations, experiments often
suffer from limited statisticsâ€”the events of interest (transi-

tions between states or visits to rare states) may occur only
a few times during the course of the measurement, and data
for additional fibers is time-consuming to collect. As a result,
while the MLE yields the most likely set of model parameters, there may be enormous uncertainty in some of these parameters, and the uncertainty in multiple parameters may be
correlated in complex nonlinear ways. While methods exist
for estimating the standard error or confidence intervals from
MLHMMs [37], these schemes can be prohibitively costly for
long traces, and may still significantly underestimate the statistical error for short traces due to the normally-distributed
error approximation inherent in the approach. The high cost
(both in terms of instrument and experimenter time) of collecting additional data also means that it is not a simple task to
judge how much data need be collected to test a particular hypothesis in a statistically meaningful way. Worse yet, the standard algorithms employed to find the MLE may not even find
the true maximum likelihood solution, instead converging to
a local maximum in likelihood that is far from optimal [38].
Here, we resolve this issue through the use of a Bayesian
extension of hidden Markov models [39â€“42] applicable to single molecule force experiments. By sampling over the posterior distribution of model parameters and hidden state assignments instead of simply finding the most likely values,
the experimenter is able to accurately characterize the correlated uncertainties in both the model parameters (transition
rates and state force or extension distributions) and hidden
state sequences corresponding to observed data. Additionally, prior information (either from additional independent
measurements or physical constraints) can be easily incorporated. We also include a reversibility constraint on the transition matrixâ€”in which microscopic detailed balance is imposed on the kinetics, as dictated by the physics of equilibrium
systems [43]â€”which has been shown to significantly reduce
statistical uncertainties in data-poor conditions [44, 45]. The
framework we present is based on Gibbs sampling [46, 47], allowing simple swap-in replacement of models for observable
distributions, extension to multiple observables, and alternative models for state transitions. Additionally, the Bayesian
method provides a straightforward way to model the statistical outcome and assess the utility of additional experiments
given some preliminary data, allowing the experimenter a
powerful tool for assessing whether the cost of collecting additional data is outweighed by their benefits. A Matlab implementation of this approach is available online [http://
simtk.org/home/bhmm].

II.

HIDDEN MARKOV MODELS FOR FORCE
SPECTROSCOPY

Suppose the temporal behavior of some observable O(x)
that is a function of molecular configuration xâ€”here, generally force or molecular extensionâ€”is observed at temporal intervals âˆ†t to produce a timeseries ot , where t = 0, 1, . . . , L.
An instantaneous observation ot does not necessarily contain
enough information to unambiguously identify the current
conformational state the molecule occupies; to infer the hidden state, we must also make use of the temporal information
in the observed trace. We restrict ourselves to consideration of
scalar functions O(x), but the generalization to multidimensional probes (or multiple probes, such as combined force and
fluorescence measurements [48]) and multiple observed tem-

3
poral traces is straightforward.
We presume the system under study has M kinetically distinct states, in the sense that the system generally remains in
a given state for several observation intervals âˆ†t, but these
states may not necessarily represent highly populated states
of the system at equilibrium. We treat these conformational
states as the hidden states of the model, because we cannot directly observe the identity of the metastable state in which the
system resides. The hidden Markov model presumes the observed data O â‰¡ {ot } was generated according to the following model dependent on parameters Î˜ â‰¡ {T, E}, where T
is an M Ã— M row-stochastic transition matrix and E a set of
emission parameters governing the observable (force or extension) distributions for each of the M hidden states, and prior
information about the initial state distribution Ï,
P(s0 ) = Ïs0
P(st | stâˆ’1 , T) = Tstâˆ’1 st , t â‰¥ 1
P(ot | st , est ) = Ï•(ot | est ).

(2)

The initial state distribution Ï reflects our knowledge of the
initial conditions of the experiment that collected data o. In the
case that the experiment was prepared in equilibrium, Ï corresponds to the equilibrium distribution Ï€ of the model transition matrix T; if the experiment was prepared out of equilibrium, Ï may be chosen to reflect some other prior distribution
(e.g. the uniform prior).
State transitions (stâˆ’1 â†’ st ) are governed by the discrete
transition probability Tstâˆ’1 st . The Markov property of HMMs
prescribes that the probability that a system originally in state
i at time t is later found in state j at time t+1 is dependent only
on knowledge of the state i, and given by the corresponding
matrix element Tij of the (row-stochastic) transition matrix T.
Alternatively, one could instead use the rate matrix K, related
to the transition matrix T through the equation T = eKâˆ†t .
If the processes described by T or K are slow compared to
the observation interval âˆ†t, then we can easily estimate the
rate matrix from the associated transition matrix in a way that
avoids the matrix logarithm, through the expansion K â‰ˆ (T âˆ’
I)/âˆ†t, where I denotes the M Ã— M identity matrix.
The probabilistic â€œemissionâ€ of observables from each state
(st â†’ ot ) is governed by the continuous emission probability
Ï•(ot | est ), parametrized by observable emission parameters e.
For example, in the force spectroscopy applications described
here, Ï•(o | es ) is taken to be a univariate normal (Gaussian)
distribution parameterized by a mean Âµ and variance Ïƒ 2 that
characterize each state, such that ei â‰¡ {Âµi , Ïƒi2 }. Other choices
of observable distribution can easily be substituted in a modular way without affecting the structure of the algorithms presented here.
Given the HMM process specified in Eq. 1, the probability
of observing data O given the model parameters Î˜ is then,
P (O | Î˜) =

X
S

Ïs0 Ï•(o0 | es0 )

L
Y
t=1

X

Tstâˆ’1 st Ï•(ot | est ), (3)

â‰¡

M X
M
X
s0 =1 s1 =1

S

M
X

Â·Â·Â·

.

(4)

sL =1

If multiple independent traces {ot } are available, the probability P (O | Î˜) is simply the product of Eq. 3 for the independent
traces.

A.

Maximum likelihood hidden Markov model
(MLHMM)

The standard approach to construct an HMM from observed data is to compute the maximum likelihood estimator
(MLE) for the model parameters Î˜ â‰¡ {T, E}, which maximize the probability of the observed data O given the model,

(1)

In diagrammatic form, the observed state data {ot } and corresponding hidden state history {st } can be represented
Ï
T
T
T
T
âˆ’â†’ s0 âˆ’â†’ s1 âˆ’â†’ s2 âˆ’â†’ Â· Â· Â· âˆ’â†’ sL
â†“Ï•
â†“Ï•
â†“Ï•
â†“Ï•
o0
o1
o2
oL

where the sum over hidden state histories S is shorthand for

Î˜Ì‚ = arg max P (O | Î˜),
Î˜

(5)

yielding MLE estimates of transition matrix TÌ‚ and state emission parameters EÌ‚. Typically, determination of the model
parameters Î˜ is carried out using the Baum-Welch algorithm [34].
Once the MLE parameters Î˜Ì‚ are determined, the most
likely hidden state history that produced the observations O
can be determined using these parameters:
SÌ‚ = arg max P (S | O, Î˜Ì‚).
S

(6)

This is typically carried out using the Viterbi algorithm [35], a
classic example of dynamic programming.

B.

Bayesian hidden Markov model (BHMM)

Instead of simply determining the model that maximizes
the likelihood of observing the data O given the model parameters Î˜, we can make use of Bayesâ€™ theorem to compute the
posterior distribution of model parameters given the observed
data:
P (Î˜ | O) âˆ P (O | Î˜)P (Î˜).

(7)

Here, P (Î˜) denotes a prior distribution that encodes any a priori information we may have about the model parameters Î˜.
This prior information might include, for example, physical
constraints (such as ensuring the transition matrix satisfies detailed balance) or prior rounds of inference from other independent experiments.
Making use of the likelihood (Eq. 3), the model posterior is
then given by,
P (Î˜ | O) âˆ P (Î˜)

X
S

Ïs0 Ï•(o0 | es0 )

L
Y

Tstâˆ’1 st Ï•(ot | est ).(8)

t=1

Drawing samples of Î˜ from this distribution will, in principle, allow the confidence with which individual parameters and
combinations thereof are known, given the data (and subject
to the validity of the model of Eq. 1 in correctly representing
the process by which the observed data is generated). While
the posterior P (Î˜|O) is complex, we could in principle use

4
a Markov chain Monte Carlo (MCMC) approach [47] to sample it. In its current form, however, this would be extremely
expensive due to the sum over all hidden state histories S
appearing in ratios involving Eq. 8. Instead, we introduce
the hidden state histories S as an auxiliary variable, sampling
from the augmented posterior,
"
#
L
Y
P (Î˜, S | O) âˆ Ïs0 Ï•(o0 | es0 )
Tstâˆ’1 st Ï•(ot | est ) P (Î˜).
t=1

(9)
which makes it much less costly to compute the ratios required
for MCMC on the augmented (Î˜, S) parameter space.
If we presume the prior is separable, such that P (Î˜) â‰¡
P (T)P (E), we can sample from the augmented posterior
(Eq. 9) using the framework of Gibbs sampling [47], in which the
augmented model parameters are updated by sampling from
the conditional distributions,
P (S | T, E, O) âˆ Ïs0 Ï•(o0 | es0 )

L
Y

Tstâˆ’1 st Ï•(ot | est )

t=1

P (T | E, S, O) = P (T | S) âˆ P (T)

L
Y

Tstâˆ’1 st

1.

We first initialize the observed distributions of each state by
fitting a Gaussian mixture model with M states to the pooled
observed data O, ignoring temporal information:
P (O | Ï€, E) =

P (E | S, T, O) = P (E | S, O) âˆ P (E)

III.
A.

ALGORITHMS

Generating an initial model

To initialize either computation of the MLHMM or sampling from the posterior for the BHMM, an initial model that
respects any constraints imposed in the model prior P (Î˜)
must be selected. Here, we employ a Gaussian observable
distribution model for Ï•(o | e) (Eq. 11) and enforce that the
transition matrix T satisfy detailed balance.

(12)

Ï€m = Nm /Ntot
âˆ’1
Âµm = N m

t=0

where Âµ is the mean force or extension characterizing a particular state, and Ïƒ is the standard deviation or width of forces or
extensions corresponding to that state. We note that marginal
posterior distributions of each mean P (Âµi |O) reflect the statistical uncertainty in how well the mean force or position is
determined, and need not correspond to the standard deviation Ïƒi , which may be much broader (or narrower, depending
on the situation).

2
Ï€m Ï•(ot | Âµm , Ïƒm
),

where the state observable emission probability vector E â‰¡
2
{e1 , . . . , eM } and em â‰¡ {Âµm , Ïƒm
} with Âµm denoting the ob2
servable mean and Ïƒm the variance for state m for the Gaussian mixture model. The vector Ï€ is composed of equilibrium
P
state populations {Ï€1 , . . . , Ï€M } with Ï€m â‰¥ 0 and M
m=1 Ï€m =
1.
A first approximation to Ï€ and E is computed by pooling
and sorting the observed ot , and defining M indicator functions hm (o) that separate the data into M contiguous regions
of the observed range of o of roughly equal population. Let
PL
total number of observations
Nm â‰¡
t=0 hm (ot ) denote the P
falling in region m, and Ntot = M
m=1 Nm . The initial parameters are then computed as,

Ï•(ot | est ). (10)

The equalities on the second and third lines reflect the conditional independence of the hidden Markov model defined
by Eq. 1. When only the model parameters Î˜ â‰¡ {T, E}
or the hidden state histories S are of interest, we can simply marginalize out the uninteresting variables by sampling
from the augmented joint posterior for {T, E, S} and examine
only the variables of interest. In addition, the structure of the
Gibbs sampling scheme above allows individual components
(such as the observable distribution model Ï•(o | e) or transition probability matrix T) to be modified without affecting the
structure of the remainder of the calculation.
In the illustrations presented here, we employ a Gaussian
observable distribution model for Ï•(o | e),


1 (o âˆ’ Âµ)2
1
, (11)
exp âˆ’
Ï•(o | e) = Ï•(o | Âµ, Ïƒ 2 ) = âˆš
2
Ïƒ2
2Ï€Ïƒ

L X
M
Y
t=0 m=1

t=1
L
Y

Observable parameter estimation

L
X

ot hm (ot )

(13)

t=0
2
âˆ’1
Ïƒm
= Nm

L
X
(ot âˆ’ Âµm )2 hm (ot ).

(14)

t=0

This approximation is then improved upon by iterating the
expectation-maximization procedure described by Bilmes [49],
0
âˆ’1
Ï€m
= Ntot

L
X

Ï‡m (ot , E, Ï€)

t=0
0
Âµ0m = (Ï€m
Ntot )âˆ’1

L
X

ot Ï‡m (ot , E, Ï€)

t=0
2

0
Ïƒ 0 m = (Ï€m
Ntot )âˆ’1

L
X
(ot âˆ’ Âµ0m )2 Ï‡m (ot , E, Ï€)

(15)

t=0

where the function Ï‡m (o, E, Ï€) is given by the fuzzy membership function,
Ï‡m (o, E, Ï€) =

Ï€m Ï•(o | em )
.
M
P
Ï€l Ï•(o | el )

(16)

l=1

The iterative procedure is terminated at iteration j when the
change in the parameters {Ï€, Âµ, Ïƒ 2 } falls below a certain relative threshold, such as kÏ€ [j] âˆ’ Ï€ [jâˆ’1] k2 /kÏ€ [j] k2 < 10âˆ’4 .

2.

Transition matrix estimation

Once initial state observable emission parameters E are determined, an initial transition matrix is estimated using an iterative likelihood maximization approach that enforces detailed
balance [50]. First, a matrix of fractional transition counts
C â‰¡ (cij ) is estimated using the membership function:
cij =

L
X
t=1

Ï‡i (otâˆ’1 , E, Ï€) Ï‡j (ot , E, Ï€)

(17)

5
A symmetric M Ã— M matrix X â‰¡ (xij ) is initialized by
(18)

xij = xji = cij + cji .

The iterative procedure described in Algorithm 1 of [50] is then
applied. For each update iteration, we first update the diagonal elements of X:
M

x0ii

In practice, the logarithms of these quantities are computed
instead to avoid numerical underflow.
The aggregate matrix of expected transition counts
C â‰¡ (cij ) is then computed from Î as,
cij =

M

X
X
cii (xiâˆ— âˆ’ xii )
; ciâˆ— =
cij ; xiâˆ— =
xij , (19)
=
ciâˆ— âˆ’ cii
j=1
j=1

followed by the off-diagonal elements:
âˆš
âˆ’b + b2 âˆ’ 4ac
(20)
x0ij = x0ji =
2a
where the quantities a, b, and c are computed from X and C,
a â‰¡ ciâˆ— âˆ’ cij + cjâˆ— âˆ’ cji

This count matrix is used to update the maximum-likelihood
transition matrix T using the method of Prinz et al. [50] described in the previous section.
The state observable distribution parameters E are then updated from the Î³ti . For the univariate normal distribution applied to force spectroscopy data here, we update the mean Âµi
and variance Ïƒi2 for state i using the scheme,
L
P

(22)

Tij = xij /xiâˆ— .

Note that the equilibrium probability vector Ï€ computed during the Gaussian mixture model fitting is not respected during
this step.

B.

Fitting a maximum likelihood HMM

The HMM model parameters Î˜ â‰¡ {T, E} are fit to the observed data O through use of the expectation-maximization
(EM) algorithm [51]. This is an iterative procedure, where the
model parameters are subsequently refined through successive iterations. The initial HMM is usually quick to compute,
and can give the experimenter a rough idea of the model parameters, as well as providing a useful starting point for sampling models from the Bayesian posterior.
During each iteration, the Baum-Welch algorithm [34] is
used to compute Î â‰¡ (Î¾tij ), which represents the probability that the system transitions from hidden state i at time t âˆ’ 1
to hidden state j at time t, and Î³ti , the probability that the system occupied state i at time t. This is accomplished by first
executing the forward algorithm,
(
Ïj Ï•(o0 | ej )
t=0
P
Î±tj =
(23)
Ï•(ot | ej ) M
Î±
T
t = 1, . . . , L
ij
(tâˆ’1)i
i=1
followed by the backward algorithm,
(
1
Î²ti = PM
j=1 Tij Ï•(ot+1 | ej )Î²(t+1)j

Î¾tij = Î±ti Ï•(ot+1 | ei )Tij Î²(t+1)j /

M
X

Î±T i

(25)

i=1
M
X
j=1

Î¾tij

=

(26)

L
P

ot Î³ti

t=0
L
P
t=0

;
Î³ti

2
Ïƒ0 i

=

(ot âˆ’ Âµ0i )2 Î³ti

t=0
L
P
t=0

.

(28)

Î³ti

Once the model parameters have been fitted by iteration of
the above update procedure to convergence (which may only
converge to a local maximum of the likelihood), the most likely
hidden state sequence can be determined given the observations O and the MLE model Î˜Ì‚ using the Viterbi algorithm [35].
Like the forward-backward algorithm employed in the BaumWelch procedure, the Viterbi algorithm also has a forward recursion component,
(
Ïj Ï•(ot | ej )
t=0
jt =
(29)
Ï•(ot | ej ) maxi i(tâˆ’1) Tij t = 1, . . . , L
(
1
t=0
Î¦jt =
arg maxi i(tâˆ’1) Tij t = 1, . . . , L
as well as a reverse reconstruction component to compute the
most likely state sequence SÌ‚,
(
arg maxi it t = L
sÌ‚t =
(30)
Î¦sÌ‚t+1 (t+1)
t = (L âˆ’ 1), . . . , 0

C.

Sampling from the posterior of the BHMM

Sampling from the posterior of the BHMM (Eq. 8) proceeds
by rounds of Gibbs sampling, where each round consists of
an update of the augmented model parameters {T, E, S} by
sampling
S0 | T, E, O âˆ¼ P (S0 | T, E, O)
T0 | S0
âˆ¼ P (T0 | S0 )
E0 | S0 , O
âˆ¼ P (E0 | S0 , O)

t=L
(24)
t = (L âˆ’ 1), . . . , 0

The LÃ—M Ã—M matrix Î is then computed for t = 0, . . . , (Lâˆ’1)
as,

Î³ti =

Âµ0i

(21)

Once a sufficient number of iterations j have been completed
to compute a stable estimate of X (such as the relative convergence criteria kX[j] âˆ’ X[jâˆ’1] k2 /kX[j] k2 < 10âˆ’4 , the maximum
likelihood transition matrix estimate T is computed as

(27)

Î¾tij .

t=0

b â‰¡ ciâˆ— (xjâˆ— âˆ’ xji ) + cjâˆ— (xiâˆ— âˆ’ xij )
âˆ’ (cij + cji )(xiâˆ— âˆ’ xij + xjâˆ— âˆ’ xji )
c â‰¡ âˆ’(cij + cji )(xiâˆ— âˆ’ xij )(xjâˆ— âˆ’ xji ).

Lâˆ’1
X

where the conditional probabilities are given by Eq. 10.

1.

Updating the hidden state sequences

We use a modified form of the Viterbi process to generate
an independent sample of the hidden state history S given the

6
transition probabilities T, state observable distribution parameters E, and observed data O. Like the Viterbi scheme, a forward recursion is applied to each observation trace o, but instead of computing the most likely state history on the reverse
pass, a new hidden state history S is drawn from the distribution P (S | O, T, E). The forward recursion uses the same
forward algorithm as used in Baum-Welch [34],
(
Ïj Ï•(o0 | ej )
t=0
P
Î±tj =
(31)
Î±
T
t = 1, . . . , L
Ï•(ot | ej ) M
ij
i=1 (tâˆ’1)i
In the reverse recursion, we now sample a state sequence by
sampling each hidden state from the conditional distribution
st âˆ¼ P (st | st+1 , . . . , sL ) starting from t = L and proceeding
down to t = 0, where the conditional distribution is given by,
P (st = i | st+1 , . . . , sL )
(
P
Î±ti / M
j=1 Î±tj
P
âˆ
Î±ti Tist+1 / M
j=1 Î±tj Tjst+1

(32)
t=L
t = (L âˆ’ 1), . . . , 0

It is straightforward to show the result of these sampling steps
reconstitutes the probability distribution P (S|T, E, O) (see
Supplementary Material: Proof of state history sampling scheme).

2.

Updating the transition probabilities

If no detailed balance constraint is used and the prior P (T)
is Dirichlet in each row of the transition matrix T, it is possible to generate an independent sample of the transition matrix
from the conditional distribution P (T0 | S0 ) by sampling each
row of the transition matrix from the conjugate Dirichlet posterior using the transition counts from the sampled state sequence S0 [44]. However, because physical systems in the absence of energy input through an external driving force should
satisfy detailed balance, we make use of this constraint in updating our transition probabilities, since this has been demonstrated to substantially reduce parameter uncertainty in the
data-limited regime [44].
The transition matrix is updated using the reversible transition matrix sampling scheme of NoeÌ [44, 52]. Here, an adjusted
count matrix C â‰¡ (cij ) is computed using the updated hidden
state sequence S0 ,
cij = bij +

L
X

Î´istâˆ’1 Î´jst ,

(33)

t=1

where the Kronecker Î´ij = 1 if i = j and zero otherwise, and
B â‰¡ (bij ) is a matrix of prior pseudocounts, which we take
to be zero following the work of NoeÌ et al. [13]. Using the
adjusted count matrix C, a Metropolis-Hastings Monte Carlo
procedure [53] is used to update the matrix and produce a new
sample from P (T0 | S0 ). Two move types are attempted, selected with equal probability, and 1000 moves are attempted to
generate a new sample T0 that is approximately uncorrelated
from the previous T. Prior to starting the Monte Carlo procedure, the vector of equilibrium probabilities for all states Ï€ is
computed according to
TT Ï€ = Ï€.

(34)

The first move type is a reversible element shift. A pair of
states (i, j), i 6= j, are selected with uniform probability, and a

random number âˆ† is selected uniformly over the interval,
âˆ† âˆˆ [max(âˆ’Tii , âˆ’

Ï€j
Tjj ), Tij ].
Ï€i

The changed elements in the proposed transition matrix T0 are
then given by:
Ï€i
âˆ†
Ï€j
Ï€i
âˆ†.
+
Ï€j

0
Tij0 = Tij âˆ’ âˆ† ; Tji
= Tji âˆ’
0
Tii0 = Tii + âˆ† ; Tjj
= Tjj

This move is accepted with probability
ï£± s
ï£²
0 2
(Tij0 )2 + (Tji
)
0
Paccept (T |T) = min 1,
2
ï£³
(Tij ) + (Tji )2
 0 cii  0 cij  0 cjj  0 cji 
Tjj
Tji
Tij
Tii
.
Ã—
Tii
Tij
Tjj
Tji

(35)

This move will leave the vector of stationary probabilities Ï€
unchanged.
The second move type is a row shift. A row i of T is selected
with uniform probability, and a random number Î· chosen uniformly over the interval


1
Î· âˆˆ 0,
1 âˆ’ Tii
and used to update row i of T according to
(
Î·Tij
j = 1, . . . , M, j 6= i
0
Tij =
Î·(Tii âˆ’ 1) + 1 j = i

(36)

This move is accepted with probability


c 
1 âˆ’ Î·(1 âˆ’ Tii ) ii
Paccept (T0 |T) = min 1, Î· (M âˆ’2) Î· (ciâˆ— âˆ’cii )
.
Tii
(37)
The row shift operation will change the stationary distribution
of Ï€ 0 , but it may be efficiently updated with
Ï€i0 =

Î· Ï€j
Ï€i
; Ï€j0 =
.
Ï€i + Î·(1 âˆ’ Ï€i )
Ï€i + Î·(1 âˆ’ Ï€i )

Since this update scheme is incremental, it will accumulate numerical errors over time that cause the updated Ï€ to drift away
from the stationary distribution of the current transition matrix. To avoid this, Ï€ is recomputed from the current sample
of the transition matrix in regular intervals (here, every 100
sampling steps).

3.

Updating the observable distribution parameters

Following the update of the transition matrix T, the observable distribution parameters E are updated by sampling
E from the conditional probability P (E0 | S0 , O). The conditional probability for the observable distribution parameters
for state m, denoted em , is given in terms of the output model
Ï•(o | e) by Bayesâ€™ theorem,
"L
#
Y
P (E | O, S) =
Ï•(ot | est ) P (E).
(38)
t=0

7
An important choice must be made with regards to the
prior, P (E). If the prior is chosen to be composed of independent priors for each state, as in
P (E) =

M
Y

P (em ),

(39)

m=1

then the full BHMM posterior (Eq. 8) will be invariant under
any permutation of the states. This behavior might be undesirable, as the states may switch labels during the posterior sampling procedure; this will require any analysis of the models
sampled from the posterior to account for the possible permutation symmetry in the states. On the other hand, breaking this
symmetry (e.g., by enforcing an ordering on the state mean observables) can artificially restrict the confidence intervals of the
states, which might additionally complicate data analysis.
Here, we make the choice that the prior be separable
(Eq. 39), which has the benefit of allowing the conditional
probability for E (Eq. 38) to be decomposed into a separate
posterior for each state. For each state m, collect all the observations ot whose updated hidden state labels st 0 = m into a
m
single dataset o â‰¡ {on }N
n=1 , where Nm is the total number of
times state m is visited, for the purposes of this update procedure. Then, the observable parameters e for this state are
given by
#
"N
m
Y
Ï•(on | e) P (e). (40)
P (e | o) = P (o | e)P (e) =

where ÂµÌ‚ is the sample mean for o, the samples in state m,
ÂµÌ‚ â‰¡

P (e) âˆ Ïƒ âˆ’1 ,

(41)

which produces the posterior
#
N
1 X
2
(on âˆ’ Âµ) ,
exp âˆ’ 2
2Ïƒ n=1
"

P (e | o) âˆ Ïƒ

âˆ’(N +1)

(42)

where we remind the reader that here and in the remainder of
this section, the symbols e, o, Ïƒ, Âµ, and N refer to em , om , Ïƒm ,
Âµm , and Nm , respectively.
Updating {Âµ, Ïƒ 2 } also proceeds by a Gibbs sampling
scheme, alternately updating Âµ and Ïƒ, as earlier described in
Ref. [52],
Âµ âˆ¼ P (Âµ | Ïƒ 2 , o)
Ïƒ 2 âˆ¼ P (Ïƒ 2 | Âµ, o)

(43)

The conditional distribution of the mean Âµ is then given by


1
2
(Âµ
âˆ’
ÂµÌ‚)
(44)
P (Âµ | Ïƒ 2 , o) âˆ exp âˆ’
2(Ïƒ 2 /N )

(45)

This allows us to update Âµ according to
Âµ0 âˆ¼ N (ÂµÌ‚, Ïƒ 2 /N )

(46)

The conditional distribution of the variance Ïƒ 2 is given by


N ÏƒÌ‚ 2
P (Ïƒ 2 | Âµ, o) âˆ Ïƒ âˆ’(N +1) exp âˆ’ 2
(47)
2Ïƒ
where the quantity ÏƒÌ‚ 2 , which is not in general identical to the
sample variance, is given by
ÏƒÌ‚ 2 â‰¡

N
1 X
(on âˆ’ Âµ)2 .
N n=1

(48)

A convenient way to update Ïƒ 2 | Âµ, o is to sample a random
variate y from the chi-square distribution with N âˆ’ 1 degrees
of freedom,
y âˆ¼ Ï‡2 (N âˆ’ 1)

(49)

and then update Ïƒ 2 as
2

Ïƒ 0 = N ÏƒÌ‚ 2 /y.

n=1

In the application presented here, we use a Gaussian output
model (Eq. 11) for the state observable distributions P (o | e),
where e â‰¡ {Âµ, Ïƒ 2 }, with Âµ the state mean observable and Ïƒ 2
the variance (which will include both the distribution of the
observable characterizing the state and any broadening from
measurement noise). Other models (including multidimensional or multimodal observation models) are possible, and
require replacing only the observation model Ï•(o | e) and corresponding prior P (e).
We use the (improper) Jeffreys prior [54] which has the
information-theoretic interpretation as the prior that maximizes the information content of the data [55], (suppressing
the state index subscript m),

N
1 X
on
N n=1

(50)

Note that Âµ and Ïƒ 2 can be updated in either order, but the updated values of Âµ or Ïƒ 2 must be used in sampling the not-yetupdated Ïƒ 2 or Âµ, and vice-versa.
Other output probabilities, such as mixtures of normal distributions or other distributions, can be substituted by simply
changing P (E | O, S) and the scheme by which E is updated.

IV.

VALIDATION USING SYNTHETIC DATA

To verify that our BHMM posterior sampling scheme reflects the true uncertainty in the model parameters, we tested
the scheme on synthetic data generated from a model with
known parameters Î˜âˆ— . Given observed data O generated
from P (O | Î˜âˆ— ), sampling from the posterior P (Î˜ | O) using the scheme described in Sampling from the posterior of the
BHMM will provide us with confidence intervals [Î¸low , Î¸high ]
for a specified confidence interval size Î± âˆˆ [0, 1]. If these computed confidence intervals are accurate, we should find that
the true model parameter Î¸âˆ— lies in the computed confidence
(Î±)
(Î±)
interval [Î¸low , Î¸high ] with probability Î±. This can be tested by
generating synthetic observed data O from P (O | Î˜âˆ— ) and
(Î±)
(Î±)
verifying that we find Î¸âˆ— âˆˆ [Î¸low , Î¸high ] in a fraction Î± of these
synthetic experiments.
As an example synthetic model, consider the three-state system intended to mimic a protein with (1) a highly-compliance,
low-force unfolded state, (2) a moderately compliant lowpopulation intermediate at intermediate force, and (3) a lowcompliance, high-force folded state. Here, the term â€œcomplianceâ€ refers to the width of the force or extension distribution
characterizing the state. Parameters of the model are given in
Table I, and the observation interval was taken to be Ï„ = 1
ms. An example realization of a model trajectory, along with

8
6

force / pN

5
4
3
2
1
0
0

1

2

3

4

5
time / s

6

7

8

9

10

FIG. 2. Synthetic force trajectory and inferred state assignments in MLHMM. Observed samples are colored by their hidden
state assignments. Dark horizontal lines terminating in triangles to the right denote state means, while lightly colored bands
indicate one standard deviation on either side of the state mean. The gray histogram on the right side shows the total observed
probability of samples, while the colored peaks show the weighted Gaussian output contribution from each state, and the black
outline the weighted sum of the Gaussian output contributions from the HMM states.

TABLE I. Estimated mean model parameters and confidence intervals for synthetic timeseries data
Estimated Model Parameters
Property
True Value
1 000 observations
10 000 observations
100 000 observations
stationary probability
Ï€1
0.308
0.228 0.480
0.318 0.407
0.324 0.355
0.074
0.244
0.292
0.155
Ï€2
0.113
0.093 0.172
0.124
0.112 0.121
0.042
0.098
0.104
Ï€3
0.579
0.679 0.870
0.558 0.648
0.564 0.599
0.415
0.455
0.531
transition probability
T11
0.980
0.970 0.987
0.972 0.978
0.979 0.981
0.945
0.966
0.978
T12
0.019
0.023 0.045
0.026 0.032
0.020 0.021
0.009
0.021
0.018
0.018
0.003
T13
0.001
0.007 0.001
0.002 0.001
0.001 0.001
0.001
T21
0.053
0.054 0.106
0.067 0.082
0.057 0.061
0.018
0.053
0.052
T22
0.900
0.868 0.931
0.890 0.907
0.897 0.903
0.790
0.870
0.892
T23
0.050
0.078 0.136
0.043 0.056
0.046 0.050
0.035
0.033
0.042
T31
0.001
0.002 0.006
0.001 0.002
0.001 0.001
0.000
0.000
0.000
T32
0.009
0.010 0.019
0.010 0.012
0.009 0.010
0.007
0.008
0.004
T33
0.990
0.988 0.995
0.990 0.992
0.990 0.991
0.978
0.987
0.989
state mean force (pN)
Âµ1
3.000
2.947 3.082
2.998 3.033
3.001 3.013
2.812
2.963
2.990
Âµ2
4.700
4.666 4.721
4.699 4.716
4.702 4.707
4.612
4.683
4.696
Âµ3
5.600
5.597 5.614
5.602 5.607
5.602 5.603
5.583
5.596
5.600
state std dev force (pN)
Ïƒ1
1.000
1.037 1.134
0.992 1.018
0.999 1.007
0.951
0.967
0.991
Ïƒ2
0.300
0.254 0.300
0.287 0.300
0.301 0.305
0.217
0.275
0.296
Ïƒ3
0.200
0.200 0.211
0.203 0.207
0.201 0.203
0.190
0.199
0.200

the MLHMM state assignment, is shown in Figure 2. We generated a trajectory of 100 000 observations, and characterized
the BHMM mean parameter estimate and 95% confidence intervals for a subset of this trajectory of varying lengths. The
results, shown in Table I, show that the confidence intervals
contract as trajectory length increases, as expected, and the
BHMM-computed 95% confidence intervals contain the true
model parameters with the expected statistics. In contrast, a
model created from simply segmenting the observed forces
into disjoint region and assigning state membership based on
the force value alone estimates model parameters with significant bias even for 1 000 000 observations (see Supporting Information).
As a more rigorous test, we sampled 50 random models
from the prior P (Î˜) with two to six states, generated a 10
000 observation synthetic trajectory for each, and accumulated
statistics on the observed fraction of time the true model parameters were within the BHMM confidence intervals for various values of the confidence interval width Î±. The results of
this test are depicted in Supplementary Figure 1. We expect that
the plot traces the diagonal if the observed and expected confi-

dence intervals are identical; an overestimate of the confidence
interval will be above the diagonal, and an underestimate will
fall below it. Because only a finite number of independent
replicates of the experiment are conducted, there is some associated uncertainty with the observed confidence intervals.
The results show that the observed confidence intervals line
up with the expected confidence intervals to within statistical
error, suggesting the BHMM confidence intervals neither underestimate nor overestimate the actual uncertainty in model
parameters.

V.

RNA HAIRPIN KINETICS IN A PASSIVE OPTICAL
TRAP

We illustrate the BHMM approach applied to real force
spectroscopy data by characterizing the average forces and
transition rates among kinetically distinct states of the p5ab
RNA hairpin in an optical trap under passive (equilibrium)
conditions.
The p5ab RNA hairpin from Tetrahymena thermophilia was

9
14.5

force / pN

14
13.5
13
12.5
12
0

10

20

30
time / s

40

50

60

FIG. 3. Experimental force trajectory of the p5ab hairpin and MLHMM state assignments. Observed samples are colored
by their hidden state assignments. Dark horizontal lines terminating in triangles to the right denote state means, while lightly
colored bands indicate one standard deviation on either side of the state mean. The gray histogram on the right side shows the
total observed probability of samples, while the colored peaks show the weighted Gaussian output contribution from each state,
and the black outline the weighted sum of the Gaussian output contributions from the HMM states.

TABLE II. BHMM model estimates for p5ab hairpin data.
Property
Value
Equilibrium probability
Ï€1
0.215 0.236
0.193
Ï€2
0.046 0.050
0.041
Ï€3
0.740 0.762
0.717
Transition probability (âˆ†t = 1 ms)
T11
0.954 0.959
0.950
T12
0.033 0.037
0.029
T13
0.013 0.015
0.011
T21
0.154 0.169
0.139
T22
0.650 0.673
0.627
T23
0.196 0.216
0.180
T31
0.004 0.004
0.003
T32
0.012 0.013
0.011
T33
0.984 0.985
0.983
12.552
State force mean (pN)
Âµ1
12.549 12.544
Âµ2
13.016 13.027
13.006
Âµ3
13.849 13.852
13.848
State force std dev (pN)
Ïƒ1
0.210 0.213
0.207
Ïƒ2
0.201 0.208
0.193
Ïƒ3
0.213 0.214
0.211
Transition rate (sâˆ’1 )

State mean lifetime (ms)

k12
k13
k21
k23
k31
k32
Ï„1
Ï„2
Ï„3

41.4 46.6
36.3
9.1 11.3
7.2
194.7 216.7
173.1
243.7 271.5
219.0
2.6 3.2
2.1
15.0 16.6
13.4
21.9 24.1
20.0
2.9 3.1
2.7
63.1 68.5
58.4

provided by Jin-Der Wen, and prepared as previously described [56]. Within the population of RNA hairpin molecules
in the examined sample, there were two chemically distinct species present in the sample (i.e. as a result of posttranscriptional or other covalent modification during sample
storage), exhibiting either apparent two-state (as reported previously [56]) or three-state behavior (studied here). For the
purposes of testing this method, we examined a fiber that appeared to consistently exhibit three-state behavior upon visual
inspection of the force timeseries data.
The instrument used in this experiment was a dual-beam
counter-propagating optical trap with a spring constant of 0.1

pN/nm. A piezoactuator controlled the position of the trap
and allowed position resolution to within 0.5 nm [57]. Drift
in the instrument was less than 1 nm/minute resulting in a
constant average force within 0.1 pN over the course of a typical 60 s experiment. For these constant trap position experiments, higher frequency data was recorded at 50 kHz recording the voltage corresponding to the force on the tether directly from the position-sensitive detectors. To ensure sequential samples obeyed Markovian statistics, these data were subsampled down to 1 kHz for analysis by the BHMM framework
after examination of autocorrelation functions for trap positions where the hairpin appeared to remain in a single conformational state (see Supplementary Material: Choice of observation
interval).
A single observed force trajectory at a fixed trap position
adequate to cause hopping among multiple states is shown in
Figure 3. The most likely state trajectory from the MLHMM fit
with three states is shown by coloring the observations most
likely to be associated with each state, with bands of color indicating the mean and standard deviation about the mean force
characterizing each state.
Table II lists the BHMM posterior means and confidence
intervals characterizing the three-state model extracted from
this single 60 s observed force trace. Several things are notable about the estimated model parameters. Surprisingly,
while there is a clearly-resolved intermediate-force state (state
2) through which most of the flux from the high- and low-force
states passes (as seen from large K12 and K23 ), there are nontrivial rate constants connecting the high and low force states
directly (K13 ), indicating that while a sequential mechanism
involving passing through the intermediate state is preferred,
it may not be an obligatory step in hairpin formation under
these conditions. While the state mean forces are clearly distinct, the state standard deviationsâ€”which reflect the width
of the observed force distribution characterizing each state,
rather than the uncertainty in state meansâ€”possess overlapping confidence intervals. These standard deviations reflect
not only contributions from both the distribution of extensions
sampled by the hairpin in each conformational state, but also
from fluctuations in the handles and beads, and other sources
of mechanical and electrical noise in the measurement. As
we would expect the unfolded hairpin to be more compliant
(i.e. possess a wider distribution of forces) than the folded hairpin, the inability to distinguish the standard deviations among
states is suggestive that, for this experimental configuration

10
and observation time, the predominant contribution to the observed force distributions for each state may be in the form of
handle or bead fluctuations or other sources of measurement
noise.
Finally, the lifetime of the intermediate-force state is significantly shorter than for the low- and high-force states by nearly
an order of magnitude, and only a few times longer than the
observation interval of 1 msâ€”despite this, the lifetime appears
to be well-determined, as indicated by the narrow confidence
intervals.

VI.

DISCUSSION

We have described an approach to determining the firstorder kinetic parameters and observable (force or extension)
distributions characterizing conformational states in singlemolecule force spectroscopy. By use of a Bayesian extension
of hidden Markov models, we are able to characterize the experimental uncertainty in these parameters due to instrument
noise and finite-size datasets. The use of a detailed balance
constraint additionally helps reduce the experimental uncertainty over standard hidden Markov models, as both transitions into and out of conformational states provide valuable
information about state kinetics and populations in data-poor
conditions [44, 45]. Additionally, the Gibbs sampling framework used to sample from the Bayesian posterior can be easily
extended to incorporate additional nuisance parameters, such
as stochastic models of instrument drift or laser power fluctuations.
We have opted to make use of a reversible transition matrix to describe the statistical kinetic behavior between the observation intervals âˆ†t, but it is possible to use a reversible

[1] F. Ritort, â€œSingle-molecule experiments in biological
physics: methods and applications,â€ J. Phys.: Condens.
Matter, 18, R531â€“R583 (2006).
[2] Keir C. Neuman and Steven M. Block, â€œOptical trapping,â€
Rev. Sci. Instrum., 75, 2787 (2004).
[3] Jeffrey R. Moffitt, Yann R. Chemla, Steven B. Smith, and
Carlos Bustamante, â€œRecent advances in optical tweezers,â€ Annu. Rev. Biochem., 77, 205â€“228 (2008).
[4] Carlos Bustamante, Zev Bryant, and Steven B. Smith,
â€œTen years of tension: single-molecule DNA mechanics,â€
Nature, 421, 423â€“427 (2003).
[5] Michael T. Woodside, CuauhteÌmoc GarcÄ±Ìa-GarcaÌ, and
Steven M. Block, â€œFolding and unfolding single RNA
molecules under tension,â€ Curr. Opin. Chem. Biol., 12,
640â€“646 (2008).
[6] Carlos Bustamante, Wei Cheng, and Yara X. Mejia, â€œRevisiting the central dogma one molecule at a time,â€ Cell,
144, 480â€“497 (2011).
[7] Susan B. Fowler, Robert B. Best, JoseÌ L. Toca Herrera,
Trevor J. Rutherford, Annette Steward, Emanuele Paci,
Martin Karplus, and Jane Clarke, â€œMechanical unfolding
of a titin Ig domain: Structure of unfolding intermediates
revealed by combining AFM, molecular dynamics simulations, NMR and protein engineering,â€ J. Mol. Biol., 322,
841â€“849 (2002).

rate matrix instead by substituting a rate matrix sampling
scheme [58] in the appropriate stage of the Gibbs sampling updates.
While the experimenter must currently choose the number
of conformational states by hand, a number of extensions of
Bayesian hidden Markov models can be used to automatically determine the number of states best supported by the
data, including reversible-jump schemes [59, 60] and variational Bayes methods [61, 62].
We note that the experimenter in principle has access to
the full posterior distribution of models given the observed
data, so that instead of looking at the confidence of single parameters, confidence intervals in more complex functions of
parametersâ€”such as the rates or lifetimes in Table IIâ€”can be
computed, or joint posterior distributions of multiple parameters examined. It is also possible to generate synthetic data
from the current model, or family of models, to examine how
the collection of additional data will further reduce uncertainties or allow discrimination among particular hypotheses. The
field of Bayesian experimental design [63] holds numerous possibilities for selecting how future experiments can maximize
information gain, and whether the information gain from the
collection of additional data will be of sufficient utility to justify the expense.

VII.

ACKNOWLEDGMENTS

The authors thank Sergio Bacallado (Stanford University)
for helpful feedback on this manuscript, and Steve Presse
(UCSF) for engaging discussions on this topic. JDC acknowledges support from a QB3-Berkeley Distinguished Postdoctoral Fellowship. FN acknowledges DFG Grant 825/2. This
work was supported in part by a grant from the NSF (SM).

[8] Ciro Cecconi, Elizabeth A. Shank, Carlos Bustamante,
and Susan Marqusee, â€œDirect observation of the threestate folding of a single protein molecule,â€ Science, 309,
2057â€“2060 (2005).
[9] J. Christof M. Gebhardt, Thomas BornschloÌˆgl, and
Matthias Rief, â€œFull distance-resolved folding energy
landscape of one single protein molecule,â€ Proc. Natl.
Acad. Sci. USA, 107, 2013â€“2018 (2010).
[10] F. Ritort, â€œNonequilibrium fluctuations in small systems:
From physics to biology,â€ Adv. Chem. Phys., 137, 31â€“123
(2008).
[11] Ch. SchuÌˆtte and W. Huisinga, â€œBiomolecular conformations can be identified as metastable states of molecular
dynamics,â€ in Handbook of Numerical Analysis - special volume on computational chemistry, Vol. X, edited by P. G. Ciaret and J.-L. Lions (Elsevier, 2002).
[12] John D. Chodera, Nina Singhal, William C. Swope, Vijay S. Pande, and Ken A. Dill, â€œAutomatic discovery of
metastable states for the construction of markov models
of macromolecular conformational dynamics,â€ J. Chem.
Phys., 126, 155101 (2007).
[13] F. NoeÌ, C. SchuÌˆtte, E. Vanden-Eijnden, L. Reich, and T. R.
Weikl, â€œConstructing the full ensemble of folding pathways from short off-equilibrium simulations,â€ Proc. Natl.
Acad. Sci. USA, 106, 19011â€“19016 (2009).

11
[14] Robert B. Best, Emanuele Paci, Gerhard Hummer, and
Olga K. Dudko, â€œPulling direction as a reaction coordinate for the mechanical unfolding of single molecules,â€ J.
Phys. Chem. B, 112, 5968â€“5976 (2008).
[15] Zu Thur Yew, Michael Schlierf, Matthias Rief, and
Emanuele Paci, â€œDirect evidence of the multidimensionality of the free-energy landscapes of proteins revealed by
mechanical probes,â€ Phys. Rev. E, 81, 031923 (2010).
[16] Greg Morrison, Changbong Hyeon, Michael Hinczewski,
and D. Thirumalai, â€œCompaction and tensile forces determine the accuracy of folding landscape parameters from
single molecule pulling experiments,â€ Phys. Rev. Lett.,
106, 138102 (2011).
[17] John D. Chodera and Vijay S. Pande, â€œSplitting probabilities as a test of reaction coordinate choice in singlemolecule experiments,â€ arÏ‡iv preprint, arXiv:1105.0710
(2011).
[18] Michael T. Woodside, Peter C. Anthony, William M.
Behnke-Parks, Kevan Larizadeh, Daniel Herschlag, and
Steven M. Block, â€œDirect measurement of the full,
sequence-dependent folding landscape of a nucleic acid,â€
Science, 314, 1001â€“1004 (2006).
[19] N. Forns, S. de Lorenzo, M. Manosas, J. M. Huguet, and
F. Ritort, â€œImproving signal/noise resolution in singlemolecule experiments using molecular constructs with
short handles,â€ Biophys. J., 100, 1765â€“1774 (2011).
[20] John D. Chodera, Phillip Elms, William C. Swope, Frank
NoeÌ, and Vijay S. Pande, â€œA robust approach to estimating rates from time-correlation functions,â€ in preparation
(2011).
[21] Lawrence R. Rabiner, â€œA tutorial on Hidden Markov
models and selected applications in speech recognitiion,â€
Proceedings of the IEEE, 77, 257â€“286 (1989).
[22] J. D. Becker, J. Honerkamp, J. Hirsch, U. FroÌˆbe, E. Schlatter, and R. Greger, â€œCompaction and tensile forces determine the accuracy of folding landscape parameters from
single molecule pulling experiments,â€ PfluÌˆgers Arch.,
426, 328â€“332 (1994).
[23] Feng Qin, Anthony Auerbach, and Frederick Sachs, â€œA
direct optimization approach to hidden Markov modeling for single channel kinetics,â€ Biophys. J., 79, 1915â€“1927
(2000).
[24] M. C. M. De Gunst, H. R. KuÌˆnsch, and J. G. Schouten,
â€œStatistical analysis of ion channel data using hidden
Markov models with correlated state-dependent noise
and filtering,â€ J. Am. Stat. Assoc., 96, 805â€“815 (2001).
[25] Feng Qin, â€œRestoration of single-channel currents using
the segmental k-means method based on hidden Markov
modeling,â€ Biophys. J., 86, 1488â€“1501 (2004).
[26] Michael Andrec, Ronald M. Levy,
and David S.
Talaga, â€œDirect determination of kinetic rates from
single-molecule photon arrival trajectories using hidden
Markov models,â€ J. Phys. Chem. A, 107, 7454â€“7464 (2003).
[27] Sean McKinney, Chirlmin Joo, and Taekjip Ha, â€œAnalysis of single-molecule FRET trajectories using hidden
Markov models,â€ Biophys. J., 91, 1941â€“1951 (2006).
[28] Tae-Hee Lee, â€œExtracting kinetics information from
single-molecule fluorescence resonance energy transfer
data using hidden Markov models,â€ J. Phys. Chem. B,
113, 11535â€“11542 (2009).
[29] Ying Li, Xiaohui Qu, Ao Ma, Glenna J. Smith, Norbert F. Scherer, and Aaron R. Dinner, â€œModels of singlemolecule experiments with periodic perturbations reveal
hidden dynamics in RNA folding,â€ J. Phys. Chem. B, 113,

7579â€“7590 (2009).
[30] Irina V. Gopich and Attila Szabo, â€œDecoding the pattern
of photon colors in single-molecule FRET,â€ J. Phys. Chem.
B, 113, 10965â€“10973 (2009).
[31] David A. Smith, Walter Steffen, Rober M. Simmons, and
John Sleep, â€œHidden-Markov methods for the analysis
of single-molecule actomoysin displacement data: The
variance-hidden-Markov method,â€ Biophys. J., 81, 2795â€“
2816 (2001).
[32] Lorin S. Milescu, Ahmet Yildiz, Paul R. Selvin, and
Frederick Sachs, â€œExtracting dwell time sequences from
processive molecular motor data,â€ Biophys. J., 91, 3135
(2006).
[33] Fiona E. MuÌˆllner, Sheyum Syed, Paul R. Sevin, and Fred J.
Sigworth, â€œImproved hidden Markov models for molecular motors, part 1: Basic theory,â€ Biophys. J., 99, 3684â€“3695
(2010).
[34] L. E. Baum, T. Petrie, G. Soules, and N. Weiss, â€œA maximization technique occurring in the statistical analysis
of probabilistic functions of Markov chains,â€ Ann. Math.
Statist., 41, 164â€“171 (1970).
[35] A. J. Viterbi, â€œError bounds for convolutional codes and
an asymptotically optimum decoding algorithm,â€ IEEE
Trans. Info. Theory, 13, 260â€“269 (1967).
[36] M. Kruithof and J. van Noort, â€œHidden Markov analysis
of nucleosome unwrapping under force,â€ Biophys. J., 96,
3708â€“3715 (2009).
[37] Tero Aittokallio and Esa Uusipaikka, Computation of
standard errors for maximum-likelihood estimates in hidden
Markov models, Tech. Rep. (University of Turku, 2008)
technical Report No. 379.
[38] Bernard Merialdo, â€œOn the locality of the forwardbackward algorithm,â€ IEEE Trans. Speech and Audio
Proc., 1, 255â€“257 (1993).
[39] Christian P. Robert, Giles Celeux, and Jean Diebolt,
â€œBayesian estimation of hidden Markov chains: A
stochastic implementation,â€ Stat. Prob. Lett., 16, 77â€“83
(1993).
[40] Siddhartha Chib, â€œCalculating posterior distributions and
modal estimates in Markov mixture models,â€ J. Econometrics, 75, 79â€“97 (1996).
[41] Steven L. Scott, â€œBayesian methods for hidden Markov
models: Recursive computing in the 21st century,â€ J. Am.
Stat. Assoc., 97, 337â€“351 (2002).
[42] Tobias RydeÌn, â€œEM versus Markov chain Monte Carlo for
estimation of hidden Markov models: A computational
perspective,â€ Bayesian Analysis, 3, 659â€“688 (2008).
[43] N. G. van Kampen, Stochastic processes in physics and chemistry, 2nd ed. (Elsevier, 1997).
[44] Frank NoeÌ, â€œProbability distributions of molecular observables computed from Markov models,â€ J. Chem.
Phys., 128, 244103 (2008).
[45] Philipp Metzner, Frank NoeÌ, and Christof SchuÌˆtte, â€œEstimating the sampling error: Distribution of transition matrices and functions of transition matrices for given trajectory data,â€ Phys. Rev. E, 80, 021106 (2009).
[46] Stuart Geman and Donald Geman, â€œStochastic relaxation,
Gibbs distributions and the Bayesian restoration of images,â€ IEEE Trans. Pattern Anal., 6, 721â€“741 (1984).
[47] Jun S. Liu, Monte Carlo strategies in scientific computing, 2nd
ed. (Springer-Verlag, New York, 2002).
[48] Matthew J. Comstock, Taekjip Ha,
and Yann R.
Chemla, â€œUltrahigh-resolution optical trap with singlefluorophore sensitivity,â€ Nature Methods, 8, 335â€“340

12
(2011).
[49] Jeff A. Bilmes, A gentle tutorial of the EM algorithm and its
application to parameter estimation for Gaussian mixture and
hidden Markov models, Tech. Rep. (University of California,
Berkeley, 1998).
[50] Jan-Hendrik Prinz, Hao Wu, Marco Sarich, Bettina
Keller, Martin Fischbach, Martin Held, John D. Chodera,
Christof SchuÌˆtte, and Frank NoeÌ, â€œMarkov models of
molecular kinetics: Generation and validation,â€ J. Chem.
Phys., 134, 174105 (2011).
[51] A. P. Dempster, N. M. Laird,
and D. B. Rubin,
â€œMaximum-likelihood from incomplete data via the EM
algortihm,â€ J. Royal Statist. Soc. B, 39, 1â€“38 (1977).
[52] John D. Chodera and Frank NoeÌ, â€œProbability distributions of molecular observables computed from Markov
models: II. uncertainties in observables and their timeevolution,â€ J. Chem. Phys., 133, 105012 (2010).
[53] W. K. Hastings, â€œMonte Carlo sampling methods using
Markov chains and their applications,â€ Biometrika, 57,
97â€“109 (1970).
[54] Harold Jeffreys, â€œAn invariant form for the prior probability in estimation problems,â€ Proc. Royal Soc. A, 186,
453â€“461 (1946).
[55] Philip Goyal, â€œPrior probabilities: An informationtheoretic approach,â€ in Bayesian Inference and Maximum
Entropy Methods in Science and Engineering, edited by
Kevin H. Knuth, Ali E. Abbas, Robin D. Morriss, and
J. Patrick Castle (American Institute of Physics, 2005) pp.
366â€“373.
[56] Jin-Der Wen, Maria Manosas, Pan T. X. Li, Steven B.
Smith, Carlos Bustamante, Felix Ritort, and Jr. Igna-

[57]

[58]

[59]

[60]

[61]

[62]

[63]

cio Tinoco, â€œForce unfolding kinetics of RNA using optical tweezers. I. Effects of experimental variables on measured results,â€ Biophys. J., 92, 2996â€“3009 (2007).
Carlos J. Bustamante and Steven B. Smith, â€œLight-force
sensor and method for measuring axial optical-trap forces
from changes in light momentum along an optic axis,â€
(2006).
Gerhard Hummer, â€œPosition-dependent diffusion coefficients and free energies from Bayesian analysis of equilibrium and replica molecular dynamics simulations,â€ New
Journal of Physics, 7, 34 (2005).
Christian P. Robert, Tobias RydeÌn, and D. M. Titterington,
â€œBayesian inference in hidden Markov models through
the reversible jump Markov chain Monte Carlo method,â€
J. R. Statist. Soc. B, 62, 57â€“75 (2000).
Mathisca C. M. De Gunst and Barry Schouten, â€œModel selection for hidden Markov models of ion channel data by
reversible Markov chain Monte Carlo,â€ Bernoulli, 9, 373â€“
393 (2003).
Matthew J. Beal, Variational algorithms for approximate
Bayesian inference, Masterâ€™s thesis, University of Cambridge, UK (2003).
Jonathan E. Bronson, Jingyi Fei, Jake M. Hofman, Ruben
L. Gonzalez Jr., and Chris H. Wiggins, â€œLearning rates
and states from biophysical time series: a Bayesian approach to model selection and single-molecule FRET
data,â€ Biophys. J., 97, 3196â€“3205 (2009).
Kathryn Chaloner and Isabella Verdinelli, â€œBayesian experimental design: A review,â€ Statist. Sci., 10, 273â€“204
(1995).

