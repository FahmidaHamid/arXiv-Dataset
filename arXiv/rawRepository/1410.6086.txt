arXiv:1410.6086v4 [math.PR] 28 Jan 2015

A model for neural activity in the absence of
external stimuli
Aline Duarte and Guilherme Ost

∗

Universidade de São Paulo and GSSI - L’Aquila

†

November 7, 2014

Abstract
We study a stochastic process describing the continuous time evolution of the
membrane potentials of finite system of neurons in the absence of external stimuli.
The values of the membrane potentials evolve under the effect of chemical synapses,
electrical synapses and a leak current. The evolution of the process can be informally
described as follows. Each neuron spikes randomly following a point process with rate
depending on its membrane potential. When a neuron spikes, its membrane potential
is immediately reset to a resting value. Simultaneously, the membrane potential of the
neurons which are influenced by it receive an additional positive value. Furthermore,
between consecutive spikes, the system follows a deterministic motion due both to
electrical synapses and the leak current. Electrical synapses push the system towards
its average potential, while the leak current attracts the membrane potential of each
neuron to the resting value.
We show that in the absence leakage the process converges exponentially fast to
an unique invariant measure, whenever the initial configuration is non null. More
interesting, when leakage is present, we proved the system stops spiking after a finite
amount of time almost surely. This implies that the unique invariant measure is
supported only by the null configuration.

Key words : piecewise deterministic Markov process, limiting distribution, neuronal
systems, chemical synapses, electrical synapses, leak current
AMS Classification: 60K35, 60F99, 60J25

1

Introduction

We study the behavior of a finite number of interacting neurons in the absence of external
stimuli our goal being to determine the long-run behavior of the process. Our system is
composed of N neurons whose state at time t ≥ 0 is specified by U(t) = (U1 (t), . . . UN (t)),
with U(t) ∈ RN
+ . For each neuron i = 1, . . . , N and each time t ≥ 0, Ui (t) represents the
∗

e-mail addresses: aline.duart@gmail and guilhermeost@gmail.com
The authors are PhD students from the Universidade de São Paulo and this work was developed during
their partial completion doctoral program at GSSI both being fully supported by CNPq.
†

1

membrane potential of neuron i at time t. We consider two kinds of interactions among
neurons and also a constant interplay between neurons and the environment.
More precisely the neurons interact via electrical and chemical synapses. Electrical
synapses are due to so-called gap-junction channels between neurons which induce a constant sharing of potential, pushing the system towards its average value. By contrast,
chemical synapses are point events which can be described as follows. Each neuron spikes
randomly at rate ϕ(U) ≥ 0 which depends on its membrane potential U, we suppose ϕ
a non decreasing function, positive at U > 0 and both integrable and vanishing at 0 (in
agreement with the assumption of non external stimuli). When neuron i spikes, its membrane potential is immediately reset to a resting potential 0. Simultaneously, the neurons
which are influenced by neuron i receive an additional positive value to their membrane
potential. This value may vary for each pair of neurons. Moreover, in the whole time, the
neurons loose potential to the environment, due to leakage channels which pushes down
the membrane potential of each neuron toward zero. This outgoing constant flow of potential is defined as the leak current. For technical details we refer the reader to Gersnter
and Kistler (2002).
Our system is inspired by the one introduced in Galves and Löcherbach (2013). This
model is an example of piecewise-deterministic Markov processes (PDPs) introduced by
Davis (1984). Such processes combine random jump events, the chemical synapses, with
deterministic continuous evolutions, in our case due both to electrical synapses and the
leak current. The PDPs have been used also to model neuronal systems by other authors,
see for instance the papers by Riedler, Thieullen and Wainrib (2012), De Masi et al.
(2014), Fournier and Löcherbach (2014) and Robert and Touboul (2014).
Chemical synapses and leakage make the system non-conservative. Moreover, there is
an evident competition between the incoming energy induced by the spikes and the outgoing energy induced by the leak current. Therefore it is natural to ask about the limiting
behavior of the system as time t → ∞. The main results of the paper, presented in Theorem 2.3 and Theorem 2.4, provide a complete description of the asymptotic distribution
of the process. The theorem 2.3 states that under the presence of the leakage then almost
surely there are only a finite number of spikes and the system converges to an “inactive
global state” interpreted as “brain sleep”.
When the leakage is absent we prove a Harris-type condition which shows exponential
convergence to an unique non trivial invariant measure, whenever the initial configuration
is non null. This is the content of the Theorem 2.4.
Our paper is organized as follows. In section 2, we introduce the process, prove its
existence and we state the main results, Theorem 2.3 and Theorem 2.4. In Section 3, we
proof the theorem 2.3, while in section 4, we prove Theorem 2.4. In section 5, we briefly
compare similar results recently obtained by Roberto and Touboul (2014).

2

Model definition and main results

Let N = {1, · · · , N } be a finite set of neurons, for some fixed integer N ≥ 1 and consider
(N )
the family of non-negative synaptic weights (Wi→j )i,j∈N ∈ R+2 such that Wi→i = 0 for
all i ∈ N . The value Wi→j corresponds to the value added to the membrane potential of
neuron j when the neuron i spikes.
2

We consider a continuous time Markov process
U(t) = (U1 (t), . . . , UN (t)), t ≥ 0,
taking values in RN
+ , whose infinitesimal generator is given for any smooth test function
N
f : R+ → R, by


X  ∂f
X
X  ∂f
(u)[ui − ū] − α
(u)[ui ] , (1)
Lf (u) =
ϕ(ui )[f (∆i (u)) − f (u)] − λ
∂ui
∂ui
i∈N

i∈N

i∈N

N
where for all i ∈ N , ∆i : RN
+ → R+ is defined by

(∆i (u))j =



uj + Wi→j , if j 6= i
,
0,
if j = i

λ, α ≥ 0 are positive parameters
Pmodelling, respectively, the strength of electrical synapses
and the leakage effect, ū = N1 N
i=1 ui and
Assumption 1. ϕ : R+ → R+ is a non-decreasing function such that ϕ(0) = 0 and
there exists a constant r > 0 satisfying
Z2r

ϕ(u)
du < +∞.
u

0

We are assuming that there is no external stimuli. This assumption appears in the
condition ϕ(0) = 0. In addition, from the neurobiological point of view, it is reasonable to
assume that ϕ is a non-decreasing function since an addition in the membrane potential
increases the probability of a spiking occurs.
The first term in (1) depicts how the chemical synapses are incorporated in our model.
Neurons whose potential is u spike at rate ϕ(u). Intuitively this means that any initial
configuration u ∈ RN
+ of the membrane potentials
P(U(t) = ∆i (u)|U(0) = u) = ϕ(ui )t + o(t), as t → 0
Thus, the function ϕ is called firing or spiking rate of the system.
The second and third terms in (1) represent the electrical synapses and the leak current
respectively. They describe the deterministic time evolution of the system between two
consecutive spikes. More specifically, in an interval of time [a, b], without occurrence of
spikes in the whole system, the membrane potential of neuron i ∈ N obeys the following
ordinary differential equation
d
Ui (t) = −αUi (t) − λ(Ui (t) − Ū(t)).
dt

(2)

Notice that the first term of the right-hand side of (2) pushes simultaneously all neurons
to the resting state, while the second term tends to attract the neurons to the average
potential.
Our first theorem proves the existence of the process.

3

Theorem 2.1. Let ϕ : R+ → R+ be any function satisfying the Assumption 1. For any
N ≥ 1 and any u ∈ RN
+ there exists a unique strong Markov process U(t) taking values in
N
R+ starting from u whose generator is given by (1).
Proof. Let Ni (t), t ≥ 0, be the simple pointPprocess on R+ which counts the jump events
of neuron i ∈ N up to time t. Define Ei = j6=i Wi→j and E = maxi∈N Ei and, following
De Masi et al. (2014), consider the following random variable, for all t > 0,
XZ t
1{Ui (s− ) ≤ 2E}dNi (s).
k(t) =
i∈N

0

The random variable k(t) counts the number of spikes of neurons whose the potential is
at most 2E.
Suppose Ui fires at time t, in this case
Ū(t) =



1
1 X
Uj (t− ) + Wi→j = Ū(t− ) +
Ei − Ui (t− ) .
N
N
j6=i

Now, using the expression of Ū(t) above and adapting the proof of Theorem 1 of De Masi
et al (2014), we have the following inequalities for all t > 0,
Ū(t) ≤ Ū(0)+

E
k(t), EN (t) ≤ N Ū(0)+2Ek(t) and Umax (t) ≤ (N +1)Umax (0)+2Ek(t),
N

where Umax (t) = maxi∈N Ui (t).
Since we can bound Ek(t) by a Poisson process of intensity N ϕ(2E), the second
inequality above shows that number of jumps of the process is finite almost surely on any
finite time interval. To conclude the proof just note that the construction of the process
can be achieved by gluing together trajectories given by the deterministic flow between
successive jump times. This procedure is feasible since the number of jumps of the process
is finite on any finite interval.

Now, we shall present an elementary argument which shows that for all leakage rate α
large enough and if the firing rate ϕ is globally Lipschitz with ϕ(0) = 0, the system goes
extinct. This result was the starting point of this paper. The idea of this proof was taken
from discussions with Galves and Löcherbach. The result is the following.
Theorem 2.2. For any N ≥ 1, α ≥ 0, λ ≥ 0 and c-Lipschitz function ϕ : R+ → R+ such
that c > 0 and ϕ(0) = 0, the following inequality holds, for all t ≥ 0 and U(0) = u ∈ RN
+,
∗ c−α)

E[Ū(t)] ≤ ūet(α
where α∗ = max
k∈N

P

j∈N

,

Wj→k . In particular, if α > α∗ c, then the process goes extinct.

Proof. For each i ∈ N , plugging f = πi in (1), where πi is the projection onto the i-th
coordinate, we have
X
d
E[Ui (t)] =
Wj→i E[ϕ(Uj (t))] − E[Ui (t)ϕ(Ui (t))] − αE[Ui (t)] + λE[Ui (t) − Ū(t)].
dt
j∈N

4

Summing over all i ∈ N and then using that ϕ is a non-negative c-Lipschitz function such
that ϕ(0) = 0, it follows that
d
E[Ū(t)] ≤ (α∗ c − α)E[ϕ(Ū(t))].
dt
Therefore applying the Grownwall’s lemma to the inequality above we finish the proof.
Even assuming that the firing rate ϕ satisfies only the Assumption 1, we claim that
for any fixed numbers of neurons, the presence of the leak current is a necessary and
sufficient condition for the extinction of the process. In fact, we shall prove a stronger
result. It states that, if there is the leakage, there will be only a finite numbers of spikes
eventually almost surely. On the other hand, it is shown that, excluding the trivial initial
configuration, the system is ergodic when there is no leakage. In particular, this results
generalize the Theorem 2.2 above.
In order to state our main result, we need to introduce the some extra notation.
For each neuron i ∈ N , let T1i = inf{s > 0 : Ui (s) = 0} be the first spiking time of
i
neuron i and for each k ≥ 2, let Tki = inf{s > Tk−1
: Ui (s) = 0} be the k-th spiking time of
neuron i. Then, the first and the k-th spiking time of the system are defined respectively
by
T1 = inf T1i and Tk = inf {Tki > Tm−1 }, k ≥ 2.
(3)
i∈N

i∈N ,m≥1

Our main theorem is given below.
Theorem 2.3. Let (U(t))t≥0 be the Markov process whose the infinitesimal generator is
given by (1)
P and Tk be as defined in (3). Assume that ϕ satisfies the Assumption 1 with
Wj→i . Then for any α > 0 and λ ≥ 0,
r > max
i∈N j∈N



X
1{Tk < ∞} < ∞ = 1.
P
k≥1

Corollary 1. Under the same hypothesis of Theorem 2.3, for all i ∈ N , it holds
lim Ui (t) = 0

t→+∞

a.s.

In particular, the delta of Dirac at point 0N , δ0N , is the unique invariant measure for the
process in the presence of the leak current.
It remains to analyse what happens in the long-run behavior of the system in the
absence of the leakage. This is the content of the next result.
Theorem 2.4. Let (U(t))t≥0 be the Markov process whose the infinitesimal generator is
given by (1) with α = 0. Then the process admits only two different invariant probability
measures either δ0N or µ 6= δ0N . If U(0) = (0, · · · , 0), the invariant measure is δ0N , while
N
whatever is the initial configuration U(0) = u ∈ RN
+ \ {0 }, the invariant measure is µ.

5

3

Proof of Theorem 2.3

First of all, observe that, from the equation (2), for any time t ∈ [Tn , Tn+1 ),

Ui (t) = Ui (Tn ) − Ū(Tn ) e−(α+λ)(t−Tn ) + Ū(Tn )e−α(t−Tn ) .

(4)

We shall explore this equation many times.

We start giving a lower bound to the probability of there is no spikes when the system
starts with a initial condition small enough.
Proposition 3.1. Let (U(t))t∈R be the Markov process whose the generator is given by
(1), Umax (t) = max{Ui (t), i ∈ N } be the maximum potential of the system at time t and
T1 as defined in (3). Suppose that ϕ satisfies the Assumption 1. If α > 0, then
P(T1 = ∞ | Umax (0) < r) ≥ e−N α

R 2r
0

ϕ(u)
du
u

> 0.

Proof. By the equation (4), we have, for all 0 ≤ t < T1 , the following inequalities
Umax (t) ≤ Ū(0)e−αt + Umax (0)e−αt ≤ 2re−αt .
Using the inequality above and the non-decreasing assumption on ϕ, we have

 Z t
max
max
N ϕ(U
(s))ds
P(T1 > t | U
(0) < r) ≥ exp −
0


Z 2r
ϕ(u)
≥ exp −N α
du .
2re−αt u

(5)

Therefore, when t goes to infinite, it follows the result.
Proposition 3.2. Under the same hypothesis of Proposition 3.1. For θ < (α + λ)−1 , it
holds that
P(T1 > θ | Umax (0) > r) ≤ e−θϕ(β) < 1,
where β = β(θ) = r(1 − (α + λ)θ) > 0.
Proof. Indeed,
θ



(6)

Umax (s) ≥ Umax (0)e−(α+λ)s ≥ re−(α+λ)s ≥ r(1 − (α + λ)θ).

(7)

max

P(T1 > θ | U

 Z
(0) > r) ≤ exp −

max

ϕ(U

0

(s))ds .

Moreover, by (4) for all 0 < s ≤ θ is true that

From (6) and (7) we deduce the desired inequality
P(T1 > θ | Umax (0) ≥ r) ≤ e−θϕ(β) < 1.

6

The next result claims that even when all membrane potentials are large, there is a
positive probability of all potentials become small after a fix time T > 0.
Proposition 3.3. Consider (U(t))t∈R the Markov process whose the generator is given by
(1), Umax (t) and T1Pas in proposition 3.1. Suppose that ϕ satisfies the Assumption 1 and
Wj→i , then there exists T > 0 such that
α > 0. If r > max
i∈N j∈N

P(Umax (T ) < r | Umax (0) > r) > 0.

Proof. Define τ1 = inf{t > 0 : Umax (t) = 0} and take θ < (α + λ)−1 (this is possible since
α > 0). By the proposition 3.2, it holds that
 1

.
P(T1 < θ, T1 = τ1 | Umax (0) > r) ≥ 1 − e−θϕ(β)
N
Define T0 = 0, T = N θ and consider the following sequence of events
A1 = {T1 < θ, T1 = τ1 , Umax (T1 ) < r, T2 > T },
and for l = 2 . . . , N ,
o
n
max
max
{U
(T
)
>
r},
U
(T
)
<
r,
T
>
T
.
Al = ∩lk=1 {Tk < Tk−1 + δ, Tk = τk , }, ∩l−1
k
l
l+1
k=1

The event Ak corresponds to the following situation. At the firsts k − 1 consecutive spikes
of the maximum, there always exists at least one neuron whose potential is larger than r.
But at k-th spike of the maximum all membrane potentials get less than r and no more
spikes happen up to time T .
From the definition of the events Ak we have
N
 max
	 [
U
(T ) < r ⊇
Ak .
k=1

Wherefore, it suffices to compute the probability of each event Ak in the right side ofthe
inclusion, conditioned to the event {Umax (0) > r}. Now, defining β0 = 1 − e−δϕ(β) N1 ,
	

cr
−αN δ ) , by the proposition 3.2 and the inequality (5), it follows
and β1 = exp −2N
α (1 − e
that
P(A1 | Umax (0) > r) ≥ β0 β1 P(Umax (T1 ) < r | Umax (0) > r, T1 < δ, T1 = τ1 ).
Similarly, we have
P(A2 | Umax (0) > r) ≥ β02 β1 P(Umax (T1 ) > r | Umax (0) > r, T1 < δ, T1 = τ1 )
×P(Umax (T2 ) < r | Umax (T1 ) > r, T2 < T1 + δ, T2 = τ2 ).
Thus, summing these two inequalities, we get the following lower bound for A1 ∪ A2 ,
P(A1 ∪ A2 | Umax (0) > r) ≥ β02 β1 P(Umax (T2 ) < r | Umax (T1 ) > r, T2 < T1 + δ, T2 = τ2 ).
Proceeding in this way for the other terms, we obtain that
P(Umax (T ) < r | Umax (0) > r)

≥ β0N β1 P Umax (TN ) < r | Umax (TN −1 ) > r, ∩N
k=1 {Tk < Tk−1 + δ, Tk = τk } .
7

Since r > max

P

i∈N j∈N

Wj→i , we easily see that


P Umax (TN ) < r | Umax (TN −1 ) > r, ∩N
k=1 {Tk < Tk−1 + δ, Tk = τk } = 1.
As a consequence of proposition 3.3 we have
Corollary 2. Le T > 0 be the positve constant given by the proposition 3.3. Define the
stopping times R1 = inf{n ≥ 1; Umax (nT ) ≤ r} and Rk = inf{n ≥ Rn−1 ; Umax (nT ) ≤ r}
for all k ≥ 2. Then, under the same hypothesis of the proposition 3.3,
∞
X

1{Rk < ∞} = ∞

P − a.s.

k=1

Proof of Theorem 2.3. For this proof we must define the following stopping times: K1 =
inf{n ≥ 1 : Tn > R1 T }, J1 = inf{n ≥ 1 : Rn T > TS1 }, and for k ≥ 2, Kk = inf{n > Jk−1 :
Tn > RJk−1 T } and Jk = inf{n ≥ 1 : Rn T > TSk−1 }. From corollary 2 all these stopping
times are finite almost surely and, in particular, well defined. Now, from Theorem 2.1 and
the definition of Kk
 ∞
 ∞
!
!
∞
∞ [
∞
 \
 \
\
\


{Ti < ∞} .
{Ti < ∞} = P
{Kk < ∞} 
1=P
{Ti > Rk T } 


i=1

k=1 i=1

k=1

i=1

∞
Thus, ∩∞
k=1 {Tk < ∞} implies ∩k=1 {Kk < ∞}.

On the other hand, by proposition 3.1, one knows that
!
n

R 2r ϕ(u) n
\
P
{Kk < ∞} ≤ 1 − e−N α 0 u du ,
k=1

which converges to 0 when n diverges.

4

Proof of Theorem 2.4

To simplify the proof of the theorem 4 we shall split it into several steps. The main part
of the argument is to find a positive recurrent regeneration set B in sense that
(i) B is positive recurrent and,
(ii) there exist some t∗ > 0, ǫ > 0 and a probability measure ν on RN
+ such that
Pt∗ (u, A) ≥ εν(A), u ∈ B,
for all measurable set A ∈ B(RN
+ ).
Usually, a Markov processes with a regeneration set are called Harris recurrent Markov
processes. For such Processes an invariant measure always exits.
8

In what follows, for any positive real number a > 0 we use the notation Ra (x), meaning
that there exists a constant l > 0 such that |Ra (x)| ≤ la for all x. When a function satisfies
such condition it is called a function of order a. Note that we are not specifying the domain
in which the function R is defined on.
For each ε > 0 define the following event
Aε = {(i − 1)ε < Ti < iε, i = 1, · · · , N },
and let (Sm )m≥1 be the spiking mark sequence of the system. The event {Sm = i}
means that the Ui (Tm ) = 0. Finally, consider the event
S = {Si = i, i = 1, . . . , N }.
The first lemma below says that, conditioning to the event Aε , when ε is sufficiently
small the process evolves, modulo an error of small order, as in the case without gap
junction (λ = 0). Before stating this lemma we need to introduce a finite sequence of
potential configurations.
N given by
Consider the sequence (v(k))k=0,...,N with v(k) ∈ R+

v(0) =

N
X

Wj→1 ,

j=2

N
X
j=3


Wj→1 , . . . , WN →N −1 , 0 ,

and for 1 ≤ k ≤ N, vk (k) = 0 and for i 6= k


vi (k) = 1{i>k} vi (0) +

k
X



Wj→i + 1{i<k}

j=1

k
 X

j=i+1


Wj→i .

Lemma 4.1. Fix δ > 0. If U(0) = u ∈ B(v(0), δ), then conditioning to the event Aǫ ∩ S,
for each k = 1, · · · , N the following equalities hold:
(i) Ui (Tk ) = vi (k) +

k
P

r=i+1

(ii) Ui (Tk ) = vi (k) +

k
P

r=1

λ(Tr − Tr−1 )di (r − 1) + Rδε (T1k , u) + Rε2 (T1k , u), if i < k;

λ(Tr − Tr−1 )di (r − 1) + Rδ (u) + Rδε (T1k , u) + Rε2 (T1k , u), if i > k;

(iii) Ū(Tk ) = v̄(k) + Rδ (u) + Rε (T1k , u), if k < N and Ū(TN ) = v̄(0) + Rε (T1N , u),
where di (m) = v̄(m) − vi (m), T0 = 0 and T1k = (T1 , . . . , Tk ). Furthermore, all the partial
derivatives of the remainder functions Rδǫ (T1k , u), Rǫ2 (T1k , u) above are either of order δ
or ε.
Proof. The proof is given by induction on k. On the event Aǫ ∩S, we have that U1 (T1 ) = 0
and for each i 6= 1 and U(0) = u ∈ B(v(0), δ),
Ui (T1 ) = W1→i + Ū(0) + (1 − λT1 + Rε2 (T1 ))(vi (0) + Rδ (u) − Ū(0))
= vi (1) + λT1 di (0) + Rδ (u) + Rǫδ (T1 , u) + Rε2 (T1 , u),
9

where in the first equality we have used the expansion series of the exponential function.
Thus, from the expression of Ui (T1 ) above, we conclude that
N
1 X
di (0) + Rδ (u) + Rεδ (T1 , u) + Rε2 (T1 , u)
Ū(T1 ) = v̄(1) + λT1
N
i=2

= v̄(1) + Rδ (u) + Rε (T1 , u).

In addition, it is easy to check that all remainders functions above have partial derivatives
with respect to T1 and all of them are either Rδ or Rǫ functions. Therefore (i), (ii) and
(iii) it is verified for k = 1.
Now, suppose that (i), (ii) and (iii) hold for some fixed 1 < k < N . As before, on the
event Aǫ ∩ S, we have Uk+1 (Tk+1 ) = 0 and, for i < k + 1, by the inductive hypothesis,
Ui (Tk+1 ) = Wk+1→i + Ū(Tk ) + (1 − λ(Tk+1 − Tk ) + Rε2 (Tk−1 , Tk ))(Ui (Tk ) − Ū(Tk ))
= vi (k + 1) +

k+1
X

λ(Tr − Tr−1 )di (r − 1) + Rδε (T1k+1 , u) + Rε2 (T1k+1 , u)

r=i+1

Using the same argument for the case when i > k + 1, we get the first equation. Using
again the inductive hypothesis and looking at the expression written in the first equality
of the membrane potential, it is readily seen that the remainder functions possesses partial
derivatives with to Tl , for l = 1, . . . , k + 1 and they are either of order δ or ǫ.
Finally, summing Ui (Tk+1 ) over all neurons i = 1, . . . , N
Ū(Tk+1 ) = v̄(k + 1) + Rε (T1k+1 , u) + Rδ (u).

Note that v(TN ) = v(0), thus, from the previous lemma it follows the
Corollary 3. Under the same assumptions of Lemma 4.1, if T = N ε, then for each
i = 1, . . . , N , the following equality is verified
Ui (T ) = vi (0) + λ(T − TN )di (0) +

N
X

λ(Tr − Tr−1 )di (r − 1) + Rδǫ (T1N , u) + Rǫ2 (T1N , u).

r=i+1

Remark 4.1. In order to simplify the notation, we shall denote the map γ 0 : Aǫ → RN
+
0 (tN ), . . . , γ 0 (tN )) where for each i = 1, . . . , N ,
)
=
(γ
by γ 0 (tN
1 1
1
N 1
γi0 (tN
1 ) = vi (0) + λ(T − tN )di (0) +

N
X

λ(tr − tr−1 )di (r − 1).

r=i+1

By corollary 3, conditioning to the event Aǫ ∩ S, we have the following representation for
all U(0) = u ∈ B(v(0), δ)
U(T ) = γ 0 (T1N ) + Rδǫ (T1N , u) + Rǫ2 (T1N , u),
where both Rδǫ (T1N , u) and Rǫ2 (T1N , u) are multivalued functions whose the L1 -norms are
remainders functions of order δǫ and ǫ2 respectively. Moreover, we have the following
corollary
10

Corollary 4. For each u ∈ B(v(0), δ), the determinant of the Jacobian of map Aε ∋ tN
1 7→
0 (tN ) + R (tN , u) + R (tN , u), is given by
γu (tN
)
=
γ
2
δǫ 1
ǫ
1
1
1
N
|Jγu (tN
1 )| = λ

N
Y

N
v̄(i) + Rǫ (tN
1 , u) + Rδ (t1 , u),

i=1

which is different from zero for δ and ε small enough for all (tN
1 ) ∈ Aǫ and u ∈ B(v(0), δ).
We shall use this representation to show that our process, at time T , satisfies a Harristype condition (see proposition 4.1). Before proving this proposition, we need an extra
lemma. Here again the non-decreasing assumption on ϕ is important.
Lemma 4.2. (i) Let fu (t1 , . . . , tN ) = fu,(S1 =1,...,SN =N ) (t1 , . . . , tN ) denote the joint density of (T1 , . . . , TN ) conditioning to the event S, when the starting configuration is
u. Then, for any 0 < δ < mini=1,...,N {vi (0)}, there exist a constant β > 0 such that
for all u ∈ B(v(0), δ),
fu (t1 , . . . , tN ) ≥ C1 , for (t1 , . . . , tN ) ∈ Aǫ .
(ii) For any given mini=1,...,N {vi (0)} > δ > 0 there exist ǫ > 0 and a constant C2 > 0
such that for u ∈ B(v(0), δ), we have
PT (u, B(v(0), δ)) ≥ C2 ǫN > 0.
In particular, defining TB(v(0),δ) = inf{t > 0 : U(t) ∈ B(v(0), δ)}, we have
sup

Eu [TB(u∗ ,δ) ] ≤

u∈B(v(0),δ)

T
.
C2 ǫ N

h R P
i
t
u (s))ds , we immediately see that the
Proof. (i) Since Pu (T1 > t) = exp − 0 N
ϕ(U
j
j=1
density function of T1 given that U(0) = u is


Z t1 X
N
N
X
(8)
ϕ(Uuj (s))ds , for t1 ≥ 0.
ϕ(Uuj (t1 )) exp −
fu,T1 (t1 ) =
j=1

0

j=1

Since u ∈ B(v(0), δ) and δ < min{Mi }, we know that there exist positive constants c1 and
c2 such that c1 < uj < c2 for all j = 1, . . . , N . But then for all u ∈ B(v(0), δ) and j, we
have that c1 < Uj (t1 ) = ū(1 − e−λt1 ) + uj e−λt1 < c2 . Thus from the previous inequality
and the identity (8) that it follows that fu,T1 (t1 ) ≥ N ϕ(c1 )e−t1 N ϕ(c2 ) . Now, from the
definition of the process one easily sees that the density function of the increment T2 given
T1 = t1 , S1 = 1 and U(0) = u, for t2 > t1 , is


Z t2 X
N
N
X
∆ (Uu (t1 ))
∆ (Uu (t1 ))
ϕ(Uj 1
(s))ds . (9)
ϕ(Uj 1
(t2 )) exp −
fT2 |T1 =t1 ,S1 =1,u (t2 ) =
t1

j=1

j=1

Note that for j 6= 1, W1→j + c1 < ∆1 (Uu (t1 ))j = W1→j + Uu (t−
1 ) < W1→j + c2 , so that
1 PN
N −1
u
Ū (t1 ) > c1 N + N j=2 W1→j . From these two inequalities and using that the average
potential is constant between successive jumps it follows that there exist positive constants
11

1

c11 and c12 such that fT2 |T1 =t1 ,S1 =1,u (t2 ) ≥ ϕ(c11 )e−(t2 −t1 )N (c2 ) , for t2 > t1 . Proceeding in this
manner we obtain sequences (cn1 )n=1,...,N −1 and (cn2 )n=1,...,N −1 satisfying for k = 2, . . . , N ,
k−1

fTk |Tk−1 =tk−1 ,Sk−1 =k−1,...,T1 =t1 ,S1 =1,u (tk ) ≥ ϕ(c1k−1 )e−(tk −tk−1 )N (c2

)

,

where tk > tk−1 > . . . > t1 ≥ 0. Thus, we have that, over Aǫ , the product of these
conditional densities is positive. This concludes the prove of item (i).
(ii) The first part follows immediately fromR the item (i) above and corollary 3. To prove
∞
the second part, we write E[TB (v(0), δ)] = t=0 P(T
 Bt (v(0), δ) > t)dt and we note that
{TB (v(0), δ) > t} ⊂ {U(lT ) ∈
/ B(v(0), δ), l = 1, . . . , T } for t > T. A simple application
of the Markov property together with the lower bound given by the first part finishes the
proof.
Proposition 4.1. (Harris-type Condition) For any u ∈ B(v(0), δ), there exists a non
N
negative function hu such that for all measurable sets A ∈ β(R+
N \ {0 }),
Z
hu (v)dv.
PT (u, A) ≥
A

Moreover, there exist a measurable set I, with positive Lebesgue measure, and a constant
C3 > 0 such that hu (v) ≥ C3 1I (v) for all u ∈ B(v(0), δ).
Proof. For each u ∈ B(v(0), δ), as in Corollary 4 let us call γu : Aǫ → Iu , the map
N
N
0 N
γu (tN
1 ) = γ (t1 ) + Rδǫ (t1 ) + Rǫ2 (t1 ),

where Iu = γu (Aǫ ). From the corollaries 3, 4 and the remark 4.1, it follows that for each
u ∈ B(v(0), δ), conditioning to Aǫ ∩ S, the random vector Uu (T ) has a density hu , where
hu (v) =



fu (gu (v))|Jgu (v)| , if v ∈ Iu
0
otherwise,

with gu : Iu → Aǫ being the inverse of γu . This concludes the first part of the proposition.
The proof of second part is more delicate and requires some work. From Corollary 4 and
Lemma 4.2 part (i) it suffices to prove that there exists a set I such that I ⊂ ∩u∈B(v(0),δ) Iu .
Now, consider the event Bǫ defined by
n
o
ε
ǫ
Bǫ = (i − 1)ε + < Ti < iε − , i = 1, · · · , N ∩ S,
4
4

define I = γ0 (Bǫ ) and fix v ∈ I. We want to show that for all u ∈ B(v(0), δ) there is
N
N
an vector tN
1 = t1 (u) = (t1 (u), . . . , tN (u)) ∈ Aǫ such that γu (t1 ) = v. To this end, we
N
N
N
introduce the function F (s, t1 ) = v − γ0 (t1 ) − s[Rδǫ (t1 , u) + Rǫ2 (tN
1 , u)], for s ∈ [0, 1] and
N
N
t1 ∈ Aǫ . Note that we need to show the existence of vector t1 ∈ Aǫ such that F (1, tN
1 ) = 0.
N as function of s. To
)
=
0
with
t
This means that we need to study the equation F (s, tN
1
1
ease the notation, from now on we will write t instead of tN
1 .
Note that by the definition of I, there exists t0 ∈ Bǫ such that F (0, t0 ) = 0. Besides,
t = t(s) is solution of the equation F (s, t) = 0 if and only if it satisfies
0 = −Dγ0 (t(s))·

dt(s)
dt(s)
−[Rδǫ (t(s), u)+Rǫ2 (t(s), u)]−s[DRδǫ (t(s), u)+DRǫ2 (t(s), u)]·
ds
ds
12

or equivalently,
h

i dt(s)
− Dγ0 (t(s))− s DRδǫ (t(s), u)+ DRǫ2 (t(s), u) ·
= Rδǫ (t(s), u)+ Rǫ2 (t(s), u), (10)
ds

where Df (·) stands for the differential operator of f . By corollary 4, the linear operator
inside the brackets is invertible and the function on the right hand side is of order δǫ + ǫ2
whose the derivative is of order δ + ǫ. Therefore, it follows that t = t(s) is a solution of
(10) if and only if it is the solution of the ODE of the form
d
t(s) = H(s, t(s)), t(0) = t0 ,
ds

(11)

where the derivative of H with respect to t exists and it is of order δ + ǫ. In particular, it
is limited and therefore the ODE has unique solution t = t(s) for all s such that t(s) ∈ Aǫ .
Since t(s) moves as H which is of order δǫ + ǫ2 and the initial condition t0 is at a distance
of order ǫ of Aǫ , we have that t(1) ∈ Aǫ , decreasing both δ and ǫ if necessary. Hence, we
have that I ⊂ ∩u∈B Iu .
Proof of Theorem 4. Item (ii) of lemma 4.2 implies that the time of B(v(0), δ) is a positive
recurrent set. Now by poposition 4.1 the Harris-type condition holds once the process reach
B(v(0), δ), so that in fact B(v(0), δ) is a positive recurrent regenerative set.

5

Final discussion

The present paper presented a new class of stochastic processes and studied its asymptotic
distribution in absence and presence of the leak current, without external stimuli in either
cases. Similar results have been recently obtained by Robert and Touboul in [7]. Their
model considers a constant leakage rate without the gap junction term (α = 1 and λ = 0
in our context) where the synaptic weights are positive i.i.d random variables with finite
R1
mean. They showed that if 0 ϕ(x)
x dx < +∞, then no spike occurs after some finite time.
This condition imposed on the spiking rate ϕ is the same type as ours. They also proved
that when ϕ(0) > 0 and the synaptic weights are bounded almost surely, the process
admits a unique non trivial invariant measure. In other words, when external stimuli are
considered the system remains active almost surely, contrasting with non external stimuli
case. We did not study this case, nonetheless it could be treated similarly as we did in
Theorem 4.

6

Acknowledges

We are indebted to R. Cofré for the discussions which originate this work. We thank E.
Presutti for helpful suggestions and stimulating discussions; A. Galves and E. Löcherbach
for the discussions which led us to the proof of Theorem 2.2. This article was produced
as part of the activities of FAPESP Center for Neuromathematics (FAPESP grant 2013/
07699-0). A. Duarte is fully supported by a CNPq fellowship (grant 141270/2013-6) and
G. Ost is fully supported by a CNPq fellowship (grant 141482/2013-3).

13

References
[1] M. H. A. Davis. Piecewise-deterministic Markov processes: a general class of nondiffusion stochastic models. J. Roy. Statist. Soc. Ser. B, 46(3):353–388, 1984.
[2] A. De Masi, A. Galves, E. Löcherbach, and E. Presutti. Hydrodynamical limit for a
system of interacting neurons. ArXiv, 2014.
[3] N. Fournier and E. Löcherbach. On a toy model of interacting neurons. ArXiv, 2014.
[4] A. Galves and E. Löcherbach. Infinite systems of interacting chains with memory
of variable lengtha stochastic model for biological neural nets. Journal of Statistical
Physics, 151(5):896–921, 2013.
[5] Wulfram Gerstner and Werner Kistler. Spiking Neuron Models: An Introduction. Cambridge University Press, New York, NY, USA, 2002.
[6] M. Thieullen M. Riedler and G. Wainrib. Limit theorems for infinite-dimensional
piecewise deterministic markov processes. applications to stochastic excitable membrane models. Electron. J. Probab., 17:no. 55, 1–48, 2012.
[7] P. Robert and J. Touboul. On the dynamics of random neuronal networks. ArXiv,
2014.

14

