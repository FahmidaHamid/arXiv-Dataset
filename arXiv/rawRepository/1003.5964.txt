arXiv:1003.5964v2 [q-bio.PE] 5 May 2011

Fast Convergence of MCMC Algorithms for Phylogenetic
Reconstruction with Homogeneous Data on Closely Related
Species
Daniel SÌŒtefankovicÌŒâˆ—

Eric Vigodaâ€ 

November 22, 2010

Abstract
This paper studies a Markov chain for phylogenetic reconstruction which uses a
popular transition between tree topologies known as subtree pruning-and-regrafting
(SPR). We analyze the Markov chain in the simpler setting that the generating tree
consists of very short edge lengths, short enough so that each sample from the generating tree (or character in phylogenetic terminology) is likely to have only one mutation,
and that there enough samples so that the data looks like the generating distribution.
We prove in this setting that the Markov chain is rapidly mixing, i. e., it quickly
converges to its stationary distribution, which is the posterior distribution over tree
topologies. Our proofs use that the leading term of the maximum likelihood function
of a tree T is the maximum parsimony score, which is the size of the minimum cut
in T needed to realize single edge cuts of the generating tree. Our main contribution
is a combinatorial proof that in our simplified setting, SPR moves are guaranteed to
converge quickly to the maximum parsimony tree. Our results are in contrast to recent
works showing examples with heterogeneous data (namely, the data is generated from
a mixture distribution) where many natural Markov chains are exponentially slow to
converge to the stationary distribution.

1

Introduction

We study Markov Chain Monte Carlo (MCMC) methods for Bayesian inference of phylogeny. We begin by presenting the relevant background material by defining phylogenetic
trees, evolutionary models (in Section 1.1), and the associated MCMC methods (in Section
1.2). We refer the interested reader to Semple and Steel [15] for a more comprehensive
âˆ—
Department of Computer Science, University of Rochester, Rochester, NY 14627. Email: stefanko@cs.rochester.edu. Research supported in part by NSF grant CCF-0910415.
â€ 
College of Computing, Georgia Institute of Technology, Atlanta GA 30332.
Email:
vigoda@cc.gatech.edu. Research supported in part by NSF grant CCF-0830298 and CCF-0910584.

1

introduction to the mathematics of phylogeny. Finally, we present our results and discuss
related work in Section 1.3.
A phylogenetic tree is an unrooted tree T on n leaves (called taxa, corresponding to
n species) where internal vertices have degree three. Let E(T ) denote the edges of T
and V (T ) denote the vertices. In the phylogenetic reconstruction problem, we observe a
collection of labelings of the leaves of T from a set â„¦, and our goal is to infer the tree T
from which they were generated from. For example, if â„¦ = {A, C, G, T } then we are given
(aligned) DNA sequences for n species, and we are trying to determine the tree describing
the evolutionary history of the present-day species.

1.1

Evolutionary models and Maximum Likelihood

The labelings on the leaves of T are the projection of labelings on all vertices of T , and
these labelings of V are generated in the following manner. There is a stochastic process
along edges of T (e. g., modeling the evolutionary process of DNA substitutions) which is
defined by a continuous-time Markov chain. Thus, for each edge e âˆˆ T there is a |â„¦| Ã— |â„¦|
rate matrix Q and a time te > 0, which is called the branch length of e. In this paper,
as is typical in the phylogenetic setting, we assume there is a single rate matrix Q that is
common to all edges. The rate matrix is assumed to be reversible with respect to some
distribution Ï€ on â„¦. Hence, fix Ï€ as the stationary vector for Q (i. e., Ï€Q = 0). (The
matrix Q is usually scaled so that we expect one â€œsubstitutionâ€ (i. e., change) per unit of
time.) The rate matrix Q defines a continuous time Markov chain, and together with te
defines a transition matrix on edge e:
Pe = exp(te Q) = I + te Q + t2e Q2 /2! + t3e Q3 /3! + . . .

(1)

The matrix Pe is a stochastic matrix of size |â„¦| Ã— |â„¦|, and thus defines a discrete-time
Markov chain, which is time-reversible with stationary distribution Ï€, i. e., Ï€Pe = Ï€, and
Ï€i (Pe )ij = Ï€j (Pe )ji (for all i, j âˆˆ â„¦).
The simplest four-state (i. e., |â„¦| = 4) evolutionary model has a single parameter for
the off-diagonal entries of the rate matrix Q; this model is known as the Jukes-Cantor
model. The most general reversible four-state model is the GTR (general time reversible)
model. For |â„¦| = 2 (often studied for mathematical interest), the model is binary and the
rate matrix has a single parameter; this model is known as the CFN (Cavender-FarrisNeyman) model. See Felsenstein [6] or Yang [22] for an introduction to these evolutionary
models.
Given T , the rate matrix Q and the branch lengths t = (te )eâˆˆE(T ) , we then define the
following distribution on labelings of the vertices of T . Let Pe = exp(te Q) for e âˆˆ E(T ).
We first orient the edges of T away from an arbitrarily chosen root r of the tree. (We
can choose the root arbitrarily since each Pe is reversible with respect to Ï€.) Then, the
probability of a labeling ` : V (T ) â†’ â„¦ is
Y
Âµ0T,Q,t (`) := Ï€(`(r))
Puv (`(u), `(v)).
(2)
âˆ’
â†’
uvâˆˆE(T
)

2

The distribution Âµ0T,Q,t can be generated in an equivalent algorithmic manner. Choose
`(r) from Ï€. Then for each edge e = (u, v) âˆˆ E(T ), given an assignment for exactly one
of the endpoints, say `(u), choose `(v) from the distribution defined by the row of Pe
corresponding to the label `(u).
Let ÂµT,Q,t be the marginal distribution of Âµ0T,Q,t on the labelings of the leaves of T
(thus ÂµT,Q,t is a distribution on â„¦n where n is the number of leaves of T ). Fix T âˆ— with
parameters Qâˆ— and tâˆ— as the generating tree. The goal of phylogeny reconstruction is to
reconstruct T âˆ— (and possibly Qâˆ— and tâˆ— ) from ÂµT âˆ— ,Qâˆ— ,tâˆ— (more precisely, from independent
samples from ÂµT âˆ— ,Qâˆ— ,tâˆ— ).
Let Q denote a set of rate matrices with non-zero entries where Qâˆ— âˆˆ Q. The set Q is
the set of possible rate matrices. The set can be arbitrary, usually it is determined by the
model considered (e. g., for the Jukes-Cantor model Q would contain rate matrices whose
off-diagonal entries are the same). One often assumes that the rate matrix Qâˆ— is known.
In this case we would set Q = {Qâˆ— }. On the other hand, our results also apply if one sets
Q to be the set of all rate matrices with non-zero entries.
We consider the likelihood of a tree T as, the maximum over rate matrices Q âˆˆ Q
and over assignments of non-zero branch lengths t to the edges of T , of the probability
that the tree (T, Q, t) generated Âµ. More formally, the maximum expected log-likelihood
of tree T for distribution Âµâˆ— is defined by
LT (Âµâˆ— ) = sup sup LT,Q,t (Âµâˆ— ),

(3)

t

QâˆˆQ

where
LT,Q,t (Âµâˆ— ) =

X

Âµâˆ— (y) ln(ÂµT,Q,t (y)).

(4)

yâˆˆâ„¦n

For a set of characters D = (D1 , . . . , DN ) where Di âˆˆ â„¦n , define the log-likelihood of
a tree T as
LT (D) =
=

sup sup ln (ÂµT,Q,t (D))
QâˆˆQ

t

sup sup
QâˆˆQ

t

N
X

ln (ÂµT,Q,t (Di )) .

i=1

Our goal is to sample from the distribution on the set of phylogenetic trees with n
leaves where the weight of a tree is LT (D). In Section 3 we will look at the straightforward
extension to the setting where we are given a prior on trees and parameters Q, t, and our
goal is to sample from the posterior distribution.

1.2

SPR Markov Chain

We analyze a Markov chain using transitions made by subtree pruning-and-regrafting
(SPR). SPR transitions are a natural combinatorial transition, which is also popular in
3

practice. In Section 4 we discuss several other well-studied choices for the transitions.
Here we consider trees weighted by their maximum likelihood. In Section 3 we discuss
how the Markov chain definition and our main result extends to sampling the posterior
distribution.
An SPR transition from a tree T works by choosing an (internal or terminal) edge
e = (u, v). If e is an internal edge we consider one of the two subtrees in T \ e, either
the subtree Su containing u or the subtree Sv containing v. Let Su denote the selected
subtree. If e is a terminal edge, let Su be the endpoint of e that is a leaf. Let T 0 denote
the tree formed by removing Su from T , in particular, we remove Su and edge e from T
and â€œsmooth awayâ€ the vertex v (that is, contract one of the two adjacent edges). We
then choose an edge eâˆ— in T 0 and we attach S onto eâˆ— by adding a new intermediate vertex
along eâˆ— . See Figure 1 for an illustration. Let SP R(T, Su , eâˆ— ) denote the tree resulting
from the above transition.
B
A

B

C

*

C

A
E

D

E
F

D

G

F

G

Figure 1: Illustration of an SPR transition. The randomly chosen edge e is marked by
an arrow. The subtree containing B and C is pruned, and then regrafted at the edge eâˆ—
marked by a starred arrow. The resulting tree is illustrated.

We analyze the following Markov chain which chooses a random subtree S to prune, and
then chooses an edge to regraft S along based on the maximum likelihood of the resulting
tree. This Markov chain is an analogous to heat bath chains studied in Statistical Physics
(as opposed to Metropolis chains), e.g., see [4], thus we refer to the below chain as the
Heat Bath SPR Markov Chain. Here is the formal definition of the transitions Tt â†’ Tt+1
of the Heat Bath SPR Markov Chain.
From a tree Tt at time t we proceed as follows.
1. Choose a random subtree S of Tt , by choosing a random edge e and then choosing
one of the two subtrees hanging off of e. Let T 0 denote the tree formed by deleting
S and e from T .
2. For each edge eâˆ— of tree T 0 , let w(eâˆ— ) = LTb (D) where Tb = SP R(T, S, eâˆ— ) is the tree
4

âˆ—
formed by pruning S from T and regrafting S onto
Pedge e . Let0 Ï‰ be the distribution
0
âˆ—
âˆ—
on edges of T where Ï‰(e ) = w(e )/Z and Z = e0 âˆˆE(T 0 ) w(e ).

3. Sample an edge eâˆ— from the distribution Ï‰ on edges of T 0 .
4. Graft S onto edge eâˆ— and move to this new tree, i. e., set Tt+1 = SP R(T, S, eâˆ— ).
We now verify that the above Markov chain is ergodic and reversible with respect to
the distribution Ï€ on trees where Ï€(T ) âˆ LT (D), and thus Ï€ is also the unique stationary
distribution. Let T1 and T2 be neighboring states of the Markov chain. Let S be the tree
that is pruned and regrafted to obtain T2 from T1 . Note that the same tree S can be
pruned and regrafted to obtain T1 from T2 . The transition probability from T1 to T2 is the
probability of choosing S in step 1 times LT2 (D)/Z. Similarly the transition probability
from T2 to T1 is the probability of choosing S in step 1 times LT1 (D)/Z (note that Z is
the same in both cases since pruning S results in the same tree T 0 ). The detailed balance
condition is satisfied for Ï€(T ) âˆ LT (D) and hence it is the unique stationary distribution.
Let dTV (Âµ, Î½) denote the (total) variation distance between a pair of probability distributions Âµ and Î½ defined on the same finite, discrete space, and let P t (T0 , Â·) denote the
t-step distribution of the Markov chain from initial state T0 . The mixing time Ï„mix is
defined as
Ï„mix = max min{t : dTV (P t (T0 , Â·), Ï€) â‰¤ 1/2e},
T0

which is the time to reach variation distance â‰¤ 1/2e of the stationary distribution from
the worst initial state. Note, it is straightforward to then â€œboostâ€ so that for any Î´ > 0,
after Ï„mix ln(1/Î´) steps we are within variation distance â‰¤ Î´ of Ï€, from the worst initial
state (see Aldous [1]).

1.3

Results on MCMC for Phylogenetic Reconstruction

MCMC algorithms are an important tool for phylogenetic reconstruction. MrBayes [10]
is a popular program that relies on MCMC methods for Bayesian inference of phylogeny.
MrBayes uses a sophisticated variant of MCMC known as Metropolis-Coupled MCMC [8].
For statistical inference problems, such as phylogenetic reconstruction, it is often easy
to design appropriate MCMC algorithms, such as the above Markov chain we defined using
SPR transitions, which converge in the limit over time to the desired posterior distribution.
However, the computational efficiency of these methods rely on their fast convergence to
the posterior distribution. Since theoretical results are typically lacking, heuristic methods
are used to measure convergence to the desired distribution. Hence, there are often no
rigorous guarantees on the scientific computations which rely on the random samples
produced by the MCMC methods. Our goal is to provide some theoretical understanding of
settings where MCMC methods for phylogenetic reconstruction are provably fast and hence
yield accurate results, and settings where the MCMC methods are slow and consequently
the samples may be misleading.
5

There are several works with computational experiments on the convergence rates
of MCMC algorithms for phylogenetic reconstruction, e. g., see the recent works [3, 11].
There is relatively little theoretical work. Diaconis and Holmes [5] proved fast convergence
of a Markov chain to the uniform distribution over phylogenetic trees. Recently, several
works have shown examples of heterogeneous data where MCMC algorithms are provably
slow to converge. Mossel and Vigoda [13, 14] proved slow convergence for a class of
examples with data arising from a uniform mixture of a pair of 5-taxa trees (with different
topologies). SÌŒtefankovicÌŒ and Vigoda [19, 20] proved slow convergence for a class of mixture
examples from a pair of 5-taxa trees that share the same topology but differ in their branch
lengths. In these slow mixing results, the convergence time is exponential in the number
of characters (i. e., sequence length).
In this paper we show fast convergence for data from a homogenous source of closely
related species. In particular, for data generated from a single tree (of any size) when
all the branch lengths are sufficiently short, we prove fast convergence. The requirement
of sufficiently short branches is for our proof technique, but it is important to note that
the slow mixing results mentioned earlier [13, 14, 19, 20] require, in an analogous manner,
sufficiently short branches. If one searches for the tree with the maximum likelihood (or
maximum a posteriori probability) our methods show that in our setting of very short
branch lengths, the space of trees (connected by the SPR moves) has no local maxima and
hence one can find the optimal tree using hill climbing.
For simplicity, we present our results here for the case where the weight of a tree is the
maximum likelihood of generating the given data D where the maximum is over a rate
matrix Q (common to all edges) and a set of branch lengths t. This is closely related to
the posterior distribution when the priors are uniformly distributed. Our results extend
to Î´-regular priors, which are priors that are lower bounded by some Î´ > 0, see Section 3
for a discussion on the extension of our results to sampling the posterior distribution. We
are interested in the mixing time Ï„mix , defined as the number of steps until the chain is
within variation distance â‰¤ 1/2e of the stationary distribution.
We prove that the Heat Bath SPR Markov Chain converges quickly to its stationary
distribution when the data is generated from a tree T âˆ— where all of the branch lengths are
sufficiently small, and there are sufficiently many samples generated from T âˆ— . Here is the
formal statement of our main result.
Theorem 1. Consider any reversible 4-state model, any phylogenetic tree T âˆ— on n taxa
and any rate matrix Qâˆ— with no zero entries. For all Î±min > 0, there exists 0 > 0, such
that for all 0 < Îµ < Îµ0 and any choice of branch lengths tâˆ—e âˆˆ (Î±min , ) for e âˆˆ E(T âˆ— ),
there exists N0 > 0 where the following holds.
For a data set with N > N0 characters, each
âˆš chosen independently from the distribution
ÂµT âˆ— ,Qâˆ— ,tâˆ— , then, with probability > 1 âˆ’ exp(âˆ’ N ) over the data generated, the Heat Bath
SPR Markov Chain has mixing time â‰¤ 50n.
Since there is considerable quantification in the above theorem we will take a moment
to dissect it at a high-level. First off, the requirement that N > N0 comes from needing
6

the data to look very much like the generating distribution Âµâˆ— = ÂµT âˆ— ,Qâˆ— ,tâˆ— . Therefore,
how much data we need depends on several quantities, such as the minimum probability
of a configuration arising in Âµâˆ— , which depends on the minimum branch length and the
minimum rate in Qâˆ— . Hence, N0 depends on Î±min and Qâˆ— , and is exponential in n. A
somewhat related question has been studied by Steel and SzeÌkely [16, 17, 18] on how large
N needs to be so that the maximum likelihood tree is the generating tree T âˆ— . In their
results one also needs N to be exponential in n.
Our proof uses the fact that in the setting of Theorem 1, where the branch lengths are
sufficiently short, the leading term of the maximum likelihood function is actually maximum parsimony. Such a result is well-known in the mathematical phylogeny community,
and was first observed by Felsenstein [7]. We require a more detailed statement of such
a result, which we present in Lemma 2 in Section 2.2. Our main technical contribution
is a combinatorial proof that, in the setting of Theorem 1, SPR moves can be used in a
greedy manner to quickly find the maximum parsimony tree. This result is presented in
Section 2.3. Finally, in Section 2.4 we show how Theorem 1 follows in a straightforward
manner from these combinatorial results. In Section 3 we discuss how Theorem 1 extends
to Bayesian inference. We make some concluding remarks in Section 4.

2
2.1

Proof of Rapid Mixing
Overview

To prove Theorem 1 we will analyze, for every tree T , the maximum expected log-likelihood
LT (Âµâˆ— ) where Âµâˆ— = ÂµT âˆ— ,Qâˆ— ,tâˆ— (recall that LT (Âµâˆ— ) is the maximum expected log-likelihood
of T maximized over all rate matrices Q and all edge lengths t, see (3)). To analyze LT (Âµâˆ— )
we will consider the dominant terms of the likelihood function. We will show that
LT (Âµâˆ— ) = E(Ï€ âˆ— ) + A(T )Îµ ln Îµ + o(Îµ ln Îµ),
where E(Ï€ âˆ— ) is the entropy of the stationary distribution of Qâˆ— and thus is the same for
every T . By taking Îµ sufficiently small the last term o(Îµ ln Îµ) can be ignored. Therefore,
the dominant term is A(T )Îµ ln Îµ. We will prove that the function A(T ) decreases with each
optimal SPR move. Hence, since ln Îµ is negative, we then have that T âˆ— has the highest
maximum expected log-likelihood, and as the Markov chain gets closer to T âˆ— the maximum
expected log-likelihood will increase. Theorem 1 will then follow in a straightforward
manner.

2.2

Analyzing Likelihood

Let T be a tree with leaves {1, . . . , n}. Let R be a partition of the leaves into two parts
(R1 , R2 ). Note that we only consider the partition of the leaves without any regard for
the internal vertices. Let cutR (T ) denote the size of the cut (i. e., a subset of edges) of
minimum size that disconnects R1 from R2 . Tuffley and Steel [21] showed that the quantity
7

cutR (T ) is the parsimony score of the character corresponding to R (see also Semple and
Steel [15, Proposition 5.1.6]), where the character corresponding to R = (R1 , R2 ) assigns
all leaves in R1 some Î± âˆˆ â„¦ and assigns all leaves in R2 some Î² âˆˆ â„¦, Î² 6= Î±.
For an edge e âˆˆ T , the removal of e splits T into two components. This induces a
partition of the leaves of T into two parts. We will call this partition R(T, e).
The following is the main technical tool for our results. The lemma describes the highorder terms of the likelihood function as Îµ â†’ 0. Throughout this paper, the asymptotic
notations o() and O() are parameterized by Îµ â†’ 0.
Roughly speaking, the lemma shows there is a function A(T ) which plays a leading
role in the maximum likelihood. In words, A(T ) looks at each partition R of leaves in the
generating tree T âˆ— realized by cutting a single edge eâˆ— . It then considers the minimum
number of edges in T to realize this partition R times the branch length of eâˆ— in the
generating tree. As mentioned in the introduction, there are earlier results which show
that the leading term of the likelihood function is the parsimony score, e.g., Felsenstein [7],
and in the below lemma, the function A is the leading term of the expected parsimony
score. We require a more detailed statement than we found in the literature.
Lemma 2. Let T âˆ— , T be trees with leaves {1, . . . , n} and Qâˆ— be a rate
P matrix reversible
with respect to Ï€ âˆ— . Assume that the matrix Qâˆ— is normalized (that is, i6=j Ï€iâˆ— Qâˆ—ij = 1) and
âˆ— , Î±âˆ—
that Qâˆ— has no zero entries. Let T âˆ— have branch lengths tâˆ—e = Î±eâˆ— Îµ, where Î±eâˆ— âˆˆ [Î±min
max ],
âˆ—
for all e âˆˆ E(T ), where Î±min > 0. Let
X
A = AT âˆ— ,Î±âˆ— (T ) =
Î±eâˆ— cutR(T âˆ— ,e) (T ).
(5)
eâˆˆE(T âˆ— )

For Âµâˆ— = ÂµT âˆ— ,Qâˆ— ,tâˆ— the following holds:
LT (Âµâˆ— ) = E(Ï€ âˆ— ) + AÎµ ln Îµ + O(Îµ ln ln(1/Îµ)),

(6)

P
where E(Ï€ âˆ— ) = iâˆˆâ„¦ Ï€iâˆ— ln Ï€iâˆ— is the entropy of Ï€ âˆ— , and the constant in the O(Â·) is indepenâˆ—
âˆ—
dent of the choice of the Î±eâˆ— (but does depend on Î±min
and Î±max
).
As a consequence of the above lemma, to analyze the expected log-likelihood on the
tree space, when Îµ is sufficiently small, we simply have to consider the function A = A(T ).
In the next subsection we will investigate the combinatorial properties of A.
Before presenting the proof of Lemma 2 we give a brief outline of its proof. Let Âµâˆ—
denote the probability distribution defined by Qâˆ— and T âˆ— on assignments of labels from
â„¦ to the leaves. In this generating distribution Âµâˆ— , to prove (6) we only need to consider
two types of assignments. The first type are constant assignments where no substitutions
occur and thus all leaves receive the same label i âˆˆ â„¦, these are denoted as Ïƒi . The second
type are assignments obtained by a substitution along just one edge eâˆ— . In this case, the
cut obtained by deleting edge eâˆ— plays an important role. By deleting eâˆ— from T âˆ— , the
leaves are partitioned into two sets R1 and R2 , denoted as R(T âˆ— , eâˆ— ). If a substitution
only occurs along edge eâˆ— , then the leaves in R1 will receive the same label i âˆˆ â„¦, and
8

the leaves in R2 will receive another label j âˆˆ â„¦, j 6= i. We denote such an assignment
eâˆ— . Any other type of assignment requires at least two substitutions, and hence has
by Ïƒij
probability at most O(Îµ2 ), which is dominated by the O(Îµ ln ln(1/Îµ)) term of (6).
For any tree T , to prove that LT (Âµâˆ— ) is lower bounded by the right-hand side of (6),
we compute the expected log-likelihood of Âµâˆ— for the rate matrix Q = Qâˆ— and the set of
branch lengths t where te = Îµ for every edge e. For each edge eâˆ— and its corresponding
eâˆ— , the quantity cut
assignment Ïƒij
R(T âˆ— ,e) (T ) is the minimum number of edges which require
eâˆ— on T . Hence, the quantity A = A(T ) plays
a substitution to obtain the assignment Ïƒij
an important role when we sum over all edges eâˆ— of T âˆ— . In particular, by a calculation
(as detailed in
the expected log-likelihood LT,Q,t (Âµâˆ— ), for this set of branch
P (13) âˆ—below),
âˆ—
lengths t, is iâˆˆâ„¦ Ï€i ln Ï€i + AÎµ ln Îµ + O(Îµ). Since O(Îµ) = O(Îµ ln ln(1/Îµ)), this implies the
lower bound of (6).
To obtain the upper bound of (6) we consider three cases: when the rate matrix Q has a
stationary distribution different from Qâˆ— , when there is an edge e where te is long (namely
â‰¥ Îµ(ln(1/Îµ))2 ), and when all edges are short. In the first case of different stationary
distributions, by considering the constant assignments it will be easy to establish that
there is a difference in the first term of the right-hand side of (6). When there is a long
edge, then the constant assignments are too unlikely to occur. Finally, if all edges are
shorter than Îµ(ln(1/Îµ))2 , then, by calculation, we show that the expected log-likelihood is
at most the right-hand side of (6).
We now present the formal proof of Lemma 2.
Proof of Lemma 2. We first make some observations about the distribution Âµâˆ— . Let P âˆ—
denote the transition matrix for Qâˆ— , as defined in (1).
Note, for any e, any i, j âˆˆ â„¦ where i 6= j, we have
(Peâˆ— )ij = Qâˆ—ij Î±eâˆ— Îµ + O(Îµ2 ).
For any i âˆˆ â„¦ we have

(Peâˆ— )ii = 1 âˆ’

X

(7)

(Peâˆ— )ij = 1 + O(Îµ).

j6=i

For i âˆˆ â„¦, let Ïƒi âˆˆ â„¦n denote the constant assignment Ïƒi (v) = i for all leaves v. Note
to achieve Ïƒi in Âµâˆ— we assign label i to the root and then we have no substitutions, or we
have at least two edges with substitutions. Thus,
Y
Âµâˆ— (Ïƒi ) = Ï€iâˆ—
(Peâˆ— )ii + O(Îµ2 ) = Ï€iâˆ— + O(Îµ).
(8)
eâˆˆE(T âˆ— )
e âˆˆ â„¦n denote the assignment of
For an edge e âˆˆ E(T âˆ— ) and i, j âˆˆ â„¦ where i 6= j, let Ïƒij
âˆ—
label i to all leaves in one of the partitions of R(T , e) and label j to all leaves in the other
partition of R(T âˆ— , e). In this case we have:
e
Âµâˆ— (Ïƒij
) = Ï€iâˆ— Qâˆ—ij Î±eâˆ— Îµ + O(Îµ2 ).

9

(9)

(To see why (9) is correct, w.l.o.g., assume that the root is a leaf in the first partition of
R(T âˆ— , e), and hence to achieve Ïƒij we need to label the root by i and have a substitution
on e, or at least two edges with substitutions.)
Now we compute LT,Q,t (Âµâˆ— ) where te = Îµ for each edge e of T and Q = Qâˆ— . Again we
will make some observations about Âµ = ÂµT,Q,t . By the same reasoning as we used for (8),
we obtain
Âµ(Ïƒi ) = Ï€iâˆ— + O(Îµ).
(10)
e on T using a substitution on cut
We can obtain assignment Ïƒij
R(T âˆ— ,e) (T ) edges, and we
cannot obtain this assignment with fewer substitutions. Hence,
e
Âµ(Ïƒij
) = Î˜(ÎµcutR(T âˆ— ,e) (T )) ).

(11)

e
ln Âµ(Ïƒij
) = Î˜(1) + cutR(T âˆ— ,e) (T ) ln Îµ.

(12)

Therefore,
In order to compute the high-order terms of LT,Q,t (Âµâˆ— ) we do not need to consider labelings
e (the other labelings have probability O(Îµ2 ) in Âµâˆ— ).
other than Ïƒi and Ïƒij
Combining (9), (8), (10), and (12) we obtain
X
LT,Q,t (Âµâˆ— ) = O(Îµ2 ln Îµ) +
(Ï€iâˆ— + O(Îµ)) ln(Ï€iâˆ— + O(Îµ))
iâˆˆâ„¦

+

X X

(Ï€iâˆ— Qâˆ—ij Î±eâˆ— Îµ + O(Îµ2 ))(Î˜(1) + cutR(T âˆ— ,e) (T ) ln Îµ)

eâˆˆE(T âˆ— ) i6=j

= O(Îµ) +

X

Ï€iâˆ— ln Ï€iâˆ— + AÎµ ln Îµ,

(13)

iâˆˆâ„¦

where in the last inequality we used the fact that Qâˆ— is normalized. This proves the lower
bound in (6).
It remains to prove the upper bound in (6). We will show that no rate matrix and
no assignment of branch lengths can do better than the bound established in (13). Let
Q be a rate matrix with stationary distribution Ï€. If Ï€ 6= Ï€ âˆ— then we bound LT,Q,t (Âµâˆ— )
as follows. First, note that the terms in the sum (4) are negative and hence to obtain an
upper bound we will only consider the constant assignments. Second, the probability of
constant assignment Ïƒi in Âµâˆ— is Âµâˆ— (Ïƒi ) â‰¤ Ï€iâˆ— and similarly Âµ(Ïƒi ) â‰¤ Ï€. Thus
X
X
LT,Q,t (Âµâˆ— ) â‰¤
Ï€iâˆ— ln Ï€i =
Ï€iâˆ— ln Ï€iâˆ— âˆ’ DKL (Ï€ âˆ— kÏ€),
iâˆˆâ„¦

iâˆˆâ„¦

where DKL (Ï€ âˆ— ||Ï€) := iâˆˆâ„¦ Ï€iâˆ— (ln(Ï€iâˆ— /Ï€i )) is the K-L divergence of Ï€ from Ï€ âˆ— . Since, by
the Gibbsâ€™ inequality, the KL-divergence is positive when Ï€ 6= Ï€ âˆ— , we have established the
upper bound in (6) for the case Ï€ 6= Ï€ âˆ— .
Now we assume Ï€ = Ï€ âˆ— . Let t be an assignment of branch lengths to T . Let Âµ = ÂµT,Q,t .
Suppose that there exists an edge f âˆˆ E(T ) with branch length tf > Îµ(ln(1/Îµ))2 . We are
P

10

going to show that such a t has a tiny log-likelihood because of the constant leaf labelings
(i. e., Ïƒi , i âˆˆ â„¦). By (1), we have (Pf )ii â‰¤ 1 âˆ’ qmin Îµ(ln(1/Îµ))2 + O(Îµ2 (ln(1/Îµ))4 ), where
qmin = mini,jâˆˆâ„¦ |Q(i, j)|. Hence,

Âµ(Ïƒi ) â‰¤ Ï€i 1 âˆ’ qmin Îµ(ln(1/Îµ))2 + O(Îµ2 (ln(1/Îµ))4 ) .
Thus
LT,Q,t (Âµâˆ— ) â‰¤ O(Îµ) +

X
iâˆˆâ„¦

Ï€iâˆ— ln(Ï€i ) âˆ’ qmin Îµ(ln(1/Îµ))2 + O(Îµ2 (ln(1/Îµ))4 )



âˆ—

â‰¤ E(Ï€ ) âˆ’ qmin Îµ(ln(1/Îµ))2 + O(Îµ).

(14)

As Îµ â†’ 0, (14) is smaller than the right-hand side of (6) and we are done.
We are now left with the case in which all edges f âˆˆ E(T ) have branch lengths
tf â‰¤ Îµ(ln(1/Îµ))2 . Since we can generate the leaf labelings starting from any vertex, then
by starting at a leaf, we see that:
ln Âµ(Ïƒi ) â‰¤ ln Ï€i .

(15)

e , we need to have substitutions across all edges
Moreover, for e âˆˆ E(T âˆ— ), to generate Ïƒij
in a cut that realizes R(T âˆ— , e). Since the edges are short this happens with probability
k
â‰¤ Îµ(ln(1/Îµ))2 where k is the size of the cut. Since there are at most 2n such cuts and
each has size at least cutR(T âˆ— ,e) (T ), we have that:
e
ln Âµ(Ïƒij
) = cutR(T âˆ— ,e) (T )(O(ln ln(1/Îµ)) + ln Îµ).

(16)

Hence,
LT,Q,t (Âµâˆ— ) â‰¤ O(Îµ2 ln Îµ) + E(Ï€)
X X
+
(Ï€iâˆ— Qâˆ—ij Î±eâˆ— Îµ + O(Îµ2 ))cutR(T âˆ— ,e) (T )(O(ln ln(1/Îµ)) + ln Îµ)
eâˆˆE(T âˆ— ) i6=j

= O(Îµ ln ln(1/Îµ)) + E(Ï€) + AÎµ ln Îµ.

2.3

Analyzing the Cut-Distance A(T )

In light of Lemma 2, we need to analyze how A(T ) changes with SPR moves. By taking N
sufficiently large, for each subtree S, we will only need to analyze the effect of the optimal
SPR move for S (optimal in terms of minimizing A(T 0 )).
The quantity A(T ) looks at cuts obtained by single edges of T âˆ— . For a tree T , we
classify the edges of T âˆ— as good or bad if their corresponding cut in T âˆ— is realizable in T
by cutting a single edge. More precisely, let
GOODT âˆ— (T ) = {eâˆ— âˆˆ E(T âˆ— ) : there exists e âˆˆ E(T ) where R(T, e) = R(T âˆ— , eâˆ— )},
11

be the set of good edges for T . Let BADT âˆ— (T ) = E(T âˆ— ) \ GOODT âˆ— (T ).
The following lemma says that for every tree TÌƒ obtained from T by an SPR move
using S, if TÌƒ has more bad edges than T , then this was not the optimal SPR move using
S. Namely, there is a tree T 0 which is also obtained from T by an SPR move using S, and
T 0 is such that A(T 0 ) < A(TÌƒ ). (More precisely, each term in A(T 0 ) is less than or equal
to the corresponding term in A(TÌƒ ) and there is a term in A(T 0 ) which is strictly smaller
than the corresponding term in A(TÌƒ )). Our proof has some similarity to those of Bruen
and Bryant [2] which connect the parsimony score of a character to the minimum number
of SPR transitions needed to obtain the character.
Lemma 3. For every generating tree T âˆ— and all trees T, TÌƒ where T and TÌƒ differ by a
prune-and-regraft of a subtree S and such that there exists f âˆ— âˆˆ BADT âˆ— (TÌƒ ) \ BADT âˆ— (T )
the following holds. There exists a tree T 0 which differs from T by a prune-and-regraft of
S and such that cutR (T 0 ) â‰¤ cutR (TÌƒ ) for every partition R realized by single edges in T âˆ—
and cutR (T 0 ) < cutR (TÌƒ ) for partition R realized by f âˆ— in T âˆ— .
Proof. Suppose an edge f âˆ— âˆˆ E(T âˆ— ) is good for T and is bad for TÌƒ . Let L1 , L2 be the
partition of the leaves induced by f âˆ— in T âˆ— . Thus, in T , there is an edge f = (v1 , v2 ) which
partitions the leaves into L1 and L2 . See Figure 2 for an illustration of the setup.

L1

fâˆ—

L1

L2

f

S1
v1

tree T âˆ—

S2

L2

v2

tree T

Figure 2: Edge f âˆ— is good for T .
Let S1 denote the subtree â€œhanging offâ€ of v1 in T . More precisely, after deleting f
from T , let S1 be the subtree containing v1 . Let L1 denote the leaves in S1 . Similarly, let
L2 denote the leaves and S2 denote the subtree hanging off of v2 . Let v denote the root
of the subtree S.
First we claim that f 6âˆˆ S. Suppose f âˆˆ S and without loss of generality suppose
S1 âŠ‚ S. See Figure 3 for an illustration of this case. Thus we must be grafting S into an
edge of S2 \ S. After such a move, the edge f still separates L1 and L2 , and thus f âˆ— is
still good. Therefore, f 6âˆˆ S.
From now on, we assume, without loss of generality, that S âŠ‚ S1 where S 6= S1 . see
Figure 4. We construct the tree T 0 by taking T , pruning S and then regrafting S along
edge f , see Figure 5.
Note that TÌƒ is obtained from T by regrafting S onto an edge in S2 (otherwise f âˆ— would
be good for TÌƒ ), see Figure 6.
12

tree T

S
f

S1

Figure 3: Case when f âˆˆ S, this scenario cannot occur.

S

f

v

L1

v1

S2
v2

L2

S1

Figure 4: Case when f 6âˆˆ S, this must be the scenario.
The following claim says that the tree T 0 satisfies the conclusion of the lemma.
Claim 4. For every partition R = (R1 , R2 ) of leaves realized by edges of T âˆ— , it holds that:
cutR (TÌƒ ) â‰¥ cutR (T 0 ),
Moreover, for the partition Râˆ— = (L1 , L2 ) (corresponding to f âˆ— ), we have that:
cutRâˆ— (TÌƒ ) > cutRâˆ— (T 0 ).
The proof of the claim proceeds by constructing a cut in T 0 realizing (R1 , R2 ) by a
small modification of a cut in TÌƒ realizing (R1 , R2 ). Assuming the claim, the proof of the
lemma is now complete.
We now prove the above claim.
Proof of Claim 4. We continue using the setup and notation from the proof of Lemma 3
in Section 2.3.
Recall, the claim says that for every partition R = (R1 , R2 ) of leaves realized by edges
of T âˆ— that cutR (TÌƒ ) â‰¥ cutR (T 0 ) and for the partition Râˆ— = (L1 , L2 ), cutRâˆ— (TÌƒ ) > cutRâˆ— (T 0 ).

13

L1

S1 \ S

w

v1

v2

f0

f

S2

L2

v
S
L1

Figure 5: Construction of the tree T 0 .

f
L1

S1 \ S

S2
v2

v1

L2

v
S
L1

Figure 6: In tree TÌƒ , S is regrafted into S2 .

First we argue that cutRâˆ— (TÌƒ ) > cutRâˆ— (T 0 ). Note that cutRâˆ— (TÌƒ ) â‰¥ 2 since f âˆ— is bad for
TÌƒ . On the other hand, cutRâˆ— (T 0 ) = 1 since cutting f 00 separates L1 and L2 . Now we just
need to argue that cutR (TÌƒ ) â‰¥ cutR (T 0 ).
Let g âˆ— âˆˆ E(T âˆ— ) be an edge in T âˆ— . Let R = (R1 , R2 ) be the corresponding partition in
âˆ—
T . Note that if g âˆ— is in the subtree with leaves L1 then
R1 âŠ† L1 and R2 âŠ‡ L2 .

(17)

On the other hand, if g âˆ— is in the subtree with leaves L2 then
R2 âŠ† L2 and R1 âŠ‡ L1 .

(18)

Consider a minimum cut C âŠ‚ E(TÌƒ ) that realizes (R1 , R2 ) in TÌƒ and amongst these
minimum cuts is the one with the fewest number of edges in subtrees S1 \ S and S.
We claim that v1 is reachable from a leaf of S1 \ S in TÌƒ \ C and that v is reachable from
a leaf of S in TÌƒ \ C. Suppose that v1 is not reachable from a leaf of S1 \ S. Let e0 be the
edge in C âˆ© (S1 \ S) closest to v1 . We claim that C 0 = (C \ {e0 }) âˆª {f } realizes (R1 , R2 ). If
there was a pair of leaves in S1 \ S each in different Ri that are connected in TÌƒ \ C 0 then
14

by the choice of e0 one of those leaves would be connected to v1 in TÌƒ \ C, a contradiction
with the assumption that v1 is not reachable from a leaf of S1 \ S in TÌƒ \ C. Thus R1 and
R2 are still separated in S1 \ S in TÌƒ \ C 0 ; R1 and R2 are still separated by C 0 in S2 âˆª S
since C = C 0 in this subtree; and f âˆˆ C 0 ensures that pairs across f are separated. Note
that |C 0 | â‰¤ |C| and C 0 has fewer edges in S1 \ S and S, a contradiction with the choice of
C. Thus v1 is reachable from some leaf of S1 \ S in TÌƒ \ C. The argument for S and v is
the same.
Since a leaf of S is reachable from v in TÌƒ \ C, then in other words a (non-empty) subset
of R1 and/or R2 are reachable from v. Moreover, since C realizes the partition (R1 , R2 ),
then a subset of only one of the Ri is reachable from v in TÌƒ \ C; we will say v is of type
Ri to signify the Ri reachable from v. Analogously, we say vi is of type Ri for the set
reachable from v1 .
If v and v1 are of the same type Ri , let
(
C
if f âˆˆ
/C
C0 =
0
(C \ {f }) âˆª {f } if f âˆˆ C
We claim C 0 realizes (R1 , R2 ) in T 0 . To see this, note that if a path (between a pair of
leaves) exists in T 0 and does not exist in TÌƒ then it must include w which is the new vertex
in T 0 where S is regrafted, see Figure 5 for an illustration. Now we argue that such a path
can not connect a leaf in R1 with a leaf in R2 in T 0 \ C 0 . Note that only a subset of Ri is
reachable from w in T 0 \ C 0 , since w can reach the same set of vertices (outside of S) in
T 0 \ C 0 as v1 does in TÌƒ \ C, and only a subset of Ri is reachable from v in S \ C. Finally,
since |C 0 | = |C| we have that cutR (T 0 ) â‰¤ cutR (TÌƒ ) which completes the proof in this case.
Now suppose v1 is of type R1 and v is of type R2 . This means a leaf of S is in R2
and since S âŠ‚ S1 , it is also in L1 and we are in case (17), thus R2 âŠ‡ L2 . Note C has
to separate v1 from L2 by some set of edges Q âŠ† C. Let C 0 = (C \ Q) âˆª {f }. The new
pairs of leaves that are connected in T 0 \ C 0 (but not in TÌƒ \ C) are either both from L2
and hence R2 , or are connected by a path that exists in T 0 and does not exist in TÌƒ . As in
the previous case, if a path (between a pair of leaves) exists in T 0 and does not exist in TÌƒ
then it must include w which is the new vertex in T 0 where S is regrafted. Note that w is
disconnected from S1 \ S in T 0 \ C 0 (since f âˆˆ C 0 ). The leaves of S2 are from R2 and the
leaves of S reachable from v in S \ C are also from R2 . Therefore the new paths do not
connect leaves of R1 and R2 . This completes this case since |C 0 | â‰¤ |C|.
Finally, suppose v1 is of type R2 and v is of type R1 . In this case a leaf of S1 \ S is in
R2 and is also in L1 and therefore we are again in case (17), thus R2 âŠ‡ L2 . Note C has to
separate v from L2 by some set of edges Q âŠ† C. Let C 0 = (C \ Q) âˆª {w, v}. Once again,
the new pairs of leaves that are connected in T 0 \ C 0 (but not in TÌƒ \ C) are either both
from L2 and hence R2 , or are connected by a path that exists in T 0 and does not exist in
TÌƒ . Note that w is disconnected from the leaves of S in T 0 \ C 0 . The leaves of S2 are from
R2 and the leaves of S1 \ S reachable from v1 are also from R2 . Therefore the new paths
in T 0 \ C 0 do not connect leaves of R1 and R2 . This completes this case since |C 0 | â‰¤ |C|.
15

This completes the proof of the claim.
Using Lemma 3, we will prove that for every subtree S the optimal SPR move using
S does not increase the number of bad edges, and there is a subtree S where the optimal
SPR move using S decreases the number of bad edges. It will then be straightforward to
prove rapid mixing by analyzing the time until the number of bad edges is zero, and hence
we have reached T âˆ— .
Lemma 5. For all trees T âˆ— , every choice of parameters Î± : E(T âˆ— ) â†’ R+ , for all trees
T 6= T âˆ— the following holds, where A = AT âˆ— ,Î± is defined in (5).
1. For any subtree S of T the following holds. Let Tmin be any tree which minimizes
A(Tmin ) amongst the SPR neighbors of T which differ by a prune-and-regraft of S.
Then,
BADT âˆ— (Tmin ) âŠ† BADT âˆ— (T ).
(19)
2. There exists a subtree S of T where the following holds. Let Tmin be any tree which
minimizes A(Tmin ) amongst the SPR neighbors of T which differ by a prune-andregraft of S. Then,
BADT âˆ— (Tmin ) ( BADT âˆ— (T ).
(20)
Part 1 of Lemma 5 follows immediately from Lemma 3. To prove part 2 we choose
a particular â€˜minimalâ€™ subtree S. Roughly speaking, we consider the bad edge f âˆ— that is
closest to the leaves in T âˆ— , and take the subtree S hanging off of f âˆ— .
Proof of Lemma 5. If (19) is violated then there exists f âˆ— âˆˆ BADT âˆ— (Tmin ) \ BADT âˆ— (T ),
and hence by Lemma 3, there exists T 0 (which differs from T and Tmin by a prune-andregraft of S) such that no cuts increased in size and the cut corresponding to f âˆ— is smaller.
Therefore, A(T 0 ) < A(Tmin ), contradicting the choice of Tmin . Therefore, part 1 holds.
We now prove part 2. We first claim that there is an SPR move that decreases the
number of bad edges.
Claim 6. For every tree T , there is an SPR move resulting in a tree T 0 where
BADT âˆ— (T 0 ) ( BADT âˆ— (T ).

(21)

Now we argue that part 2 of the lemma follows from the above claim and part 1. We
then go back to prove the claim.
Consider a subtree S of T . Let NS (T ) denote those trees obtainable from T by a
prune-and-regraft of S. Note, for any T 0 âˆˆ NS (T ), we have that NS (T 0 ) = NS (T ), since
when we prune S from T and T 0 we have the same subtree remaining.
Let T 0 denote the neighboring tree from Claim 6 with fewer bad edges, and let S denote
the subtree where T 0 âˆˆ NS (T ). Let Tmin denote the tree in NS (T ) which minimizes
A(Tmin ). As noted above we must have that NS (T 0 ) = NS (T ). Thus, Tmin is also the
neighbor of T 0 that minimizes A(Tmin ). Therefore, we can apply part 1 of the lemma for
16

tree T 0 and subtree S, and conclude that BADT âˆ— (Tmin ) âŠ† BADT âˆ— (T 0 ). Combined with
(21) we then have that:
BADT âˆ— (Tmin ) ( BADT âˆ— (T ),
which proves part 2.
We now prove Claim 6.
Proof of Claim 6. Let f âˆ— in T âˆ— be an edge in BADT âˆ— (T ) that is â€œclosestâ€ to the leaves in
the following precise sense. Say f âˆ— joins subtrees S âˆ— and Z âˆ— in T âˆ— where the number of
vertices in S âˆ— is at most the number of vertices in Z âˆ— . Then we say the distance of f âˆ— to
the leaves is the number of vertices of S âˆ— .
Note, by the choice of f âˆ— , S âˆ— contains no bad edges for T . First, note that S âˆ— must
contain at least two leaves because, in any tree, any single leaf can be separated from the
rest of the leaves by deleting one edge (which would contradict that f âˆ— is bad). Let S1âˆ—
and S2âˆ— denote the two subtrees of S âˆ— hanging from the root of S âˆ— in T âˆ— . Both S1âˆ— and S2âˆ—
must exist since S âˆ— contains at least two leaves.
Let L1 and L2 denote the leaves in S1âˆ— and S2âˆ— , respectively. Since f âˆ— is the closest bad
edge to the leaves, there is a subtree S1 in T whose leaves are L1 , and also a subtree S2
whose leaves are L2 . Moreover, by induction, S1 = S1âˆ— and S2 = S2âˆ— . In T , by pruning S2
and then regrafting along the edge incident to S1 we obtain a copy of S âˆ— in T . See Figure
7 for an illustration. Let T 0 be the tree resulting from this SPR move. Note, f âˆ— is now a
good edge in T 0 .
tree T 0

tree T
f1

S1

f10

f30
f3 f2

f1

S2

S1

f3

f2
S2

Figure 7: Construction of the tree T 0 with fewer bad edges.
It remains to argue that other edges of T âˆ— did not change from good for T to bad for
Note, edges in S1âˆ— and S2âˆ— remain good in T 0 since they are realizable in S1 and S2 ,
respectively. Consider an edge eâˆ— of T âˆ— where eâˆ— âˆˆ
/ S âˆ— . Let (R1 , R2 ) be the partition of
âˆ—
âˆ—
the leaves realized by e in T . Note that L1 , L2 are in the same partition, since tree S âˆ— is
not cut by eâˆ— . Let g be the edge in T that realizes (R1 , R2 ). After pruning-and-regrafting
S2 (to form T 0 ), g still realizes the partition (R1 , R2 ) since L1 and L2 are in the same
partition. Hence, eâˆ— is still good for T 0 . Therefore, GOODT âˆ— (T 0 ) âŠ‡ GOODT âˆ— (T ) âˆª {f âˆ— },
which completes the proof of the claim.

T 0.

17

Finally, we prove that when the number of bad edges increases then A(T ) also increases
by a significant amount. As a consequence, in our analysis of the Markov chain, by taking
Îµ sufficiently small, we can focus on how a transition changes A(T ) and hence on the
change in the number of bad edges.
Lemma 7. For any trees T and T 0 which differ by one SPR move, if |BADT âˆ— (T 0 )| >
|BADT âˆ— (T )| then
A(T 0 ) â‰¥ A(Tmin ) + Î±min .
Proof. Let S be the subtree used to move between T and T 0 . Let NS (T ) denote those
trees obtainable from T by a prune-and-regraft of S. Note NS (T ) = NS (T 0 ).
Consider Tmin which minimizes A(Tmin ) amongst the SPR neighbors of T which differ
by a prune-and-regraft of S. Since NS (T 0 ) = NS (T 0 ) then Tmin is also the neighbor of T 0
that minimizes A(Tmin ). Fix e âˆˆ BADT âˆ— (T 0 ) \ BADT âˆ— (T ). By Part 1 of Lemma 5,
BADT âˆ— (Tmin ) âŠ† BADT âˆ— (T ) âˆ© BADT âˆ— (T 0 ).

2.4

Proof of Rapid Mixing: Theorem 1

The proof of our main theorem now follows from a straightforward argument. We show
that the Heat Bath SPR Markov Chain behaves like a local search algorithm, and then a
simple coupling argument gives the mixing result.
Proof of Theorem 1. Let T denote the space of phylogenetic trees on n taxa. For a tree
T âˆˆ T , and subtree S of T , let NS (T ) denote those trees obtainable from T by pruningand-regrafting S.
Let C be the constant in the O(Â·) notation of (6) for the chosen Î±min and Î±max = 1.
By choosing Îµ0 (note that Îµ0 is an upper bound on Îµ) sufficiently small then for every
tree T , in (6), the CÎµ ln ln(1/Îµ) is smaller than |Î±min (Îµ ln Îµ)/10| and therefore:
LT (Âµâˆ— ) = E(Ï€ âˆ— ) + (A(T ) + Î´T )Îµ ln Îµ,

(22)

for some |Î´T | < Î±min /10.
Fix a tree T 6= T âˆ— and a subtree S of T . By Lemma 7 and (22), for every T 0 âˆˆ NS (T )
where |BADT âˆ— (T 0 )| > |BADT âˆ— (T )| we have:
LT 0 (Âµâˆ— ) < LTmin (Âµâˆ— ) âˆ’ (9/10)Î±min Îµ ln(1/Îµ).
For a character Ïƒ âˆˆ â„¦n , let D(Ïƒ) = |{i : Di = Ïƒ}|. A straightforward application of
Hoeffdingâ€™s inequality [9] and a union bound over Ïƒ âˆˆ â„¦n implies, for all Î´ > 0:
Pr (for all Ïƒ âˆˆ â„¦n , |D(Ïƒ) âˆ’ Âµâˆ— (Ïƒ)N | â‰¤ Î´N ) â‰¥ 1 âˆ’ 2 Â· 4n exp(âˆ’2Î´ 2 N ).
18

Let qmin = mini,jâˆˆâ„¦:i6=j Qi,j denote a lower-bound on the off-diagonal entries in the rate
matrix. For Îµ0 sufficiently small, every labeling of the leaves has probability at least Îµ2n ;
this follows from the fact that for every edge, every transition has probability â„¦(Îµ), see
(7) for a precise statement. Hence, by choosing Îµ0 sufficiently small (relative to Î±min , qmin
and the constant in the error term of (7)), then for all Ïƒ âˆˆ â„¦n , Âµâˆ— (Ïƒ) â‰¥ Îµ2n . Let
Î´ = Î±min Îµ ln(1/Îµ)/(20 Â· 4n n ln Îµ).
Then, for D âˆ¼ Âµâˆ— ,
LT 0 (D) < LTmin (D) âˆ’ (7/10)Î±min Îµ ln(1/Îµ)N,
âˆš
with probability â‰¥ 1 âˆ’ exp(âˆ’ N ) for N sufficiently large. The probability of moving from
T to T 0 after choosing S in step 1 is at most:
exp(LT 0 (D))
< exp(âˆ’(7/10)Î±min Îµ ln(1/Îµ)N ) < exp(âˆ’10n)
exp(LTmin (D))

(23)

for N sufficiently large. Therefore, with probability â‰¥ 1 âˆ’ 4n exp(âˆ’10n), the chain will
move from T to some Tmin (where Tmin is a tree that can be obtained from T by an
SPR move and such that it minimizes A(Tmin )), and thus by part 1 of Lemma 5 the
number of bad edges will not increase. Moreover, if we choose the subtree S satisfying
part 2 of Lemma 5 then the number of bad edges will decrease. Hence, with probability
â‰¥ 1/(4n) âˆ’ 4n exp(âˆ’10n) â‰¥ 1/(5n) the number of bad edges decreases by at least one. In
expectation, after â‰¤ 5n steps of the chain the number of bad edges will be zero, in which
case we have reached T âˆ— . By Markovâ€™s inequality, with probability â‰¥ 9/10 after 50n steps
we reach T âˆ— . Once we reach T âˆ— the probability of moving to a different tree within 50n
steps is at most 50n(4n)2 exp(âˆ’10n) < 1/100. Hence the claimed mixing time follows by
an elementary coupling argument (c.f., [12] for an introduction to the coupling technique)
since from any pair of initial trees, both chains (run independently) reach T âˆ— at time 50n
with probability â‰¥ 1 âˆ’ 1/2e.

3

Bayesian Inference

The goal is often to randomly sample from the posterior distribution over trees. To do
this, we consider a Markov chain whose stationary distribution is the posterior distribution
and analyze the chainâ€™s mixing time, which is a measure of the convergence time of the
chain to its stationary distribution. Let Î¦(T, Q, t) denote a prior density where
Z
XZ
Î¦(T, Q, t)dtdQ = 1.
T

QâˆˆQ

t

Our results extend to priors that are lower bounded by some Î´ > 0 as in Mossel and
Vigoda [14]. In particular, for all trees T and all branch lengths t where te â‰¤ t0 for all
edges e, we require Î¦(T, Qâˆ— , t) â‰¥ Î´. We refer to these priors as (Î´, t0 )-regular priors.
19

Applying Bayes law we get the posterior distribution:
Pr (T, Q, t | D) =
where
Pr (D) =

XZ
T0

Z

Q0 âˆˆQ

t0

ÂµT,Q,t (D)Î¦(T, Q, t)
,
Pr (D)

ÂµT 0 ,Q0 ,t0 (D)Î¦(T 0 , Q0 , t0 )dt0 dQ.

Each tree T then has a posterior weight
Z
Z
ÂµT,Q,t (D)Î¦(T, Q, t)dtdQ.
w(T ) =
QâˆˆQ

Finally, the posterior distribution Âµ on trees is defined as Âµ(T ) = w(T )/

3.1

(24)

t

P

T0

w(T 0 ).

Extension of Theorem 1 to Sampling the Posterior

To sample from the posterior distribution, the Markov chain is defined as in Section 1.2
except that in step 2 the weight w(eâˆ— ) is now set as w(T âˆ— ) defined in (24). This ensures
that the Markov chain is reversible with respect to the posterior distribution, and hence
this is the unique stationary distribution.
Theorem 1 then extends to hold for any priors which are (Î´, 2Îµ0 )-regular The proof
easily extends to this case in the following manner.
In particular, we need to modify the statement of Lemma 2 so that, for any tree T , (6)
is achieved for Q = Qâˆ— and for every set of branch lengths t where te âˆˆ (Îµ/2, 2Îµ) for all
edges e. Then we can use the same proof as Lemma 21 in Mossel and Vigoda [14] to get
an analog of (23) to hold for the posterior weights defined in (24) in place of the maximum
likelihood function exp(L(D)), and the remainder of the proof of Theorem 1 remains the
same.

4

Discussion

NNI Transitions: In a NNI transition, an internal edge e is chosen. Since internal
vertices have degree three, there are four subtrees hanging off of e. There are three
possible ways of attaching these four subtrees to e, and an NNI transition moves to one of
these rearrangements. There are trees T (different from the generating tree T âˆ— ) where no
NNI neighbor (strictly) improves A(T ); moreover, there are cases where there is also no
improvement in the next term of (6). We are uncertain as to whether Theorem 1 holds
for a Markov chain based on NNI transitions. It would be especially intriguing if there
are cases where chains based on NNI transitions are slow to converge (so-called torpidly
mixing), whereas a chain based on SPR transitions is provably fast to converge (rapidly
mixing).

20

Possible Future Work: There are now several works with proofs of convergence of
MCMC algorithms for phylogenetic reconstruction in certain settings â€“ rapid mixing results in this paper and torpid mixing results in Mossel and Vigoda [13, 14] and SÌŒtefankovicÌŒ
and Vigoda [19, 20]). All of these results require that the branch lengths are sufficiently
small so that only the dominant terms of the likelihood function need to be considered.
A natural avenue for extending this paper, is to allow arbitrary branch lengths on the
terminal edges.
Rapid or Torpid Mixing for General Pure Distributions: The most tantalizing
question to the authors is whether there exists a pure distribution (i. e., a single generating
tree as in the setting of this paper) where Markov chains based on all of the natural
transitions (e. g., NNI, SPR and TBR transitions) are slow to converge to the stationary
distribution (in other words, they are torpidly mixing). We expect simulations can be quite
useful for finding such a bad example, if one exists; in fact, our previous work [19, 20] on
this topic was inspired by some intriguing findings from some simple simulations.

5

Acknowledgements

We are thankful to the anonymous referees for many useful comments on the manuscript.

References
[1] D. Aldous. Random walks on finite groups and rapidly mixing Markov chains. Lecture
Notes Math, 986:243-297, 1983.
[2] T. C. Bruen and D. Bryant. Parsimony via Consensus. Systematic Biology, 57(2):251256, 2008.
[3] R. Beiko, J. Keith, T. Harlow, and M. Ragan. Searching for convergence in phylogenetic Markov Chain Monte Carlo. Systematic Biology, 55(4):553â€“565, 2006.
[4] B. A. Berg. Introduction to Markov Chain Monte Carlo simulations and their statistical analysis. In Markov Chain Monte Carlo: Innovations and Applications, eds.,
W. S. Kendall, F. Liang, and J.-S. Wang, World Scientific Publishing Co., Singapore.
Pages 1-52, 2005.
[5] P. Diaconis and S. P. Holmes. Random walks on trees and matchings. Electronic
Journal of Probability, 7, article 6, 2002.
[6] J. Felsenstein. Inferring Phylogenies. Sinauer Associates, Inc., Sunderland, MA, 2004.
[7] J. Felsenstein. A likelihood approach to character weighting and what it tells us about
parsimony and compatibility. Biological Journal of the Linnean Society, 16:183-196,
1981.
21

[8] C. J. Geyer and E. A. Thompson. Annealing Markov Chain Monte Carlo with applications to ancestral inference. Journal of American Statistical Association, 90(431):909â€“
920, 1995.
[9] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal
of the American Statistical Association, 58(301):13â€“30, 1963.
[10] J. P. Huelsenbeck and F. Ronquist. MRBAYES: Bayesian inference of phylogenetic
trees. Bioinformatics, 17(8):754-755, 2001.
[11] C. Lakner, P. van der Mark, J. P. Huelsenbeck, B. Larget, and F. Ronquist. Efficiency
of Markov Chain Monte Carlo tree proposals in Bayesian phylogenetics. Systematic
Biology, 57(1):86â€“103, 2008.
[12] D. A. Levin, Y. Peres and E. L. Wilmer. Markov Chains and Mixing Times. American
Mathematical Society (AMS), Providence, RI, 2008.
[13] E. Mossel and E. Vigoda. Phylogenetic Markov Chain Monte Carlo algorithms are
misleading on mixtures of trees. Science, 309(5744):2207â€“2209, 2005.
[14] E. Mossel and E. Vigoda. Limitations of Markov Chain Monte Carlo algorithms for
Bayesian inference of phylogeny. Annals of Applied Probability, 16(4):2215â€“2234, 2006.
[15] C. Semple and M. Steel. Phylogenetics. Oxford Lecture Series in Mathematics and its
Applications, vol. 24, 2003.
[16] M. A. Steel and L. A. SzeÌkely. Inverting random functions. Annals of Combinatorics,
3(1):103-113, 1999.
[17] M. A. Steel and L. A. SzeÌkely. Inverting random functions II: explicit bounds for the
discrete maximum likelihood estimation, with applications. SIAM J. Discrete Math
15(4):562-575, 2002.
[18] M. A. Steel and L. A. SzeÌkely. Inverting random functions III: Discrete MLE revisited.
Annals of Combinatorics, 13(3):365-382, 2009.
[19] D. SÌŒtefankovicÌŒ, and E. Vigoda. Pitfalls of heterogeneous processes for phylogenetic
reconstruction. Systematic Biology, 56(1):113â€“124, 2007.
[20] D. SÌŒtefankovicÌŒ, and E. Vigoda. Phylogeny of mixture models: Robustness of maximum likelihood and non-identifiable distributions. Journal of Computational Biology,
14(2):144â€“155, 2007.
[21] C. Tuffley and M. Steel. Links between Maximum Likelihood and Maximum Parsimony under a Simple Model of Site Substitution. Bull. Math. Biology, 59(3):581-607,
1997.
22

[22] S. Yang. Computational Molecular Evolution. Oxford University Press, New York,
2007.

23

