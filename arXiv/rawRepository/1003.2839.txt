On the Border Length Minimization Problem (BLMP) on a
Square Array
∗

Vamsi Kundeti

arXiv:1003.2839v2 [cs.DS] 16 Mar 2010

371 Fairfield Way, U-2031,
A-38
Storrs, CT 06269

vamsik@engr.uconn.edu

Sanguthevar
†

Rajasekaran

371 Fairfield Way, U-2155
Storrs, CT 06269

rajasek@engr.uconn.edu

‡

Hieu Dinh

371 Fairfield Way, U-2031,
A-42
Storrs, CT 06269

hdinh@engr.uconn.edu

ABSTRACT

1. INTRODUCTION

Protein/Peptide microarrays are rapidly gaining momentum in the diagnosis of cancer. High-density and highthroughput peptide arrays are being extensively used to detect tumor biomarkers, examine kinase activity, identify antibodies having low serum titers and locate antibody signatures. Improving the yield of microarray fabrication involves solving a hard combinatorial optimization problem
called the Border Length Minimization Problem (BLM P ).
An important question that remained open for the past seven
years is if the BLMP is tractable or not. We settle this
open problem by proving that the BLMP is N P-hard. We
also present a hierarchical refinement algorithm which can
refine any heuristic solution for the BLMP problem. We
also prove that the TSP+1-threading heuristic is an O(N )approximation.

Cancer diagnosis research has taken a new direction recently
by adopting peptide microarrays for reliable detection of tumor biomarkers (Chatterjee, et al., [1]), (Melle, et al., [7]),
(Welsh, et al., [8]). These high-throughput arrays also find
application in examining kinase activity, identifying antibody signatures against tumor antigens, etc. High-density
peptide arrays are currently fabricated using technologies
such as photolithography or in-situ synthesis based on micromirror arrays. The manufacturers of these arrays are
facing serious fabrication challenges due to unintended illumination effects such as diffraction and scattering of light.
These illumination effects can be reduced dramatically by
selecting a right placement of the peptide probes before fabrication. Finding this placement can be formulated as a
combinatorial optimization problem, known as the Border
Length Minimization Problem (BLMP). Hannenhalli, et al.
first introduced BLMP in 2002 [4]. Although the BLMP
was formulated in the context of DNA microarrays, peptide
arrays share a similar fabrication technology.

The hierarchical refinement solver is available as an opensource code at http://launchpad.net/blm-solve.

Categories and Subject Descriptors
F.2 [Analysis of Algorithms and Problem Complexity]: [Complexity of proof procedures]; G.2.2 [Graph Theory]: [Graph Algorithms]; F.1.3 [Complexity Measures
and Classes]: [Reducibility and completeness]

General Terms
Border length minimization, Quadratic assignment, Microarray optimization, Appoximation algorithms, Computational
biology
∗Graduate student, Computer Science and Engineering Department
†Dr. Rajasekaran, UTC Chair processor, Computer Science
and Engineering Department
‡Graduate student, Computer Science and Engineering Department

The BLMP can be stated as follows. Given N 2 strings of
the same length, how do we place them in a grid of size
N × N such that the Hamming distance summed over all
the pairs of neighbors in the grid is minimized? The BLMP
has received a lot of attention from many researchers. The
earliest algorithm suggested by Hannenhalli, et al. reduces
BLMP to TSP (Traveling Salesman Problem) by computing a tour of the strings and then threading the tour on the
grid [4]. Kahng, et al. have proposed several other heuristic
algorithms which are considered the best performing algorithms in practice [5]. De Carvalho, et al. introduced a
quadratic program formulation of the BLMP but unfortunately the quadratic program is an intractable problem [3].
Later, Kundeti and Rajasekaran formulated the problem as
an integer linear program which performs better than the
quadratic program in practice [6].
Despite many studies on the BLMP, the question of whether
BLMP is tractable or not remained open for the past 7 years.
In this paper, we show that the BLMP is N P-hard. We
also consider a generalization of the BLMP called the Hamming Graph Placement Minimization Problem (HGPMP).
We show that some special cases of the HGPMP are also
N P-hard. On the algorithmic side, we show that a simple
version of the algorithm suggested by Hannenhalli, et al. is
an O(N )-approximation. On the practical side, we propose

a refinement algorithm which takes any solution and tries
to improve it. An experimental study of this refinement
algorithm is also included.
Our paper is organized as follows. Section 2 formally defines the BLMP and HGPMP. Section 3 provides the N Phardness proof of the BLMP and some special cases of the
HGPMP. Section 4 gives the O(N )-approximation algorithm
and the refinement algorithm for the BLMP. Section 5 provides an experimental evaluation of the refinement algorithm. Finally, Section 6 concludes our paper and discusses
some open problems.

2.

PROBLEM DEFINITION

Let S be a set of strings of the same length with S =
{s1 , . . . , sn } and let G = (V, E) be a graph with |V | = n.
A placement of S on G is a bijective map f : S → V . Let
f −1 (u) be the string that is mapped to vertex u by the
placement f . We denote the Hamming distance between
two strings siP
and sj as δ(si , sj ). The cost of placement f
is Cost(f ) = e=(u,v)∈E δ(f −1 (u), f −1 (v)). The Hamming
Graph Placement Minimization Problem (HGPMP) is defined as follows. Given S and G, find a placement of S on G
of minimum cost. We denote the optimal cost as OP T (S, G),
or simply as OP T if it is clear what S and G are.
Obviously, if G is a ring graph, then HGPMP is the same
as the well-known Hamming Traveling Salesman Problem
(HTSP). If G is a grid graph of size N × N (where N 2 =
n), then HGPMP becomes the Border Length Minimization
Problem (BLMP), which is the main study of our paper.

3.

N P -HARDNESS

OF THE BLMP

AND HGPMP
Theorem 1. The BLMP is N P-hard.
We will show that the Hamming traveling salesperson problem (HTSP) for strings (with the Hamming distance metric)
polynomially reduces to the BLMP. The HTSP is already
defined in Section 2.
The idea of the proof is that given 4N strings for the HTSP
we construct (N + 1)2 strings for the BLMP such that from
an optimal solution to this BLMP, we can easily obtain an
optimal solution for the HTSP. So we need to consider the
variant of the HTSP in which the number of strings is divisible by 4. The proof will be presented in stages. The next
three subsections present some preliminaries needed for the
proof of the theorem. Followed by these subsections, the
proof is presented.

3.1

4N -strings traveling salesperson problem
Define an instance of the HTSP as a 4N -strings HTSP if
the number of strings in the input is 4N (for some integer
N ). In this section we show that the 4N -strings HTSP is
N P-hard.
Theorem 2. 4N -strings HTSP is N P-hard.
Proof: We will show that the HTSP polynomially reduces
to the 4N -strings HTSP. Let S = {s1 , s2 , . . . , sn } be the

input for any instance of the HTSP. Let ℓ be the length
of each input string. Append a string of 2nℓ 0’s to the
left of each si to get s′i (for 1 ≤ i ≤ (n − 1)). For example, if n = 4, ℓ = 3 and s1 = 101, then s′1 will be
000000000000000000000000101. We append 2nℓ 1’s to the
left of sn to get s′n . We will generate an instance S ′ of
the 4N -strings HTSP that has as input 4N strings, where
N = ⌈ n4 ⌉. S ′ will have s′1 , s′2 , . . . , s′n−1 and 1, 2, 3 or 4 copies
of s′n depending on whether n = 4N, 4N − 1, 4N − 2, or
4N − 3, respectively.
It is easy to see that in an optimal tour for the above 4N strings HTSP instance, all the copies of s′n will be successive
and that an optimal solution for S can be obtained readily
from an optimal solution for S ′ . 2

3.2 A special instance of the BLMP
Consider the following (N + 1)2 strings as an input for the
BLMP:
t1 , t2 , . . . , t4N , t, t, . . . , t. Here there are N 2 −2N +1 copies of
t. There is a positive integer k such that δ(ti , t) = k for any
1 ≤ i ≤ 4N and 2k ≥ δ(ti , tj ) > 47 k for any 1 ≤ i 6= j ≤ 4N .
Lemma 1. In any optimal solution to the above BLMP
instance, t1 , t2 , . . . , t4N will lie on the boundary of the (N +
1) × (N + 1) grid (see Figure 1).
Proof: This can be proven by contradiction. Let T be
the collection of the strings t1 , t2 , . . . , t4N . Let q be one of
the strings from T that has a degree of 4 in an optimal
placement. Let r be one of the strings equal to t that lies
in the boundary. Next we show that we can get a better
solution by exchanging q and r.
Let u be the number of neighbors of q from T . Let v be the
number of neighbors of r from T . Note that 0 ≤ u ≤ 4 and
0 ≤ v ≤ 3. In the current solution, the total cost incurred by
q and r is at least 47 ku + k(4 − u) + kv = 43 ku + kv + 4k. If we
exchange q and r, the new total cost incurred by q and r is
strictly less than ku + 2kv + k(3 − v) = ku + kv + 3k. The old
cost minus the new cost is strictly greater than k − 14 ku ≥ 0.
We thus conclude that all the strings of T lie on the boundary of the grid in any optimal solution. 2

3.3 A special set of strings and some operations on strings
We denote the (ordered) concatenation of two strings x and
y as x + y. If x and x′ (respectively y and y ′ ) have the same
length then, clearly, δ(x + y, x′ + y ′ ) = δ(x, x′ ) + δ(y, y ′ ).
Given a string x = x1 x2 . . . xl and an integer h, let REPh (x)
be the string x1 x1 . . . x1 x2 x2 . . . x2 . . . xl xl . . . xl , where each
xi appears h times (REP stands for “replicate”). It is not
hard to see that if x and y have the same length, then
δ(REPh (x), REPh (y)) = hδ(x, y).
Given an integer n, we can construct a set of n strings of
length n each, An = {a1 , a2 , . . . , an }, such that δ(ai , aj ) = 2
for any 1 ≤ i 6= j ≤ n. One way to construct An is to let
ai = 00 . . . 0100 . . . 0, where there are (i − 1) 0’s before 1. It
is easy to check that δ(ai , aj ) = 2 for any 1 ≤ i 6= j ≤ n.

This completes the proof of Theorem 1. 2

3.5
t

t

t

t

t

t

t

t

t

N P -hardness of the HGPMP for other spe-

cial cases
We can generalize the result in Theorem 1 for other special
cases of the HGPMP. We say graph G is “bordered-ring” if G
is undirected and G has a ring of size Ω(nα ) for some constant α > 0 such that every vertex in the ring has degree no
greater than d and every vertex outside the ring has degree
greater than d for some d ≥ 3. For grid graphs, α = 12 and
d = 3. Some variants of grid graphs like Manhattan grids
are bordered-ring as well.

Figure 1: An illustration for Lemma 1 with N = 4.
Each ti lies on a dark vertex in the grid.

Theorem 3. The HGPMP is N P-hard even if G is borderedring.

3.4 Proof of the main theorem

Proof: By a similar reduction to that of the BLMP above,
the theorem follows. 2

Now we are ready to present the proof of Theorem 1. Let
S = {s1 , s2 , . . . , s4N } be the input for any instance of the
4N -strings HTSP. Each si has the length l. We will generate
(N +1)2 strings such that an optimal solution for the BLMP
on these (N + 1)2 strings will yield an optimal solution for
the 4N -strings HTSP on S.
The input for the BLMP instance that we generate will be
T = {t1 , t2 , . . . , t4N , t, t, . . . , t} where t occurs N 2 − 2N + 1
times. We set ti = REPh (ai ) + REP2 (si ), where ai is the
i-th string in the set A4N defined in subsection 3.3. We will
choose h later. Also, we set t = REP4Nh (0) + 0101 . . . 01,
where the string 01 is repeated l times. We can easily check
that:
δ(ti , t)
δ(ti , tj )

= h + l for any 1 ≤ i ≤ 4N
= 2h + 2δ(si , sj ) ≤ 2h + 2l
for any 1 ≤ i 6= j ≤ 4N

(1)
(2)

We choose h so that T satisfies the condition in Lemma
1. Particularly, choose h = 8l. Now we will show that
OP TBLM P (T ) = 4(N − 1)(h + l) + 8N h + 2OP THT SP (S),
which in turn means that an optimal solution for the BLMP
on T will yield an optimal solution for the 4N -strings HTSP
on S.
Let A = si1 , si2 , . . . , si4N be an optimal tour for the 4N string HTSP on S. We construct a solution A′ for the BLMP
on T by placing ti ’s on the border of the grid in the order
ti1 , ti2 , . . . , ti4N and placing the copies of t on the center of
the grid. By the equalities (1) and (2), the cost of A′ is
Cost(A′ ) = 4(N − 1)(h + l) + 8N h + 2Cost(A). Therefore,
OP TBLM P (T ) ≤ 4(N − 1)(h + l) + 8N h + 2OP THT SP (S).
On the other hand, let B be an optimal solution for the
BLMP on T . By Lemma 1, ti ’s lie on the border of the
grid and the copies of t lie on the center of the grid. Assume that ti ’s lie in the order ti1 , ti2 , . . . , ti4N . We can construct a tour B ′ for the 4N -strings HTSP on S in the order
si1 , si2 , . . . , si4N . By the equalities (1) and (2), Cost(B) =
4(N −1)(h+l)+8N h+2Cost(B ′ ). Hence, OP TBLM P (T ) ≥
4(N − 1)(h + l) + 8N h + 2OP THT SP (S).

3.6 An alternate N P -hardness proof
for the BLMP
In this section, we give an alternate N P-hardness proof for
the BLMP by showing that another variant of the HTSP
called k-Segments HTSP polynomially reduces to the BLMP.
We believe that the techniques introduced in both of our
proofs will find independent applications.

3.6.1 k-Segments traveling salesperson problem
We define the k-segments HTSP and show that it is NPhard. Consider an input of n strings: s1 , s2 , . . . , sn . The
problem of k-segments HTSP is to partition the n strings
into k parts such that the sum of the optimal tour costs for
the individual parts is minimum.
Theorem 4. The k-segments HTSP for strings is N Phard.
Proof: We will prove this for k = 4 (since this is the instance that will be useful for us to prove the main result)
and the theorem will then be obvious.
We will show that the HTSP polynomially reduces to the
4-segments HTSP. Let S = {s1 , s2 , . . . , sn } be the input to
any instance of the HTSP. We will generate an instance of
the 4-segments HTSP that has as input (n + 3) strings. Let
l be the length of each string in S. Note that the optimal
cost for the HTSP with input S is ≤ nl.
Consider the 4 strings: 1110, 1101, 1011, 0111. The distance between any two of them is 2. Now replace each 1 in
each of these 4 strings with a string of nl 1’s. Also, replace
each 0 in each of these strings with a string of nl 0’s. Call
these new strings t1 , t2 , t3 , t4 . The distance between any two
of these strings is 2nl.
The input strings for the 4-segments HTSP are q1 , q2 , . . . , qn+3
and are constructed as follows: qi is nothing but si with t1

appended to the left, for 1 ≤ i ≤ n. qn+1 is a string of
length 4nl + l whose l LSBs are 0’s and whose 4nl MSBs
equal t2 . qn+2 is a string of length 4nl + l whose l LSBs are
0 and whose 4nl MSBs equal t3 . Also, qn+3 has all 0’s in its
l LSBs and its 4nl MSBs equal t4 .

to be 1. String tn+1 has all 1’s. Clearly, δ(ti , tj ) = 2 for any
i and j less than or equal to n. Also, δ(ti , tn+1 ) = 1 for any
1 ≤ i ≤ n.

Now, in each ti (for 1 ≤ i ≤ (n + 1)) replace every 1 with a
string of eight 1’s and replace each 0 with a string of eight
Clearly, in an optimal solution for the 4-segments HTSP in1’s. After this change, δ(ti , tj ) = 16 for any 1 ≤ i, j ≤ n and
stance, the four parts have to be {q1 , q2 , . . . , qn }, {qn+1 }, {qn+2 }, δ(ti , tn+1 ) = 8 for any 1 ≤ i ≤ n.
and {qn+3 }. As a result, we can get an optimal solution
for the HTSP instance given an optimal solution for the 4Finally, append a 0 to the left of each ti (for 1 ≤ i ≤ n) as
segments HTSP instance. 2
the MSB. Also, append a 1 to the left of tn+1 . In this case,
δ(ti , tj ) = 16 for any 1 ≤ i, j ≤ n and δ(ti , tn+1 ) = 9 for any
1 ≤ i ≤ n.
3.6.2 A special instance of the BLMP
Consider the following n2 strings as an input for the BLMP:
t1 , t2 , . . . , tn , t, t, . . . , t. Here there are n2 − n copies of t.
Also, δ(ti , tj ) = 16 for any i and j less than or equal to n.
δ(ti , t) = 9 for any i ≤ n.
Lemma 2. In an optimal solution to the above BLMP instance, t1 , t2 , . . . , tn lie on the boundary of the n × n grid
and moreover these strings are found in four segments of
successive nodes.
Proof: Let T be the collection of strings t1 , t2 , . . . , tn . By
Lemma 1, we conclude that all the strings of T lie on the
boundary of the grid in an optimal solution.
Let S1 and S2 be two segments such that S1 and S2 consist of
strings from T , strings in S1 are in successive nodes, strings
in S2 are in successive nodes, and these two segments are
not successive. Consider the case when none of these strings
is in a corner of the grid. Let S1 =P
{a1 , a2 , . . . , an1 } and
n1 −1
δ(ai , ai+1 ) and
S2 = {b1 , b2 , . . . , bn2 }. Let C(S1 ) = i=1
Pn2 −1
C(S2 ) =
δ(b
,
b
).
The
total
cost
for these two
i
i+1
i=1
segments is C(S1 ) + C(S2 ) + 9(n1 + n2 ) + 36. If we join
these two segments into one, the new cost will be C(S1 ) +
C(S2 ) + 9(n1 + n2 ) + 34.
Thus it follows that all the strings of T will be on the boundary and they will be found in successive nodes in any optimal solution. Also it helps to utilize the corners of the grid
since each use of a corner will reduce the total cost by 9.
Therefore in an optimal solution there will be four segments
such that all the segments are in the boundary of the grid,
each segment has strings from T in successive nodes, and one
string of each segment occupies a corner of the grid. In other
words, an optimal solution for the BLMP instance contains
an optimal solution for the 4-segments TSP corresponding
to T . The optimal cost for this BLMP instance is 25n − 28.
2

3.6.3 Construction of strings for the above BLMP instance
We can construct n2 strings that have the same properties
as the ones in the above BLMP instance.
To begin with, we construct (n + 1) binary strings of length
n each. The string ti has all 1’s except in position i, for
1 ≤ i ≤ n. The position of the LSB of any string is assumed

3.6.4 The alternate proof of the main theorem
Let S = {s1 , s2 , . . . , sn } be the input for any instance of
the HTSP. We will generate n2 strings such that an optimal solution for the BLMP on these n2 strings will yield an
optimal solution for the 4-segments HTSP on S.
We will use as the basis the (n + 1) strings generated in the
above section. Recall that these strings t1 , t2 , . . . , tn+1 are of
length (8n + 1) each. Also, δ(ti , tj ) = 16 for any 1 ≤ i, j ≤ n
and δ(ti , tn+1 ) = 9 for any 1 ≤ i ≤ n.
Replace each 0 in each of the above strings with nl 0’s and
replace each 1 in each of these strings with nl 1’s. Now,
δ(ti , tj ) = 16nl for any 1 ≤ i, j ≤ n and δ(ti , tn+1 ) = 9nl for
any 1 ≤ i ≤ n. Each of these strings is of length (8n + 1)nl.
Replace each 0 in each si with two 0’s (for 1 ≤ i ≤ n)
and replace each 1 in each si with two 1’s and let s′i be
the resultant string. Note that an optimal solution for the
4-segments HTSP on the revised S will also be an optimal
solution for the 4-segments HTSP on the old S. If l is the
length of each string in the old S, then 2l will be the length
of each revised input string.
The input for the BLMP instance that we generate will be
q1 , q2 , . . . , qn , t, t, . . . , t where t occurs n2 − n times. Each of
these strings will be of length (8n + 1)nl + 2l. The string qi
will have s′i in its 2l LSBs and it will have ti in its (8n + 1)nl
MSBs, for 1 ≤ i ≤ n. The string t will have tn+1 in its (8n +
1)nl MSBs. Its 2l LSBs will be 0101 . . . 01, i.e., the string 01
is repeated l times. Note that δ(qi , qj ) = 16nl + δ(s′i , s′j ) for
any 1 ≤ i, j ≤ n. Also, δ(qi , t) = 9nl + l for any 1 ≤ i ≤ n.
Note that strings of this BLMP instance are comparable
to the strings we had for Lemma 2. This is because the
interstring distances are very nearly in the same ratios for
the two cases. As a result, using a proof similar to that
of Lemma 2, we can show that the strings t1 , t2 , . . . , tn will
all lie in the boundary of the grid in an optimal solution
to the above BLMP. Let T = {t1 , t2 , . . . , tn }. Also, the
strings of T will be found in four segments such that one
string of each segment occupies one of the corner nodes of
the grid. Let S1 , S2 , S3 , and S4 stand for the strings in these
four segments, respectively. Let C1 , C2 , C3 , and C4 be the
optimal tour costs for S1 , S2 , S3 , and S4 , respectively.
Let |Si | = ni for 1 ≤ i ≤ 4. The total cost (i.e., the border
length) for the above BLMP solution can be computed as

(1,1)

(1,N)

distance satisfies the triangular inequality, δ(si1 , siN 2 ) ≤
Cost(P ′ ). Consider the tour that starts at si1 , traverses
along the path P ′ to siN 2 and comes back to si1 . Obviously,
the cost of the tour is Cost(P ′ )+δ(si1 , siN 2 ) ≤ 2Cost(P ′ ) ≤
2Cost(A). Hence, OP THT SP (S) ≤ 2OP TBLM P (S). 2

Theorem 5. The above algorithm yields an O(N )-approximate
solution.

(N,1)

(N,N)

Figure 2: The thick dark line corresponds to an optimal tour on the input strings

follows. Consider S1 alone. The cost due to this segment
is C1 + 2(9nl + l) + (n1 − 1)(9nl + l). The cost 2(9nl + l)
is due to the two end points of the segment S1 . The cost
(n1 − 1)(9nl + l) is due to the fact that each string of S1
(except for the one in a corner of the grid) is a neighbor of a
t. Upon simplification, the cost for S1 is C1 +(n1 +1)(9nl+l).
Summing over all the four segments, the total cost for the
BLMP solution is C1 + C2 + C3 + C4 + (n + 4)(9nl + l). The
minimum value of this is obtained when S1 , S2 , S3 , and S4
form a solution to the 4-segments HTSP on T .
Clearly, an optimal solution for the 4-segments HTSP on T
will also yield an optimal solution for the 4-segments HTSP
on S. This can be seen as follows. Consider the strings in
Si and let Qi = ai1 , ai2 , . . . , aini be the corresponding input
strings (of S), for 1 ≤ i ≤ 4. Note that Ci is nothing but
(ni − 1)(16nl) plus twice the optimal tour cost for Qi , for
1 ≤ i ≤ 4. Thus, C1 + C2 + C3 + C4 is equal to (n − 4)16nl +
2(C1′ + C2′ + C3′ + C4′ ) where Ci′ is the optimal tour cost for
Qi , for 1 ≤ i ≤ 4.
This completes the proof of Theorem 1. 2

4. ALGORITHMS FOR THE BLMP
4.1 An O(N )-approximation algorithm
In this section, we will show that a simple version of the
algorithm suggested by Hannenhalli, et al. is actually an
O(N )-approximation algorithm. This algorithm can be described as follows. Assume that the input is the set of strings
S = {s1 , s2 , . . . , sN 2 }. The algorithm first computes a tour
T on strings in S. Then it threads the tour T into the
grid in row-major order (see Figure 2). The first step can
be done by calling the 32 -approximation algorithm for the
HTSP suggested by [2].
Lemma 3. OP THT SP (S) ≤ 2OP TBLM P (S).
Proof: Let A be an optimal solution for the BLMP on S.
Consider the path P ′ drawn as the thick dark line in Figure
2. Obviously, Cost(P ′ ) ≤ Cost(A) = OP TBLM P (S). Let
si1 and siN 2 be the two endpoints of P ′ . Since the Hamming

Proof: First, we see that Cost(T ) ≤ 32 OP THT SP (S) ≤
3OP TBLM P (S). The first inequality is due to the 32 -approximation
for the HTSP. The second inequality is due to Lemma 3.
Now let us analyze the cost of the solution F produced by
the algorithm. Consider the path P drawn as the thick dark
line in Figure 2. Obviously, Cost(P ) ≤ Cost(T ). Also, the
total cost of the N rows in F is no more than Cost(P ).
By the triangle inequality, it is not hard to see that the
cost of each column in F is no more than Cost(P ). Therefore, Cost(F ) ≤ (N + 1)Cost(P ) ≤ (N + 1)Cost(T ) ≤
3(N + 1)OP TBLM P (S) = O(N )OP TBLM P (S). 2

4.2 A hierarchical refinement algorithm
Several heuristics such as the Epitaxial growth have been
proposed to solve the BLMP problem earlier. However most
of these heuristics do not improve the cost monotonically.
Local search based algorithms are often employed to solve
hard combinatorial problems. We now introduce a hierarchical refinement algorithm (HRA). This refinement technique can be applied to any heuristic placement to refine
the cost and get a better placement. Let N be the number of probes in the placement, d a positive integer such
that dx = N, x ≥ 1 is called the degree of refinement. The
refinement algorithm starts with a given placement, then it
divides the placement into s01 , s02 . . . s0N/d2 sub-problems with
d2 probes per sub-problem. Each of these sub-problems
is solved optimally – an optimal permutation among the
probes is found. After this every d2 sub-problems are com2
bined into a new sub-problem s1i = ∪dj=1 s0id2 +j , 1 ≤ i ≤
3
N/d . To solve s1i optimally we identify an optimal permutation among s0id2 +j ∈ s1i , 1 ≤ j ≤ d2 . This process
continues until we are left with no sub-problems to solve.
See Figure 3.
We should remark that while solving a sub-problem optimally, we also consider the cost contributed from the neighboring sub-problems. This ensures the monotonic improvement in the placement cost. The refinement algorithm asymptotically runs in Θ(d2 !N ) time. If d = O(1), the refinement
algorithm runs in linear time. For small values of d, the algorithm performs well in practice. HRA is a deterministic
refinement algorithm. We further extend this by introducing
randomness. The Randomized Hierarchical Refinement Algorithm (RHRA) is similar to the HRA algorithm. RHRA
randomly selects a sub-square within the given placement
and applies the HRA technique to the selected sub-square.
Similar to local search algorithms, repeating RHRA algorithm several times improves the placement cost monotonically. We study the performance of both these algorithms
in section 5.

3

2

5

8

1

1
6

3

9

4

7

k−1

3

5

7

2

9

8

6

4

as an input for refinement/local search algorithms such as
RHRA. In the next sub-section we provide our experimental
study of HRA and RHRA algorithms on various placement
heuristics.

5.2 Performance of refinement algorithms
3

k

We have applied our HRA, RHRA refining algorithms on
the following placement heuristics.
• (RAN D) Random placement: in this placement we
just use the order in which the probes are provided to
our algorithm.
• (SORT ) Sort placement: in this placement the input
probes are sorted lexicographically

Figure 3: Illustration of the hierarchical refinement
algorithm with degree of refinement 3. This shows
the possible optimal solutions (i.e. permutation
among sub-problems) at the top-most and penultimate levels

4.3 Quad epitaxial algorithm
The epitaxial (EP X) placement suggested in [5] places a
randomly selected probe at the center of the array, it continues placing the probes greedily around the locations adjacent to the placed probes to minimize the cost (i.e. the algorithm almost spends O(N 2 ) time to place each probe). The
epitaxial algorithm gives good results for small arrays but
for larger arrays the epitaxial algorithm is impractical and
extremely slow. We propose the Quad Epitaxial (QEP X)
algorithm as a simple extension to the epitaxial algorithm.
QEP X yields good performance and is very fast compared
to the EP X algorithm. The basic idea behind the QEP X
algorithm is to divide the array into four parts, apply EP X
algorithm for each of the four parts and finally find an optimal arrangement among the four parts. In section 5 we
compare the QEP X algorithm with EP X algorithm.

5. EXPERIMENTAL STUDY
5.1 Performance of the QEP X algorithm
In this section we compare the performance of QEP X algorithm introduced earlier. We use randomly generated probe
arrays of size 322 ,642 ,1282 and 2562 . In all of our experimental studies we compute a lower bound on the solution
by picking the smallest 2N (N − 1) edges from the complete
Hamming distance graph. Column-4(INIT COST) in the
table 1 indicates the placement cost obtained by placing
the probes in the row major order as given by the input.
Column-5(8) indicates the final placement cost obtained by
the epitaxial (quad) algorithm. As we can see from columns
7 and 10, the refinement obtained by the QEP X algorithm
is very close to the EP X algorithm. On the other hand
QEP X runs 3.6X faster than the EP X algorithm. As we
can see from table 1, as the chip size increases EP X algorithm becomes very slow. We ran both EP X and QEP X
algorithms on a chip size of 243 × 243 with a time limit of
60 minutes. The QEP X algorithm took around 12 minutes
to complete and improved the input placement cost by 36%.
On the other hand the EP X algorithm did not complete
the placement. From our experiments we conclude that the
QEP X can provide a good placement which we can use

• (SW M ) Sliding Window Matching placement is obtained by running the SW M [5] algorithm with parameters (6, 3).
• (REP X) Row epitaxial placement is obtained by running the row-epitaxial algorithm with 3 look-ahead
rows.
• (EP X) Epitaxial placement is obtained by running the
EP X algorithm
• (QEP X) Quad epitaxial placement obtained by our
quad-epitaxial algorithm
The cost of the placement obtained by running the HRA algorithm exactly once is given in column-5 (HRA). Column6 (RHRA) indicates the placement cost obtained by running
our randomized refinement algorithm RHRA for 350 iterations. From table 2 we can see that as initial placement
moves closer and closer towards the lower bound the refinement percentage decreases, which is logical. For test cases
with 729, 6561 (1024, 4096) probes we use a refinement degree d = 3 (d = 2). Choosing a bigger refinement degree
gives better refinements, however takes more time. Finally
we conclude that our refinement algorithms would be very
useful when applied in conjunction with fast initial placement heuristics. A fully function program called blm-solve
implementing all our algorithms can be downloaded from the
website http://launchpad.net/blm-solve, the web-site also
has all the supplementary details used in the our experimental study.

6. CONCLUSIONS
In this paper we have studied the Border Length Minimization Problem (BLMP) that has numerous applications in
biology and medicine. We have solved a seven-year old open
problem in this area by showing that the BLMP is N Phard. Two different proofs have been given and we believe
that the techniques in these proofs will find independent applications. We have also shown that certain generalizations
of the BLMP are N P-hard as well. In addition, we have
presented a hierarchical refinement algorithm (HRA) for the
BLMP. Deterministic and randomized versions of this algorithm can be used to refine the solutions obtained from any
algorithm for solving the BLMP. Our experimental results
indicate that indeed HRA can be useful in practice.

TEST
CASE

PROBES

LOWER
BOUND

INIT
COST

EPX

TIME
(sec)

REFINED
PRECENT

QEPX

TIME
(sec)

REFINED
PRECENT

t-0
t-1
t-0
t-1
t-0
t-1
t-0
t-1

1024
1024
4096
4096
16384
16384
65536
65536

23480
23427
86818
86897
322129
-

37192
37029
151116
151176
609085
608928
2447885
2427143

27591
27472
106471
106430
410301
409625
2447885
2427143

0.60
0.62
10.70
10.37
180.00
185.88
-

25.81%
25.81%
29.54%
29.60%
32.64%
32.73%
0.00%
0.00%

28060
28151
107805
107634
411746
410902
1563369
1562630

0.42
0.43
3.05
3.23
43.93
44.70
765.79
774.33

24.55%
23.98%
28.66%
28.80%
32.40%
32.52%
36.13%
35.62%

Table 1: Comparison between epitaxial and quad epitaxial

PROBES

ALGO

729
729
729
729
729
6561
6561
6561
6561
6561
1024
1024
1024
1024
1024
1024
4096
4096
4096
4096
4096
4096

RAND
SORT
SWM
REPTX
EPTX
RAND
SORT
SWM
REPTX
EPTX
RAND
SORT
SWM
QEPX
REPTX
EPTX
RAND
SORT
SWM
QEPX
REPTX
EPTX

LOWER
BOUND
17087
17087
17087
17087
17087
136820
136820
136820
136820
136820
23480
23480
23480
23480
23480
23480
86818
86818
86818
86818
86818
86818

INIT
COST
26401
24082
22267
21115
19733
243125
210326
204955
185386
168676
37192
33784
31424
28060
29574
27591
151116
131291
127516
107805
116406
106471

HRA

RHRA

23970
22415
22195
21107
19726
221090
198972
204525
185362
168623
35236
32326
31383
28035
29557
27567
143246
127033
127357
107766
116395
106462

22631
21649
22069
21101
19726
209514
191915
203412
185341
168544
33046
31026
31323
28028
29546
27565
134485
121742
127092
107702
116376
106448

REFINED
PRECENT
14.280%
10.103%
0.889%
0.066%
0.035%
13.825%
8.754%
0.753%
0.024%
0.078%
11.148%
8.164%
0.321%
0.114%
0.095%
0.094%
11.005%
7.273%
0.333%
0.096%
0.026%
0.022%

TIME
2.83(min)
2.81(min)
2.81(min)
2.81(min)
2.81(min)
17.55(min)
17.02(min)
17.20(min)
17.16(min)
17.15(min)
0.28(sec)
0.26(sec)
0.13(sec)
0.47(sec)
0.11(sec)
0.11(sec)
6.93(sec)
4.46(sec)
1.27(sec)
5.04(sec)
1.02(sec)
1.04(sec)

Table 2: Cost refinement for various placement heuristics by applying HRA (hierarchical refinement algorithm) and RHRA (randomized hierarchical refinement algorithm) with 350 iterations

One of the best performing algorithms for the BLMP is the
epitaxial algorithm (EPX). This algorithm takes too much
time especially when the number of probes is large. In this
paper we present a variant called the quad-epitaxial algorithm (QEPX) that is much faster than EPX while yielding
a solution that is very close to that of EPX in quality. QEPX
partitions the input into four parts, works on each part separately, and finally combines these solutions. This idea can
be extended further to partition the input into more parts
and hence this algorithm is ideal for parallelism.
Some of the open problems are: 1) In this paper we have
used a simple lower bound on the quality of solution for the
BLMP. It will be nice to develop tighter lower bounds; 2)
Develop more efficient algorithms than EPX; and 3) Design
parallel algorithms for the BLMP.
Acknowledgements. This work has been supported in
part by the following grants: NSF 0326155, NSF 0829916
and NIH 1R01GM079689-01A1.

7.

REFERENCES

[1] M. Chatterjee, S. Mohapatra, A. Ionan, G. Bawa,
R. Ali-Fehmi, X. Wang, J. Nowak, B. Ye, F. A.
Nahhas, K. Lu, S. S. Witkin, D. Fishman,
A. Munkarah, R. Morris, N. K. Levin, N. N. Shirley,
G. Tromp, J. Abrams, S. Draghici, and M. A. Tainsky.
Diagnostic markers of ovarian cancer by
high-throughput antigen cloning and detection on
arrays. Cancer research, 66(2):1181–1190, 2006.
[2] N. Christofides. Worst-case analysis of a new heuristic
for the travelling salesman problem. Graduate School of
Industrial Administration, Report 388, 1976.
[3] S. de Carvalho Jr. and S. Rahmann. Microarray layout
as a quadratic assignment problem. In Proc. German
Conference on Bioinformatics, volume P-83 of Lecture
Notes in Informatics, pages 11–20, 2006.
[4] S. Hannenhalli, E. Hubell, R. Lipshutz, and P. A.
Pevzner. Combinatorial algorithms for design of dna
arrays. Advances in biochemical
engineering/biotechnology, 77:1–19, 2002.
[5] A. Kahng, I. Mandoiu, P. Pevzner, S. Reda, and
A. Zelikovsky. Engineering a scalable placement
heuristic for dna probe arrays. In Intl. Conf. on
Research in Computational Molecular Biology, pages
148–156, April 2003.
[6] V. Kundeti and S. Rajasekaran. On the hardness of the
border length minimization problem. In IEEE
International conference on bioinformatics and
bio-engineering, pages 248–253, 2009.
[7] C. Melle, G. Ernst, B. Schimmel, A. Bleul, S. Koscielny,
A. Wiesner, R. Bogumil, U. MÃűller, D. Osterloh, K. .
Halbhuber, and F. Von Eggeling. A technical triade for
proteomic identification and characterization of cancer
biomarkers. Cancer research, 64(12):4099–4104, 2004.
[8] J. B. Welsh, L. M. Sapinoso, S. G. Kern, D. A. Brown,
T. Liu, A. R. Bauskin, R. L. Ward, N. J. Hawkins, D. I.
Quinn, P. J. Russell, R. L. Sutherland, S. N. Breit,
C. A. Moskaluk, H. F. Frierson Jr., and G. M.
Hampton. Large-scale delineation of secreted protein
biomarkers overexpressed in cancer tissue and serum.
Proceedings of the National Academy of Sciences of the

United States of America, 100(6):3410–3415, 2003.

On the Border Length Minimization Problem
(BLMP) on a Square Array

arXiv:1003.2839v2 [cs.DS] 16 Mar 2010

Vamsi Kundeti, Sanguthevar Rajasekaran, and Hieu Dinh
Department of Computer Science and Engineering
University of Connecticut
Storrs, CT 06269, USA
{vamsik,rajasek,hdinh}@engr.uconn.edu

Abstract. Protein/Peptide microarrays are rapidly gaining momentum
in the diagnosis of cancer. High-density and high-throughput peptide
arrays are being extensively used to detect tumor biomarkers, examine
kinase activity, identify antibodies having low serum titers and locate antibody signatures. Improving the yield of microarray fabrication involves
solving a hard combinatorial optimization problem called the Border
Length Minimization Problem (BLM P ). An important question that remained open for the past seven years is if the BLMP is tractable or not.
We settle this open problem by proving that the BLMP is N P-hard.
We also present a hierarchical refinement algorithm which can refine
any heuristic solution for the BLMP problem. We also prove that the
TSP+1-threading heuristic is an O(N )-approximation.
The hierarchical refinement solver is available as an open-source code at
http://launchpad.net/blm-solve.

1

Introduction

Cancer diagnosis research has taken a new direction recently by adopting peptide
microarrays for reliable detection of tumor biomarkers (Chatterjee, et al., [?]),
(Melle, et al., [?]), (Welsh, et al., [?]). These high-throughput arrays also find
application in examining kinase activity, identifying antibody signatures against
tumor antigens, etc. High-density peptide arrays are currently fabricated using
technologies such as photolithography or in-situ synthesis based on micromirror
arrays. The manufacturers of these arrays are facing serious fabrication challenges due to unintended illumination effects such as diffraction and scattering
of light. These illumination effects can be reduced dramatically by selecting a
right placement of the peptide probes before fabrication. Finding this placement
can be formulated as a combinatorial optimization problem, known as the Border Length Minimization Problem (BLMP). Hannenhalli, et al. first introduced
BLMP in 2002 [?]. Although the BLMP was formulated in the context of DNA
microarrays, peptide arrays share a similar fabrication technology.
The BLMP can be stated as follows. Given N 2 strings of the same length,
how do we place them in a grid of size N × N such that the Hamming distance
summed over all the pairs of neighbors in the grid is minimized? The BLMP

2

Vamsi Kundeti, Sanguthevar Rajasekaran, and Hieu Dinh

has received a lot of attention from many researchers. The earliest algorithm
suggested by Hannenhalli, et al. reduces BLMP to TSP (Traveling Salesman
Problem) by computing a tour of the strings and then threading the tour on the
grid [?]. Kahng, et al. have proposed several other heuristic algorithms which
are considered the best performing algorithms in practice [?]. De Carvalho, et al.
introduced a quadratic program formulation of the BLMP but unfortunately the
quadratic program is an intractable problem [?]. Later, Kundeti and Rajasekaran
formulated the problem as an integer linear program which performs better than
the quadratic program in practice [?].
Despite many studies on the BLMP, the question of whether BLMP is tractable
or not remained open for the past 7 years. In this paper, we show that the BLMP
is N P-hard. We also consider a generalization of the BLMP called the Hamming
Graph Placement Minimization Problem (HGPMP). We show that some special
cases of the HGPMP are also N P-hard. On the algorithmic side, we show that
a simple version of the algorithm suggested by Hannenhalli, et al. is an O(N )approximation. On the practical side, we propose a refinement algorithm which
takes any solution and tries to improve it. An experimental study of this refinement algorithm is also included.
Our paper is organized as follows. Section 2 formally defines the BLMP and
HGPMP. Section 3 provides the N P-hardness proof of the BLMP and some
special cases of the HGPMP. Section 4 gives the O(N )-approximation algorithm
and the refinement algorithm for the BLMP. Section 5 provides an experimental
evaluation of the refinement algorithm. Finally, Section 6 concludes our paper
and discusses some open problems.

2

Problem definition

Let S be a set of strings of the same length with S = {s1 , . . . , sn } and let
G = (V, E) be a graph with |V | = n. A placement of S on G is a bijective
map f : S → V . Let f −1 (u) be the string that is mapped to vertex u by the
placement f . We denote the Hamming distance between
two strings si and sj
P
as δ(si , sj ). The cost of placement f is Cost(f ) = e=(u,v)∈E δ(f −1 (u), f −1 (v)).
The Hamming Graph Placement Minimization Problem (HGPMP) is defined as
follows. Given S and G, find a placement of S on G of minimum cost. We denote
the optimal cost as OP T (S, G), or simply as OP T if it is clear what S and G
are.
Obviously, if G is a ring graph, then HGPMP is the same as the well-known
Hamming Traveling Salesman Problem (HTSP). If G is a grid graph of size
N ×N (where N 2 = n), then HGPMP becomes the Border Length Minimization
Problem (BLMP), which is the main study of our paper.

3

N P-hardness of the BLMP
and HGPMP

Theorem 1. The BLMP is N P-hard.

On the Border Length Minimization Problem

3

We will show that the Hamming traveling salesperson problem (HTSP) for
strings (with the Hamming distance metric) polynomially reduces to the BLMP.
The HTSP is already defined in Section 2.
The idea of the proof is that given 4N strings for the HTSP we construct
(N +1)2 strings for the BLMP such that from an optimal solution to this BLMP,
we can easily obtain an optimal solution for the HTSP. So we need to consider the
variant of the HTSP in which the number of strings is divisible by 4. The proof
will be presented in stages. The next three subsections present some preliminaries
needed for the proof of the theorem. Followed by these subsections, the proof is
presented.
3.1

4N -strings traveling salesperson problem

Define an instance of the HTSP as a 4N -strings HTSP if the number of strings in
the input is 4N (for some integer N ). In this section we show that the 4N -strings
HTSP is N P-hard.
Theorem 2. 4N -strings HTSP is N P-hard.
Proof: We will show that the HTSP polynomially reduces to the 4N -strings
HTSP. Let S = {s1 , s2 , . . . , sn } be the input for any instance of the HTSP. Let ℓ
be the length of each input string. Append a string of 2nℓ 0’s to the left of each
si to get s′i (for 1 ≤ i ≤ (n − 1)). For example, if n = 4, ℓ = 3 and s1 = 101,
then s′1 will be 000000000000000000000000101. We append 2nℓ 1’s to the left
of sn to get s′n . We will generate an instance S ′ of the 4N -strings HTSP that
has as input 4N strings, where N = ⌈ n4 ⌉. S ′ will have s′1 , s′2 , . . . , s′n−1 and 1, 2, 3
or 4 copies of s′n depending on whether n = 4N, 4N − 1, 4N − 2, or 4N − 3,
respectively.
It is easy to see that in an optimal tour for the above 4N -strings HTSP
instance, all the copies of s′n will be successive and that an optimal solution for
S can be obtained readily from an optimal solution for S ′ . 
3.2

A special instance of the BLMP

Consider the following (N + 1)2 strings as an input for the BLMP:
t1 , t2 , . . . , t4N , t, t, . . . , t. Here there are N 2 − 2N + 1 copies of t. There is a positive integer k such that δ(ti , t) = k for any 1 ≤ i ≤ 4N and 2k ≥ δ(ti , tj ) > 47 k
for any 1 ≤ i 6= j ≤ 4N .
Lemma 1. In any optimal solution to the above BLMP instance, t1 , t2 , . . . , t4N
will lie on the boundary of the (N + 1) × (N + 1) grid (see Figure 1).
Proof: This can be proven by contradiction. Let T be the collection of the
strings t1 , t2 , . . . , t4N . Let q be one of the strings from T that has a degree of 4
in an optimal placement. Let r be one of the strings equal to t that lies in the
boundary. Next we show that we can get a better solution by exchanging q and
r.

4

Vamsi Kundeti, Sanguthevar Rajasekaran, and Hieu Dinh

Let u be the number of neighbors of q from T . Let v be the number of
neighbors of r from T . Note that 0 ≤ u ≤ 4 and 0 ≤ v ≤ 3. In the current
solution, the total cost incurred by q and r is at least 74 ku + k(4 − u) + kv =
3
4 ku + kv + 4k. If we exchange q and r, the new total cost incurred by q and r
is strictly less than ku + 2kv + k(3 − v) = ku + kv + 3k. The old cost minus the
new cost is strictly greater than k − 41 ku ≥ 0.
We thus conclude that all the strings of T lie on the boundary of the grid in
any optimal solution. 

t

t

t

t

t

t

t

t

t

Fig. 1. An illustration for Lemma 1 with N = 4. Each ti lies on a dark vertex in the
grid.

3.3

A special set of strings and some operations on strings

We denote the (ordered) concatenation of two strings x and y as x + y. If x and
x′ (respectively y and y ′ ) have the same length then, clearly, δ(x + y, x′ + y ′ ) =
δ(x, x′ ) + δ(y, y ′ ).
Given a string x = x1 x2 . . . xl and an integer h, let REPh (x) be the string
x1 x1 . . . x1 x2 x2 . . . x2 . . . xl xl . . . xl , where each xi appears h times (REP stands
for “replicate”). It is not hard to see that if x and y have the same length, then
δ(REPh (x), REPh (y)) = hδ(x, y).
Given an integer n, we can construct a set of n strings of length n each,
An = {a1 , a2 , . . . , an }, such that δ(ai , aj ) = 2 for any 1 ≤ i 6= j ≤ n. One way
to construct An is to let ai = 00 . . . 0100 . . . 0, where there are (i − 1) 0’s before
1. It is easy to check that δ(ai , aj ) = 2 for any 1 ≤ i 6= j ≤ n.
3.4

Proof of the main theorem

Now we are ready to present the proof of Theorem 1. Let S = {s1 , s2 , . . . , s4N }
be the input for any instance of the 4N -strings HTSP. Each si has the length l.

On the Border Length Minimization Problem

5

We will generate (N + 1)2 strings such that an optimal solution for the BLMP
on these (N + 1)2 strings will yield an optimal solution for the 4N -strings HTSP
on S.
The input for the BLMP instance that we generate will be
T = {t1 , t2 , . . . , t4N , t, t, . . . , t} where t occurs N 2 − 2N + 1 times. We set
ti = REPh (ai ) + REP2 (si ), where ai is the i-th string in the set A4N defined in
subsection 3.3. We will choose h later. Also, we set t = REP4N h (0) + 0101 . . . 01,
where the string 01 is repeated l times. We can easily check that:
δ(ti , t) = h + l for any 1 ≤ i ≤ 4N
δ(ti , tj ) = 2h + 2δ(si , sj ) ≤ 2h + 2l

(1)
(2)

for any 1 ≤ i 6= j ≤ 4N
We choose h so that T satisfies the condition in Lemma 1. Particularly,
choose h = 8l. Now we will show that OP TBLMP (T ) = 4(N − 1)(h + l) + 8N h +
2OP THT SP (S), which in turn means that an optimal solution for the BLMP on
T will yield an optimal solution for the 4N -strings HTSP on S.
Let A = si1 , si2 , . . . , si4N be an optimal tour for the 4N -string HTSP on S.
We construct a solution A′ for the BLMP on T by placing ti ’s on the border of
the grid in the order ti1 , ti2 , . . . , ti4N and placing the copies of t on the center
of the grid. By the equalities (1) and (2), the cost of A′ is Cost(A′ ) = 4(N −
1)(h+ l)+ 8N h+ 2Cost(A). Therefore, OP TBLMP (T ) ≤ 4(N − 1)(h+ l)+ 8N h+
2OP THT SP (S).
On the other hand, let B be an optimal solution for the BLMP on T . By
Lemma 1, ti ’s lie on the border of the grid and the copies of t lie on the center
of the grid. Assume that ti ’s lie in the order ti1 , ti2 , . . . , ti4N . We can construct
a tour B ′ for the 4N -strings HTSP on S in the order si1 , si2 , . . . , si4N . By the
equalities (1) and (2), Cost(B) = 4(N − 1)(h + l) + 8N h + 2Cost(B ′ ). Hence,
OP TBLMP (T ) ≥ 4(N − 1)(h + l) + 8N h + 2OP THT SP (S).
This completes the proof of Theorem 1. 
3.5

N P-hardness of the HGPMP for other special cases

We can generalize the result in Theorem 1 for other special cases of the HGPMP.
We say graph G is “bordered-ring” if G is undirected and G has a ring of size
Ω(nα ) for some constant α > 0 such that every vertex in the ring has degree no
greater than d and every vertex outside the ring has degree greater than d for
some d ≥ 3. For grid graphs, α = 12 and d = 3. Some variants of grid graphs like
Manhattan grids are bordered-ring as well.
Theorem 3. The HGPMP is N P-hard even if G is bordered-ring.
Proof: By a similar reduction to that of the BLMP above, the theorem follows.


6

3.6

Vamsi Kundeti, Sanguthevar Rajasekaran, and Hieu Dinh

An alternate N P-hardness proof
for the BLMP

In this section, we give an alternate N P-hardness proof for the BLMP by showing that another variant of the HTSP called k-Segments HTSP polynomially
reduces to the BLMP. We believe that the techniques introduced in both of our
proofs will find independent applications.
k-Segments traveling salesperson problem
We define the k-segments HTSP and show that it is NP-hard. Consider an input
of n strings: s1 , s2 , . . . , sn . The problem of k-segments HTSP is to partition the n
strings into k parts such that the sum of the optimal tour costs for the individual
parts is minimum.
Theorem 4. The k-segments HTSP for strings is N P-hard.
Proof: We will prove this for k = 4 (since this is the instance that will be useful
for us to prove the main result) and the theorem will then be obvious.
We will show that the HTSP polynomially reduces to the 4-segments HTSP.
Let S = {s1 , s2 , . . . , sn } be the input to any instance of the HTSP. We will
generate an instance of the 4-segments HTSP that has as input (n + 3) strings.
Let l be the length of each string in S. Note that the optimal cost for the HTSP
with input S is ≤ nl.
Consider the 4 strings: 1110, 1101, 1011, 0111. The distance between any
two of them is 2. Now replace each 1 in each of these 4 strings with a string of
nl 1’s. Also, replace each 0 in each of these strings with a string of nl 0’s. Call
these new strings t1 , t2 , t3 , t4 . The distance between any two of these strings is
2nl.
The input strings for the 4-segments HTSP are q1 , q2 , . . . , qn+3 and are constructed as follows: qi is nothing but si with t1 appended to the left, for 1 ≤ i ≤ n.
qn+1 is a string of length 4nl + l whose l LSBs are 0’s and whose 4nl MSBs equal
t2 . qn+2 is a string of length 4nl + l whose l LSBs are 0 and whose 4nl MSBs
equal t3 . Also, qn+3 has all 0’s in its l LSBs and its 4nl MSBs equal t4 .
Clearly, in an optimal solution for the 4-segments HTSP instance, the four
parts have to be {q1 , q2 , . . . , qn }, {qn+1 }, {qn+2 }, and {qn+3 }. As a result, we
can get an optimal solution for the HTSP instance given an optimal solution for
the 4-segments HTSP instance. 
A special instance of the BLMP
Consider the following n2 strings as an input for the BLMP:
t1 , t2 , . . . , tn , t, t, . . . , t. Here there are n2 − n copies of t. Also, δ(ti , tj ) = 16
for any i and j less than or equal to n. δ(ti , t) = 9 for any i ≤ n.
Lemma 2. In an optimal solution to the above BLMP instance, t1 , t2 , . . . , tn lie
on the boundary of the n × n grid and moreover these strings are found in four
segments of successive nodes.

On the Border Length Minimization Problem

7

Proof: Let T be the collection of strings t1 , t2 , . . . , tn . By Lemma 1, we conclude
that all the strings of T lie on the boundary of the grid in an optimal solution.
Let S1 and S2 be two segments such that S1 and S2 consist of strings from
T , strings in S1 are in successive nodes, strings in S2 are in successive nodes,
and these two segments are not successive. Consider the case when none of
these strings is in a corner of the grid. Let S1 = {a1 , a2 , . . . , an1 } and S2 =
Pn2 −1
Pn1 −1
δ(bi , bi+1 ).
δ(ai , ai+1 ) and C(S2 ) = i=1
{b1 , b2 , . . . , bn2 }. Let C(S1 ) = i=1
The total cost for these two segments is C(S1 )+C(S2 )+9(n1 +n2 )+36. If we join
these two segments into one, the new cost will be C(S1 )+C(S2 )+9(n1 +n2 )+34.
Thus it follows that all the strings of T will be on the boundary and they
will be found in successive nodes in any optimal solution. Also it helps to utilize
the corners of the grid since each use of a corner will reduce the total cost by
9. Therefore in an optimal solution there will be four segments such that all the
segments are in the boundary of the grid, each segment has strings from T in
successive nodes, and one string of each segment occupies a corner of the grid.
In other words, an optimal solution for the BLMP instance contains an optimal
solution for the 4-segments TSP corresponding to T . The optimal cost for this
BLMP instance is 25n − 28. 
Construction of strings for the above BLMP instance
We can construct n2 strings that have the same properties as the ones in the
above BLMP instance.
To begin with, we construct (n + 1) binary strings of length n each. The
string ti has all 1’s except in position i, for 1 ≤ i ≤ n. The position of the LSB
of any string is assumed to be 1. String tn+1 has all 1’s. Clearly, δ(ti , tj ) = 2 for
any i and j less than or equal to n. Also, δ(ti , tn+1 ) = 1 for any 1 ≤ i ≤ n.
Now, in each ti (for 1 ≤ i ≤ (n + 1)) replace every 1 with a string of eight
1’s and replace each 0 with a string of eight 1’s. After this change, δ(ti , tj ) = 16
for any 1 ≤ i, j ≤ n and δ(ti , tn+1 ) = 8 for any 1 ≤ i ≤ n.
Finally, append a 0 to the left of each ti (for 1 ≤ i ≤ n) as the MSB. Also,
append a 1 to the left of tn+1 . In this case, δ(ti , tj ) = 16 for any 1 ≤ i, j ≤ n
and δ(ti , tn+1 ) = 9 for any 1 ≤ i ≤ n.
The alternate proof of the main theorem
Let S = {s1 , s2 , . . . , sn } be the input for any instance of the HTSP. We will
generate n2 strings such that an optimal solution for the BLMP on these n2
strings will yield an optimal solution for the 4-segments HTSP on S.
We will use as the basis the (n + 1) strings generated in the above section. Recall that these strings t1 , t2 , . . . , tn+1 are of length (8n + 1) each. Also,
δ(ti , tj ) = 16 for any 1 ≤ i, j ≤ n and δ(ti , tn+1 ) = 9 for any 1 ≤ i ≤ n.
Replace each 0 in each of the above strings with nl 0’s and replace each 1 in
each of these strings with nl 1’s. Now, δ(ti , tj ) = 16nl for any 1 ≤ i, j ≤ n and
δ(ti , tn+1 ) = 9nl for any 1 ≤ i ≤ n. Each of these strings is of length (8n + 1)nl.
Replace each 0 in each si with two 0’s (for 1 ≤ i ≤ n) and replace each 1
in each si with two 1’s and let s′i be the resultant string. Note that an optimal

8

Vamsi Kundeti, Sanguthevar Rajasekaran, and Hieu Dinh

solution for the 4-segments HTSP on the revised S will also be an optimal
solution for the 4-segments HTSP on the old S. If l is the length of each string
in the old S, then 2l will be the length of each revised input string.
The input for the BLMP instance that we generate will be q1 , q2 , . . . , qn , t, t, . . . , t
where t occurs n2 −n times. Each of these strings will be of length (8n+1)nl+2l.
The string qi will have s′i in its 2l LSBs and it will have ti in its (8n + 1)nl
MSBs, for 1 ≤ i ≤ n. The string t will have tn+1 in its (8n + 1)nl MSBs. Its
2l LSBs will be 0101 . . . 01, i.e., the string 01 is repeated l times. Note that
δ(qi , qj ) = 16nl + δ(s′i , s′j ) for any 1 ≤ i, j ≤ n. Also, δ(qi , t) = 9nl + l for any
1 ≤ i ≤ n.
Note that strings of this BLMP instance are comparable to the strings we
had for Lemma 2. This is because the interstring distances are very nearly in
the same ratios for the two cases. As a result, using a proof similar to that of
Lemma 2, we can show that the strings t1 , t2 , . . . , tn will all lie in the boundary
of the grid in an optimal solution to the above BLMP. Let T = {t1 , t2 , . . . , tn }.
Also, the strings of T will be found in four segments such that one string of
each segment occupies one of the corner nodes of the grid. Let S1 , S2 , S3 , and
S4 stand for the strings in these four segments, respectively. Let C1 , C2 , C3 , and
C4 be the optimal tour costs for S1 , S2 , S3 , and S4 , respectively.
Let |Si | = ni for 1 ≤ i ≤ 4. The total cost (i.e., the border length) for the
above BLMP solution can be computed as follows. Consider S1 alone. The cost
due to this segment is C1 + 2(9nl + l) + (n1 − 1)(9nl + l). The cost 2(9nl + l) is
due to the two end points of the segment S1 . The cost (n1 − 1)(9nl + l) is due
to the fact that each string of S1 (except for the one in a corner of the grid) is
a neighbor of a t. Upon simplification, the cost for S1 is C1 + (n1 + 1)(9nl + l).
Summing over all the four segments, the total cost for the BLMP solution is
C1 + C2 + C3 + C4 + (n + 4)(9nl + l). The minimum value of this is obtained
when S1 , S2 , S3 , and S4 form a solution to the 4-segments HTSP on T .
Clearly, an optimal solution for the 4-segments HTSP on T will also yield
an optimal solution for the 4-segments HTSP on S. This can be seen as follows.
Consider the strings in Si and let Qi = ai1 , ai2 , . . . , aini be the corresponding input
strings (of S), for 1 ≤ i ≤ 4. Note that Ci is nothing but (ni − 1)(16nl) plus
twice the optimal tour cost for Qi , for 1 ≤ i ≤ 4. Thus, C1 + C2 + C3 + C4 is
equal to (n − 4)16nl + 2(C1′ + C2′ + C3′ + C4′ ) where Ci′ is the optimal tour cost
for Qi , for 1 ≤ i ≤ 4.
This completes the proof of Theorem 1. 

4
4.1

Algorithms for the BLMP
An O(N )-approximation algorithm

In this section, we will show that a simple version of the algorithm suggested by
Hannenhalli, et al. is actually an O(N )-approximation algorithm. This algorithm
can be described as follows. Assume that the input is the set of strings S =
{s1 , s2 , . . . , sN 2 }. The algorithm first computes a tour T on strings in S. Then it

On the Border Length Minimization Problem

9

threads the tour T into the grid in row-major order (see Figure 2). The first step
can be done by calling the 32 -approximation algorithm for the HTSP suggested
by [?].

(1,1)

(1,N)

(N,1)

(N,N)

Fig. 2. The thick dark line corresponds to an optimal tour on the input strings

Lemma 3. OP THT SP (S) ≤ 2OP TBLMP (S).
Proof: Let A be an optimal solution for the BLMP on S. Consider the path
P ′ drawn as the thick dark line in Figure 2. Obviously, Cost(P ′ ) ≤ Cost(A) =
OP TBLMP (S). Let si1 and siN 2 be the two endpoints of P ′ . Since the Hamming
distance satisfies the triangular inequality, δ(si1 , siN 2 ) ≤ Cost(P ′ ). Consider the
tour that starts at si1 , traverses along the path P ′ to siN 2 and comes back to
si1 . Obviously, the cost of the tour is Cost(P ′ ) + δ(si1 , siN 2 ) ≤ 2Cost(P ′ ) ≤
2Cost(A). Hence, OP THT SP (S) ≤ 2OP TBLMP (S). 
Theorem 5. The above algorithm yields an O(N )-approximate solution.
Proof: First, we see that Cost(T ) ≤ 23 OP THT SP (S) ≤ 3OP TBLMP (S). The
first inequality is due to the 32 -approximation for the HTSP. The second inequality is due to Lemma 3. Now let us analyze the cost of the solution F produced by
the algorithm. Consider the path P drawn as the thick dark line in Figure 2. Obviously, Cost(P ) ≤ Cost(T ). Also, the total cost of the N rows in F is no more
than Cost(P ). By the triangle inequality, it is not hard to see that the cost of each
column in F is no more than Cost(P ). Therefore, Cost(F ) ≤ (N + 1)Cost(P ) ≤
(N + 1)Cost(T ) ≤ 3(N + 1)OP TBLMP (S) = O(N )OP TBLMP (S). 
4.2

A hierarchical refinement algorithm

Several heuristics such as the Epitaxial growth have been proposed to solve the
BLMP problem earlier. However most of these heuristics do not improve the cost

10

Vamsi Kundeti, Sanguthevar Rajasekaran, and Hieu Dinh

monotonically. Local search based algorithms are often employed to solve hard
combinatorial problems. We now introduce a hierarchical refinement algorithm
(HRA). This refinement technique can be applied to any heuristic placement
to refine the cost and get a better placement. Let N be the number of probes
in the placement, d a positive integer such that dx = N, x ≥ 1 is called the
degree of refinement. The refinement algorithm starts with a given placement,
then it divides the placement into s01 , s02 . . . s0N/d2 sub-problems with d2 probes
per sub-problem. Each of these sub-problems is solved optimally – an optimal
permutation among the probes is found. After this every d2 sub-problems are
2
combined into a new sub-problem s1i = ∪dj=1 s0id2 +j , 1 ≤ i ≤ N/d3 . To solve
s1i optimally we identify an optimal permutation among s0id2 +j ∈ s1i , 1 ≤ j ≤
d2 . This process continues until we are left with no sub-problems to solve. See
Figure 3.
We should remark that while solving a sub-problem optimally, we also consider the cost contributed from the neighboring sub-problems. This ensures the
monotonic improvement in the placement cost. The refinement algorithm asymptotically runs in Θ(d2 !N ) time. If d = O(1), the refinement algorithm runs in
linear time. For small values of d, the algorithm performs well in practice. HRA
is a deterministic refinement algorithm. We further extend this by introducing
randomness. The Randomized Hierarchical Refinement Algorithm (RHRA) is
similar to the HRA algorithm. RHRA randomly selects a sub-square within
the given placement and applies the HRA technique to the selected sub-square.
Similar to local search algorithms, repeating RHRA algorithm several times
improves the placement cost monotonically. We study the performance of both
these algorithms in section 5.

3

2

5

8

1

6

1

3

9

4

7

k−1

3

5

7

2

9

8

6

4

3

k

Fig. 3. Illustration of the hierarchical refinement algorithm with degree of refinement
3. This shows the possible optimal solutions (i.e. permutation among sub-problems) at
the top-most and penultimate levels

On the Border Length Minimization Problem

4.3

11

Quad epitaxial algorithm

The epitaxial (EP X) placement suggested in [?] places a randomly selected
probe at the center of the array, it continues placing the probes greedily around
the locations adjacent to the placed probes to minimize the cost (i.e. the algorithm almost spends O(N 2 ) time to place each probe). The epitaxial algorithm
gives good results for small arrays but for larger arrays the epitaxial algorithm
is impractical and extremely slow. We propose the Quad Epitaxial (QEP X)
algorithm as a simple extension to the epitaxial algorithm. QEP X yields good
performance and is very fast compared to the EP X algorithm. The basic idea
behind the QEP X algorithm is to divide the array into four parts, apply EP X
algorithm for each of the four parts and finally find an optimal arrangement
among the four parts. In section 5 we compare the QEP X algorithm with EP X
algorithm.

5
5.1

Experimental study
Performance of the QEP X algorithm

In this section we compare the performance of QEP X algorithm introduced earlier. We use randomly generated probe arrays of size 322 ,642 ,1282 and 2562 . In
all of our experimental studies we compute a lower bound on the solution by picking the smallest 2N (N − 1) edges from the complete Hamming distance graph.
Column-4(INIT COST) in the table 1 indicates the placement cost obtained by
placing the probes in the row major order as given by the input. Column-5(8)
indicates the final placement cost obtained by the epitaxial (quad) algorithm.
As we can see from columns 7 and 10, the refinement obtained by the QEP X
algorithm is very close to the EP X algorithm. On the other hand QEP X runs
3.6X faster than the EP X algorithm. As we can see from table 1, as the chip size
increases EP X algorithm becomes very slow. We ran both EP X and QEP X
algorithms on a chip size of 243 × 243 with a time limit of 60 minutes. The
QEP X algorithm took around 12 minutes to complete and improved the input
placement cost by 36%. On the other hand the EP X algorithm did not complete
the placement. From our experiments we conclude that the QEP X can provide
a good placement which we can use as an input for refinement/local search algorithms such as RHRA. In the next sub-section we provide our experimental
study of HRA and RHRA algorithms on various placement heuristics.
5.2

Performance of refinement algorithms

We have applied our HRA, RHRA refining algorithms on the following placement heuristics.
– (RAN D) Random placement: in this placement we just use the order in
which the probes are provided to our algorithm.
– (SORT ) Sort placement: in this placement the input probes are sorted lexicographically

12

Vamsi Kundeti, Sanguthevar Rajasekaran, and Hieu Dinh

TEST PROBES LOWER INIT
CASE
BOUND COST

EPX

TIME REFINED QEPX TIME REFINED
(sec) PRECENT
(sec) PRECENT

t-0
t-1
t-0
t-1
t-0
t-1
t-0
t-1

27591
27472
106471
106430
410301
409625
2447885
2427143

0.60
0.62
10.70
10.37
180.00
185.88
-

1024
1024
4096
4096
16384
16384
65536
65536

23480
23427
86818
86897
322129
-

37192
37029
151116
151176
609085
608928
2447885
2427143

25.81%
25.81%
29.54%
29.60%
32.64%
32.73%
0.00%
0.00%

28060
28151
107805
107634
411746
410902
1563369
1562630

0.42
0.43
3.05
3.23
43.93
44.70
765.79
774.33

24.55%
23.98%
28.66%
28.80%
32.40%
32.52%
36.13%
35.62%

Table 1. Comparison between epitaxial and quad epitaxial

PROBES ALGO LOWER
BOUND
729
RAND 17087
729
SORT 17087
729
SWM 17087
729
REPTX 17087
729
EPTX 17087
6561
RAND 136820
6561
SORT 136820
6561
SWM 136820
6561
REPTX 136820
6561
EPTX 136820
1024
RAND 23480
1024
SORT 23480
1024
SWM 23480
1024
QEPX 23480
1024
REPTX 23480
1024
EPTX 23480
4096
RAND 86818
4096
SORT 86818
4096
SWM 86818
4096
QEPX 86818
4096
REPTX 86818
4096
EPTX 86818

INIT
COST
26401
24082
22267
21115
19733
243125
210326
204955
185386
168676
37192
33784
31424
28060
29574
27591
151116
131291
127516
107805
116406
106471

HRA
23970
22415
22195
21107
19726
221090
198972
204525
185362
168623
35236
32326
31383
28035
29557
27567
143246
127033
127357
107766
116395
106462

RHRA REFINED
PRECENT
22631 14.280%
21649 10.103%
22069 0.889%
21101 0.066%
19726 0.035%
209514 13.825%
191915 8.754%
203412 0.753%
185341 0.024%
168544 0.078%
33046 11.148%
31026 8.164%
31323 0.321%
28028 0.114%
29546 0.095%
27565 0.094%
134485 11.005%
121742 7.273%
127092 0.333%
107702 0.096%
116376 0.026%
106448 0.022%

TIME
2.83(min)
2.81(min)
2.81(min)
2.81(min)
2.81(min)
17.55(min)
17.02(min)
17.20(min)
17.16(min)
17.15(min)
0.28(sec)
0.26(sec)
0.13(sec)
0.47(sec)
0.11(sec)
0.11(sec)
6.93(sec)
4.46(sec)
1.27(sec)
5.04(sec)
1.02(sec)
1.04(sec)

Table 2. Cost refinement for various placement heuristics by applying HRA (hierarchical refinement algorithm) and RHRA (randomized hierarchical refinement algorithm)
with 350 iterations

On the Border Length Minimization Problem

13

– (SW M ) Sliding Window Matching placement is obtained by running the
SW M [?] algorithm with parameters (6, 3).
– (REP X) Row epitaxial placement is obtained by running the row-epitaxial
algorithm with 3 look-ahead rows.
– (EP X) Epitaxial placement is obtained by running the EP X algorithm
– (QEP X) Quad epitaxial placement obtained by our quad-epitaxial algorithm
The cost of the placement obtained by running the HRA algorithm exactly once
is given in column-5 (HRA). Column-6 (RHRA) indicates the placement cost
obtained by running our randomized refinement algorithm RHRA for 350 iterations. From table 2 we can see that as initial placement moves closer and closer
towards the lower bound the refinement percentage decreases, which is logical.
For test cases with 729, 6561 (1024, 4096) probes we use a refinement degree
d = 3 (d = 2). Choosing a bigger refinement degree gives better refinements, however takes more time. Finally we conclude that our refinement algorithms would
be very useful when applied in conjunction with fast initial placement heuristics.
A fully function program called blm-solve implementing all our algorithms can be
downloaded from the website http://launchpad.net/blm-solve, the web-site
also has all the supplementary details used in the our experimental study.

6

Conclusions

In this paper we have studied the Border Length Minimization Problem (BLMP)
that has numerous applications in biology and medicine. We have solved a sevenyear old open problem in this area by showing that the BLMP is N P-hard.
Two different proofs have been given and we believe that the techniques in
these proofs will find independent applications. We have also shown that certain
generalizations of the BLMP are N P-hard as well. In addition, we have presented
a hierarchical refinement algorithm (HRA) for the BLMP. Deterministic and
randomized versions of this algorithm can be used to refine the solutions obtained
from any algorithm for solving the BLMP. Our experimental results indicate that
indeed HRA can be useful in practice.
One of the best performing algorithms for the BLMP is the epitaxial algorithm (EPX). This algorithm takes too much time especially when the number
of probes is large. In this paper we present a variant called the quad-epitaxial
algorithm (QEPX) that is much faster than EPX while yielding a solution that
is very close to that of EPX in quality. QEPX partitions the input into four
parts, works on each part separately, and finally combines these solutions. This
idea can be extended further to partition the input into more parts and hence
this algorithm is ideal for parallelism.
Some of the open problems are: 1) In this paper we have used a simple lower
bound on the quality of solution for the BLMP. It will be nice to develop tighter
lower bounds; 2) Develop more efficient algorithms than EPX; and 3) Design
parallel algorithms for the BLMP.

14

Vamsi Kundeti, Sanguthevar Rajasekaran, and Hieu Dinh

Acknowledgements. This work has been supported in part by the following
grants: NSF 0326155, NSF 0829916 and NIH 1R01GM079689-01A1.

C1

C’1 = C1− 4
x

y’

C2
y’

C’2 = C2+ 3
x

