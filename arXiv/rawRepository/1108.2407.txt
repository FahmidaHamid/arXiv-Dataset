On the dynamics of mean-field equations for stochastic neural
fields with delays
Jonathan Touboul

arXiv:1108.2407v3 [math.DS] 31 Oct 2011

The Mathematical Neuroscience Laboratory, Collège de France, Center for Interdisciplinary Research in
Biology (CIRB) and INRIA Paris, BANG Laboratory, 11 place Marcelin Berthelot, 75005 Paris, France

Abstract
The cortex is composed of large-scale cell assemblies sharing the same individual properties
and receiving the same input, in charge of certain functions, and subject to noise. Such
assemblies are characterized by specific space locations and space-dependent delayed interactions. The mean-field equations for such systems were rigorously derived in a recent paper
for general models, under mild assumptions on the network, using probabilistic methods.
We summarize and investigate general implications of this result. We then address the dynamics of these stochastic neural field equations in the case of firing-rate neurons. This is
a unique case where the very complex stochastic mean-field equations exactly reduce to a
set of delayed differential or integro-differential equations on the two first moments of the
solutions, this reduction being possible due to the Gaussian nature of the solutions. The
obtained equations differ from more customary approaches in that it incorporates intrinsic
noise levels nonlinearly and make explicit the interaction between the mean activity and
its correlations. We analyze the dynamics of these equations, with a particular focus on
the influence of noise levels on shaping the collective response of neural assemblies and
brain states. Cascades of Hopf bifurcations are observed as a function of noise amplitude,
for noise levels small enough, and delays, in a finite-population system. The presence of
spatially homogeneous solutions in law is discussed in different non-delayed neural fields
and an instability, as noise amplitude is varied, of the homogeneous state, is found. In
these regimes, very complex irregular and structured spatio-temporal patterns of activity
are exhibited including in particular wave or bump splitting.
Keywords: mean-field equations, neural fields, bifurcations, integro-differential equations,
noise, infinite-dimensional dynamical systems, propagation of chaos, delayed stochastic
integro-differential equations.

Contents
1 Introduction

2

Email address: jonathan.touboul@college-de-france.fr (Jonathan Touboul)

Preprint submitted to Elsevier

November 1, 2011

2 Mathematical results on mean-fields models for neural fields
2.1 Finite number of populations . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Infinite number of populations . . . . . . . . . . . . . . . . . . . . . . . . . .

4
4
6

3 Mean-field equations for firing-rate models
12
3.1 Finite-population Wilson and Cowan Equations . . . . . . . . . . . . . . . . . 13
3.2 Spatially extended neural fields . . . . . . . . . . . . . . . . . . . . . . . . . . 16
4 Analysis of the solutions
4.1 Finite-populations networks . . .
4.2 Neural Fields Dynamics . . . . .
4.2.1 A single layer neural field:
4.2.2 Dynamic Turing patterns
boundary conditions . . .

. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
Turing-Hopf instabilities and noise . . . . .
in a two layers neural field with periodic
. . . . . . . . . . . . . . . . . . . . . . . . .

5 Discussion

21
21
25
26
30
40

Appendix A
Existence and Uniqueness of solutions of the synchronized
mean-field solution
43
Appendix B
Spatio-temporal patterns for one-dimensional neural fields
with reflective or zero boundary conditions
45
Appendix B.1
Reflective boundary conditions . . . . . . . . . . . . . . . . . . 45
Appendix B.2
Zero boundary conditions . . . . . . . . . . . . . . . . . . . . . 45
1. Introduction
The activity of the brain is often characterized by large-scale macroscopic states resulting of the structured interaction of a very large number of neurons. This interaction yield
meaningful signals accessible from non-invasive imaging techniques (EEG/MEG) and often
used for diagnosis. Finer analysis of the brain’s constitution identifies anatomical structures, such as the cortical columns, composed of the order of few thousands to one hundred
thousand neurons belonging to a few different species, in charge of specific functions, sharing the same input and strongly interconnected, communicating through the emission of
action potentials after delays depending on the anatomical or functional distance between
themselves. Two interesting well-understood cortical areas are the primary visual cortex
of certain mammals and the rat’s barrel cortex. In the primary visual cortex V1, neurons
can be divided into orientation preference columns responding to specific orientations in
visual stimuli, forming specific patchy connections [1, 2]. Similarly, the rat’s barrel cortex
presents a clear columnar organization, and neurons responding to the sensory information
of a particular whisker anatomically gather into barrels [3, 4]. More generally, it is now
understood that brain’s activity results from the interactions of different cells, among which
neurons, manifesting highly complex behaviors often characterized by the intense presence
of noise. The communication between two such columns is characterized by a delay due to
the transport of information through axons at a finite speed and to the typical time the
synaptic transmission takes. These delays have a clear role in shaping the neuronal activity,
as established by different authors (see e.g. [5, 6, 7, 8]). Several relevant brain states rely
on the coordinated behaviors of large neural assemblies, and recently raised the interest of
2

Stochastic Neural Fields Dynamics

3

physiologists and computational neuroscientists, among which we shall cite the rapid complex answers to specific stimuli [9], decorrelated activity [10, 11], large scale oscillations [12],
synchronization [13, 14], and spatio-temporal pattern formation [15, 16, 17].
Most computational models of neural area have relied on continuum limits ever since
the seminal work of Wilson, Cowan and Amari [18, 19, 20, 21], in which the activity is
represented through a macroscopic variable, the population-averaged firing rate, that is
generally assumed to be deterministic. This approach successfully accounted for several
phenomena, for instance for the problem of spatio-temporal pattern formation in spatially
extended models (see e.g. [16, 22, 23, 24]). In this approach, the effect of noise is neglected
in large populations. Increasingly many models now consider that the different intrinsic
or extrinsic noise sources are a meaningful part of the neuronal signal rather than a pure
disturbing effect, that conveys important information [25]. For instance, sparsely connected
neural networks were studied, in wihch the sparse connectivity assumption allows studying
regimes where the activity is uncorrelated [26, 27, 28]. Again, the emergent global activity
of the population in the limit of an infinite number of neurons is deterministic, and evolves
according to a mean-field firing rate equation. The stochastic nature of the firing was also
developed through such methods as the population density [29], or from a Markovian model
governing the firing dynamics of the neurons in the network, where the transition probability
satisfies master equation. This modeling recently gathered the interest of the community (see
e.g.[30, 31, 32, 33, 34, 35]). Most of these approaches are proved correct in some parameter
regions using statistical physics tools such as path integrals and Van-Kampen expansions,
and involve a moment expansion and truncation.
We are interested here to start include noise at the level of each individual neuron. From
the mathematical viewpoint, each neuron is a diffusion process, and different neurons belong
to particular populations in which they receive a common input and are interconnected in
a similar fashion. These equations motivated by biological analysis, are studied in a probabilistic setting in a recent paper [36], taking into account spatial information and delayed
interconnections, using probabilistic methods from the propagation of chaos domain (see
e.g. [37]). The results of this analysis are summarized in section 2. Under mild assumptions
on the neuronal dynamics, the regularity of the interactions and the number of neurons per
population, this analysis provides a mean-field equation corresponding to the behavior of a
particular neuron in the network. These equations, in the original general framework, apply
to such neurons as the Hodgkin-Huxley and Fitzhugh-Nagumo neuron models, which are
relevant representations of individual neuronal activity. We analyze here the implications
of this result in that general setting in the discussion section. We also discuss an extended
notion of synchronization, the synchronization in law, and provide general conditions for
this property to occur in spatially extended mean-field neural fields systems. The meanfield equations obtained in [36] are implicit equations on probability distributions, and as
such extremely hard to tame in general. Simulations of such equatiosn are relatively hard
to perform, as illustrated in the case where no delay and no spatial extension is taken into
account [38], and this difficulty makes it hard to identify qualitative effects of noise levels in
such systems. In order to start uncovering the structure of the solutions of these equations
and possible effects of noise levels each individual neuron is submitted to, we consider in
section 3 a simplified neuron model based on the description of firing-rates. In this particular case, we demonstrate that the solution of the complicated mean-field equations are
Gaussian, similarly to what was done in a way simpler model in [39]. Characterizing the
mean and covariance functions are hence sufficient to describe the dynamics of the solu-

Stochastic Neural Fields Dynamics

4

tions, and these are shown to exactly reduce to a set of deterministic delayed differential
or integro-differential equations. The derivation of these equations and the bifurcations of
these equations are studied, in the finite-population case (section 3.1) and an intricate bifurcation structure with the appearance of a cascade of Hopf bifurcations as a function of
noise levels and delays, that tend to accumulate as delays are increased in a restricted region
of noise levels. This bifurcation structure results in the presence of an increasing number
of unstable cycles that provoke extremely complex and irregular transient behaviors. In
the continuous-populations case (section 3.2), we show noise levels very strongly shape the
form of the spatio-temporal solutions of the system. Spatially homogeneous stationary or
periodic solutions are found as a function of the noise levels. An additional state is exhibited
for small values of the noise levels, in which spatially inhomogeneous, non-stationary spatiotemporal patterns with a complicated shape arise. These qualitative results are reproduced
for different unidimensional neural fields with different topologies and connectivities, and
either satisfying or not the synchronization in law condition, in appendix Appendix B.
Implications of these results are eventually discussed in section 5 together with some open
problems the present study motivates.
2. Mathematical results on mean-fields models for neural fields
In [36] a rigorous derivation of the mean-field equations is performed for a wide class of
models encompassing classical neuron models such as the Hodgkin-Huxley or the FitzhughNagumo models, taking into account neural fields spatial structures and space-dependent delays. The general results obtained in that article are summarized here. In all the manuscript,
we are working in a complete probability
space (Ω, F, P) satisfying the usual conditions, and

endowed with a filtration Ft t . The state of each neuron i in the network is described by

a d-dimensional variable X i ∈ E = Rd , typically corresponding to the membrane potential
of the neuron and possibly additional variables such as those related to ionic concentrations
and channels. We distinguish the cases where the number of populations is finite and the
case where it is infinite corresponding to the spatially extended neural fields.
def

2.1. Finite number of populations
We start by summarizing the results of [36] in the case where the network is composed
of a finite number of populations α ∈ {1, · · · , P }. Let p denote the population function that
associates to a neuron i the population α it belongs to: p(i) = α, and Nα (N ) the number
of neuron in population α for a network size N (we will generally drop the dependence of
Nα in N since no confusion is possible), and assume that this size tends to infinity in the
limit N → ∞. The state of a neuron i in population α, X i is characterized by its intrinsic
dynamics governed by a drift function Gα : R × E 7→ E (including the intrinsic dynamics
and the deterministic inputs) and a diffusion matrix gα : R × E 7→ Rm .
dXti = Gα (t, Xti ) dt + gα (t, Xti ) dWti ,
where (Wti )i∈N constitutes a sequence of m × d-dimensional adapted Brownian motions.
Neuron j of population γ produces an infinitesimal current received at time t by the neuron
i modeled as:
Z 0
Z 0
1
1
j
j
i
bαγ (Xt , Xt+s )dηαγ (s) dt +
βαγ (Xti , Xt+s
)dµαγ (s) dBtiγ .
Nγ −τ
Nγ −τ

Stochastic Neural Fields Dynamics

5

The terms ηαγ and µαγ denote signed finite measures modeling the delays occurring in the
information transmission through the axons, bαγ and βαγ are smooth functions of E×E 7→ E
and (Bti,γ )i∈N,γ=1,··· ,P are d × d independent adapted Brownian motions. In this setting,
the network equations read:
d Xti,N = Gα (t, Xti,N ) dt +

P
X

N
X

γ=1 j=1,p(j)=γ

+

gα (t, Xti,N )

·

dWti

P
X
1
+
N
γ
γ=1

1
Nγ

Z

0
j,N
bαγ (Xti,N , Xt+s
)dηαγ (s) dt

−τ

N
X
j=1,p(j)=γ

Z

0
j,N
βαγ (Xti,N , Xt+s
)dµαγ (s) · dBtiγ . (1)

−τ

For the sake of simplicity, we will denote in the sequel the sum over neurons of population γ,
PN
PNγ
j=1 . These equations are well-posed stochastic differential equaj=1,p(j)=γ , simply by
tions on the infinite-dimensional space Cτ = C([−τ, 0], E) (see e.g. [40, 41]) of continuous
functions of [−τ, 0] with values in E. We will generally make the assumption that the network has chaotic initial condition, i.e. independent initial conditions, identically distributed
for neurons belonging to the same population. In details, for (ζ0α (t), α = 1 · · · P ) ∈ Cτ a
stochastic process with independent components, a chaotic initial condition on the network
consists in setting the initial condition of all neurons i of population α to an independent
copy of ζ0α . The following mild technical assumptions on the functions governing the network’s dynamics:
(H1). Gα and gα are uniformly locally Lipschitz-continuous with respect to their second
variable
(H2). bαγ and βαγ are L-Lipschitz-continuous, i.e. there exists a positive constant L such
that for all (x, y) and (x0 , y 0 ) in E × E we have:
|Θαγ (x, y) − Θαγ (x0 , y 0 )| ≤ L (|x − x0 | + |y − y 0 |)
for Θ ∈ {b, β}.
(H3). There exists a K̃ > 0 such that:
max(|bαγ (x, z)|2 , |βαγ (x, z)|2 ) ≤ K̃(1 + |x|2 )
(H4). The drift and diffusion functions satisfy the monotone growth condition:
1
xT fα (t, x) + |gα (t, x)|2 ≤ K (1 + |x|2 ).
2
Under these assumptions, the following theorem is proved in [36]:
Theorem 1. Let us consider a network composed of N neurons belonging to P populations. Assuming the regularity assumptions (H1)-(H4) are satisfied and that each neuron of the network chaotic initial conditions (with law ζ α,0 for neurons belonging to the
same population α). Fix l ∈ N∗ and {i1 , · · · , il } l neurons of the network. The law of
(Xti1 ,N , · · · , Xtil ,N , −τ ≤ t ≤ T ) solution of the network equations (1) converges towards
mp(i1 ) ⊗ · · · ⊗ mp(il ) when N → ∞ where mα is the law of the unique solution X̄ of the

Stochastic Neural Fields Dynamics

6

equation:
dX̄tα = Gα (t, X̄tα ) dt +

P Z
X
γ=1

0

−τ

β
EȲ [bαβ (X̄tα , Ȳt+s
)]dηαβ (s) dt

+ gα (t, X̄tα ) dWtα +

P Z
X
γ=1

0

−τ

β
EȲ [βαβ (X̄tα , Ȳt+s
)]dµαβ (s) dBtαγ

(2)

with initial condition (ζtα , α = 1 · · · P, t ∈ [−τ, 0]). In equation (2), the process (Ȳtγ ) is a
process independent of, and has the same law as X̄tγ and EȲ denotes the expectation with
respect to the process Ȳ . In other terms, if mγt denotes the probability distribution of Xtγ ,
the mean-field equations can be equivalently written as:
dX̄tα = Gα (t, X̄tα ) dt +

P Z
X
γ=1

0

−τ

Z

bαβ (X̄tα , y)mγt+s (dy)dηαβ (s) dt

E

+ gα (t, X̄tα ) dWtα +

P Z
X
γ=1

0

−τ

Z

βαβ (X̄tα , y) mγt+s (dy)dµαβ (s) dBtαγ .

E

2.2. Infinite number of populations
We now summarize results of [36] in the case of neural fields spanning over an infinite,
continuous compact space Γ, modeling a spatial or functional extension of a cortical structure. These structures are made of several sub-areas, the neural populations, composed of a
large number of strongly interconnected neurons. The set Γ considered is a finite-dimensional
compact set. Typically, it is considered, when populations are described by their location
on the cortex, that Γ as a compact subset of Rq for some q ∈ N∗ (in Appendix B, we will
consider in particular the case Γ = [0, 1]). When considering that populations are defined
by the neuron’s function, the shape of Γ can take different forms depending on the geometry
of the feature space. For instance in the case of the modeling of pinwheels primary visual
area, neurons are indexed by their preferred orientation that can be represented in the torus
Γ = S1 (this is the case of section 4.2).
In that case, the total number of populations is a diverging function P (N ) of the total
number of neurons N , and populations densely cover Γ in the limit N → ∞ with a certain
density. The populations are characterized by their location (r1 , · · · , rP (N ) ) ∈ ΓP (N ) (see
figure Fig. 1), and by the number of neurons in each population in a network of size N ,
PP (N )
(N1 (N ), . . . , NP (N ) (N )) (we hence have γ=1 Nγ (N ) = N ). Populations are randomly
and independently drawn in a law λ(·)/λ(Γ) where λ is a square integrable1 , finite density
R
def
over Γ (i.e. a positive function such that Γ λ2 (r) dr = (λ2 )(Γ) < ∞), accounting for possible
inhomogeneities of the neural field. Note that the square integrability condition readily
R
def
implies that Γ λ(r) dr = λ(Γ) < ∞. This degree of freedom allowed for accounting for
different topologies and geometries of the physical space or feature space (for instance, a
recent model of texture neural field involves a feature space having an hyperbolic geometry,
1 Only integrability was assumed in [36], the square integrability will be needed for our developments on
firing-rate models.

Stochastic Neural Fields Dynamics

7

Figure 1: A typical architecture of neural field: cylinders represent neural populations as cortical columns
spanning across the cortex. Neuron j in the green population at location rj ∈ Γ communicates with neuron
i in the red population at location ri , and the information sent is received with a delay τ (ri , rj ) depending
on the spatial location of each neuron, and with a synaptic weight J(ri , rj ) also depending on the neural
populations of i and j.

see e.g. [42, 43]). The expectation over the realizations of the space locations (rα ) is denoted
E .
As observed in [36], there is a critical competition between the number of populations
and the total number of neurons. Indeed, in order for averaging effects to occur in the
neural field, a very large number of neurons in each population is required, competing with
the number of populations, which is required to fill the space Γ. A technical assumption
made in [36] was the following:
1
e(N ) def
=
P (N )

P (N )

X
γ=1

1
−→ 0
Nγ (N ) N →∞

(3)

This assumption ensures heuristically most populations are made of a diverging number of
neurons. This interpretation is relevant for our neuronal populations problem since, it is understood that populations roughly contain the same number of neurons and that this number
of neurons is orders of magnitude larger than the number of populations(see e.g. [44]). In
order to comply with more standard approaches in neural field theory, the functions Gα (t, x)
are replaced by G(rα , t, x) and similarly gα (t, x) by g(rα , t, x). The number of populations
diverging, it is necessary in our description to scale the synaptic weight as the number of
populations increases, in order to ensure the finiteness of the input to one single neuron2 , and
define the interaction functions between population α and population γ at location rγ , denoted in the finite-population case bα,γ (x, y) (resp. βα,γ (x, y)) by Pλ(Γ)
(N ) b(rα , rγ , x, y) (resp.
λ(Γ)
P (N ) β(rα , rγ , x, y)).

The delay measures η(rα , rγ , s) and µ(rα , rγ , s) typically involve the
distance krα − rγ kΓ and the transmission velocity of the information in the axons. These are
again assumed to be finite signed measures, charging the interval [−τ, 0], with total variation
uniformly (in rα and rγ ) bounded by a quantity denoted κ.
We define a sequences of independent Brownian motions (Wti ) in M(C([0, T ]), Rm×d ) and
2 This

is due in particular to the limited resources of the neuronal environment.

Stochastic Neural Fields Dynamics

8

2
a sequence of independent cylindrical Brownian motions Bti (r0 ) in M(C([0,
(Γ, Rδ×d ))
 i T ]), L
j 0
for i ∈ N, i.e. centered Gaussian processes with covariance function E Bt (r)Bs (r ) = t ∧ s
if i = j and r = r0 , and 0 otherwise (see [40] for existence and properties of this process).
The stochastic network equations in the read, for p(i) = α and a network of size N :

d Xti,N = G(rα , t, Xti,N ) dt +
+ g(rα , t, Xti,N ) · dWti +

Z 0
P (N ) Nγ
λ(Γ) X X 1
j,N
b(rα , rγ , Xti,N , Xt+s
)dη(rα , rγ , s) dt
P (N ) γ=1 j=1 Nγ −τ

Z 0
P (N ) Nγ
λ(Γ) X X 1
j,N
β(rα , rγ , Xti,N , Xt+s
)dµ(rα , rγ , s) · dBti (rγ ).
P (N ) γ=1 j=1 Nγ −τ
(4)

They make the following suitable regularity assumptions on the functions governing the
dynamics of the neurons:
(GH1). G(r, t, ·) and g(r, t, ·) are uniformly locally Lipschitz-continuous,
(GH2). b(r, r0 , ·, ·) and β(r, r0 , ·, ·) are uniformly Lipschitz-continuous, i.e. there exists a positive constant L such that for all (x, y) and (x0 , y 0 ) in E × E and all (r, r0 ) ∈ Γ2 we
have:
|Θ(r, r0 , x, y) − Θ(r, r0 , x0 , y 0 )| ≤ L (|x − x0 | + |y − y 0 |)
for Θ ∈ {b, β}.
(GH3). There exists a K̃ > 0 such that:
max(|b(r, r0 , x, z)|2 , |β(r, r0 , x, z)|2 ) ≤ K̃(1 + |x|2 )
(GH4). The drift and diffusion functions satisfy, uniformly in space (r) and time (t), the
inequality:
1
xT f (r, t, x) + |g(r, t, x)|2 ≤ K (1 + |x|2 )
2
Under these assumptions, it is proved in [36] the following:
Theorem 2. Let us consider a network composed of N neurons belonging to P (N ) populations such that the assumptions (3), (GH1)-(GH4) are satisfied and that each neuron of
the network has independent initial conditions, that are identically distributed population per
population (with law ζ 0 (r) for neurons belonging to the population located at r ∈ Γ). Fix
l ∈ N∗ and {i1 , · · · , il } l neurons of the network. The law of (Xti1 ,N , · · · , Xtil ,N , −τ ≤ t ≤ T )
solution of the network equations (4) converges towards mt (rp(i1 ) ) ⊗ · · · ⊗ mt (rp(il ) ) when
N → ∞, where mt (r) is the law of X̄t (r), the unique solution of the mean-field equations:
Z Z

0

d X̄t (r) = G(r, t, X̄t (r)) dt +
Γ

Z Z

0

+ g(r, t, X̄t (r)) · dWt (r) +
Γ

−τ

−τ

EZ̄ [b(r, r0 , X̄t (r), Z̄t+s (r0 ))]dη(r, r0 , s) λ(r0 )dr0 dt

EZ [β(r, r0 , X̄t (r), Z̄t+s (r0 ))]dµ(r, r0 , s) · dBt (r, r0 )λ(r0 ) dr0
(5)

with initial condition (ζt0 (r), r ∈ Γ, t ∈ [−τ, 0]), where the process (Zt (r)) is independent of,
and has the same law as X̄t (r).

Stochastic Neural Fields Dynamics

9

These results presented in the summary can appear very formal due to the generality of
the models considered and to the mathematical approach developed of [36]. However, even at
this level of generality, one can deduce several implications of these properties with regards
to neuroscience applications (see section 5). It is important to realize at this point that
even if the mean-field approach reduces an infinite system of interacting diffusion processes
into a single well-posed equation, a major issue one faces is the concrete identification,
characterization and simulation of the solution of the mean-field equations. In our spatial
and delayed setting, this concern is even more true. We show in this paper that there exists a
particular model of neuronal dynamics related to the so-called firing-rate neurons, for which
mean-field equations are rigorously reducible to a set of deterministic equations that are
amenable to analytic treatment (see sections 3 and 4). We emphasize the fact that firingrate neurons, though highly popular and widely used in the computational neuroscience
community, are not the most relevant from the biological viewpoint, and this restriction
justifies the necessity of dealing with such complex models as those described in [36].
A phenomenon of great interest in neuroscience is the synchronization, understood as the
fact that neurons manifest the same behavior after a transient phase, or the polychronization
corresponding to the fact that the neural field form a few synchronized clusters (see e.g. [14]).
Our stochastic setting suggests to extend this notion to the wider notion of synchronization
in law, corresponding to the fact that the law of state of different neurons or different
populations converge (as time increases) towards the same probability distribution. This
notion of synchronization generalizes the notion of synchronized oscillations to solutions
that are stationary in time, or that present specific time evolutions. In particular, spatially
homogeneous in law solutions of the neural field equations will be said synchronized in law.
It is clear from the results of theorem 2 that different neurons of the same population in the
network have, in the mean-field limit, the same probability distribution provided that they
had iid initial conditions, hence synchronize in law, and that different neurons belonging to a
few different populations polychronize in law, forming clusters depending on the population
the neurons belong to. A more complex question is the synchronization or polychronization
in law between different populations. The following proposition provides sufficient conditions
for solutions to be spatially homogeneous in law in the neural field setting. To this end, we
extend the simple sufficient conditions of [45] in the stochastic mean-field setting. We have
the following:
Proposition 3. Assume that the distribution of the initial condition ζt0 (r) is chaotic and
does not depend on r ∈ Γ, and that the functions G(r, t, x) and g(r, t, x) does not depend on
r. Assume moreover that the law of the quantities:
(
R0
def R
B(r, x, ϕ) = Γ λ(r0 )dr0 −τ dη(r, r0 , u)b(r, r0 , x, ϕ(u)) and
(6)
R0
Rt
def R
H(r, x, ϕ) = Γ λ(r0 )dr0 −τ dµ(r, r0 , u) 0 β(r, r0 , x, ϕ(u))dBs (r, r0 )
do not depend on r for any ϕ a measurable function (this ensures that the global input a
neuron receives is independent of its spatial location). Then the solution of the mean-field
equation (5) is spatially homogeneous (i.e. does not depend on r, or is fully synchronized).
Moreover, if we fix r0 ∈ Γ, then for each r ∈ Γ, the law of Xt (r) solution of the mean-field

Stochastic Neural Fields Dynamics

10

equations is equal to the law of the solution of the equation:
t

Z
Xt (r0 ) = X0 (r0 ) +
0



ds G(r0 , s, Xs (r0 )) + EZ [B(r0 , Xs (r0 ), Z(·) (r0 ))]
Z t
+ EZ [H(r0 , Xs (r0 ), Z(·) (r0 ))] +
dWs (r)g(r0 , s, Xs (r0 )). (7)
0

which has a unique solution.
Under the spatial homogeneity condition of proposition 3, two neurons belonging to
two different populations r and r0 are heuristically interchangeable in the limit N → ∞, and hence
the finite-population setting heuristically applies with only one population network, made of neurons
with intrinsic dynamics function G(r0 , s, x) and diffusion function g(r0 , s, x), deterministic (resp.
stochastic) interaction functions B(r0 , x, Z(·) (r0 )) (resp.H(r0 , x, Z(·) (r0 ))).

Remark.

Proof. The existence and uniqueness property of solutions proved in [36] is based on showing

a contraction property on the map Φ acting on M = M1 (C([−τ, T ], L2λ (Γ)) the space of laws
def

of stochastic processes taking values in L2λ (Γ) = L2λ (Γ, E) defined, for any X ∈ M, by:


Rt 
R
R
0
0
0 0
0
0
0

ζ
(r)
+
ds
G(r,
s,
X
(r))
+
λ(r
)
dr
dη(r,
r
,
u)
E
[b(r,
r
,
X
(r),
Z
(r
))]

s
Z
s
s+u
0

0
Γ
−τ

R
R
Rt


0
0 0
0
0
0
0

+
λ(r
)
dr
dµ(r,
r
,
u)
E
[β(r,
r
,
X
(r),
Z
(r
))]
·
dB
(r,
r
)

Z
s
s+u
s
−τ
0
RΓt
Φ(X)t (r) =
+ 0 dWs (r)g(r, s, Xs (r))
,
t>0


0


,
t ∈ [−τ, 0]
ζt (r)



L
(Z ) =(X
) ∈ M independent of (X ), (W (·)) and (B (·, ·))
def

t

t

t

t

t

It can be shown that the unique solution of the mean-field equations is the limit of the recursion Xtn+1 (r) = Φ(Xtn (r)) starting from any initial process Xt0 (r). Let Xt0 (r) a stochastic
L
process whose law does not depend on r: Xt0 (r) = Xt0 (r0 ) for any (r, r0 ) ∈ Γ. We can then

Stochastic Neural Fields Dynamics

11

show that Φ(X 0 )t (r) do not depend on r either, since we have:
Z t 
Z
Z 0

0
Φ(X 0 )t (r) = X00 (r) +
ds G(r, s, Xs0 (r)) + λ(r0 ) dr0
dη(r, r0 , u)EZ 0 [b(r, r0 , Xs0 (r), Zs+u
(r0 ))]
0

Z
+

0

λ(r ) dr

0

L

0

Z

0

dµ(r, r , u)
−τ

Γ

= X00 (r0 ) +

t

Z

0

t

E

+

λ(r0 ) dr0

Γ

Z0

[β(r, r

0
, Xs0 (r), Zs+u
(r0 ))]

0

−τ
t

dµ(r, r0 , u)

Z
0

t

EZ

0

0

t

Z

dWs (r)g(r, s, Xs0 (r))

· dBs (r, r ) +
0

Γ

Z

0

Z
Z

ds G(r0 , s, Xs0 (r0 )) + λ(r0 ) dr0

0

Z

−τ

Γ

Z

0

−τ


0
dη(r, r0 , u)EZ 0 [b(r, r0 , Xs0 (r0 ), Zs+u
(r0 ))]

0
[β(r, r0 , Xs0 (r0 ), Zs+u
(r0 ))] · dBs (r0 , r0 ) +

Z

t

0



0
= X00 (r0 ) +
ds G(r0 , s, Xs0 (r0 )) + EZ 0 [B(r, Xs0 (r0 ), Z(·)
(r0 ))]
0
Z t
0
0
+ EZ 0 [H(r, Xs (r0 ), Z(·) (r0 ))] +
dWs (r0 )g(r0 , s, Xs0 (r0 ))
0
Z t 

L
0
0
= X0 (r0 ) +
ds G(r0 , s, Xs0 (r0 )) + EZ [B(r0 , Xs0 (r0 ), Z(·)
(r0 ))]
0
Z t
0
+ EZ 0 [H(r0 , Xs0 (r0 ), Z(·)
(r0 ))] +
dWs (r0 )g(r0 , s, Xs0 (r0 ))
Z

0

= Φ(X 0 )t (r0 )
because of the assumption (6). The sequence of processes Xtn (r) is hence a sequence of
processes whose law is independent of r by an immediate recursion, and so is the limit.
Hence the unique solution is spatially homogeneous in law. Moreover, a fixed point of Φ,
the solution of the mean-field equation satisfies equation (7).
The existence and uniqueness of solutions of equations (7) is more complicated to prove
and necessitates different stochastic calculus arguments. It is outlined in Appendix A.
A similar sufficient condition can be derived for polychronization in law is more intricate
notationally. Sufficient conditions of the type of that of proposition 3 would involve defining
a partition of Γ into different clusters and ensuring that the input received by any neuron in a
particular cluster from the other clusters only depends on the cluster the neuron belongs to.
Conceptually, this does not differ from the full synchronization in law condition, and would
correspond to a new partition of the neural field Γ into subpopulations in which neurons
are interchangeable. This condition is more delicate to write. Pathwise synchronization of
mean-field neural fields, i.e. the actual convergence of the trajectories of the solution of the
network equations for two different neurons is more complicated to ensure in our mean-field
setting. Such a property would for instance involve a stochastic contraction property of
the mean-field equations that would indeed ensure that the distance between two solutions
starting from different initial conditions decreases towards zero. Such properties are not
addressed here.
The present section summarized a few theoretical results and gave a formal condition
for spatially homogeneous solutions to exist. This very formal approach is now used to
investigate a simple neural field model that is amenable to analytic treatment, firing-rates
neuron models. We start by theoretically deriving the reduced equations in section 3 before
turning our attention to their dynamics in section 4.

dWs (r0 )g(r0 , s, Xs0 (r0 ))

Stochastic Neural Fields Dynamics

12

3. Mean-field equations for firing-rate models
We revisit, from our probabilist point of view, the seminal works of Wilson and Cowan
for finite-populations networks [20] and neuronal tissue [21], and the neural fields with propagation delays [46]. In the work of Wilson and Cowan, the authors emphasized the importance of the presence of delays in the dynamics, a feature which has long been overlooked
in the neuroscience community, but that has recently proved essential to shape cortical
activity [5, 7, 47]. In that model, the state of each neuron is described by a scalar quantity representing the voltage of each neuron, assumed to have a linear intrinsic dynamics
Gα (t, x) = −x/θα + Iα (t) (respectively G(r, t, x) = −x/θ(r) + I(r, t) in the spatially extended case), that interact through their mean firing rate which is given by a sigmoidal
transform of the voltage variable. The delayed interactions are written as a sum over all
neurons of a synaptic coefficient only depending on the populations the interacting neurons belong to and a sigmoidal transform of the pre-synaptic neuron (the one sending a
current), bαγ (x, y) = Jαγ Sγ (y) (resp. b(r, r0 , x, y) = J(r, r0 )S(r0 , y)). The functions Sγ (x)
and S(r, x) are assumed uniformly bounded and uniformly Lipschitz-continuous with respect to x. We enrich the model by taking into account intrinsic noise, i.e. that each
neurons integrates a stochastic current driven by a Brownian motion, with a diffusion coefficient gα (t, x) = λα (t) (resp. g(r, t) = λ(r, t)). We also consider that the interconnection
weights are noisy, as done in section 2, and specify the function βαγ (x, y) = σαγ Sγ (y) (resp.
β(r, r0 , x, y) = σ(r, r0 )S(r0 , y)). These models clearly satisfy the assumptions of theorems 1
and 2, and we use these to derive a stochastic version of their equations as the number of
neurons tends to infinity. In order to simplify further our analysis, we will assume that the
delay measures ηαγ (u) and µα,γ (u) (respectively η(r, r0 , u) and µ(r, r0 , u)) are Dirac measures at fixed times ταγ (respectively at τ (r, r0 ), which generally can be considered to be
kr − r0 kΓ /c where c would the transport velocity in the axons).
The equation of the dynamics of neuron i of population α in the network with N neurons
reads, in the case of a finite number of populations,

dV

i,N

(t) =

!
Nγ
P
X
1 X
1 i,N
j,N
V
(t) + Iα (t) +
Jαγ
Sγ (V
(t − ταγ )) dt+
−
θα
Nγ j=1
γ=1
!
Nγ
P
X
X
1
σαγ
λα (t)dWti +
Sγ (V j,N (t − ταγ )) dBtiγ
N
γ
γ=1
j=1

(8)

and in the case of spatially extended networks:
!
Nγ
P (N )
X
X
1
1
dV i,N (t) = −
V i,N (t)+I(rα , t)+
J(rα , rγ )
S(rγ , V j,N (t−τ (rα , rγ ))) dt+
θ(rα )
Nγ j=1
γ=1
!
Nγ
P (N )
X
1 X
i
j,N
Λ(rα , t)dWt +
σ(rα , rγ )
S(rγ , V
(t − τ (rα , rγ ))) dBti (rγ ). (9)
N
γ
γ=1
j=1
Note that we will also consider neural fields composed of different layers (for instance excitatory and inhibitory neurons). The above theoretical analysis will readily extend to that
case.

Stochastic Neural Fields Dynamics

13

In these models, θα (resp. θ(rα )) are the characteristic time of return to rest of neurons
in population α if it is not submitted to any input, and these times are assumed to be
lowerbounded by a strictly positive value θm . The current Iα (t) (resp. I(rα , t)) corresponds
to a deterministic input current fed to the neuron, Jαγ (resp. J(rα , rγ )) is the synaptic
weight of the incoming connections from neurons of population γ to neurons of population
α and σαγ (resp. σ(rα , rγ )) its standard deviation, λα (t) (resp. Λ(rα , t)) the standard
deviation of the external noise, and the functions Sγ (·) (resp. S(rγ , ·)) are smooth sigmoidal
transforms, assumed bounded with bounded derivative.
3.1. Finite-population Wilson and Cowan Equations
As an application of theorem 2, we have:
Proposition 4. Let T > 0 a fixed time. Assume that the initial condition on the network
are chaotic, i.e. independent and identically distributed for neurons in the same population.
The the state V i,N of neuron in population α, solution of equation (8), converges in law as
N goes to infinity towards the process V̄α unique solution of the mean-field equation:

dV̄α (t) = −

1
V̄α (t) + Iα (t) +
θα

P
X



Jαβ E[Sβ (V̄β (t − ταβ ))] dt

β=1

+ λα (t)dWtα +

P
X

σαβ E[Sβ (V̄β (t − ταβ ))]dBtαβ

(10)

β=1

where the processes (W α t) and Btαγ for α, γ ∈ {1 . . . P }2 are independent standard real
Brownian motions, and the propagation of chaos property applies.
Moreover, if V̄ 0 = (V̄α0 )α=1···P ∈ M2 (Cτ ) is a P-dimensional Gaussian process with independent coordinates, the solution of the mean-field equations (10) with initial conditions
V̄ 0 are Gaussian processes for all time, with mean denoted µ(t) = (µα (t))α=1...P and variance v(t) = (vα (t))α=1...P . Let fβ (x, y) denote the expectation of Sβ (U ) for U a Gaussian
random variable of mean x and variance y. We have:

P

X

1



µ̇α (t) = − µα (t) +
Jαβ fβ µβ (t − ταβ ), vβ (t − ταβ ) + Iα (t) α = 1 . . . P


θα
β=1
(11)
P
X


2

2
2
2


σαβ fβ µβ (t − ταβ ), vβ (t − ταβ ) + λα (t) α = 1 . . . P
v̇ (t) = − vα (t) +

 α
θα
β=1

with initial condition µα (t) = E[V̄α0 (t)] and vα (t) = E[(V̄α0 (t) − µα (t))2 ] for t ∈ [−τ, 0]. In
equation (11), the dot denotes the differential with respect to time.
Proof. It is indeed easy to show that the network equations are of the form of equation (1)
with smooth parameters that satisfy the assumptions (H1)- (H4). The first part of the
proposition is a direct application of theorem 1. The P equations (10), which are P implicit
stochastic differential equations, describe the asymptotic behavior of the network. The
unique solution of the mean-field equations (10) with initial condition V 0 can be written in

Stochastic Neural Fields Dynamics

14

an implicit form as:
t

t

Vα (t) = e− θα Vα0 + e− θα

Z

t

s

e θα (Iα (s) +

0
P Z
X

+

β=1

t

0

P
X

Jαβ E[Sβ (Vβ (s − ταβ ))])ds

β=1

e θα σαβ E[Sβ (Vβ (s − ταβ ))])dBsαβ +
s

Z

t


s
e θα λα (s) dWsα . (12)

0

It is clear from this formulation that the righthand side is a Gaussian process as the sum of
a deterministic function and of a sum of stochastic integrals of deterministic functions with
respect to Brownian motions, and hence so is Vα (t)3 . Its distribution is hence characterized
by its mean and covariance functions. The term E[Sβ (Vβ (s − ταβ ))], because of the Gaussian
nature of Vβ , only depends on µβ (s − ταβ ) and vβ (s − ταβ ), and is denoted by fβ (µβ (s −
ταβ ), vβ (s − ταβ )). Taking the expectation of both sides of the equality (12), we obtain the
equation satisfied by the mean of the process µα (t) = E[Vα (t)]:


 
Z t
P
X
t
s
Jαβ fβ (µβ (s − ταβ ), vβ (s − ταβ )) + Iα (s) ds .
µα (t) = e− θα µα (0) +
e θα 
0

β=1

Taking the variance of both sides of the equality (12), we obtain the following equation:


 
Z t
P
X
2t
2s
2
vα (t) = e− θα vα (0) +
σαβ
fβ2 (µβ (s − ταβ ), vβ (s − ταβ )) + λ2α (s) ds .
e θα 
0

β=1

These two formulae are exactly an integral representation of equations (11) which concludes
the proof.
Remark.
• The variance by itself does not characterize the law of the process V , it is necessary to describe
the covariance matrix function Cαβ (t1 , t2 ) for t1 and t2 in +∗ . It is easily shown that for
α 6= β, this correlation is null, and moreover:

R

t +t
− 1θ 2

Cαα (t1 , t2 ) = e

α

v α (0) +

t1 ∧t2

Z

2s

e θα λα (s)2 ds

0
t1 ∧t2

Z
+

2s

e θα
0

P
X

2
σαβ
fβ2 (µβ (s − ταβ ), vβ (s − ταβ )) ds

β=1

R

for t1 , t2 ∈ +∗ , hence only depends on the parameters of the system and on the functions
µβ (t) and vβ (t). The description of the solution given by equations (11) is hence sufficient to
fully characterize the solution of the mean-field equations (10).

3 This property can also be proved by using the classical characterization of the solution of the mean-field
equation as the limit of the iteration of the map Φ as proved in [36]. In that case, similarly to the proof
provided in [39] for non-delayed finite-populations equations, V̄ is defined the limit of a sequence of Gaussian
processes, hence Gaussian itself.

Stochastic Neural Fields Dynamics

15

• In the case where the sigmoidal transforms are of the form Sα (x) = erf(gα x + hα ), a simple
calculation based on a change of variable shows that the functions fα (µα , vα ) involved in the
reduced mean-field equations (11) take the simple form (see [39] for the calculations):
!
gα µ + hα
fα (µ, v) = erf p
.
2v
1 + gα
This simple expression motivates the choice of erf sigmoidal functions for further developments.

We can hence reduce the complex mean-field equation (10) into the simpler system
of coupled delayed differential equations (DDEs) (11), which allows a simple analysis of
the solutions and the dependency of the behavior upon the parameters. Even if we know
that there exists a unique solution to the mean-field equations, and that necessarily the
solution starting from Gaussian chaotic initial condition is Gaussian with mean and standard
deviation satisfying equations (11), it is necessary to show that these latter equations are
well-posed and that they have a unique solution. Let us denote by C the Banach space of
continuous functions mapping [−τ, 0] into E 2P endowed with the topology of the uniform
convergence. Following Hale and Lunel [48], we consider the moment equations (11) as
ordinary differential equations on C. We have the following:
Theorem 5. Under the assumptions that:
• t 7→ Iα (t) and t 7→ λα (t) are continuous,
• λα (t) ≥ λ0 > 0 for any t > 0 and α ∈ {1, · · · , P },
• the sigmoid functions Sγ have bounded derivatives
• The initial conditions on the variance vα0 ∈ C([−τ, 0], R) satisfies vα0 (t) ≥ v0 > 0 for
t ∈ [−τ, 0] and α ∈ {1, · · · , P }.
there exists a unique solution to the moment equations (11) starting from an initial condition
µ0α ∈ C([−τ, 0], R) and vα0 .
Proof. This theorem is a simple application of theorems [48, Thm 2.1 and 2.3]. These
results ensure existence and uniqueness of solutions as soon as the vector field is Lipschitzcontinuous in C and continuous in time. Thanks to uniform lowerbound of the functions λα
def
and of the initial condition on the variances vα0 , we clearly have v(t) ≥ vm = min(v0 , λ20 θm /2)
(we recall that θm is the strictly positive lower bound of the characteristic times θα ). We
have:
Z
Z
2
(z−x)2
1
e−z /2
√
Sγ (z y + x) √
fγ (x, y) = √
Sγ (z)e 2y =
dz.
2πy R
2π
R
Thanks to the properties of the integral term and in particular the fast convergence towards
0 of the exponential term ensuring existence of moments of any order of the Gaussian, it is
straightforward to show that derivative with respect to x and y read:

2 /2
 ∂fγ (x,y) = R S 0 (z √y + x) e−z
√
dz
∂x
R γ
2π
(13)
R
−z 2 /2
√
∂f
(x,y)
 γ
=
z S 0 (z y + x) e √
dz
∂y

R

γ

2 2πy

Stochastic Neural Fields Dynamics

16

which are both uniformly bounded on R × [vm , ∞) thanks to the fact that the sigmoids have
bounded derivatives and that y ≥ v0 , since we have:


 ∂fγ (x,y)  ≤ kSγ0 k∞
 ∂x 
0
2
R
kSγ0 k∞ R ∞ e−z2 /2
kSγ0 k∞
e−z /2
 ∂fγ (x,y)  ≤ kS√γ k∞
∂y
2 v0
R |z| √2π dz = √v0 0 z √2π dz = √2πv0
This property ensures global Lipschitz continuity of the vector field. The differentiability of
the vector field with respect to time readily follows from the differentiability of the input and
standard deviation functions. We are hence is the application domain of theorems [48, Thm
2.1 and 2.3] and we have existence and uniqueness of solutions for the reduced mean-field
equations (11).
Note that the conditions of theorem 5 are not optimal. For instance in the case
Sα (x) = erf(gα x + hα ), we already remarked that we had
!
gα µ + hα
fα (µ, v) = erf p
2v
1 + gα

Remark.

R R

and this function is globally Lipschitz continuous on × + , as one can easily check by computing
exactly the derivatives of this function with respect to µ and v. The condition on the uniform
lowerbound of λα (t) in theorem 5 can hence be omitted when one chooses erf sigmoidal transforms
for instance.

3.2. Spatially extended neural fields
We now consider the infinite-population case of firing-rate neuron models. We assume
that we are a priori given a connectivity map with mean J(r, r0 ) and standard deviation
σ(r, r0 ), a density λ(r) such that the neural populations are drawn independently according
to this law (after renormalization) as the number of neurons increases. Using theorem 2, we
can state the following proposition:
Proposition 6. Let T > 0 a fixed time. The process V i,N for i in population α at location
rα ∈ Γ, solution of equation (9) with chaotic initial conditions, converges in law towards the
process V̄ (rα ) where (V̄t (r)) is the unique solution of the mean-field equation:

Z
1
V̄t (rα ) + I(rα , t) + λ(r0 )dr0 J(rα , r0 )E[S(r0 , V̄t−τ (rα ,r0 ) (r0 ))] dt
θ(rα )
Γ
Z
+ Λ(rα , t)dWt (rα ) + σ(rα , r0 )E[S(r0 , V̄t−τ (rα ,r0 ) (r0 ))]dBt (rα , r0 )λ(r0 )dr0 (14)


dV̄t (rα ) = −

Γ

where the processes (Wt (r)) and (Bt (r, r0 )) are independent cylindrical Brownian motions,
and the propagation of chaos applies.
Moreover, if the initial condition V̄ 0 (r) ∈ M2 (C([−τ, 0]), L2λ (Γ)) is a cylindrical Gaussian process, the solution of the mean-field equations (10) with initial conditions V̄ 0 (r) is
Gaussian for all time. Denoting by µ(r, t) its mean, by v(r, t) its variance, and by f (r, x, y)
the expectation of S(r, U ) for U a Gaussian random variable of mean x and variance y. We

Stochastic Neural Fields Dynamics

17

have:
Z

1
∂µ


(r, t) = −
µ(r, t) + λ(r0 )dr0 J(r, r0 )f (r, µ(r0 , t − τ (r, r0 )), v(r0 , t − τ (r, r0 ))) + I(r, t)


θ(r)
 ∂t
Γ
Z


∂v
2


v(r, t) + λ(r0 )2 dr0 σ(r, r0 )2 f (r, µ(r0 , t − τ (r, r0 )), v(r0 , t − τ (r, r0 )))2 + Λ2 (r, t)
 (r, t) = −
∂t
θ(r)
Γ
(15)


with initial condition µ(r, t) = E V̄t0 (r) and v(r, t) = E[(V̄ 0 (r, t) − µ(r, t))2 ] for t ∈ [−τ, 0]
and r ∈ Γ.
Proof. This proof is essentially identical to that of proposition 4. Indeed, it again appears
clearly that the network equations are of the form of equation (4) with parameters satisfying
the assumption of section 2.2, and hence that theorem 2 applies. The unique solution of the
mean-field equations (10) with initial condition V 0 satisfies the implicit equation:
Z
Z t s
t
t
e θ(r) (I(r, s)+ λ(r0 )dr0 J(r, r0 )E[S(r0 , V̄t−τ (r,r0 ) (r0 ))])ds
V (r, t) = e− θ(r) V00 (r)+e− θ(r)
Γ
0
Z
Z t
Z t

s
s
e θ(r) E[S(r0 , V̄s−τ (r, r0 ) (r0 ))]dBs (r, r0 )λ(r0 )dr0 .
+
e θ(r) Λ(r, s)dWs (r) + σ(r, r0 )
Γ

0

0

(16)
and is hence necessarily Gaussian since the righthand side obviously is. Its mean, denoted
by µ(r, t), and variance v(r, t), satisfy the equations:


Rt s 
t

− θ(r)

µ(r, t) = e
µ(r, 0) + 0 e θ(r) I(r, s)





 

R

0
0
0
0
0
0
0
0

+
λ(r
)dr
J(r,
r
)f
(r
,
µ(r
,
s
−
τ
(r,
r
)),
v(r
,
s
−
τ
(r,
r
)))
ds

Γ

R t 2s 
2t


v(r, t) = e− θ(r) v(r, 0) + 0 e θ(r) Λ(r, s)2





 

R 2 0 0 2

0 2 0
0
0
0
0

+
λ
(r
)dr
σ
(r,
r
)f
(r
,
µ(r
,
s
−
τ
(r,
r
)),
v(r
,
s
−
τ
(r,
r
)))
ds ,

Γ
which concludes the proof.
Remark. Here again, mean and variance are enough to define the law of the stochastic process,

R

since the covariance matrix between V̄ (r, t1 ) and V̄ (r0 , t2 ), C(r, r0 , t1 , t2 ) for t1 and t2 in +∗ and
(r, r0 ) ∈ Γ only depends on these two quantities. Indeed, for r 6= r0 , the covariance is null because
of the cylindrical nature of the Brownian motions involved and of the initial conditions. It is easily
shown that for r = r0 :
Z t1 ∧t2
2s
e θ(r) Λ(r, s)2 ds
v (0) +
0
Z t1 ∧t2
Z
2s
+
e θ(r)
λ(r0 )2 dr0 σ(r, r0 )2 f 2 (r0 , µ(r0 , s − τ (r, r0 )), v(r0 , s − τ (r, r0 )) ds

C(r, r, t1 , t2 ) = e

t +t2
−( 1
) α
θ(r)

0

Γ

The description of the solution given by equations (15) is hence sufficient to fully characterize the
solution of the mean-field equations (14).

Stochastic Neural Fields Dynamics

18

We therefore describe in this formalism the stochastic dynamics of a neural field through
two coupled integro-differential equations, instead of just one in the classical cases: the
standard deviation of the process is now incorporated to the dynamics of the mean activity,
and in the non-noisy case, correspond exactly to more classical neural field equations [46,
49, 50].
Similarly to what was done in the finite-population case, it remains to show that reduced system on the moments (14) is well-posed, to ensure that these equations uniquely
characterize the mean-field equations. This is the case under the following conditions:
Theorem 7. For the sake of simplicity, we assume here that the density function λ(r) is
upperbounded by a constant A. Under the assumptions that:
• J is square integrable with respect to Lebesgue’s measure on Γ2 , i.e. belongs to L2 (Γ2 , R),
• σ 2 is square integrable with respect to Lebesgue’s measure on Γ2 ,
• the external current I(r, t) is a bounded, continuous functions of time taking values in
L2 (Γ, R)

• the external noise Λ2 (r, t) is a continuous functions of time taking values in L2 (Γ, R),
and is uniformly lowerbounded by a strictly positive constant: Λ(r, t)2 ≥ Λ20 > 0 for all
(r, t) ∈ Γ × R+ ,
• The derivative of S(r, x) with respect to its second variable is uniformly bounded,
then for any initial condition µ(r, t) ∈ C([−τ, 0], L2 (Γ, R)) and v(r, t) ∈ C([−τ, 0], L2 (Γ))
uniformly lowerbounded by a quantity v0 > 0, there exists a unique solution to the moments
mean-field equations (14) which moreover belongs to C([−τ, T ], L2 (Γ, R2 )).
Proof. The moment mean-field equations (14) constitute a dynamical system in the Banach
spaces of functions of Γ with values in R2 . It is well known that the space of functions in
def
B = C([−τ, T ], L2 (Γ, R2 )) endowed with the norm:
Z
Z
1/2
2
|ϕ1 (r, s) dr| + |ϕ2 (r, s)|2 dr
k(ϕ1 , ϕ2 )kB = sup
s∈[−τ,T ]

Γ

Γ

is a Banach space. We will show the existence and uniqueness of solutions in this space. We
further define the norm up to time t > 0 of two elements of B by:
Z
Z

2
Dt (ϕ1 , ϕ2 ) = sup
|ϕ1 (r, s) dr| + |ϕ2 (r, s)|2 dr .
s∈[−τ,t]

Γ

Γ

Let us start by ensuring that any possible solution is bounded in this space. We recall
that:
µ(r, t) = e

t
− θ(r)

Z t

 0 
s
E V0 (r) + e θ(r)
I(r, s)
0
Z
 
+ λ(r0 )dr0 J(r, r0 )f (r0 , µ(r0 , s − τ (r, r0 )), v(r0 , s − τ (r, r0 ))) ds



Γ

Stochastic Neural Fields Dynamics

19

and hence we have:

Z
Z t


kµ(·, t)k2L2 (Γ,R) ≤ 3 kE V00 (·) k2L2 (Γ,R) +
dr|
I(r, s) ds|2
Γ

Z

Z

t

Z

0

0

0

ds λ(r )dr J(r, r0 )f (r0 , µ(r0 , s − τ (r, r0 )), v(r0 , s − τ (r, r0 )))|2
+
dr|
0
Γ
 Γ
 0  2
≤ 3 kE V0 (·) kL2 (Γ,R) + T 2 sup kI(·, s)k2L2 (Γ,R)



t∈[0,T ]

t


+ T λ(Γ)
dr
ds λ(r0 )dr0 |J(r, r0 )|2 kf k2∞
Γ
0
Γ


 0  2
≤ 3 kE V0 (·) kL2 (Γ,R) + T 2 sup kI(·, s)k2L2 (Γ,R) + T 2 A λ(Γ)kJ(r, r0 )k2L2 (Γ2 ,R) kf k2∞
Z

Z

Z

t∈[0,T ]

where kf k∞ is the uniform upperbound of f (r, µ, v) in R, which is smaller or equal to the
uniform supremum of the function x 7→ S(r, x) (which exists by assumption). The same
types of calculations allow proving that:


kv(·, t)k2L2 (Γ,R) ≤ 3 kv(·, 0)k2L2 (Γ,R) +T 2 sup kΛ2 (·, s)k2L2 (Γ,R) +T 2 (λ2 )(Γ)A2 kσ 2 (r, r0 )k2L2 (Γ2 ,R) kf k4∞ .
t∈[0,T ]

These bounds do not depend upon time t and hence prove that any solution of the
moment mean-field equations have bounded norms in the space B.
Routine methods for this type of infinite-dimensional systems ensure existence and
uniqueness of solutions as soon as the vector field of the equation is Lipschitz-continuous for
this norm. In our case, similarly to what was done in the proof of theorem 5, it is easy to
show under the assumptions of the theorem that the standard deviation v(r, t) is uniformly
lowerbounded by a minimal strictly positive value vm = min(v0 , Λ20 θm /2), and hence ensuring the global uniform in r Lipschitz-continuity of the function (µ, v) 7→ f (r, µ, v). Let us
define for (µ, v) ∈ B the transformation Φ(µ, v) taking values in B and defined by:


 
Rt s 
R
t
− θ(r)
0
0
0
0
0
0
0
0
θ(r)
µ(0, r) + 0 e
I(r, s) + Γ λ(r )dr J(r, r )f (r , µ(r , s − τ (r, r )), v(r , s − τ (r, r ))) ds
 e



 
 − 2t
R t 2s
R
2
0 2 0 2
0 2 0
0
0
0
0
θ(r)
θ(r)
Λ (r, s) + Γ λ(r ) dr σ (r, r )f (r , µ(r , s − τ (r, r )), v(r , s − τ (r, r ))) ds
e
v(0, r) + 0 e
It is clear that any solution of the moment equations are fixed points of Φ and reciprocally.
Since (B, k · kB ) is a Banach space, showing existence and uniqueness of solutions, i.e. of
fixed points of Φ, amounts showing a contraction property on Φ. First of all, similarly
to what was done to show that any solutions of the moment equations were bounded in
B, it is very easy to show that for any (µ, v) ∈ B, we have Φ(µ, v) ∈ B. We use the
classical iteration method to show existence and uniqueness of fixed point. To this end, we
fix ϕ0 = (µ0 , v 0 ) ∈ B arbitrarily and define the sequence ϕn = (µn , v n )n∈N iteratively by
setting (µn+1 , v n+1 ) = Φ(µn , v n ). We recall that f is Lipschitz-continuous as shown in the
proof of theorem 5, and we denote by L the uniform Lipschitz constant of f (r, x, y) in its
two last variables. The function f 2 (r, x, y) is hence also uniformly Lipschitz-continuous in
its two last variables with the Lipschitz constant 2kf k∞ L. Let us now show that the vector






Stochastic Neural Fields Dynamics

20

field Φ is Lipschitz-continuous on B. Let us fix ϕ1 = (µ1 , v1 ) and ϕ2 = (µ2 , v2 ) two elements
of B. We have:
Z
Z tZ


λ(r0 )dr0 J(r, r0 ) f (r0 , µ1 (r0 , s − τ (r, r0 )), v1 (r0 , s − τ (r, r0 )))
Dt (ϕ1 , ϕ2 ) = sup
dr
s∈[−τ,t]

0

Γ

Γ

 2

− f (r0 , µ2 (r0 , s − τ (r, r0 )), v2 (r0 , s − τ (r, r0 ))) ds
Z
Z t Z


+
λ2 (r0 )dr0 σ 2 (r, r0 ) f 2 (r0 , µ1 (r0 , s − τ (r, r0 )), v1 (r0 , s − τ (r, r0 )))
dr
0
Γ
Γ
 2 

2 0
0
0
0
0
− f (r , µ2 (r , s − τ (r, r )), v2 (r , s − τ (r, r ))) ds
The two terms of the righthand side are treated similarly, let us hence deal with the first
one. We have:
Z
Z t Z


dr
λ(r0 )dr0 J(r, r0 ) f (r0 , µ1 (r0 , s − τ (r, r0 )), v1 (r0 , s − τ (r, r0 )))
Γ

0

Γ

 2

− f (r0 , µ2 (r0 , s − τ (r, r0 )), v2 (r0 , s − τ (r, r0 ))) ds
Z
Z tZ


≤T
dr
 λ(r0 )dr0 J(r, r0 ) f (r0 , µ1 (r0 , s − τ (r, r0 )), v1 (r0 , s − τ (r, r0 )))
Γ
0
Γ
 2

0
− f (r , µ2 (r0 , s − τ (r, r0 )), v2 (r0 , s − τ (r, r0 ))) ds
Z
Z tZ
 Z

0
0
0 2
≤T
dr
λ(r )dr J(r, r )
λ(r0 )dr0 f (r0 , µ1 (r0 , s − τ (r, r0 )), v1 (r0 , s − τ (r, r0 )))
Γ
Γ
0
Γ
2 
0
0
0
0
0
− f (r , µ2 (r , s − τ (r, r )), v2 (r , s − τ (r, r )))
ds

Z
Z tZ
 Z
≤ 2 L2 T
dr
λ(r0 )dr0 J(r, r0 )2
λ(r0 )dr0 |µ1 (r0 , s − τ (r, r0 )) − µ2 (r0 , s − τ (r, r0 ))|2
Γ
Γ
0
Γ

0
0
0
0
2
+ |v1 (r , s − τ (r, r )) − v2 (r , s − τ (r, r )))| ds
Z tZ

Z

dr0 J(r, r0 )2 kϕ1 (·, s − τ (r, r0 )) − ϕ2 (·, s − τ (r, r0 ))k2L2 (Γ,R2 )
Z t
≤ 2 A2 L2 T 2 kJk2L2 (Γ×Γ)
Ds (ϕ1 , ϕ2 ) ds
≤ 2 A2 L2 T

dr

Γ

0

Γ

0

Similarly, the second term is upperbounded by 8 A4 L2 kf k2∞ T 2 kσ 2 k2L2 (Γ×Γ)
These two bounds do not depend upon time, hence we have:
Z t
Dt (ϕ1 , ϕ2 ) ≤ K 0
Ds (ϕ1 , ϕ2 ) ds

Rt
0

Ds (ϕ1 , ϕ2 ) ds.

0
0

2

2

with K = 2 A L T

2

(kJk2L2 (Γ×Γ) + 4 A2 kσ 2 k2L2 (Γ×Γ) ).
Dt (ϕn , ϕn−1 ) ≤

Routine methods allow showing that:

(K 0 t)n
DT (ϕ1 , ϕ0 )
n!

Stochastic Neural Fields Dynamics

21

p
and hence kϕn − ϕn−1 kB ≤ (K 0 t)n /n!kϕ1 − ϕ0 kB , readily implying that ϕn is a Cauchy
sequence in the complete space B and hence converging in B towards a fixed point of Φ.
Uniqueness of the solution is also classically deduced from the above inequality.
The well-posedness of the moment equations allows studying the solutions of the stochastic mean-field equations through the analysis of the dynamical system (15).
4. Analysis of the solutions
Now that mean-field equations for firing-rate models were derived, we numerically and
analytically analyze their solutions. We investigate separately the effect of delays on the
solutions in a finite-population network, and spatially extended neural fields equations with
or without delays. The specific effects of propagation delays in neural fields is not analyzed
here.
4.1. Finite-populations networks
We analyze in this section the effect of delays in the dynamics of finite-populations
neural fields. To fix ideas, we start by a numerical analysis of a simple 2-populations model
composed of one excitatory and one inhibitory population, with synaptic weights:


15 −12
J=
,
(17)
16 −5
and constant delays τ . The time constants θ1 and θ2 are set to 1, and the noise on the
synaptic connectivity σαγ are all null. The currents are fixed to I1 = 0, I2 = −3, and both
populations have a common noise intensity λ. The bifurcation diagram of the system (11)
as a function of the noise intensity λ and the delay τ is produced in Figure 2. Two branches
of fixed points are identified, and as delays are increased, one of the branches of fixed
points undergoes a cascade of Hopf bifurcations. The different curves of Hopf bifurcations
accumulate on the first curve as delays are increased, on a parameter region where the related
fixed point is unstable (see Figure 2(e)). These Hopf are not related to eigenvalues having the
largest real part, and hence has no effect on the number or stability of fixed point. These Hopf
bifurcations are associated with unstable limit cycles, the accumulation of which produce a
complex landscape resulting in very irregular transient behaviors. When fixing a value for
the delays to τ = 0.5 (green line of figure 2(a)), we observe in this diagram different ranges of
parameter values corresponding to different asymptotic behaviors: stationary solution (blue
region), bistability between a stationary and a periodic solution (yellow region), and periodic
solutions (orange region, see Fig. 2(e)). The oscillatory region corresponds to a precise
synchronization of all neurons (see figure 2(f)), a highly relevant phenomenon in neural
systems sometimes related to pathologies such as epilepsy. Fixing τ = 5 corresponds to a
case where the system displays seven Hopf bifurcations. The codimension two bifurcation
diagram of the system as a function of the input current I1 and the noise level λ is given
in Figure 2(c). We observe that the saddle-node bifurcation forms a cusp, and on one
of the branch of the codimension two saddle-node bifurcation curve appears a BogdanovTakens and two degenerate Bodganov-Takens bifurcations. These degenerate bifurcations
correspond to the tangential pairing of two Hopf bifurcations merging with the saddlenode manifold. These are non-generic bifurcation but appear structurally stable in these
equations. The precise study of the unfolding of these bifurcations is not in the scope

Stochastic Neural Fields Dynamics

22

of the present paper, but the unfolding of that bifurcations would be helpful to further
understand the local behavior of the system around these singular points. An interesting
feature of the system is that delays create complex phenomena only in a restricted region
of parameters corresponding to small noise values: for noise large enough, no instability
is found, illustrating quantitatively the stabilization effect of the noise. It also illustrates
the strong impact of noise in shaping the qualitative form of the solution of the mean-field
equations: for small noise, a stationary behavior is found, for intermediate values a periodic
orbit is found corresponding to the synchronization of different neurons in the network (see
Figure 2(f)) and for larger values of noise, a stationary behavior is again obtained.
The previous example was particularly interesting because noise induced the presence of
regular oscillations. However, its particular dynamics and parameters do not allow further
mathematical investigation because the characterization of fixed point was not analytically
explicit and hence it was hard to characterize the local bifurcations. In order to analytically
investigate this kind of system and in particular account for the cascade of Hopf bifurcations
numerically exhibited in the previous example, we now simplify the problem by considering
a similar neural network model with connectivity matrix:


1 −1
J=
,
1 1
and input currents I1 = 0 and I2 = −1. In this case, µ1 = µ2 = 0, v1 = v2 = λ2 /2 is a
fixed point of the equation. The characteristic matrix governing the linear stability of this
solution reads:
g
Je−ζτ ,
A(ζ) = −(ζ + 1)Id + p
2π(1 + g 2 λ2 /2)
whose eigenvalues (the caracteristic roots) are:
ν± = −(ζ + 1) + p

g
2π(1 +

g 2 λ2 /2)

e−ζτ (1 ± i)

with i2 = −1, and the characteristic equation, defined as ∆(ζ) = det(A(ζ)), vanishes for
values of ζ such that at least one of the characteristic roots vanishes, i.e. for ζ such that
−(ζ + 1) + p

g
2π(1 + g 2 λ2 /2)

e−ζτ (1 ± i) = 0

(18)

This equation can be solved using the complex branches of Lambert’s (Wk )k∈Z functions
(see e.g. [53]). Simple algebra yields the following formula for the characteristic roots of the
system:
!
1
g
k
τ
ζ± = −1 + Wk p
τ e (1 ± i) .
(19)
τ
2π(1 + g 2 λ2 /2)
The stability of the solution 0 is governed by the sign of the real part of the uppermost
eigenvalue. There can be an infinite number of characteristic roots of the equations, but for
any real ν0 there exists a finite number such that <(ζ) > ν0 . The eigenvalue with largest
real part is given by the real branch W0 , and if the argument has a real part greater than
−e−1 the root is unique. If this is not the case, two eigenvalues have the same real part,
namely the ones corresponding to k = 0 or k = −1.

Stochastic Neural Fields Dynamics

23

I1
3

70

10

2

60

8

50

6

40

1

0

30

4

-1

20

2
-2

10

0

0

1

2

3

4

5

6

(a) Codimension 2 bifurcations in
(λ, τ )

0
0

-3

1

2

3

4

5

(b) 3 first Hopf bifurcations,
large delays

SN

6

0

1

2

3

4

5

6

(c) Codimension 2 bifurcations
in (I1 , λ), τ = 5

10

2
8

SH

1

4

6

0

2

4

H

2

-1
0

0

-2
ï2
-2

-3

ï4
ï6

-4

-4

0
0

50

100

150

t

1

2

3

4

5

200

(d) solution, t 7→ µ1 (t) for τ =
10, λ = 0.1

(e) Codimension 1 bifurcation
diagram, τ = 0.5

ï8

0

5

10

15

20

25

30

35

40

(f) 100 trajectories for the network equations, λ = 1.5, τ = 0.5

Figure 2: Dynamics and bifurcations of the mean-field equations (11). (a): Codimension 2 bifurcation
diagram as a function of the noise intensity λ and the delay τ : saddle-node bifurcations (blue line) and
cascade of Hopf bifurcation (pink curves) that all have a common vertical asymptote (b). (c): Codimension
2 bifurcation diagram as a function of I1 and λ for τ = 0.5 (green line in diagram (a)): 2 degenerate
bifurcations appear, corresponding to the tangential merging of two Hopf with the saddle-node bifurcation,
and one Bogdanov-Taken bifurcation. (d) Codimension 1 bifurcation diagram as a function of the noise
intensity λ and I1 = 0, segmented into three zones: blue for stationary solution, yellow for bistability
between a stationary/ periodic solutions, and orange: periodic behavior. In the periodic region, all neurons
in the network are synchronized (2(f): blue (resp. red): 50 trajectories from neurons of the excitatory
(resp. inhibitory) population 1 (resp. 2)). The diagrams were obtained using DDE-BIFTOOL [51, 52] and
a specific code for the network equations.

Stochastic Neural Fields Dynamics

24

We observe that the argument of the Lambert function in the expression (19) has a
modulus that decreases towards 0 as λ or τ are increased towards infinity. For fixed values
of τ , the rightmost eigenvalue is given by k = 0, we hence observe that as λ is increased, the
value of W0 decreases towards 0, and there exists a value λ(τ ) such that for any λ > λ∗ (τ ) the
fixed point 0 is stable: noise has a stabilizing effect on this fixed point. This stabilization
appears through a Hopf bifurcation, and periodic behaviors are found for λ < λ∗ (τ ), as
shown in the bifurcation diagram 3(e) for a fixed value of the delays, τ = 0.5. For fixed
values of λ, as τ is increased, the real parts of the eigenvalues increase and might switch
from positive to negative (see figure 3(a)). As delays are increased, a growing number of
eigenvalues have positive real part, which accounts for the more complex phenomena of
creation and destruction of limit cycles and for the increasing number of fixed points as a
function of λ and τ .
Let us now investigate more thoroughly the location of possible Hopf bifurcations. These
are necessarily points where the real part of the characteristic roots vanishes. Following [48,
54], we use the fact that a necessary condition for the existence of a Hopf bifurcation is
the existence of a purely imaginary characteristic root ζ = iω. This condition, plugged into
formula (18) yields the condition:
−(iω + 1) = − p

g
2π(1 + g 2 λ2 /2)

e−iωτ (1 ± i)

(20)

which, taking the squared modulus of these imaginary numbers, give the equality:
1 + ω2 =
i.e.
ω2 =

g2
π(1 + g 2 λ2 /2)

−π + g 2 (1 − πλ2 /2)
.
π(1 + g 2 λ2 /2)

(21)

This quantity is necessary positive, hence Hopf bifurcations only arise for values of the noise
intensity satisfying the inequality:


1
1
2
∗ 2 def
λ ≤ (λ ) = 2
−
.
π g2
This inequality precisely corresponds to the vertical asymptote: for any value of the noise
intensity greater than λ∗ , no Hopf bifurcation is possible in the system. This property
indicates that necessarily, for noise intensities greater than λ∗ , the fixed point (0, λ2 /2) is
stable. Indeed, we already mentioned that for large noise values that fixed point is stable. It
can gain stability only if the real part of one of the characteristic roots crosses the imaginary
axis, corresponding either to purely imaginary roots or null roots, and the previous inequality
showed that such roots do not exist for parameters λ > λ∗ .
Under the condition λ < λ∗ , we can perfectly characterized Hopf bifurcations by equating
the argument of both sides of equality (20). We obtain that way:
τ=

− arctan(ω) ±
ω

π
4

+ 2 kπ

for k ∈ Z. This relationship can be written in closed form as a function of the parameters
using the expression of ω obtained in equation (21). The different curves of Hopf bifurcations

Stochastic Neural Fields Dynamics

(a) x 7→ <(W0 (x(1 + i)))

25

(b) Hopf bifurcations

(c) Accumulation

0.6
0.8

0.4
0.6

0.2

0.4

H

0.2

0
0

-0.2

-0.2

-0.4

-0.4
-0.6

-0.6

-0.8
0

50

100

150

200

0

(d) Chaotic transient, τ = 2

0.5

1

1.5

2

2.5

(e) τ = 0.5: stabilization by noise

Figure 3: Analysis of the characteristic roots of the delayed neural field equations related to the equilibrium
point µ1 = µ2 = 0 and v1 = v2 = λ2 /2. Results are similar to those of Fig. 2: as delays are increased,
several Hopf bifurcations arise and accumulate around the same value. (a): Shape of the Lambert function
x 7→ <(W0 (x(1+i))), (b): cascade of Hopf bifurcations and (c): locus of the Hopf bifurcations for the 15 first
characteristic root continuated for very large values of τ (d): Transient regime for τ = 5 and (e): Bifurcation
diagram as a function of λ for τ = 0.5.

are plotted in Figure 3. We observe the exact same phenomenon of cascade, accumulation
and pairing of Hopf bifurcations as delays are increased. The accumulation is related to the
fact that the value of λ related to a Hopf bifurcation increases as a function of τ , and that
the value of λ is upperbounded. This case again is characterized, for large values of τ , by the
presence of irregular transient behaviors corresponding to the very complex landscape of the
phase plane, as displayed in Fig. 3(d). We chose for instance to display the transient solution
for τ = 4, a case where the delays are small enough so that we can resolve the presence
of different limit cycles trapping the solution transiently. This case hence makes explicit
the dependence on noise levels of qualitative behaviors of the system for finite populations
networks. We now extend the study to infinite populations networks in a neural field setting.
4.2. Neural Fields Dynamics
We now turn our attention to the study of the dynamics of the neural field equations
derived in section 3.2. We start by an analytic study of the dynamics of a single layer
neural field with delays with a particular focus on Turing-Hopf bifurcations from spatially

Stochastic Neural Fields Dynamics

26

homogeneous states, before addressing numerically the same questions in the more complex
and more relevant dynamics of a two-layers neural field composed of an excitatory and an
inhibitory layers.
4.2.1. A single layer neural field: Turing-Hopf instabilities and noise
In this section, we consider the case of a single-layer neural field, whose mean and
standard deviation satisfy equations (15). In order to analyze the dynamics of the solutions,
we chose a particular set of parameters that allow analytical studies. We consider a onedimensional periodic neural field on Γ = S1 distributed homogeneously (i.e. λ(x) = 1), with
connectivity functions J(r, r0 ) and σ(r, r0 ) only depending on |r − r0 |, θ(r) constant, and
denote with a slight abuse of notation θ the constant value of the function θ(r), J(r, r0 ) =
R1
J(r − r0 ) (resp. σ(r, r0 ) = σ(r − r0 )). We denote by J (resp. σ̃ 2 ) the integral 0 J(r, r0 )dr0
R1
(resp. 0 σ 2 (r, r0 )dr0 ) assumed finite (these obviously do not depend on r). Taking S(r, x) =
p
erf(gx), the function f (r, x, y) is equal, as already stated, to erf(gx/ 1 + g 2 y). Hence
def

F0 = f (r, 0, v) does not depend on v. Let us further set I = −J F0 . In that case, µ(r, t) ≡ 0
for any (r, t) ∈ Γ × R+ is a solution of the mean equation whatever the standard deviation
v is, and this variable v is solution of the equation:
∂v
2
(r, t) = − v(r, t) + σ̃ 2 F02 + Λ2 (r, t).
∂t
θ
Let us further assume that Λ(r, t) is constant. Then the spatially homogeneous, constant in
time function:


θ
def
(µ(r, t), v(r, t)) ≡ 0, (σ̃F02 + Λ2 ) = (µ0 , v0 )
2
is solution of the neural field equations.
It is clear that since the domain is periodic and both the connectivity and the delay
functions only depend on |r − r0 |, the related neural field satisfies the full synchronization
property of proposition 3. The spatially homogeneous state (or synchronized state) satisfies
the equations:
(
R1
dµ
µ
J(r)f (µ(t − τ (r)), v(t − τ (r))) dr + I
dt = − θ + R
0
1 2
2v
dv
2
2
dt = − θ + 0 σ (r)f (µ(t − τ (r)), v(t − τ (r))) dr + Λ
which is a delayed differential equation that is formally equivalent to a differential equation
with delays distributed on the interval [minr τ (r), maxr τ (r)] with the density J(r). The
spatially homogeneous equilibria (µ̄, v̄) are hence solution of the equations:
(
˜ (µ̄, v̄) + I
=0
− µ̄θ + Jf
2 v̄
2 2
2
− θ + σ̃ f (µ̄, v̄) + Λ = 0
and are obviously the same equilibria as for the non-delayed spatially homogeneous equations:
(
dµ
µ
˜
dt = − θ + Jf (µ, v) + I
dv
2v
2 2
2
dt = − θ + σ̃ f (µ, v) + Λ
The bifurcation diagram of this non-delayed system as a function of thepslope of the noise
Λ is produced in Figure 4. If the slope g is small enough (precisely g < 2π/J ), the fixed

Stochastic Neural Fields Dynamics

27

point (µ0 , v0 ) is the unique fixed point and is always stable whatever the noise parameters
are. For larger values of g (in Fig. 4 we chose g = 3), the system presents two distinct regimes
as a function of the noise parameters: for small values of the parameters σ and Λ, the system
presents three fixed points, (µ0 , v0 ) that is unstable and two distinct fixed points denoted
(µ1 , v1 ) and (µ2 , v2 ) which are stable, and that hence constitute spatially homogeneous and
constant in time solutions of the mean-field equations. The system undergoes a pitchfork
bifurcation on the manifold defined by σ̃ 2 F02 + Λ2 = −1/g 2 + J /2π, and on this line the
two fixed points (µ1 , v1 ) and (µ2 , v2 ) collapse with (µ0 , v0 ) and disappear. For σ and Λ such
that σ̃ 2 F02 + Λ2 > −1/g 2 + J /2π system is left with a unique equilibrium, (µ0 , v0 ), which is
now stable. Hence again appears a stabilization effect of noise on the spatially homogeneous
fixed point (µ0 , v0 ).
Let us now return to the full spatially extended system with delays, and address the
problem of pattern formation in these equations beyond a Turing instability at the spatially homogeneous steady state (µ0 , v0 ). To this purpose, we analyze the linear stability of
this fixed point (see e.g. [55, 56, 46]), which amounts characterizing p
the eigenvalues of the
linearized equations around this fixed point. Since f (x, y) = erf(gx/ 1 + g 2 y), we clearly
have:

∂f
g
1 def 0

 ∂x |(µ0 ,v0 ) = √ 2 √2π = F0
1+g v0


 ∂f |(µ0 ,v0 ) = √
∂y

2
µ2
0g

−
−g 3 µ0
e 1+g2 v0
π(1+g 2 v0 )3

=0

and hence the linearized equations around (µ0 , v0 ) read:
(
R
1
∂A
0
0
− r)A(r0 , t − τ (r0 − r)) dr0
∂t (r, t) = − θ A(r, t) + F0 Γ J(r
R 2 0
.
∂B
2
0
0
0
0
∂t (r, t) = − θ B(r, t) + 2F0 F0 Γ σ (r − r)A(r , t − τ (r − r)) dr
We recall that the integral operators are convolutions on S1 , these can hence be diagonalized on the Fourier basis. Let us consider perturbations of the equilibrium of the form
Aν,k (r, t) = <(eν t+2π k r ) with ν = iω + l, and leave B(r, t) unspecified. Let us denote by
(ak (ν)) and (bk (ν)) the Fourier coefficients of the functions J(r)e−ντ (r) and σ(r)e−ντ (r) :
(
R
0
0
ak (ν) = Γ J(r0 )e−ν τ (r ) e−2iπk r dr0
R 2 0 −ν τ (r0 ) −2iπk r0 0 .
bk (ν) = Γ σ (r )e
e
dr
The functions Aν,k are eigenfunctions for the first equation the linearized system provided
that ν satisfies the relationship:
1
def
ν = νk = − + F00 ak ,
θ
defining a dispersion relationship. The characteristic roots of the linearized equations is
hence composed of the eigenvalues of the matrix:
 1

− θ + F00 ak 0
2F0 F00 bk
−2/θ
which are exactly {−2/θ, −1/θ + F00 ak , k ∈ N}. In particular, this shows that no instability
can occur on the standard deviation equation, and the whole stability of the homogeneous

Stochastic Neural Fields Dynamics

28

fixed point only depends on the Fourier coefficients of the deterministic connectivity function
multiplied by the exponential of the delay function, J(r)e−τ (r) , and on the quantity F00 which
depends on the different parameters of the system. Explicitly, in the original parameters, the
spectrum is hence composed of the eigenvalues {− θ2 , − θ1 + ak √ g 2 , k ∈ N} with v0 =
2π(1+g v0 )

Let aM be the largest Fourier coefficient of J: aM = maxk∈N ak . By our integrability assumption on J, this maximum necessarily exists because of Parcheval-Plancherel
√
theorem. The solution (0, v0 ) is hence linearly stable as soon as −1/θ + aM g/ 2π < 0.
This particular form of the spectrum of the linearized operator quantifies directly a
stabilization effect of√the noise already mentioned. Indeed, let us assume that θ and g are
fixed. If −1/θ + αg/ 2π < 0, then all the eigenvalues are negative whatever v0 and hence
the solution (0, v0 ) is stable
whatever the noise connectivity matrix σ and the additive noise
√
Λ. If now −1/θ + αg/ 2π > 0, then for Λ and σ small, the fixed point (0, v0 ) is unstable.
When σ or Λ are increase, the fixed point will gain stability, since the maximal eigenvalue
tends to −1/θ when v0 goes to infinity4 .
An instability occurs when the characteristic roots such that ν has a positive real part.
A Turing bifurcation point is defined by the fact that there exists an integer k such that
<(νk ) = 0. It is said to be static if at this point =(νk ) = 0, and dynamic if =(νk ) = ωk 6= 0.
In that latter case, the instability is called Turing-Hopf bifurcation, and generates a global
pattern with wavenumber k moving coherently at speed ωk /k as a periodic wavetrain. If
the maximum of λk is reached for k = 0, another homogeneous is excited.
In order to investigate the presence of Turing-Hopf instabilities, we specify further our
system. We choose an exponential connectivity function J(r) = e−|r|/s for some s > 0,
and a delay taking into account both propagation (or axonal) delays corresponding to the
transport of the information along the axons at a constant finite speed c, and constant delays
accounting for the transmission time of the spike through the synaptic machinery. In that
case,
|r|
τ (r) =
+ τd .
c
The coefficient ak (ν) hence read:
θ
2
2
2 (σ̃F0 + Λ ).

1

ν

e−ντd (1 − e−( s + c ) )
1
ν
s + c + i2πk
The related characteristic equation reads:
1

ν+

ν

1
e−ντd (1 − e−( s + c ) )
= F00
1
ν
θ
s + c + i2πk

These equations are relatively complex to solve analytically in that general form (see e.g. [57]).
However, when considering purely synaptic delay case (corresponding formally to c = ∞,
i.e. disregarding the transport phenomenon), a similar analysis as the one performed in
the previous section allows computing in closed form the curves of Turing-Hopf instabilities.
4 The stabilization effect already mentioned in previous examples comes from the fact that noise tends to
make the sigmoidal transform less sharp,
p which explicitly appears in our equations, since the slope of the
effective sigmoidal transform reads g/ (2π(1 + g 2 v0 )) which is always smaller than the slope of the sigmoid
√
that intervenes in the deterministic case, g/ 2π, and which decreases as noise is increased.

Stochastic Neural Fields Dynamics

29

1.5

0

0

1

0.5

0.5

-0.5

-0.5

1

1

One stable fixed point

0.5

Pitchfork
Bifurcations

Three fixed points

0

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

300

(b) σ ≡ 0 and Λ ≡ 0.1, IC1

(a) Bifurcations of the synchronized system
0

100

0

1

(c) σ ≡ 1 and Λ ≡ 0.1, IC1

0

1

0.5

0.5

0.5

-0.5

-0.5

-0.5

300

500

(d) σ ≡ 0 and Λ ≡ 0.1, IC2

1

300

(e) σ ≡ 0 and Λ ≡ 0.2, IC2

(f) σ ≡ 0 and Λ ≡ 0.1

Figure 4: Stabilization by noise of the spatially homogeneous state (µ0 , v0 ). (a) shows the bifurcation
diagram as a function of σ and Λ of the fully synchronized state, (b): for initial conditions IC1 given by (22)
and small noise levels, the system stabilizes on a non-spatially homogeneous state (mode k = 1) but for
larger values of Λ (not shown) or σ (c) the state (µ0 , v0 ) is stabilized and attractive. (d): same thing for
initial conditions IC2 given by (23) shows mode k = 2 excited, and as Λ is increased (e), this state looses
stability in favor of the mode k = 1 before (µ0 , v0 ) is stabilized. (f) shows the instability of the mode related
to k = 4.
F 02 (1−e−1/s )2

0
Dynamical instabilities occur for parameters such that: 1/s
>
2 +4π 2 k 2
us denote by ωk the quantity:
s
1
F002 (1 − e− s )2
1
ωk =
− 2
1
2 k2
θ
+
4π
s2

1
θ2 .

In that case, let

the instability arises for parameters such that there exists m ∈ Z for which is valid the
relationship:

1 
τdk =
− arctan(θωk ) − arctan(2πks) + 2πm .
ωk
Let us now numerically explore the bifurcations of this system. We aim at exhibiting
different solutions that are not spatially homogeneous, corresponding to different wavenumbers, and to this end specify particular initial conditions. We have seen that for small
values of the noise parameters and large values of the slope g, the fully synchronized system
presented two different stable equilibria that we denoted (µ1 , v1 ) and (µ2 , v2 ). If all initial
conditions belong to the attraction bassin of the same stable fixed point, or are equal to zero,
then the only mode to be excited corresponds to k = 0, and the neural field stabilizes on a

Stochastic Neural Fields Dynamics

30

constant mode. If we now partition Γ into different connected zones and set up the initial
condition so that neurons belonging to different subsets of Γ belong to the attraction bassin
of the two different stable fixed points, in a balanced manner, higher modes are excited. As
examples, setting the initial condition to:


r ∈ [0, 0.25], t ∈ [−τ, 0]
1
µ(r, t) = −1 r ∈ [0.75, 1], t ∈ [−τ, 0]
(22)


0
overwise
we excite a non-constant mode k
condition to:


1
µ(r, t) = −1


0

= 1 as illustrated in Fig. 4(b), and setting the initial
r ∈ [0, 0.05] ∪ [0.5, 0.55], t ∈ [−τ, 0]
r ∈ [0.25, 0.3] ∪ [0.75, 0.8], t ∈ [−τ, 0]
overwise

(23)

we excite the mode k = 2, as illustrated in Fig. 4(d). In these plots, the abcissa represents
the location on the space Γ and the ordinate corresponds to time. As noise is increased,
this mode looses stability in favor of the mode k = 1 first, before this mode looses again
stability in favor of the spatially homogeneous solution (µ0 , v0 ) as expected from the analysis
of the stability of that fixed point. Higher modes prove relatively unstable, and illustrated
in Fig. 4(f) for an initial condition corresponding to a wavenumber k = 4. After a short
transient, the system stabilizes on a stationary solution corresponding to the mode k = 1.
For axonal delays, the characteristic equation obtained is very complicated to solve analytically. These are simplified when considering wizard hat connectivity kernels, as shown
in [46]. We can however obtain complex closed form expressions of the Turing-Hopf bifurcation curves using [57]. These are not made explicit here, and simulations performed using
an adaptation of the the code provided by Venkov in [58] yield very similar phenomena (not
shown).
The dynamic Turing bifurcations in the particular one-layer case treated, chosen for
simplifying analytical exploration, presented a limited set of spatio-temporal behaviors: we
observed that only stationary solutions are found, either spatially homogeneous or characterized by a few typical modes. In particular, no wave or oscillatory activity was observed,
because of our particular choice of connectivity function which was chosen of constant sign
(here, positive) for biological reasons. In order to go beyond these behaviors, we now turn
to the numerical study of a more complex neural field system composed of two layers in
order to include both excitation and inhibition phenomena. We will observe in these cases
complex irregular dynamic Turing patterns are observed in a particular range of noise levels.
4.2.2. Dynamic Turing patterns in a two layers neural field with periodic boundary conditions
We now consider a two layers neural field composed of an excitatory (labeled by the index
1) and an inhibitory (labeled by an index 2) populations distributed over the space Γ = S1
or Γ = [0, 1]. Both layer produce currents sent to both the excitatory and the inhibitory
layers, and the interconnection strength depends on the (functional or anatomical) distance
between themselves in Γ (see e.g. [59]). We consider here constant sign connectivities with
0
an exponential shape, with a width depending on the population: Ja (r, r0 ) = e−|r−r |/sa /sa
for a = 1 or 2. The choice of the typical spatial extension of the kernel depends on type of
modeling chosen. For instance, if one considers that Γ is a functional space (i.e. each r ∈ Γ

Stochastic Neural Fields Dynamics

31

corresponds to a function of the population, for instance the preferred orientation of the
column), then local interactions are dominated by excitation and the connection between
two very distinct functions (e.g. orientations) are dominated by inhibition, which motivates
a choice s1 > s2 . If Γ models the spatial location of each neuron, it is known that inhibitory
connections are generally characterized by shorter axons and excitatory synapses project
their synapses further, which would motivate a choice of typical sizes such that s2 > s1 .
In the present section, we consider periodic boundary condition. This choice hence is
close from functional neural fields settings, and for instance classically models orientation
preference. The neural field can be seen as defined on the torus S1 . Two other types of
connectivity will be dealt with in Appendix B: (i)reflective boundary conditions in which
the solution is virtually evenly continued at the boundaries 0 and 1 and the convolution is
done on R instead of [0, 1] and (ii) zero boundary conditions where the convolution only
occurs on [0, 1] (which would correspond to a convolution on R with a null continuation
outside the interval [0, 1]).
We define the type function of a neuron (excitatory or inhibitory) by the function
ν(i) ∈ {1, 2}. The connectivity kernels Jν are multiplied by a typical connectivity coefficient between the different populations, wνν 0 for the deterministic interactions, chosen,
consistently with the previous finite-population example, equal to:


15 −12
w=
.
16 −5
Noisy interaction σν,ν 0 (r, r0 ) are considered equal to a coefficient σν,ν 0 multiplied by the
exponential kernels Jν 0 (r, r0 ), and the typical time constants θν (r) of all neurons is considered
equal to 1. In a network composed of Nγν neurons of type ν in the population located at
rγ ∈ Γ, the equation of neuron i of type ν(i) = a ∈ {1, 2}, in population α at location rα
reads:
dV i (t) =

P (N ) 2
X X

−V i (t)+Ia (rα , t)+

γ=1

+

P (N ) 2
X X
γ=1

1
N
γν
ν=1

1
N
γν
ν=1

!
X

waν Jν (rα , rγ ) S(rγ , V j (t−τ (rα , rγ ))) dt

j, p(j)=γ,ν(j)=ν

!
X

σaν Jν (rα , rγ ) S(rγ , V (t−τ (rα , rγ ))) dBtα,a,γ,ν +Λ(rα , t)dWti
j

j, p(j)=γ,ν(j)=ν

A straightforward multi-dimensional extension of theorem 2 and proposition 6 allows proving
that the same propagation of chaos property applies, deriving the stochastic mean-field
equations, proving that the solutions of these are Gaussian, with mean and covariances

Stochastic Neural Fields Dynamics

32

satisfying the coupled equations:

n
R
1

(r, t) = −µ1 (r, t) + Γ λ(r0 )dr0 w11 J1 (r, r0 )f (r, µ1 (r0 , t − τ (r, r0 )), v1 (r0 , t − τ (r, r0 )))
 ∂µ
∂t


o


0
0
0
0
0

+w
J
(r,
r
)f
(r,
µ
(r
,
t
−
τ
(r,
r
)),
v
(r
,
t
−
τ
(r,
r
)))
+ I1 (r, t)

12
2
2
2


n

R

∂µ
0
0
0
0
0
0
0
2


∂t (r, t) = −µ2 (r, t) + Γ λ(r )dr w21 J1 (r, r )f (r, µ1 (r , t − τ (r, ro)), v1 (r , t − τ (r, r )))





+w22 J2 (r, r0 )f (r, µ2 (r0 , t − τ (r, r0 )), v2 (r0 , t − τ (r, r0 ))) + I2 (r, t)
n
R
∂v1
2

(r, t) = −2 v1 (r, t) + Γ λ(r0 )2 dr0 σ11
J12 (r, r0 )f 2 (r, µ1 (r0 , t − τ (r, r0 )), v1 (r0 , t − τ (r, r0 )))

∂t


o


2
2
0 2
0
0
0
0

+σ
J
(r,
r
)f
(r,
µ
(r
,
t
−
τ
(r,
r
)),
v
(r
,
t
−
τ
(r,
r
)))
+ Λ21 (r, t)

2
2
12 2


n

R

∂v2
2

= −2 v2 (r, t) + Γ λ(r0 )2 dr0 σ21
J12 (r, r0 )f 2 (r, µ1 (r0 , t − τ (r, r0 )), v1 (r0 , t − τ (r, r0 )))

∂t (r, t)


o


2

+σ22
J22 (r, r0 )f 2 (r, µ2 (r0 , t − τ (r, r0 )), v2 (r0 , t − τ (r, r0 ))) + Λ22 (r, t)
(24)
We numerically explore the solutions of these equations and the dependence upon noise,
initial datum and boundary conditions on the neural field. We further simplify the problem
by considering that the additive noise parameters Λ1 (r, t) and Λ2 (r, t) are identical and
constant, and denote their common value Λ. Similarly, we assume that σνν 0 are all equal to
a quantity denoted σ. The (space-dependent) delays are neglected in the present study, i.e.
set τ (r, r0 ) ≡ 0. Their specific influence will be analyzed in a separate study.
Since the connectivity matrix is convolutional and the domain is periodic, the system
clearly satisfies the total synchronization condition given in equation (6), and we expect
to find spatially homogeneous solutions when the initial conditions are themselves spatially
homogeneous. Let us denote by Ja the quantity:
Z
Ja =
λ(r0 )dr0 Ja (r, r0 )
Γ

and by J˜a2 :
J˜a2 =

Z

λ(r0 )2 dr0 Ja2 (r, r0 )

Γ

for a ∈ {1, 2}. The infinite dimensional system (24) reduces, when considering its spatially
homogeneous solutions, to the 4-dimensional ordinary differential equation:

µ˙1 = −µ1 + w11 J1 f (µ1 , v1 ) + w12 J2 f (µ2 , v2 ) + I1 (t)




µ˙2 = −µ2 + w21 J1 f (µ1 , v1 ) + w22 J2 f (µ2 , v2 ) + I2 (t)


(25)
2
˜2 f 2 (µ1 , v1 ) + J˜2 f 2 (µ2 , v2 ) + Λ2
v
˙
=
−2
v
+
σ
J

1
1
1
2
1





v˙ = −2 v + σ 2 J˜2 f 2 (µ , v ) + J˜2 f 2 (µ , v ) + Λ2
2

2

1

1

1

2

2

2

2

Functional Connectivity Case. We start by considering a case where s1 > s2 , which is
related as stated to functional connectivity: local interactions are governed by excitation
and distal interconnections by inhibition. We fix s1 = 0.02 and s2 = 0.0125. Figure 5
represents the bifurcation diagram of the fully-synchronized system for I1 = 0 and I2 = −3,
as a function of the additive noise amplitude Λ1 = Λ2 = Λ, and of the constant common
synaptic noise amplitude σ = σνν 0 . The codimension two bifurcation diagram presents a
Hopf, a saddle-node and a saddle-homoclinic bifurcation curves, separating the diagram

Stochastic Neural Fields Dynamics

33

6

H

5

(D)

(A)

(D)

(B)(C)

(A) (B)

3

4

SN

2

SH

SN

(B)
SH
(A)

1

0

(C)

0

2

(a) Codimension 2 diagram

3

H

0

H

-1

-1

-2

-2

-3

-3
-4

-4

1

SH

1

0

2

(D)

SN

2

1

3

(C)

3

0

1

1.6 1.8

2

3

(b) σ = 0.1

4

5

-5
0

2

4

6

8

10

(c) Λ = 0.1

Figure 5: Bifurcation diagram of the fully synchronized state. (a) Codimension 2 bifurcation diagram with
respect to σ and Λ presents three bifurcations manifolds: saddle-homoclinic (SH, green), saddle-node (SN,
blue) and Hopf (H, brown) separating the bifurcation diagram into 4 zones: two stationary (cyan, A and
D), a periodic zone (C, orange) and a bistable zone (B, yellow). Codimension 1 bifurcation diagram as a
function of Λ or σ for fixed values of the other noise parameter are displayed in (b) and (c).

into three qualitatively distinct zones: two stationary zone (A and D, colored in cyan)
where the moment system presents a stable fixed point, i.e. the solution of the meanfield equation is stationary, separated by the saddle-homoclinic bifurcation curve from a
bistable zone (B, yellow) where the moment equations present both a stable fixed point
and a periodic orbit, and a zone where there only exists a periodic orbit (C, orange). The
bistable zone is separated from the periodic behavior zone through a saddle-node bifurcation,
and the periodic behavior zone from the stationary zone (D) through a Hopf bifurcation.
The analysis of this diagram underlines the similar but asymmetrical role of σ and Λ. As
examples we plotted codimension 1 bifurcation diagrams for Λ = 0.1 as a function of σ and
for σ = 0.1 as a function of Λ (black lines in the codimension two diagram). We observe
that for small noise amplitude (either σ or Λ) corresponding to parameter region (A), the
system presents a stable high fixed point and two unstable fixed points, a saddle and a
repulsive fixed point. As this noise parameter is further increased, a cycle appears from
the saddle fixed point through a saddle homoclinic bifurcation, and we enter the bistable
(yellow) region (B). As noise is further increased, the stable and the saddle fixed point merge
and disappear through a saddle-node bifurcation, and the system is left with an unstable
fixed point and a stable periodic orbit (region C). This orbit decreases amplitude disappears
through a Hopf as noise is increased, and at this bifurcation the unstable fixed point gains
stability (region D).
The diagrams presented in Figure 5 characterize the existence and the nature of the synchronized states. As soon as the initial condition is homogeneous, the system will present
solutions that are constant in space, and their time profile is given by the solutions of the
fully-synchronized system. However, these synchronized states might not be stable, and inhomogeneities in the initial condition might lead the system to different states. Bifurcations
from these spatially homogeneous states could be analytically as outlined in the previous
section. However, due to the dimensionality of the system and to the particular coefficients
chosen, the equilibria are complicated to characterize and hence the analytical characterization of Turing-Hopf bifurcations is here intricate. This is why we address that problem
numerically by computing the solutions of the neural-field moment equation for different
values of the parameter Λ or σ, with homogeneous or inhomogeneous initial condition. Re-

Stochastic Neural Fields Dynamics

34

sults are displayed in Figures 6 and 7. In these figures is plotted the mean of the activity of
the excitatory population, µ1 (r, t), as a function of r (abscissa) and t (ordinate).
The behaviors identified through the bifurcation analysis of the synchronized equations
are perfectly recovered in the analysis of the neural field when taking an homogeneous
initial condition, as expected from proposition 3. The stability of these synchronized states
is analyzed through perturbations of the synchronized initial condition. Let us start by
analyzing the effect of the parameter Λ for a fixed value of σ = 0.1. In the stationary
regime (A) corresponding to low noise amplitudes, very intricate phenomena take place that
tend to destabilize the spatially homogeneous solutions. Let us fix for instance the initial
condition of µ1 (r) to 5 for r ∈ [0, 0.05] and 0 otherwise5 . If the noise is small enough,
the initial condition creates bi-directional waves that travel through the neural field. These
waves split into different secondary waves, themselves potentially splitting. All these waves
interact together, and this phenomenon results in highly irregular transient behaviors (see
e.g. Fig. 6(a) for Λ = 0.1). This wave splitting and interaction phenomenon becomes
sustained in time as Λ is further increase. For our parameters, the wave splitting and
interaction phenomenon is sustained for Λ ≥ 1. Figure 6(d) illustrates this behavior for
Λ = 1. From the analysis of the spatio-temporal patterns, we observe the fact that waves
tend to spread increasingly fast across the neural field as noise is increased. The irregular
pattern suddenly turn as noise is further increased into a space-time quasi-periodic waves.
These states, presented in Fig. 6(f), show more regular quasi-periodic spatial patterns that
oscillate quasi-periodically in time. These waves appear not to interfere together, which
explains the increased regularity observed in contrast with the patterns observed for smaller
values of the noise parameter. These are present for increasing values of Λ until the bistable
parameter region (B) is reached.
For noise levels corresponding to the bistable region (B) or to the oscillatory region (C), it
appears that the spatially homogeneous periodic regime is attractive: for non-homogeneous
initial conditions, the system smoothly goes from a non-synchronized irregular activity to
spatially homogeneous time-periodic activity corresponding to the spatially homogeneous
solution of system (25) identified, see Figure 7(a). In the bistable region (B), one may wonder
if similarly to the case of section 4.2.1, the presence of two stable equilibria could excite higher
wavenumber. In order to numerically investigate this question we set up the initial condition
at equilibrium for one part of the neural field and on the limit cycle for the other part. In the
extensive numerical tests performed, we always obtained that the oscillatory regime would
take over and govern the dynamics after very short transients, even if most of the neural
field has its initial condition in the attraction basin of the spatially homogeneous stable fixed
point. The unconditional stability of the spatially homogeneous behavior is also recovered
for values of Λ in the stationary region (D): the spatially homogeneous stationary state is
attractive for the system with inhomogeneous initial conditions and the system converges
towards a constant spatially homogeneous solution (see Fig. 7(c)).
For fixed values of Λ and a value of σ varying, the exact same type of instability of the
spatially homogeneous equilibrium in region (A) and stability in regions (B)-(D) appears
(see Figure 8). Similarly, instabilities in region (A) is characterized by irregular wave interactions. The main difference between the effects of Λ or σ lies in the fact that in the former
case, the standard deviation of the solution of the mean-field equation has a constant value,
5 We observed in our numerical simulations that the qualitative phenomena described do not sensitively
depend on the choice of the initial condition.

Stochastic Neural Fields Dynamics

0

35

1

4
3

2

1

0

-1

-2

-4

-3

-4
0

20

40

60

80

100

100

(a) Λ = 0.1
0

(b) Λ = 0.1

0

1

50

1

50

4

4

4

3

2

1

0

-1

-2

-4

-4
-3

-4
200

50

220

240

260

280

300

320

340

150

(c) Λ = 1, homogeneous IC
0

150

(d) Λ = 1
1

(e) Λ = 1

r

4

t=160

t=190

r

-4

200

r

t

(f) Λ = 1.6

Figure 6: Irregular transient phases for inhomogeneous initial conditions, for parameters in region (A):
µ1 (r, t) solutions of the two-layers mean-field equations is plotted. Spatio-temporal patterns: Abscissa:
location on Γ = [0, 1]. Ordinate: time. Plots (b) and (e) represent µ1 (r = 0.1, t) (black) and µ1 (r = 0.5, t)
(red). Initial conditions are set to µ1 (r, 0) = 5 for r ∈ [0, 0.05] and 0 elsewhere (orange box) µ2 (r, 0) ≡ 0 and
v(r, 0) ≡ 0, except in (c) where we show the solution for spatially homogeneous (null) initial conditions. (f):
Space-time oscillation: Λ = 1.55. Waves no more interfere and the activity forms a non-stationary quasiperiodic structure. Right: plots of the activity for two different times as a function of r ∈ Γ. Animations
of the activity are available in the supplementary material. Figures and animations were obtained using
XPPAut [60].

Stochastic Neural Fields Dynamics

0

r

36

1

r

4

2

1

0

-1

-2

-4
-3

-4
0

20

40

60

80

100

300

t

(a) Λ = 1.7
0

r

(b) Λ = 1.7
1

r

4
3

2

1

0

-1

-4

-2

-3

-4
0

20

40

60

80

100

300

t

(c) Λ = 3

(d) Λ = 3

Figure 7: Spatially homogeneous stable behaviors: Λ belongs to the bistable (B), periodic (C) and stationary
(D) regimes. Same inhomogeneous initial conditions and setting as in Fig. 6. Animations of the activity are
available in the supplementary material.

Stochastic Neural Fields Dynamics

0

r

1

37

0

r

4

4

-4

-4

r

200

t

r

1

r

t

(b) σ = 1.1
1

1

100

(a) σ = 0.2
r

r

-4

t

(c) σ = 1.6
0

r
10

2.5

0

r
4

300

0

1

r

4

8
6

4

2
0

0

-4

−2

−4
−6
100

0

5

10

15

20

25

30

35

40

45

t

50

100

t

(d) σ = 1.6, Variance

(e) Different trajectories

(f) σ = 6

Figure 8: Behaviors of the two-layers mean-field equations for different initial conditions and synaptic noise
amplitude σ. (e): 100 trajectories from the fully-synchronized equations stochastic equations in a network
of 10 000 neurons show a sharp synchronization when the standard deviation reaches zero (blue: excitatory
neurons (population 1) and red: inhibitory neurons (2)).

i.e. each individual trajectory in the network deviates from the mean activity by a Gaussian process with constant standard deviation, whereas in the latter case corresponds the
standard deviation of the solution of the mean-field equations depends on the mean activity
of the neural populations. This phenomenon is particularly visible in the non-stationary
solutions. For instance for σ = 1.6, we observe that the standard deviation reaches very
small values at times close to the onset of an oscillation, as shown in Figure 8(d), resulting
in the fact that all neurons sharply synchronize at these times, a phenomenon particularly
visible in the network simulations plotted in Fig. 8(e) where we plotted 100 trajectories of
a network with 10 000 neurons.
All this analysis illustrate the fact that noise strongly shapes the form of the activity
of large network, governing the nature of spatially homogeneous states, and their stability,
yielding different spatio-temporal patterns that can be either transient or sustained. Irregular behaviors arose exclusively for relatively small values of the noise parameter, which
again illustrates a stabilizing effect of noise in favor of the spatially homogeneous in law
solutions. Very interesting irregular patterns of activity characterized by the irregular formation of waves and their splitting was observed in parameter regions close the presence of
a saddle-homoclinic bifurcation in the fully synchronized system. This same phenomenon
is found for similar systems defined on [0, 1] with different boundary conditions: reflective
or zero. Results are displayed in Appendix B. This formation of complex spatio-temporal
patterns arising from a wave-splitting phenomenon strongly evokes Turing patterns as found
in different reaction diffusion equations in biological mathematics. In particular, these pat-

Stochastic Neural Fields Dynamics

38

terns are similar to those exhibited by [61] and obtained from the analysis of the dynamics
of reaction diffusion equations related to pattern formation on the shells of mollusks, in a
case where the system induces the formation of forward and backward running waves that
interact. These can be analyzed as done in [62] in a case where are identified self-replicating
bumps, compared with dynamics observed dissipative equations such as Ginzburg-Landau’s,
but to our knowledge not observed in neural field equations. This structuring effect of noise
and the presence of the particular type of spatio-temporal patterns observed in that system,
beyond illustrating the fact that noise levels strongly shape the dynamics of neural fields.
Anatomical Connectivities: Bumps, bump-splitting and interfering waves. In this section we
consider the anatomical case where distal interconnections are dominated by excitation and
local interactions by inhibition. In that case, the codimension one bifurcation diagram of
the synchronized system is provided in Figure 9(a) and shows the same global structure.
This diagram characterizes the nature of spatially homogeneous solutions. We investigate
solutions of the mean-field equations corresponding to non-homogeneous initial conditions,
which, as observed in the previous sections, do not necessarily converge towards spatially
homogeneous patterns.
The observations are essentially of the same kind as identified in the functional connectivity case. First of all, the bifurcation diagram of the spatially homogeneous equations
can be segmented, like the previous case, into the same qualitative regions (A)-(D), and it
appeared in that case again that the spatially homogeneous states were unstable only in
the small noise region (A). However, we observed that depending on the ratio r = s1 /s2 ,
different kind of spatio-temporal patterns appeared (see Figure 9). For ratios greater than
r∗ ≈ 0.6, the same phenomenon of wave splitting and interactions as in the functional case
is observed (see Fig. 9(l)).
For r < r∗ , a new type of dynamics appear, characterized by the presence of bumps of
activity, i.e. localized stationary patterns. These are evidenced for initial conditions equal to
zero except on an interval [a, b] ∈ S1 . For Λ = 1, non spatially homogenous solutions appear,
composed of a specific number of bumps that depends on the width of the interval [a, b]. As
noise is increased, these bumps tend to split in two different bumps of the same spatial size
and either stabilize, or split again, a phenomenon strongly evocative of the patterns observed
in a different context by Coombes and Owen in [62]. This sequence of bump splitting can
either stabilize into a stationary pattern composed of several bumps or appear repetitively
depending on the value of the noise parameter. For relatively small noise values (Λ < 1.7),
the activity stabilizes on a stationary pattern which is not spatially homogeneous, and the
number of bumps and their shape depend both on the initial condition chosen and on the
value of the noise. It is interesting to note that we observed that for Λ > 1.5, the stationary
pattern found is spatially periodic, characterized by a specific wavenumber increasing as
noise is increased, and depending on the type of initial condition chosen. For instance, we
show in Figure 9(e) the case of Λ = 1.5 and initial condition zero except on [0, 0.05] where
it is equal to 5 (IC1), where the stationary behavior is characterized a spatially periodic
pattern with wavenumber 8, and for initial condition zero except on [0, 0.05] ∪ [0.1, 0.15]
where it is equal to 5, the same kind of phenomenon appears and stabilizes on a pattern
with wavenumber equal to 9. For Λ = 1.6 and initial conditions IC1, the wave number is
10. For values of Λ larger than 1.7, the system no more stabilizes into stationary patterns
and presents very complex irregular sequences of bump splitting, before presenting irregular
spatio-temporal waves (Λ = 2.2, Fig. 9(i)).

Stochastic Neural Fields Dynamics

39

0

3

0

1

3

3

-3

-3

1

SN
SH

2
1
0

H

-1
-2
-3
-4
0

2

4

6

500

8

500

t

t

(a) Spatially Homogeneous
0

(b) Λ = 1
0

1

(c) Λ = 1.2
1
0

3

3

1

0

5

-3

-3
-5

500

500

t

t

500

(d) Λ = 1.3
0

(e) Λ = 1.5
0

1

(f) Λ = 1.5,IC2
0

1

3

3

3

-3

-3

-3

300

500

t

t

300

t

(g) Λ = 1.6
0

(h) Λ = 1.7
0

1

100

1

(i) Λ = 2.2
0

1

0.5

3

-3

-3

1

0.5

-3

150

200

t

200

t

(j) Λ = 2.3

t

(k) Λ = 2.5

(l) Λ = 1.6,

s1
s2

> r∗

Figure 9: Anatomical connectivity case with µ1 (r, t) represented as a function of r ∈ S1 (abscissa) and t
(ordinate). (a)-(k): s1 /s2 = 0.57 show a sequence of bump splitting as Λ is increased in the parameter
region where stationary spatially homogeneous solutions related to small values of Λ. Orange Box: initial
condition µ1 (r, 0) = 5 for r ∈ [0, 0.05] and 0 otherwise, and blue box: −µ1 (r, 0). (l): s1 /s2 = 0.65: wave
splitting phenomenon, for Λ =

Stochastic Neural Fields Dynamics

40

These bumps disappear in favor of a spatially homogeneous periodic activity when noise
levels reach the bistable region, and this region turns into spatially homogeneous stationary
solutions as noise is further increased, in the case corresponding to regions (B), (C) and (D).
For ratios s1 /s2 greater than r∗ , as stated,s we find phenomena resembling to what was
found the functional connectivity case: for small values of the noise parameter, a wavesplitting and interference phenomenon is found, progressively turning into quasi-periodic
spatio-temporal waves, spatially-homogeneous oscillations and spatially homogeneous stationary solutions as the noise is increased. The wave splitting phenomenon is displayed on
Figure 9(l), other cases resemble to the other patterns found and are not displayed.
5. Discussion
In this article, we addressed the problem of characterizing the dynamics of large assemblies of neurons that are spatially extended and interact after delays. Modeling each
individual neuron and characterizing their interactions, we derived neural field equations
using a rigorous probabilistic mean-field theorem developed in [36]. The equations that
were obtained in that article were implicit equations on stochastic processes probability distributions, and are extremely hard to tame in that general framework. We summarized the
mathematical results obtained in a general setting, and provided a simple condition for the
existence of solutions that are spatially homogeneous in law. The main result of the present
article is the precise analysis of the dynamics of these equations in the particular case of
firing-rate neurons, corresponding to the fact that membrane potential of the neurons have
a linear intrinsic dynamics with nonlinear interactions. We were able in that case to reduce the mean-field equations to a set of delayed differential or integro-differential equations
that are mathematically tractable, and using this description evidenced the important role
of the microscopic noise in the dynamics in a series of examples of increasing complexity:
finite-populations networks with delays, one-layer neural fields and two-layers neural fields.
Several phenomena such as cascades of Hopf bifurcations, Turing-Hopf bifurcations and irregular spatio-temporal patterns were evidenced for specific levels of noise, and different
levels of noise were identified corresponding to specific typical behaviors.
The first obvious limitation of the study is the fact that this precise analysis reduces to
the case of firing-rate neurons. Though popular in the study of neural fields in the neuroscience community and widely used, this choice can be discussible in the present study.
Indeed, the model does not takes into account the highly nonlinear nature of several neuronal phenomena. This linear approximation is often seen as a heuristic mean-field limit
itself, which can raise questions of the relevance of our present approach for neuroscience applications. The analysis developed here might hence better account for large-scale neuronal
areas in which a very large number of cortical columns, described by the Wilson and Cowan
model,s interact. As far as neurons are concerned, we may add that a large body of experimental results have proved successfully accounted by linear dynamics, generally relative to
the specific nonlinearly determined fixed point of normal brain activity. These linear models,
though less general and accurate that the nonlinear ones, yielded much greater insight, and
in particular analytic treatment, of the cortical dynamics within their regimes of validity.
Probably the main reason why we chose this type of models is the reduction allowed by the
Gaussian nature of the solutions. Indeed, as we demonstrated here, these models have the
great advantage to be exactly and rigorously reducible to a set of tractable deterministic
equations. This is hence a breach to go further in analyzing the complex dynamics of the

Stochastic Neural Fields Dynamics

41

neural fields mean-field equations, with the aim of further understanding nonlinear neuron
models. The analysis provided here is the first (and to our understanding, the only case
where such an analytical study is possible) to address precisely the dynamics of such complex mean-field equations, and can also be seen as a proof of concept of the dynamics of this
class of equations, in particular the effects of noise and delays in these equations. All these
reasons motivated the investigation of linear models to uncover the dynamics of the more
complex nonlinear neural fields models. We expect that nonlinear equations of this kind will
also reveal other behaviors that are not reproduced in our linear approach. A perspective
of great interest also would be to derive from the non-linear mean-field equations systems
governing macroscopic variables such as the mean firing-rate. This is a complex and deep
question we are currently investigating.
The general version of the results of theorems 1 and 2 have several implications in
neuroscience that are discussed in [36]. Among these, we may recall that the propagation
of chaos property implies an independence between the behavior of finite sets of neurons, a
surprising property in the classical view of neural populations postulating that the activity
of neurons belonging to the same neuronal population, since sharing common input and
strongly interconnected, was highly correlated. This view was recently contradicted by
several experimental articles that exhibited high quality data corresponding to extremely
low levels of correlation, in the primary visual area V1 of awake macaques [10] and in the
rodent neocortex [11]. These results confirm that the chaotic state is biologically relevant,
and also have several implications in neural coding, as shown in the analysis of the coding
efficiency of decorrelated activity in [10]. The independence of the neural activity also
reconciles spike [9, 63] and rate [64] coding, two opposing conceptions of the neural activity,
by emphasizing the role of collectivity: observing the state of several independent neurons
during a time corresponding of the emission of a few spikes allows precise evaluation of the
probability distribution of the interspike interval, hence the firing rate of neurons.
One of the main new feature of the mean-field model is the consubstantial integration
of the noise in the dynamics of the neural field: in the reduced equations obtained for the
linear systems, we observed that noise appeared as a parameter in the sigmoidal function
transforming the voltage into a mean-firing rate. This property quantified precisely the effect of noise in the behavior of neural fields through our mean-field equations. We studied
a finite-population mean-field model with delays, and showed noise and delays shape the
behavior of the system. As delays are increased, a sequence of Hopf bifurcation appears in
the system, that was observed numerically and accounted for analytically by the analysis
of the characteristic roots for a particular system. This sequence of Hopf bifurcations produces complex transient behaviors, and simultaneously with noise levels shape the nature
of the response of the neural field, by producing either stationary or oscillatory responses.
Continuous neural fields were then analyzed with the particular aim of exhibiting qualitative changes in the behavior of a neural field due to noise levels. This is why we addressed
the presence of Turing-Hopf bifurcations, first in a one-layer system semi-analytically, and
then numerically in a more complex two-layers neural field. We evidenced several noiseinduced dynamical Turing-Hopf bifurcations generating different spatio-temporal patterns,
sometimes irregular and extremely complex. We also observed that noise also governs the
stability of synchronized in law behaviors.
The periodic solutions triggered by an increase in the noise standard deviation correspond at the level of the network to solutions in which in neuron fires synchronously, in
systems that present a unique stable fixed point when neurons are not driven by noise.

Stochastic Neural Fields Dynamics

42

Noise hence appear to have a very strong structuring effect on the dynamics of the system, which can appear at first sight counterintuitive as noise is usually seen as altering the
structure of the activity. These observations add to different phenomena documented in the
neural networks literatures, as for instance coherent oscillations in neural networks [65, 66]
where evidences for the presence of noise-induced in noisy integrate-and-fire neurons. The
phenomena observed in our analysis differs from the more classical stochastic resonance or
coherence resonance phenomena well documented in the neurocomputational literature (see
e.g. [67] for a review of the effect of noise in excitable systems), since these correspond to the
fact that there exists a particular level of noise maximizing the regularity of an oscillatory
output related to periodic forcing (stochastic resonance) or to intrinsic oscillations (coherence resonance). Another important result that can be derived from the present analysis
is the ability to define classes of parameter ranges attached to a few generic bifurcation
diagrams as functions of the parameters. This property suggests model-based strategies for
parameter evaluations in experimental measurements.
The present analysis, reducing noise levels to a simple parameter of a dynamical system,
allowed going further into the analysis of the functional role of noise in the brain, a question
currently widely debated in the neuroscience community, and relates this question to the
presence of synchronized oscillations, as we showed in our different numerical simulations
that noise levels determined the presence of oscillations, and the type of noise (additive
or synaptic) the sharpness of the synchronization of oscillations across different neurons or
areas.
Several extensions of the present study with applications in neuroscience and in applied
mathematics are envisioned. The first would be, as already mentioned, to investigate similar
phenomena in biologically relevant models and to study the behavior of the solutions of the
mean-field equations in that mathematically much more complex setting that would include
in particular nonlinear, excitable intrinsic dynamics and different ionic populations. The
mean-field equations in that setting are complicated implicit stochastic equations, that can
be written as a non-local nonlinear partial differential equation on the space of probability distributions. These equations are extremely hard to analyze directly, and the proof of
concepts provided by this simpler analysis foreshadows interesting behaviors. An additional
intricacy of the model is that, as we observed in our analysis, the system does not always
present stationary solutions, and when these exist, they are not necessarily stable. Cycles
were found, and bistable states between stationary and periodic behaviors were found, as
well as irregular states in spatially extended systems. Analyzing the solutions of the nonlinear mean-field equations will hence present interesting technical difficulties and probably
complex behaviors, and necessitates the development of techniques to analyze the convergence of such stochastic equations towards periodic solutions. Another important direction
in the development of this work would consist in fitting the microscopic model to biological
measurements. This would yield a new neural assembly model for large scale areas and develop studies on the appearance of stochastic seizures and rhythmic activity in relationship
with different parameters of the model, integrating the presence of noise in a mathematically
and biologically relevant manner.
A question that remains largely open is the question of the number of neurons per population. Motivated by the fact that typical sizes of the neuronal populations are generally
orders of magnitude larger than the number of neural populations (see e.g. [44]), we followed
the choice made in [36] and assumed that the number of neurons in each population was
increasing fast enough with the number of neurons (assumption of equation (3)), which left

Stochastic Neural Fields Dynamics

43

room for the propagation of chaos to occur in each population. If this condition is not satisfied, different limits can appear, and characterizing these different limits is a challenging
mathematical question. Another very interesting and deep question in this setting is the
question of mean-field limits in slow-fast systems, problem that is highly relevant for different aspects of the neuronal behavior, and which raises a number of technical and applied
questions.
Appendix A. Existence and Uniqueness of solutions of the synchronized meanfield solution
In this appendix we prove the existence and uniqueness of solutions of the fully synchronized system given by equations (7). This proof is similar to the proof given in [36]. Since
the quantities B(r, x, ϕ) and H(r, x, ϕ) do not depend on r, we drop the dependence of these
functions in r. As usually done, we transform the equation (7) into a fixed point equation
on the space of stochastic processes. To this end, let us define the map Φ as follow:


Rt 
0

ζ
+
ds
G(s,
X
)
+
E
[B(X
,
Z
)]
s
Z
s

(·)
0
0


Rt

+EZ [H(Xs , Z(·) )] + 0 dWs g(s, Xs )
,
t>0
Φ(X)t =
0

,
t ∈ [−τ, 0]
ζt (r)




L
(Zt ) =(Xt ) ∈ M independent of (Xt ), (Wt (·)) and (Bt (·, ·))
The solutions of equation (7) are exactly the fixed points of Φ. We assume here that G
and g are K-Lipschitz-continuous and satisfy the linear growth condition, and b and β are
L-Lipschitz continuous in both their variables. It is easy to show that any possible solution
has a bounded second moment following [36].
Existence:
L
Let X 0 ∈ M2 (C) such that X 0 |[−τ,0] = ζ0 a given stochastic process. We introduce the
sequence of probability distributions (X k )k≥0 on M(C) defined by induction as X k+1 =
(Φ(X k )). We denote by (Z k ) a sequence of processes independent of the collection of
processes (X k ) and having the same law. We decompose into six elementary terms the
difference:
Z t

Xtk+1 − Xtk =
G(s, Xsk ) − G(s, Xsk−1 ) ds
0
Z t
h
i
+
EZ B(Xsk , Z·k ) − B(Xsk−1 , Z·k ) ds
0
Z t
h
i
+
EZ B(Xsk−1 , Z·k ) − B(Xsk−1 , Z·k−1 ) ds
0
Z t

+
g(s, Xsk ) − g(s, Xsk−1 ) dWs
0
h
i
+ EZ H(Xsk , Z·k ) − H(Xsk−1 , Z·k )
h
i
+ EZ H(Xsk−1 , Z·k ) − H(Xsk−1 , Z·k−1 )
def

= At + B̃t + Ct + Dt + Et + Ft

Stochastic Neural Fields Dynamics

44

where we simply identify each of the six terms At , B̃t , Ct , Dt , Et and Ft with the corresponding expression in the previous formulation. By a simple convexity inequality (Hölder)
we have:


|Xtk+1 − Xtk |2 ≤ 6 |At |2 + |B̃t |2 + |Ct |2 + |Dt |2 + |Et |2 + |Ft |2
and treat each term separately.
The term At is easily controlled using Cauchy-Schwarz inequality, Fubini identity and
standard inequalities and we obtain:
Z t h
i
i
h
E sup |As |2 ≤ K 2 t E sup |Xuk − Xuk−1 |2 ds
sups∈[0,t]

−τ ≤u≤s

0

Similarly, the martingale term DtR is bounded using the Burkholder-Davis-Gundy theorem
t
to the d-dimensional martingale ( 0 (g(s, Xsk ) − g(s, Xsk−1 )) dWs ) and we obtain:

E

h

sup |Ds |

2

i

≤ 4K

2

t

Z

0≤s≤t

0

E

h

i
|Xuk − Xuk−1 |2 ds

sup
−τ ≤u≤s

Let us now deal with the deterministic interaction terms B̃t and Ct . We have:
2
Z t Z
Z 0


k
k
ds λ(r0 )dr0
dη(r, r0 , u)(EZ [b(r, r0 , Xsk , Zs+u
) − b(r, r0 , Xsk−1 , Zs+u
)])
|B̃t |2 = 
0
Γ
−τ
Z t Z
Z 0


0
0
k
k
≤ t λ(Γ) κ
ds λ(r )dr
dη(r, r0 , u)EZ |b(r, r0 , Xsk , Zs+u
) − b(r, r0 , Xsk−1 , Zs+u
)|2 )
0

−τ

Γ

≤ tλ(Γ)2 κ2 L2

Z

t

|Xsk − Xsk−1 |2 ds ≤ tλ(Γ)2 κ2 L2

0

Z

t

sup
0 −τ ≤u≤s

|Xuk − Xuk−1 |2 ds

hence easily conclude that

E[ sup

|B̃s | ] ≤ tλ(Γ) κ L

E[ sup

|Cs |2 ] ≤ tλ(Γ)2 κ2 L2

2

2 2

2

t

Z

s∈[0,t]

E[

sup

E[

sup

−τ ≤u≤s

0

|Xuk − Xuk−1 |2 ] ds

and similarly

s∈[0,t]

t

Z
0

−τ ≤u≤s

|Xuk − Xuk−1 |2 ] ds

Eventually, the terms Et and Ft use Burkholder-David-Gundy (BDG) inequality in place
of Cauchy-Schwarz’ together with similar arguments as used for B̃t and Ct . Using the
cylindrical nature of the Brownian motions (Bt (r, r0 )), BDG inequality yields, for the term
Et (and similarly for the term Ft ):
Z t
E[ sup |Θs |2 ] ≤ 4λ(Γ)2 κ2 L2 E[ sup |Xuk − Xuk−1 |2 ] ds
s∈[0,t]

0

−τ ≤u≤s

for Θt equal to Et or Ft . Putting all these evaluations together, we get:
Z t
h
i
E sup |Xsk+1 −Xsk |2 ≤ 6(T +4)(K 2 +2λ(Γ)2 L2 κ2 ) E[ sup |Xuk −Xuk−1 |2 ]ds (A.1)
s∈[0,t]

0

−τ ≤u≤s

Stochastic Neural Fields Dynamics

45

Moreover, since Xtk+1 ≡ Xtk for t ∈ [−τ, 0] by definition, we have, noting
i
h
Mtk = E sup |Xsk+1 − Xsk |2 ,
−τ ≤s≤t

Rt
the recursive inequality Mtk ≤ K 00 0 Msk−1 ds with K 00 = 6(T + 4)(K 2 + 2λ(Γ)2 + κ2 ), which
classically allows concluding on the existence and uniqueness of solutions.
Appendix B. Spatio-temporal patterns for one-dimensional neural fields with
reflective or zero boundary conditions
In this appendix, we reproduce the results obtained in section 4.2.2 with different boundary conditions. The system considered is essentially the same, i.e. a two layers excitatoryinhibitory neural field with exponential connectivity kernels, and set the connectivity matrix
as done in the main text. In section 4.2.2, the neural field was defined on S1 , i.e. characterized with convolutional interactions and periodic boundary conditions on the connectivity
kernel. We now choose in the present section different boundary conditions: reflective, in
which case the total synchronization conditions are satisfied, and zero boundary conditions,
in which case no spatially homogeneous solution exists.
Appendix B.1. Reflective boundary conditions
In the case of a connectivity defined by an exponential connectivity kernel on Γ = [0, 1]
with reflective boundary conditions, the stationary solutions are the same as in 4.2.2 and
their bifurcation diagram given in Fig. 5. Numerical simulation of the spatio-temporal
activity of this neural field for non-homogeneous initial conditions and different values of the
noise parameter Λ are displayed in Figure B.10. For small values of the noise parameter in the
parameter region (A), the inhomogeneous initial conditions produce large amplitude waves
that interact together, creating a transient complex structure of spatio-temporal activity
that stabilizes on a fully synchronized stationary solution (i.e. constant in space and time).
For slightly larger noise amplitude, this transient irregular phase takes over and produce
sustained dynamic irregular activity on the neural field. As noise is further increased, a
spatio-temporal periodic activity arises as the waves become faster and stop interacting.
When the noise parameter reaches parameter regions (B) or (C), the spatially homogeneous
oscillatory activity is recovered after a short transient. For noise levels in the parameter
region (D), the whole neural field integrates fast the inhomogeneous initial condition and
converges towards a spatially stationary activity.
Appendix B.2. Zero boundary conditions
In the case of zero boundary conditions of the connectivity kernel, the system does not
fulfills the synchronization condition on the kernel given by equation (6). We analyze here
the dynamics of this system for homogeneous initial conditions, and simulation results are
displayed in Figure B.11. In that case, neurons at locations r close to the boundaries 0 and
1 receive less input that neurons far from the boundaries, and non-synchronized patterns
are observed for any parameter values. We however make similar qualitatively observations
on the activity of the neural field. In details, for small noise intensities, the mean of the
state of each neuron converge towards a constant value depending on its space location
(see Fig. 11(a) for Λ = 0.1). As noise is increased, irregular spatio-temporal patterns of

Stochastic Neural Fields Dynamics

0

r

1

4

46

0

r

r

1

4

-4

4

-4

100

t

t

(a) Λ = 0.3
0

r

1

r

100

-4

100

4

0

r

200

t

(b) Λ = 0.6
r

100

1

0

r

(c) Λ = 1
r

1

r

4

-4

-4

200

100

t

t

(d) Λ = 1.5

(e) Λ = 3

Figure B.10: Solutions of the mean-field equations for the reflective exponential connectivity kernel, as
noise amplitude is increased. Abscissa represents the location on Γ = [0, 1], Ordinate the time. Initial
conditions are 5 on [0, 0.025] (orange box) and else equal 0. Animations of the activity are available in the
supplementary material.

Stochastic Neural Fields Dynamics

0

r

47

1

0

r

r

1

r

1

r

100
2

2

-2

-2

200

100

t

t

(a) Λ = 0.1
0

r

2

(b) Λ = 0.6
1

0

r

r

2

-2

-2

100

100

t

t

(c) Λ = 2

(d) Λ = 3

Figure B.11: Solutions of the mean-field equations for the exponential convolutional kernel with no (zero)
boundary condition, as noise amplitude is increased. Abscissa represents the location on Γ = [0, 1], Ordinate
the time. Initial conditions are deterministic equal to 0. Animations of the activity are available in the
supplementary material.

activity similar to these obtained in the previous cases are observed, and as noise increases
this activity converges towards a periodic in time, non-synchronized activity that disappears
into a constant in time, non-synchronized activity for larger values of the noise intensity. In
this particular case, no transient irregular activity, nor spatio-temporal waves are found.
Acknowledgements
The author wants to warmly thank Romain Veltz for interesting technical discussions on
the content and on relevant references, and David Colliaux for discussions on the choice of
connectivity kernels. This work was partially supported by the ERC grant 227747, NerVi.
References
[1] D. H. Hubel, T. N. Wiesel, M. P. Stryker, Anatomical demonstration of orientation
columns in macaque monkey, J. Comp. Neur. 177 (1978) 361–380.

Stochastic Neural Fields Dynamics

48

[2] W. Bosking, Y. Zhang, B. Schofield, D. Fitzpatrick, Orientation selectivity and the
arrangement of horizontal connections in tree shrew striate cortex, The Journal of
Neuroscience 17 (6) (1997) 2112–2127.
[3] T. Woosley, H. Van Der Loos, The structural organization of layer iv in the somatosensory region (si) of mouse cerebral cortex: The description of a cortical field composed
of discrete cytoarchitectonic units, Brain Research (1969) 205–238.
[4] E. Kandel, J. Schwartz, T. Jessel, Principles of Neural Science, 4th Edition, McGrawHill, 2000.
[5] A. Roxin, N. Brunel, D. Hansel, Role of Delays in Shaping Spatiotemporal Dynamics of
Neuronal Activity in Large Networks, Physical Review Letters 94 (23) (2005) 238103.
[6] S. Coombes, C. Laing, Delays in activity based neural networks, Submitted to the Royal
Society.
[7] A. Roxin, E. Montbrio, How effective delays shape oscillatory dynamics in neuronal
networks, Physica D: Nonlinear Phenomena 240 (3) (2011) 323–345.
[8] P. Series, S. Georges, J. Lorenceau, Y. Frégnac, Orientation dependent modulation
of apparent speed: a model based on the dynamics of feed-forward and horizontal
connectivity in v1 cortex, Vision research 42 (25) (2002) 2781–2797.
[9] S. Thorpe, A. Delorme, R. VanRullen, Spike based strategies for rapid processing.,
Neural Networks 14 (2001) 715–726.
[10] A. Ecker, P. Berens, G. Keliris, M. Bethge, N. Logothetis, A. Tolias, Decorrelated
neuronal firing in cortical microcircuits, science 327 (5965) (2010) 584. doi:DOI:10.
1126/science.1179867.
[11] A. Renart, J. De la Rocha, P. Bartho, L. Hollender, N. Parga, A. Reyes, K. Harris, The
asynchronous state in cortical circuits, science 327 (5965) (2010) 587.
[12] G. Buzsaki, Rhythms of the brain, Oxford University Press, USA, 2004.
[13] N. Tabareau, J. Slotine, Q. Pham, How synchronization protects from noise, PLoS
computational biology 6 (1) (2010) e1000637
[14] E. Izhikevich, Polychronization: Computation with spikes, Neural Computation 18 (2)
(2006) 245–282
[15] G. Ermentrout, J. Cowan, Large scale spatially organized activity in neural nets, SIAM
Journal on Applied Mathematics (1980) 1–21.
[16] S. Coombes, M. R. Owen, Bumps, breathers, and waves in a neural network with spike
frequency adaptation, Phys. Rev. Lett. 94 (14).
[17] R. Spreng, C. Grady, Patterns of brain activity supporting autobiographical memory,
prospection, and theory of mind, and their relationship to the default mode network,
Journal of Cognitive Neuroscience 22 (6) (2010) 1112–1123.

Stochastic Neural Fields Dynamics

49

[18] S. Amari, Characteristics of random nets of analog neuron-like elements, Syst. Man
Cybernet. SMC-2.
[19] S.-I. Amari, Dynamics of pattern formation in lateral-inhibition type neural fields,
Biological Cybernetics 27 (2) (1977) 77–87.
[20] H. Wilson, J. Cowan, Excitatory and inhibitory interactions in localized populations of
model neurons, Biophys. J. 12 (1972) 1–24.
[21] H. Wilson, J. Cowan, A mathematical theory of the functional dynamics of cortical and
thalamic nervous tissue, Biological Cybernetics 13 (2) (1973) 55–80.
[22] B. Ermentrout, Neural networks as spatio-temporal pattern-forming systems, Reports
on Progress in Physics 61 (1998) 353–430.
[23] G. Ermentrout, J. Cowan, Temporal oscillations in neuronal nets, Journal of mathematical biology 7 (3) (1979) 265–280.
[24] C. Laing, W. Troy, B. Gutkin, G. Ermentrout, Multiple bumps in a neuronal model of
working memory, SIAM J. Appl. Math. 63 (1) (2002) 62–97.
[25] E. Rolls, G. Deco, The noisy brain: stochastic dynamics as a principle of brain function,
Oxford university press, 2010.
[26] L. Abbott, C. Van Vreeswijk, Asynchronous states in networks of pulse-coupled neuron,
Phys. Rev 48 (1993) 1483–1490.
[27] D. Amit, N. Brunel, Model of global spontaneous activity and local structured delay
activity during delay periods in the cerebral cortex, Cerebral Cortex 7 (1997) 237–252.
[28] N. Brunel, V. Hakim, Fast global oscillations in networks of integrate-and-fire neurons
with low firing rates, Neural Computation 11 (1999) 1621–1671.
[29] D. Cai, L. Tao, M. Shelley, D. McLaughlin, An effective kinetic representation of
fluctuation-driven neuronal networks with application to simple and complex cells in
visual cortex, Proceedings of the National Academy of Sciences 101 (20) (2004) 7757–
7762.
[30] T. Ohira, J. Cowan, Master-equation approach to stochastic neurodynamics, Physical
Review E 48 (3) (1993) 2259–2266.
[31] M. Buice, J. Cowan, Field-theoretic approach to fluctuation effects in neural networks,
Physical Review E 75 (5).
[32] M. Buice, J. Cowan, C. Chow, Systematic fluctuation expansion for neural network
activity equations, Neural computation 22 (2) (2010) 377–426.
[33] P. Bressfloff, Stochastic neural field theory and the system-size expansion, Submitted
(2009).
[34] S. El Boustani, A. Destexhe, A master equation formalism for macroscopic modeling
of asynchronous irregular activity states, Neural computation 21 (1) (2009) 46–100.

Stochastic Neural Fields Dynamics

50

[35] J. Touboul, G. B. Ermentrout, Finite-size and correlation-induced effects in meanfield dynamics finite-size and correlation-induced effects in mean-field dynamics, Arxiv
preprint arXiv:1008.2839.
[36] J. Touboul, Propagation of chaos in neural fields, (submitted).
[37] A. Sznitman, Topics in propagation of chaos, Ecole d’Eté de Probabilités de Saint-Flour
XIX (1989) 165–251.
[38] Javier Baladron, Diego Fasoli, Olivier Faugeras, and Jonathan Touboul. Mean field description of and propagation of chaos in recurrent multipopulation networks of HodgkinHuxley and Fitzhugh-Nagumo neurons. arXiv:1110.4294, 2011.
[39] J. Touboul, G. Hermann, O. Faugeras, Noise-induced behaviors in neural mean field
dynamics, SIAM J. on Applied Dynamical Systems (in press).
[40] G. Da Prato, J. Zabczyk, Stochastic equations in infinite dimensions, Cambridge Univ
Pr, 1992.
[41] X. Mao, Stochastic Differential Equations and Applications, Horwood publishing, 2008.
[42] P. Chossat, O. Faugeras, Hyperbolic planforms in relation to visual edges and textures
perception, Plos Comput Biol 5 (12) (2009) e1000625. doi:doi:10.1371/journal.
pcbi.1000625.
URL http://dx.doi.org/doi:10.1371/journal.pcbi.1000625
[43] G. Faye, P. Chossat, O. Faugeras, Hyperbolic bumps, in: Neurocomp (Ed.), Proceedings
of the fifth French conference on Computational Neuroscience, 2010.
[44] Y. Fregnac, M. Blatow, J. Changeux, J. De Felipe, A. Lansner, W. Maass,
D. Mc Cormick, C. Michel, H. Monyer, E. Szathmáry, R. Yuste, Ups and downs in
cortical computation, Microcircuits: the interface between neurons and global brain
function, The MIT Press 393–433.
[45] O. Faugeras, J.-J. Slotine, Synchronizing a 2d continuum of two populations of neural
masses, in: W. R. Holmes, R. Jung, F. Skinner (Eds.), Sixteenth Annual Computational
Neuroscience Meeting (CNS), Vol. 8, Suppl 2 of BMC Neuroscience, 2007, p. 105.
URL http://www.cnsorg.org
[46] N. Venkov, S. Coombes, P. Matthews, Dynamic instabilities in scalar neural field equations with space-dependent delays, Physica D: Nonlinear Phenomena 232 (2007) 1–15.
[47] S. Coombes, C. Laing, Delays in activity based neural networks, Philosophical Transactions of the Royal Society A. 367 (2009) 1117–1129.
[48] J. Hale, S. Lunel, Introduction to functional differential equations, Springer Verlag,
1993.
[49] R. Ben-Yishai, R. Bar-Or, H. Sompolinsky, Theory of orientation tuning in visual cortex, Proceedings of the National Academy of Sciences 92 (9) (1995) 3844–3848.
[50] D. Hansel, H. Sompolinsky, Modeling feature selectivity in local cortical circuits, Methods of neuronal modeling (1997) 499–567.

Stochastic Neural Fields Dynamics

51

[51] K. Engelborghs, T. Luzyanina, D. Roose, Numerical bifurcation analysis of delay differential equations using dde-biftool, ACM Transactions on Mathematical Software
(TOMS) 28 (1) (2002) 1–21
[52] K. Engelborghs, T. Luzyanina, G. Samaey, Dde-biftool v. 2.00: a matlab package for
bifurcation analysis of delay differential equations, Technical Report TW-330, Department of Computer Science, K.U.Leuven, Leuven, Belgium (2001).
[53] R. Corless, G. Gonnet, D. Hare, D. Jeffrey, D. Knuth, On the lambertw function,
Advances in Computational mathematics 5 (1) (1996) 329–359
[54] L. Shayer, S. Campbell, Stability, bifurcation, and multistability in a system of two
coupled neurons with multiple time delays, SIAM Journal on Applied Mathematics
61 (2) (2000) 673–700.
[55] P. Bressloff, New mechanism for neural pattern formation, Physical Review Letters
76 (24) (1996) 4644–4647
[56] A. Hutt, M. Bestehorn, T. Wennekers, Pattern formation in intracortical neuronal fields,
Network: Computation in Neural Systems 14 (2) (2003) 351–368
[57] R. Veltz, An analytical method for computing hopf bifurcation curves in neural field
networks with space-dependent delays, Comptes Rendus Mathematique 1631-073X.
[58] N. Venkov, Dynamics of neural field models, Ph.D. thesis, University of Nottingham,
uRL: http://www.maths.nottingham.ac.uk/personal/pmxnav/nikola venkov 09 neural fields - phd thesis.pdf (2008).
URL
http://www.maths.nottingham.ac.uk/personal/pmxnav/
nikolavenkov09-neuralfields-phdthesis.pdf
[59] V. Dragoi, M. Sur, Dynamic properties of recurrent inhibition in primary visual cortex:
contrast and orientation dependence of contextual effects, Journal of Neurophysiology
83 (2) (2000) 1019
[60] B. Ermentrout, Simulating, Analyzing, and Animating Dynamical Systems: A Guide
to XPPAUT for Researchers and Students, Society for Industrial Mathematics, 2002.
[61] H. Meinhardt, M. Klinger, A model for pattern generation on the shells of molluscs,
Journal of Theoretical Biology 126 (1987) 63–89.
[62] S. Coombes, M. Owen, Exotic dynamics in a firing rate model of neural tissue with
threshold accommodation, Contemporary Mathematics 440 (2007) 123 0271–4132.
[63] S. Thorpe, Spike arrival times: A highly efficient coding scheme for neural networks,
Parallel processing in neural systems and computers (1990) 91–94.
[64] P. Dayan, L. Abbott, Theoretical Neuroscience : Computational and Mathematical
Modeling of Neural Systems, MIT Press, 2001.
[65] J. Pham, K. Pakdaman, J. Vibert, Noise-induced coherent oscillations in randomly
connected neural networks, Physical Review E 58 (3) (1998) 3610.

Stochastic Neural Fields Dynamics

52

[66] W. Nesse, A. Borisyuk, P. Bressloff, Fluctuation-driven rhythmogenesis in an excitatory
neuronal network with slow adaptation, Journal of computational neuroscience 25 (2)
(2008) 317–333
[67] B. Lindner, J. Garcia-Ojalvo, A. Neiman, L. Schimansky-Geier, Effects of noise in
excitable systems, Physics Reports 392 (6) (2004) 321–424

