Backward Renormalization Priors and the Cortical
Source Localization Problem with EEG or MEG
Leonardo S Barbosa1,2 and Nestor Caticha1

arXiv:1502.03481v1 [q-bio.QM] 11 Feb 2015

1

2

Instituto de Física, Universidade de São Paulo
Laboratoire de Science Cognitive et Psycholinguistique, Ecole Normale Supérieure

(Dated February 10, 2015)

Abstract We study source localization from high dimensional M/EEG data by extending a multiscale method based on Entropic inference devised to increase the spatial resolution of inverse problems . This method is used to construct informative prior distributions in a manner inspired in the context of fMRI (Amaral et al 2004 [1]) . We construct
a set of renormalized lattices that approximate the cortex region where the source activity is located and address the related problem of defining the relevant variables in a
coarser scale representation of the cortex. The priors can be used in conjunction with
other Bayesian methods such as the Variational Bayes method (VB, Sato et al 2004 [22]).
The central point of the algorithm is that it uses a posterior obtained at a coarse scale
to induce a prior at the next finer scale stage of the problem. We present results which
suggest, on simulated data, that this way of including prior information is a useful aid
for the source location problem. This is judged by the rate and magnitude of errors in
source localization. Better convergence times are also achieved. We also present results
on public data collected during a face recognition task.

Keywords: Maximum Entropy, priors, cortical source localization, EEG, MEG,Bayesian algorithms, inverse problems

1

1

Introduction

Designing improved methods of inference by updating probabilities as new information is
aquired, depends partly on the development of ideas of how to incorporate prior information.
In this paper we concentrate on the construction of prior distributions for the localization
of cortical dipole sources in EEG or MEG high resolution experiments. Several groups have
presented very encouraging results with respect to source localization (for a review see [21]).
After data aquisition, the source localization analysis problem is divided into three essentially
separate problems: first, construction of the prior; second, of the likelihood or model and
third, of the algorithm to extract the information from the resulting posterior. Several such
information extraction algorithms have been systematically analyzed by Wipf and Nagarajan
[24] in a useful unification that simplifies the literature, since different Bayesian approaches
differ only in the available information. Sato et al [22] have tackled the construction of
empirical priors. Our approach, which can also focuses on an empirical prior is based on
a previously introduced multiscale-prior method [4] which formalized ideas applied in the
context of fMRI by [1], [2]. It relies on the maximum entropy (ME) method to transfer
information gained at a coarse scale to start inference at a finner scale.
Advances by Dale and Sereno [5] permitted the inclusion of information from structural
MRI to construct a Green’s function that is specific to the subject under study. This can
be used in a Bayesian approach as such additional information can be incorporated in the
construction of the model and therefore of the likelihood. Once a posterior distribution
is constructed, it still remains to choose an appropriate numerical algorithm in order to
extract relevant information in the form of expected values or MAP estimates. Sato et al [22]
use the Automatic Relevance Determination approach of Neal [15] through the Variational
Bayes (VB) approximation, (see also [3]) while Nummemaa et al. [16] analyzed essentially
the same mathematical structure using both VB and Monte Carlo methods. A schematic
description of the VB method we employ is given in section 2.1.
In sections 2.2 and 2.3 we describe the central part of this work, the method of backward renormalization priors based on entropic inference. We show how to use information
obtained at a coarse renormalized scale to improve the starting point on a finner scale. This
can be iterated up from a very coarse description of the system to the finnest scale. The presentation is very general and the particular form of the set of renormalized lattices depends
on the application at hand.
Validation of the method appears in section 3 using simulated data sets. After calculating
the forward model with a set of known dipole sources, we assess the results comparing
the performance of the source localization of 2 dipoles at random positions and intensities
using both the original VB method and the multscale approach. This is done comparing the
distance between the localized dipole (or first N strongest dipoles) and the real one (used to
generate the simulated data), and also the sensitivity of both algorithms (ROC curves) when
the noise is increased. Finally, we applied our method to the problem of source localization
of a face/mask paradigm in the data available at http://www.fil.ion.ucl.ac.uk/spm/data/
(SPM - Wellcome Trust Centre for Neuroimaging). A discussion and conclusions appear in
the final section.

2

2

Methods

Since our contribution lies in the proposed method, this is the central part of the paper. But
first we describe the inverse problem in section 2.1. In this part we don’t make any advances
and follow the work of Sato et al. [22]). Then, in 2.2 we discuss the central part of our
contribution, the multiscale approach to the prior distribution.
2.1

The forward and inverse problems: from cortical sources to M/EEG data and
back

We follow Sato [22]) using a linear model that has been discussed by [17], [8] [13], [14]
[18] (see also [23]). We denote by V the electric potential at the scalp surface, by J the
density of dipole sources that give rise to those potentials and G the Green function that
describes the electromagnetic medium. It represents the macroscopic physiological and geometrical details of the head known in this area as the Lead Field.
Measurements are subject to noise ξ that has been supposed to be gaussian and independent of space and time, so that for data V and dipole sources density J the model is
V = GJ + ξ.

(1)

For M sensors, T the length of the time series of measurements, N the number of sites of the
lattice where the dipoles live, the dimensions of the matrices V, J, G and ξ are respectively
M × T , N × T , N × M and M × T . Bayes theorem leads to
P(J|V I) =

P0 (J|I)P(V |J I)
P(V |I)

.

(2)

The method that we follow is essentially the variational Bayes used by Sato et al. with a
twist. In their method, the prior distribution of the dipole density P(Jd ) = P({Jd (r d (i))}) is
parametrized by a set of parameters αd = {αd (i)} where Λd is the lattice of positions r d (i)
used to represent the cortex where the dipoles live. αd (i) is the inverse of the variance of
the zero mean gaussian also called the precision, used as prior for the dipole density vector
amplitude Jd (r d (i)) at each site i ∈ Λd . The direction of each Jd (r d (i)) vector is fixed and
normal to the surface representing the cortex at that point. Incomplete knowledge of αd
prompts the use of hyperpriors for each individual αd (i). It is reasonable, as they do, to use
a set of Gamma hyperprior distributions for the set of αd (i). Integrating the gaussian priors
over the Gamma distributed αd (i) leads to t-student distributions. So that at each site, the
distributions of the density belong to the family given by
Z ∞ 1/2
α
αJ 2
P(J|ᾱ, γ) =
p e− 2 Γ(α|α̂, γ̂)dα
2π
0
r
1
ᾱ Γ(γ + 2 )
1
=
(3)
h
iγ+1/2 .
2πγ Γ(γ)
J 2 ᾱ
1 + 2γ
Call collectively the set of hyperparameters of the Gammas θd0 . The inclusion of the data
read from the electrodes, V , by the Variational Bayes method, leads to a dynamics of the
f
hyperparameters of the gammas, which converges to a final value θd . Their method can be
succinctly described by the mapping
θd0 = (α̂0d , γ0d )

Variational Bayes

−→
3

f

f

f

θd = (α̂d , γd )

(4)

Their starting point is that every θd0 (r d (i)) ∈ θ0d is the same, i.e a prior of the dipole density
spatially invariant over the cortex.
Our contribution consists of considering a different choice of the prior distributions. For
this we use the theory of backward [4] renormalization priors, which are inspired in the
multigrid prior approach used by [1] to study fMRI. By using entropic inference, we can use
the posterior in a coarse scale to generate an informed prior in the next finner scale.
2.2

The Multiscale problem

The relevant variables are the dipoles densities J and the electric potential (or magnetic
fluxes) to be measured at the electrodes. The space where the variables live is a representation of the cortex obtained from structural magnetic resonance data using Free Surfer image
analysis suite, which is documented and freely available for download online
(http://surfer.nmr.mgh.harvard.edu/ - [6] [7]). This representation can be done at different
levels of resolution, so that we define a set of renormalized lattices {Λd }d=0,...D . Each lattice
is composed by sites r d (i) ∈ Λd with i = 1, ....|Λd |. The particular form of how a coarser or
renormalized lattice Λd−1 is obtained from the previous Λd depends on the particular type of
problem. For the problem at hand of EEG dipole sources, the first lattice Λ0 was generated by
introducing an icosahedron in the spherical surface (Figure 1) obtained by inflating a brain
hemisphere. Deflating the spheres results in a first lattice with 40 faces. Next, to generate the
finer scales lattices Λ1 , ..., Λ D , each trianguar face of the inflated brain was into 4 triangles.
More details about the position of dipoles in each face are given in section 3.1.
At each scale d there is a set of dipole density amplitudes, collectively denoted Jd =
{Jd (r d (i))}. We denote the integration measure over the set of |Λd | variables by dJd .
Consider just two consecutive scales, a coarse one d − 1 and the higher resolution d.
Suppose we have solved the case at the d − 1 level and now we want to solve the problem at
scale d. This will give a map to be iterated from the coarsest to the finest scale. The method
to be used follows the simpler case of discrete variables presented in [4] is the maximum
entropy (ME), and the aim is to obtain a distribution P(Jd , Jd−1 , V ) from a prior distribution
Q(Jd , Jd−1 , V ). The method of choice is the ME because (i) when constraints are imposed
and the prior already satisfies the constraints ME will result in a posterior equal to the prior
and (ii) when the results of a measurement are considered as constraints, ME gives the same
results as Bayes [9]. Bayesian usual update can be thought of as a special case of Maximum
Entropy where the constraints arise from the data.
To show (i) we perform a simple Maximum Entropy exercise: Consider a variable X that
takes values x and that Q(x) represents our prior state of knowledge. New information
is obtained, e.g that the expected value < f (x) > is known to have a particular value:
< f (x) >= E. The update from Q(x) to P(x) is done by maximizing
Z

Z

Z
P(x)
dx +λ
f (x)d x − E + λ0
P(x)d x − 1
(5)
S[P||Q] = − P(x) log
Q(x)
The result, after satisfying normalization is the Boltzmann-Gibbs probability density P(x) =
exp λ f (x)
Q(x) Z(λ) , with λ to be chosen to satisfy < f (x) > P = E. What is the value of λ if <
f (x) >Q = E, if the prior already satisfied the constraint? It is simple to see that λ = 0 and
Z(λ) = 1, so the reuse of old data in the from of constraints, again and again doesn’t change
the density Q(x) that already satisfies the constraint. While this sounds trivial, it will be
useful since data in Bayes updates can be written as constraints for Maximum Entropy.
To prove condition (ii) we follow [9]. Consider the problem where a distribution P(θ )
has to be obtained, first, from the knowledge that a measurement of X has yielded a datum
4

x 0 ; second, that prior to the inclusion of such information our knowledge of θ is codified by
a distribution Q(θ ) and third, that the relation between X and Θ is Rcodified by a likelihood
Q(x|θ ). Thus we have to consider P(x, θ ) subject to constraints dθ P(x, θ ) = P(x) =
δ(x − x 0 ). This is not a single constraint, i.e. instead of a single Lagrange multiplier, we have
to consider a function λ(x) and maximize
Z
P(x, θ )
S[P||Q] = − P(x, θ ) log
d x dθ +
Q(x, θ )
Z

Z

Z
λ(x)

dθ P(x, θ ) − δ(x − x 0 ) d x + λ0

P(x)d x − 1 .

(6)

After obtaining the joint Rdensity P(x, θ ) by maximizing the entropy, we can calculate the
desired marginal P(θ ) = d x P(x, θ ). The result is
P(θ ) = Q(θ |x 0 ),

(7)

Q(θ )Q(x 0 |θ )

is the Bayes posterior Q(θ |x 0 ) given by Bayes theorem, which
where Q(θ |x 0 ) =
Q(x 0 )
just follows from the rules of probability. This proves that maximum entropy as an inference
engine justifies the usual Bayes procedure when the constraint is a datum such as knowing
that a measurement of X turned out to give a value x 0 . Maximum entropy is used to show
that Bayes theorem should be used in the inference process.
Going back to the EEG problem we consider that the relevant space is formed by the
dipole variables at the two scales and the electrode potentials. Thus we seek the maximization of
Z
P(Jd , Jd−1 , V )
dJd dJd−1 dV
(8)
S[P||Q] = − P(Jd , Jd−1 , V ) log
Q(Jd , Jd−1 , V )
to obtain P(Jd , Jd−1 , V ), which in addition to normalization, is subject to
• (A) The marginal P(V ) = δ(V − v 0 ), the measured data is v 0 .
Q
• (B) Q(Jd−1 ) is given, e.g. by i∈Λd−1 f (Jd−1,i |θ d−1 (r d−1 (i))) for some parametric family
f (J|θ ).
• (C) Knowledge about the process of renormalization is coded by Q(Jd |Jd−1 ) (see section
2.3.)
• Given Jd , knowledge of Jd−1 is irrelevant for V : Q(V |Jd Jd−1 ) = Q(V |Jd )
Marginalization and the product rule of probability give
Z
Q(Jd ) =

Q(Jd−1 )Q(Jd |Jd−1 )dJd−1 .

(9)

which can be calculated from (B) and (C). Solving the maximization problem and taking the
marginal of the maximum entropy distribution we obtain
P(Jd ) =

Q(Jd )Q(v 0 |Jd )
Q(v 0 )
5

(10)

is given by what we would have expected, Bayes theorem, with the extra important ingredient brought in by equation 9, that the prior at this new scale is obtained by whatever
information we have on Jd−1 , i.e. Q(Jd−1 ) and the renormalization procedure Q(Jd |Jd−1 ).
If we wish to restrict the distributions to products of some parametric form , e.g. tstudents, we have to use Q(Jd |α̂0d , γ0d ), from equation 9 given by
Z
f

f

Q(Jd−1 |α̂d−1 , γd−1 )Q(Jd |Jd−1 )dJd−1 .

Q(Jd |α̂0d , γ0d ) ≈

(11)

This is the backward renormalization step. The idea is to determine which distribution of Jd
in the parametric space of the t-student distributions is closest to the integral on the right
side of equation 11. So a posterior in the coarsest scale d − 1 induces a prior in the d scale.
We can start at the lowest resolution with the same (α0i , γ0i ) initial values at every site of the
coarsest lattice Λ0 , obtain via variational Bayes the parameters for the posterior of the J0
from equation 10 and proceed for the next scales as represented by the map:
f

f

(α̂d−1 , γd−1 )

BackRenorm

−→

(α̂0d , γ0d )

(12)

Beginning with a uniform prior at the coarsest scale, that is a set of parameters
uniform over the lattice Λ0 , we iterate the mapping
VB

(α̂00 , γ00 ) → ....

BackRenorm

→

VB

f

f

(α̂0d , γ0d ) → (α̂d , γd )

BackRenorm

−→

VB

(α̂0d+1 , γ0d+1 ) → ....
f

(α̂00 , γ00 )

(13)

f

to finally obtain a posterior at the finest scales described by (α̂ D , γ D ), the desired answer to
the inference problem.
Finally Rit must be emphasized that the marginal of the maximum entropy distribution
P(Jd−1 ) = P(Jd , Jd−1 , V )dJd dV is given by Bayes theorem at the coarser level, P(Jd−1 ) =
Q(Jd−1 )Q(V 0 |Jd−1 /Q(V 0 ), showing that the new maximization of the entropy didn’t alter the
result obtained by the previous step. Reusing the same data within the realm of maximum
entropy is not the same as naively reusing the data using just Bayes theorem updating. For
ME it is harmless, as it imposes a constraint already satisfied, while for Bayes it represents
the belief that the data were independently re-obtained, leading to a unwarranted decrease
of uncertainty.
2.3

Backward renormalization priors

We now investigate the backward renormalization step given by equation 11. Renormalization is seldom a tidy business, and the fact that the dipoles are vectors and that the direction
of their sum is not necessarily the same as the perpendicular to the surface of the cortex,
does complicate things even further. The cortex is represented by triangular faces that are
not in the same plane and thus the renormalized face is not simply related to the finner scale
faces. Approximations are needed to advance and suggest a specific form for the mapping
in display 12. Numerically we have investigated this suggestion and found some variations
on the theme that lead to good results. We can analyze the backward renormalization in the
following simplified context. Call λ = |Λd−1 |/|Λd | the ratio of degrees of freedom of a lattice
at a stage d − 1 of renormalization, to the next, finner stage d. In this work λ = 1/4. When
going from one lattice to a coarser one density variables are approximately renormalized
according to a scaled block average:
X
Jd−1 ( j) = λ
Jd (i)
(14)
i( j)

6

where i( j) means that the sum is over the set of degrees of freedom at r d (i) that are blocked
to form the coarser degree of freedom at position r d−1 ( j). For independent Jd (i) probability
theory leads to the convolution


Z
Y
X

Q d−1 (Jd−1 ( j)) =
dJd (i)Q(Jd (i)) δ Jd−1 ( j) − λ
Jd (i) .
i( j)

i( j)

Using the characteristic functions, Φ(k) = F T (Q(J)), the Fourier transforms of the distributions:

1/λ
Φd−1 (k) = Φd (λk)
(15)
So the prior distribution of the dipole at the finner scale position i can be chosen by inverting
a Fourier transform:


k λ
(16)
Q(Jd (i)) = I F T [Φd−1 ( )]
λ
For distributions stable under additions, this entails a simple backward renormalization of
the distribution parameters.
From all this development, the main information we obtained is that the expected value of
the precision of the gaussian prior of the dipole density should decrease at the new lattice in
comparison with that of the posterior at the previous coarser lattice. For the more intricate
renormalization we have to consider, a further improvement obtained numerically in the
simulations is that not only the variance of the prior should be larger, but that at a given
lattice the inferred position of a dipole might be a little off and seem to be at a neighboring
site. Thus we introduced what we call contamination, by adding variance from the nearest
and next nearest sites:
1
i( j),0
ᾱd+1

=

1
j, f
ᾱd

+

1X 1
3

jn

j ,f
ᾱdn

+

1X
9

jnn

1
j ,f
ᾱdnn

,

(17)

bringing in information from j, the parent site of i as well as the nearest neighbors ( jn ) and
the next nearest neighbors ( jnn ) of j. This prevents early commitment of the position of a
dipole. In the average, the precision scales as the backward renormalization step suggests.
For all the sites i in the finner lattice Λd that give rise to a given renormalization block at
r d−1 ( j), the prior of J(r d (i)) will have renormalized parameters inherited from the block
variable distribution of J(r d−1 ( j)).

7

Figure 1: Cortex, left hemisphere (Left) original as obtained from a the structural MR image,
(center) inflated, (right) spherical. Gray scale code the curvature in the original image.

Figure 2: Representation of the cortex at different scales. Gray scale is used to represent
finer scale balanced surfaces derived from the first representation, the icosahedral. The
dipole density vectors are constrained to be at a site at the center and perpendicular to each
triangular face.

3

Results and discussions

Depending on the choice of the initial and final renormalized lattice several methods can
be defined. We denote them by BRVBkk0 , standing for Backward Renormalization Variational
Bayes and k indicates the initial coarsest scale, k0 the finest scale. The final lattice is obtained
by five steps of renormalization. The full method is BRVB04 . The original Variational Bayes
is VB, and since it runs at the finest scale only, and is the same as BRVB44 . We have also
compared its performance to that of the MNE [11].
We first present results obtained by solving the inverse problem for artificial data generated by the forward problem, obtained by simulating two dipoles in the original lattice
generated by Free Surfer. Manipulations include (i) the positions and magnitudes of the
dipoles and (ii) noise corruption of the data. To quantify the quality of the inference we considered the distance between real and localized dipoles (defined in section 3.2) estimated by
averaging over ≈ 200 runs.
Since studying the effect of adding noise to cases where BRVB44 fails is not informative,
we considered five different configurations where both algorithms performed equally well.
Then, noise level was increased to study the decay of performance of the different methods.
We end by showing the results of applying the method to publicly available EEG data in a
face recognition task (http://www.fil.ion.ucl.ac.uk/spm/data/ SPM - Wellcome Trust Centre
for Neuroimaging).

8

3.1

Simulations

We start by considering two main active dipoles in the original lattice at random positions
r(1) and r(2), separated by a distance L ( 55 ≤ L ≤ 65 mm) from each other.
Each simulation starts at t = 0 runs for T = 51 samples, with each dipole intensity
. Results shown in the images comes from analysis made from data
proportional to sin tπ
T
collected at the peak value t = 26.
The forward problem was solved using SPM (http://www.fil.ion.ucl.ac.uk/spm/ - Wellcome Trust Centre for Neuroimaging) and Fieldtrip (http://fieldtrip.fcdonders.nl/ - Centre
for Cognitive Neuroimaging of the Donders Institute for Brain, Cognition and Behaviour)
MATLAB toolboxes. They permit calculating the Lead Field using 3-spheres aproximation
(for simulations) and realistic head model BEM solutions (for analyses of real data). The
Lead Field using 3-spheres was used to generate the potential in the head surface and the
signal was then corrupted with NSR of 0.1. We used 128 electrodes positions and MRI template image available with the SPM toolbox. The inversion methods (MNE, BRVB04 through
BRVB44 ) were implemented in MATLAB.
As described in section 2.2, each one of the 5 lattices Λ0 , ..., Λ4 has 40, 160, 640, 2560,
10240 faces, respectively. The dipoles are positioned using then mean of vertices and faces of
the original lattice generated by Free Surfer. In the spherical surface, it is possible to localize
the vertices and faces above each divided face (or the faces of the icosahedron for Λ0 ). These
original faces and vertices have their position in the original folded surface. Each dipole was
located in the mean of those vertices and oriented as the mean of the normals of those faces.
This could introduce localization bias since the density of faces in the original lattice is not
homogeneous. The same set of lattices was generated using balanced representations, but
the performance in all algorithms did not change (results not reported for brevity), so this
option was discarded. For more information see [12].
Figure 3 shows as an example a particular run. In the first line, first panel in the left, the
real dipole sources used to generate the data V . The following panels shows the estimated
sources using Minimum Norm or MNE, BRVB04 and BRVB44 , respectively. Figure 4 shows the
variance or inverse precision 1/αn initial value in the two first columns and after convergence
in the two last columns, at each scale for BRVB04 in the first 5 lines and in the single grid
BRVB44 in the last.
This is a nice example to show because it exemplifies the case when the random choice
of the location of the sources placed one on a deep position, which is known to present a
problem since the Lead field is very small. This problem has been addressed by [20] using
the precision matrix, but here this information is extracted from the data. While the BRVB44
found only one, the more superficial source, the BRVB04 was able to find both sources. For the
two source case, about 10% of the 100 runs analyzed showed this difference in the behavior
for deep sources and the method was seen to be robust under this condition.

9

Figure 3: The top left box represents the real current densities used to simulate the potential,
while the others represents the different localization methods. Top and bottom line of each
box represents the bottom and right view of the inflated cortex facing right, respectively. The
first column represents the partially inflated brain and the second zooms into the indicated
region in the first column. Color codes the current intensity, negative facing inwards and
positive outwards. The cortex is represented by a lattice of ≈ 2.8 × 104 triangles and the fifth
order grid is made up by 10240 = 2 × 20 × 44 possible dipole locations.

10

Figure 4: Evolution of 1/α under iteration of the backward renormalization algorithm. First
and third columns shows the bottom view of the cortex surface, while second and fourth
columns shows the right hemisphere, both facing right. First and second columns shows
initial variance in each region, while third and fourth columns shows variance after the VB
algorithm converged in that specific grid, with a given initial value. First to fifth rows show
the stages of BRVB algorithm in each grid with: first BRVB00 (40 = 2 × 20 faces), second
BRVB01 (160) third BRVB02 (640), fourth BRVB03 (2560) and fifth BRVB04 (10240) (2 × 20 ×
4d faces). The last line is the variance in the VB algorithm starting with a hyperparameter ᾱ
uniform in the fifth lattice, BRVB44 . Notice how aggressive is the convergence in the variance
when the VB method starts with a uniform prior in the last surface, as evidenced by the
zoomed square.

11

3.2

Validation of the method

The main reason for simulating this problem is that it allows for comparisons with the real
sources, used to generate the data. We used mainly the distance between the strongest and
second strongest localized dipoles.
To begin we compare the distances in mm between real and localized dipoles in the 200
simulations. As we can see in Figure 5, we first compare the strongest dipole found by
the methods BRVB44 and BRVB04 and the strongest real dipole. We also compare it to the
minimum between the distances of the first and second strongest localized ones, ignoring
differences in amplitudes. It is interesting to see that even in this case there is a peak at 60
mm, specially for the BRVB44 .
Finally we show the ordered errors for all simulations, from BRVB44 to BRVB04 , evidencing
the improvement in the use of each additional localization.

12

Figure 5: First line shows the histograms for the distance (mm) between the strongest real
source and that found by different algorithms: BRVB04 and BRVB44 . Second line is the same
type of histogram but showing the smallest distance between the strongest real source and
the two strongest localized dipoles. This way, since we used two sources for generating the
field, we don’t care which one was localized as the strongest, only it’s position. The last line
shows the same information as the second histogram, but ordering the errors in ascendant
fashion. Obtained from inference made on 200 different forward problems, with random
positions and distances between the two dipoles randomly chosen between 55 and 65 mm.
We also analyzed how different algorithms fare under the addition of different levels of
noise.
We identified 5 simulations where both the BRVB44 , BRVB34 and BRVB04 obtained similar
good results from data corrupted at low noise-to-signal ratios (NSR=0.1). In order to compare runs with similar good results, we have to restrict to cases where the sources are located
in more superficial regions, since the performance for sources in deep locations are quite different with the single scale method not even identifying a result to be compared. We added
uncorrelated gaussian noise of zero mean and variance σ2 to each of the M components
(electrodes) of the vector V of data voltages so that
N SR =

M σ2
V 0V

.

(18)

Finally, in Figure 6, besides the error distance in localization, we also plotted the ROC
space for the three algorithms, and the curves for the distance between the average false pos13

itives positions to the real dipoles. Notice how the the BRV B04 has a clear superior resistance
to the increased noise.

14

Figure 6: Performance of the different algorithms. The horizontal axis represents the noise to
signal ratio (apart from the ROC space). First line : average distance between false positives
and the most intense real dipole on the left, and the second most intense on the right. Second
line : ROC space on the left (the size of the markers represents the intensity of noise), and
average distance between the second most intense true positive and the second most intense
real dipole on the right.
3.3

Real Data from EEG

We applied the BRVB04 and the BRVB44 in real data available at
http://www.fil.ion.ucl.ac.uk/spm/data/mmfaces/ (SPM - Wellcome Trust Centre for Neuroimaging). The experiment consists of 128 electrodes set in a Face / No-Face stimulus. We
preprocessed the data as specified in the tutorial for source reconstrucion in SPM, and computed the difference between the average of each condition. The resulting scalp potential
was used for the source localization. The sources shown in Figure 7 were found by BRVB04 ,
BRVB44 and MNE. Although they are in general consistent with the literature for this experiment [10], the location of the sources as well the intensity is clearly different. Further
studies are necessary to understand the nature of this difference in this specific protocol.

4

Conclusion

The main contribution of this paper is to use a Maximum Entropy inferential chain of projections to systematically transfer posterior information from one coarse scale to the prior of a
finner scale. This approach is not restricted to M/EEG or fMRI imaging methods, but should
apply to inference about the localization of sources in any spatially extended system and thus
has a potentially wide scope of applications. We analyzed the behavior of the hierarchical
Bayesian approach for solving the M/EEG inverse problem. This was introduced into this
context by [22] who used one coarser grid to restrict the search in a finer scale, without
propagating the converged variance to the next scale. As shown by Nummenmaa et al [16],
the Variational Bayes approach is very sensitive to the initial values of the hyperparameters.
15

Figure 7: Bottom view of the cortex looking up (most significant locations are on the bottom
in that instant). From left to right : BRVB04 , BRVB44 and MNE at approximately 170 ms.
Our multiscale approach was inspired by the fMRI work of [1] to systematically construct
prior distributions. An important difference is that the M/EEG data introduces a further complication by not being a scalar as in the fMRI case, leading us to defining a priori directions
for the dipoles, or risk having to deal with a nonlinear problem with a too large number of
degrees of freedom. This simplicity permitted the more sophisticated formulation based on
the backward renormalization group for discrete variables done in [4] Our method permits
setting the initial value for the hyperparameters in the VB approach on a finer scale from the
value obtained after VB convergence in the coarser previous scale. The method being robust
to choices in the coarsest scale. However the renormalization group anlysis of this harder
problems remains incomplete and further work in this direction will follow.
Second, we have done extensive simulations to validate the method. We presented results
in simulated data that suggests that this approach is a valid aid to increase the precision of
the localized dipoles and also to increase the performance in the presence of noise. The
backward renormalization priors permitted a more systematic localization of deep sources
far from the skull.
Third, as mentioned by Nummenmaa et al [16] in the discussion, there appears to be a
natural trade-off between choosing a method providing smoother but unique solution, and
the hierarchical approach with better spatial resolution and a multitude of candidate solutions. We claim that our method might represent a good direction in finding a compromise
between both solutions.
We stress that the main advantages of a Maximum Entropy or Bayesian approach is the
clear identification of the often hidden assumptions underlying an algorithmic approach.
This permits concentrating on the different pieces needed to solve the puzzle. It illustrates
the fact that prior information not only goes into the prior but can improve the likelihood.
While we need a better electromagnetic model of the brain as well as understanding the noise
processes, here we just looked at the prior, but this can certainly be improved. A natural idea
is to improve our method with anatomical prior information as used by [19].
Acknowledgements We thank Ariel Caticha, Selene Amaral and Said Rabbani for discussions on different aspects of inference and neuroimaging. This work received support from
CNAIPS-USP and CNPq.

16

References
[1] Amaral, R., Rabbani, S. R., and Caticha, N. (2004). Multigrid priors for a Bayesian
approach to fMRI. NeuroImage, 23:654 – 662.
[2] Amaral, R., Rabbani, S. R., and Caticha, N. (2006). BOLD response analysis by iterated
local multigrid priors. NeuroImage, 36:361 – 369.
[3] Bishop, C. M. (2007). Pattern Recognition and Machine Learning (Information Science
and Statistics). Springer, 1 edition.
[4] Caticha, N. (2015). Source localization by entropic inference and backward renormalization group priors. Entropy, submitted.
[5] Dale, A. M. and Sereno, M. I. (1993). Improved Localization of Cortical Activity by
Combining EEG and MEG with MRI Cortical Surface Reconstruction: A Linear Approach.
Journal of Cognitive Neuroscience, 5:162–176.
[6] Fischl, B., Sereno, M. I., and Dale, A. M. (1999a). Cortical surface-based analysis. I.
Segmentation and Surface Reconstruction. NeuroImage, 9(2):195–207.
[7] Fischl, B., Sereno, M. I., and Dale, a. M. (1999b). Cortical surface-based analysis. II:
Inflation, flattening, and a surface-based coordinate system. NeuroImage, 9(2):195–207.
[8] Geselowitz, D. (1967). On Bioelectric Potentials in an Inhomogeneous Volume Conductor. Biophysical Journal, 7(1):1–11.
[9] Giffin, A. and Caticha, A. (2007). Updating probabilities with data and moments.
Bayesian Inference and Maximum Entropy Methods in Science and Engineering, AIP Conf.
Proc, Ed. A. Mohammad-Djafari, 872:31.
[10] Halgren, E. (2000). Cognitive Response Profile of the Human Fusiform Face Area as
Determined by MEG. Cerebral Cortex, 10(1):69–81.
[11] Hamalainen, M. and Ilmoniemi, R. (1994). Interpreting magnetic fields of the brain:
minimum norm estimates. Medical and Biological Engineering and Computing, 32:35–42.
10.1007/BF02512476.
[12] Lin, F.-H., Belliveau, J. W., Dale, A. M., and Hämäläinen, M. S. (2006). Distributed
current estimates using cortical orientation constraints. Human brain mapping, 27(1):1–
13.
[13] Mosher, J. C., Leahy, R. M., and Lewis, P. S. (1999). EEG and MEG: forward solutions
for inverse methods. IEEE transactions on bio-medical engineering, 46(3):245–59.
[14] Mosher, J. C., Lewis, P. S., and Leahy, R. M. (1992). Multiple dipole modeling and
localization from spatio-temporal MEG data. IEEE transactions on bio-medical engineering,
39(6):541–57.
[15] Neal, R. M. (1997). Bayesian Learning for Neural Networks (Lecture Notes in Statistical
Vol. 118). Journal of the American Statistical Association, 92(438):791.

17

[16] Nummenmaa, A., Auranen, T., Hämäläinen, M. S., Jääskeläinen, I. P., Lampinen, J.,
Sams, M., and Vehtari, A. (2007). Hierarchical Bayesian estimates of distributed MEG
sources: theoretical aspects and comparison of variational and MCMC methods. NeuroImage, 35(2):669–85.
[17] Nunez, P. L. and Srinivasan, R. (2006). Electric Fields of the Brain: The Neurophysics of
EEG, 2nd Edition. Oxford University Press.
[18] Oostendorp, T. F. and van Oosterom, a. (1989). Source parameter estimation in inhomogeneous volume conductors of arbitrary shape. IEEE transactions on bio-medical
engineering, 36(3):382–91.
[19] Ou, W., Nummenmaa, A., Ahveninen, J., Belliveau, J. W., Hämäläinen, M. S., and
Golland, P. (2010). NeuroImage Multimodal functional imaging using fMRI-informed
regional EEG / MEG source estimation. NeuroImage, 52(1):97–108.
[20] Pascual-Marqui, R. D. (2002). Standardized low-resolution brain electromagnetic tomography (sLORETA): technical details. Methods and findings in experimental and clinical
pharmacology, 24 Suppl D:5–12.
[21] Ramirez, R. R. (2008). Source localization. Scholarpedia, 3(11):1733.
[22] Sato, M.-a., Yoshioka, T., Kajihara, S., Toyama, K., Goda, N., Doya, K., and Kawato,
M. (2004). Hierarchical bayesian estimation for meg inverse problem. NeuroImage,
23(3):806–826.
[23] Scharf, G. and Scharf, C. (2010). Electrophysiology of living organs from first principles. page 11.
[24] Wipf, D. and Nagarajan, S. (2009). A unified Bayesian framework for MEG / EEG
source imaging. NeuroImage, 44(3):947–966.

18

