Neural Population Coding of Multiple Stimuli

arXiv:1410.1093v2 [q-bio.NC] 31 Dec 2014

A. Emin Orhan†

Wei Ji Ma†,‡

†Center for Neural Science and ‡Department of Psychology
New York University, New York, NY 10003

Abbreviated title: Neural Population Coding of Multiple Stimuli
Corresponding author: A. Emin Orhan
E-mail: eorhan@cns.nyu.edu
Address: Center for Neural Science, 4 Washington Place, New York NY 10003

Number of pages: 43
Number of figures: 12
Number of tables: 1
Number of words (Abstract): 218
Number of words (Introduction): 496
Number of words (Discussion): 1410
January 5, 2015

Acknowledgments: WJM was supported by grant R01EY020958 from the National Eye Institute and grant
W911NF-12-1-0262 from the Army Research Office. The authors declare no competing financial interests.
1

Abstract
In natural scenes, objects generally appear together with other objects. Yet, theoretical studies of neural
population coding typically focus on the encoding of single objects in isolation. Experimental studies suggest
that neural responses to multiple objects are well described by linear or nonlinear combinations of the responses
to constituent objects, a phenomenon we call stimulus mixing. Here, we present a theoretical analysis of the
consequences of common forms of stimulus mixing observed in cortical responses. We show that some of these
mixing rules can severely compromise the brain’s ability to decode the individual objects. This cost is usually
greater than the cost incurred by even large reductions in the gain or large increases in neural variability,
explaining why the benefits of attention can be understood primarily in terms of a stimulus selection, or demixing,
mechanism rather than purely as a gain increase or noise reduction mechanism. The cost of stimulus mixing
becomes even higher when the number of encoded objects increases, suggesting a novel mechanism that might
contribute to set size effects observed in myriad psychophysical tasks. We further show that a specific form of
neural correlation and heterogeneity in stimulus mixing among the neurons can partially alleviate the harmful
effects of stimulus mixing. Finally, we derive simple conditions that must be satisfied for unharmful mixing of
stimuli.

2

Introduction
In natural vision, objects typically appear within the context of other objects rather than in isolation. It is,
therefore, important to understand how cortical neurons encode multiple objects. Experimental studies suggest
that in many cortical areas, neural responses to the presentation of multiple stimuli can be successfully described
as a linear or nonlinear combination of the responses to the individual stimuli. In area IT, for example, responses
of many individual neurons to pairs and triplets of objects are well described by the average of their responses to
individual stimuli (Zoccolan et al., 2005; Zoccolan et al., 2007). A similar weighted averaging model also provides
a good description of the responses of motion-selective MT (Recanzone et al., 1997; Britten and Heuer, 1999) and
MST neurons (Recanzone et al., 1997) to pairs of moving objects, responses of V4 neurons to composite shapes
consisting of several oriented line segments (Nandy et al., 2013), population responses in V1 to simultaneously
presented gratings (Busse et al., 2009; MacEvoy et al., 2009), and at a larger scale, fMRI responses to multiple
objects in object selective area LOC (MacEvoy and Epstein, 2009) and in V4 (Beck and Kastner, 2007).
In working memory and associative learning tasks, when multiple stimuli have to be stored in memory simultaneously, responses of single neurons in prefrontal cortex are again a potentially complex function of multiple stimuli
as well as other task parameters (Duncan, 2001; Warden and Miller, 2007; Warden and Miller, 2010). Such “mixed
selectivity” has been argued to be crucial for successful performance in context-dependent behavioral tasks (Rigotti
et al., 2010; Rigotti et al., 2013). However, mixed selectivity is not unreservedly beneficial (Barak et al., 2013). By
mapping two similar points in the input space to points that are farther apart in the output space, stimulus mixing
can make them more easily discriminable. The same, however, applies to noisy versions of the same stimulus that
one would not want to make more discriminable, thus creating a problem of generalization or robustness against
noise (Barak et al., 2013). The extent of this problem for commonly observed forms of stimulus mixing in the brain
is unknown and an analysis of what types of mixing are more or less vulnerable to this problem is lacking.
In this article, using both analytical and numerical tools, we present a systematic analysis of some common forms
of stimulus mixing observed in cortical responses with regard to their consequences for stimulus encoding in the
presence of neural variability. We show that some of these common mixing rules, such as weighted averaging,
can be profoundly harmful for stimulus encoding. Another commonly observed, divisive form of stimulus mixing
(Allman et al., 1985; Cavanaugh et al., 2002) can also be harmful for stimulus encoding, although much less so than
weighted averaging. We also derive mathematical conditions that must be satisfied for unharmful mixing of stimuli,
and provide geometric explanations for what makes particular forms of stimulus mixing more or less harmful than
others.

3

Symbol
n
Iij
f
si
∆s = |s1 − s2 |
φk
R
S
Q, Σ
α
c0
L
β
w
2
σw
U
ã
ă
N
ν
kw
J
K = Tr[JT J]

Meaning
Number of neurons per group
ij-th term of the Fisher information matrix
Column vector of the mean responses of all neurons
i-th stimulus
Angular distance between two stimuli
Preferred stimulus of k-th neuron
Correlation matrix
Diagonal matrix of standard deviations
Covariance matrix of neural responses
Gain of tuning functions (Equation 2)
Maximum noise correlation between neurons (Equation 74)
Correlation length scale parameter (Equation 74)
Scaling factor for across-group correlations (Equation 75)
Mixing weight in the linear mixing model (Equation 73)
Variance of the distribution over mixing weights
The unitary discrete Fourier transform (DFT) matrix
DFT of a vector a
Inverse DFT of a vector a
Set size (number of encoded stimuli)
Exponent in the nonlinear mixing model of Britten and Heuer (Equation 78)
Divisive normalization scaling factor (Equation 79)
Jacobian matrix for the mean responses of the neurons (Equation 53)
Total resource, i.e. the sum of the squares of the derivatives in J
Table 1: List of frequently used symbols.

Materials and Methods
Derivation of the Fisher information matrix
We use a multivariate Gaussian distribution to model neural variability. For a Gaussian distribution, the ij-th term
of the Fisher information matrix (FIM) is given by (Abbott and Dayan, 1999):

Iij

= Iij,mean + Iij,cov
=



1
∂f T −1 ∂f
−1 ∂Q −1 ∂Q
,
Q
+ Tr Q
Q
∂si
∂sj
2
∂si
∂sj

(1)

where f is a column vector of the mean responses of all neurons in the population. In the linear mixing model (see
Results), the mean response of a neuron k to a pair of stimuli (s1 , s2 ) is assumed to be a weighted average of its
mean responses to each individual stimulus alone (Equation 73). The individual tuning functions describing the
mean responses of neurons to single stimuli are assumed to be von Mises:


f (s; φ) = α exp γ[cos(s − φ) − 1] + η,

4

(2)

where α, γ and η are the tuning function parameters and φ is the neuron’s prefered stimulus. Here and in the
rest of the paper, differences between circular variables should always be understood as angular differences. The
covariance matrix Q can be expressed as Q = SRS, where S is a diagonal matrix of the standard deviations of
neural responses and R is the correlation matrix. In our problem, R has a block structure:




A
R=
B

B

A

(3)

with A representing the correlations between the neurons within the same group and B representing the across-group
correlations. We assume that within-group correlations decay exponentially with the angular difference between
the prefered stimuli of neurons:

Akm = δkm + (1 − δkm )c0 exp




|φk − φm |
−
,
L

(4)

where δ is the Kronecker delta function. Across-group correlations are simply scaled versions of the within-group
correlations:
Bkm′ = βc0 exp




|φk − φm′ |
.
−
L

(5)

The inverse of the covariance matrix is given by Q−1 = S −1 R−1 S −1 . Since S is diagonal, its inverse is straightforward. The inverse of R is less so. From Equation 3, blockwise inversion of R yields:



R−1 = 

−1

(A − BA
−1

−A

−1

−1

B)

−1

B(A − BA

−A
−1

B)

−1

B(A − BA
−1

(A − BA

B)

−1

B)

−1




.

(6)

Importantly, A and B are circulant matrices, hence they are both diagonalized in the Fourier basis. This implies
that Equation 6 can be written as:

R−1


U
=
0


(Ã − B̃ Ã−1 B̃)−1
0

−Ã−1 B̃(Ã − B̃ Ã−1 B̃)−1
U


−Ã−1 B̃(Ã − B̃ Ã−1 B̃)−1  U ∗

0
(Ã − B̃ Ã−1 B̃)−1


0
,
U∗

(7)

√
where U is the unitary discrete Fourier transform matrix with entries Ukj = exp(−2πikj/n)/ n (where n is the
number of neurons in each group), U ∗ is its conjugate transpose, and Ã and B̃ are diagonal matrices of eigenvalues
of A and B respectively which can be found by taking the discrete Fourier transforms (DFT) of the first columns
of A and B.
Let us denote ã = diag(Ã) and b̃ = diag(B̃) to be the diagonals of Ã and B̃. Note that because Ã and B̃ are

5

diagonal matrices:


(Ã − B̃ Ã−1 B̃)−1

1

= Ckk =

ãk −

kk

b̃2k

=

ãk

ãk
ã2k

− b̃2k

.

(8)

Similarly:


−1

− Ã

−1

B̃(Ã − B̃ Ã

−1

B̃)



kk

= Dkk = −

b̃k
ã2k

− b̃2k

.

(9)

Poisson-like noise
We first derive Imean and Icov for a Poisson-like noise model where the mean responses of neurons are equal to their
variance.
Derivation of Imean : We can write down the first term of the Fisher information matrix as follows:

Iij,mean


∂f T −1 ∂f
∂f T S1
=
Q
=

∂si
∂sj
∂si 0

−1 
0  U
 
0
S2



0  C

D
U


D  U ∗

0
C


0  S1

0
U∗

−1

0

S2

∂f
,
∂sj

(10)

where S1 and S2 are diagonal matrices of the standard deviations σk of the responses of neurons in the first and
√
second group respectively. For Poisson-like noise, σk = fk . In the following we denote by gi the vector whose
k-th entry is equal to σk−1 ∂fk /∂si , i.e. the derivative of the k-th neuron’s mean response with respect to si divided
by the standard deviation of its variability, where k ranges only over the neurons in the first group. Similarly, we
denote by hi the vector whose k-the entry is equal to σk−1 ∂fk /∂si , but where k now ranges over the neurons in the
second group only.
With this notation, we can re-write Equation 10 as follows:
Iij,mean = (g̃i )T C ğj + (h̃i )T Dğj + (g̃i )T Dh̆j + (h̃i )T C h̆j ,
where g̃i =

(11)

√
√
nU gi and ği = U ∗ gi / n represent the DFT and the inverse DFT of gi respectively. Similarly, h̃i and

h̆i are the DFT and the inverse DFT of hi . Recall also that C and D are diagonal matrices defined in Equations 8
and 9 respectively. Note that there are different conventions on how to compute the DFT and the inverse DFT;
our usage is consistent with Matlab’s implementation of fft and ifft functions.
The scaling of Iij,mean with n is similar to the corresponding scaling relationship in the case of the encoding of a
single stimulus analyzed previously in Sompolinsky et al. (2001) and in Ecker et al. (2011): for a homogeneous
population, Equation 11 saturates to a finite value in the presence of noise correlations (c0 6= 0), but diverges for
an independent population (c0 = 0). A detailed analysis of the asymptotic behavior of Imean is provided below for

6

the simpler case of additive noise.
Derivation of Icov : We now derive the second term of the FIM, Iij,cov . We first recall that Q = SRS and then
note that

∂Q
∂si

Iij,cov

= ∂i SRS + SR∂i S where we use the shorthand notation ∂i S to denote
=
=

∂S
∂si .

We then have:







1
1
∂Q −1 ∂Q
= Tr S −1 R−1 S −1 ∂i SRS + S −1 ∂i S S −1 R−1 S −1 ∂j SRS + S −1 ∂j S
Q
Tr Q−1
2
∂si
∂sj
2


 −1







1
Tr S ∂i SS −1 ∂j S + Tr S −1 RS −1 ∂i SR∂j S + Tr S −1 RS −1 ∂j SR∂i S + Tr S −1 ∂i SS −1 ∂j S , (12)
2

where in the second line we used the fact that the trace operator is invariant under cyclic permutations of the
products of matrices. We now note that S −1 ∂i SS −1 ∂j S is a diagonal matrix and its trace is given by:
n−1
X
X

 n−1
Tr S −1 ∂i SS −1 ∂j S =
pik pjk +
tik tjk ,
k=0

(13)

k=0

where we introduced the notation pi for the vector consisting of the diagonal entries of S1−1 ∂i S1 and ti for the
diagonal of S2−1 ∂i S2 . For the second term on the right hand side in Equation 12, we have:


Tr S −1 RS −1 ∂i SR∂j S







= Tr U ∗ V1i U ÃU ∗ V1j U C + Tr U ∗ V1i U B̃U ∗ V2j U D + Tr U ∗ V2i U B̃U ∗ V1j U D


(14)
+ Tr U ∗ V2i U ÃU ∗ V2j U C .

where we denote V1i = S1−1 ∂i S1 and V2i = S2−1 ∂i S2 . Note that the diagonals of V1i and V2i are the vectors pi and ti .
Considering the first term on the right hand side in Equation 14, we can write it as follows:


Tr U ∗ V1i U ÃU ∗ V1j U C

=

X

∗
∗
[V1j ]oo Uok Ckk
Ukl
[V1i ]ll Ulm Ãmm Umo

k,l,m,o

=

1 X i
p̃m−k Ãmm (p̃jm−k )∗ Ckk
n2
m,k

=

1
n2

n−1
X
k=0



Ckk conv(p̃i ◦ (p̃j )∗ , ã) k ,

(15)

where ◦ is the Hadamard (element-wise) product and conv(·, ·) denotes the circular convolution of two vectors.
From the first to the second line above, we simply used the definition of the DFT of a vector. The third line follows
from the definition of the circular convolution. For vectors, we use (·)∗ to denote element-wise conjugation (without
transposition). We can express the other terms in Equation 14 in a similar fashion. Thus, Equation 14 becomes:



Tr S −1 RS −1 ∂i SR∂j S

 n−1
n−1
X




1 X
i
j ∗
Ckk conv(p̃i ◦ (t̃j )∗ , b̃) k
+
conv(p̃
◦
(p̃
)
,
ã)
C
=
kk
k
n2
k=0
k=0

n−1
n−1
X
X




Ckk conv(t̃i ◦ (t̃j )∗ , ã) k .
Dkk conv(t̃i ◦ (p̃j )∗ , b̃) k +
+
k=0

k=0

7

(16)

We can derive a similar expression for the third term in Equation 12:
 n−1
n−1
X




1 X
j
i ∗
conv(p̃
◦
(p̃
)
,
ã)
+
Ckk conv(p̃j ◦ (t̃i )∗ , b̃) k
C
kk
2
k
n
k=0
k=0

n−1
n−1
X
X




Ckk conv(t̃j ◦ (t̃i )∗ , ã) k .
+
Dkk conv(t̃j ◦ (p̃i )∗ , b̃) k +



Tr S −1 RS −1 ∂j SR∂i S

=

(17)

k=0

k=0

We now note that the first term on the right hand side of Equation 12 is equal to the last term. Thus, putting it
all together, Iij,cov can be written as:

Iij,cov

=

+

n−1
X

n−1
X

 n−1
n−1
X




1 X
+ 2
+
Ckk conv(p̃i ◦ (t̃j )∗ , b̃) k
Ckk conv(p̃i ◦ (p̃j )∗ , ã) k +
2n
k=0
k=0
k=0
k=0

n−1
n−1
X
X




Ckk conv(t̃i ◦ (t̃j )∗ , ã) k
Dkk conv(t̃i ◦ (p̃j )∗ , b̃) k +
pik pjk

tik tjk

k=0

k=0

+

+

1
2n2

n−1
X
k=0

 n−1
X
k=0

Dkk



n−1
X




Ckk conv(p̃j ◦ (t̃i )∗ , b̃) k
Ckk conv(p̃j ◦ (p̃i )∗ , ã) k +
k=0

j

i ∗


conv(t̃ ◦ (p̃ ) , b̃) k +

n−1
X

Ckk

k=0





conv(t̃ ◦ (t̃ ) , ã) k .
j

i ∗

(18)

As for Iij,mean , the scaling of Iij,cov with n is identical to the corresponding scaling relationship studied in Ecker et
al. (2011) for the case of the encoding of a single stimulus: asymptotically Iij,cov scales linearly with n regardless
of the amount of correlations in the population.
Effects of heterogeneity in mixing weights in the linear mixing model on Imean and Icov : For Poisson-like
noise, it is difficult to analytically quantify the effect of heterogeneity in mixing weights on Imean . Considering a
single neuron k, when there is heterogeneity in mixing weights, we have:
(wi + δwi )f ′ (si ; φk )
gki = σk−1 ∂fk /∂si = p
,
(wi + δwi )f (si ; φk ) + (w−i + δw−i )f (s−i ; φk )

(19)

where we use δwi and δw−i to denote the random fluctuations around the mean mixing weights (the subscript −i
indicates the stimulus that is not the i-th stimulus). In this expression, it is not possible to separate out the effect
of mixing weights, as it is in the case of the encoding of a single stimulus studied in Ecker et al. (2011). This makes
it difficult to compute expectations over the random fluctuations of mixing weights.
Similarly, unlike in Ecker et al. (2011), heterogeneity in mixing weights also affects the Icov term in our model.
Again, this is because the k-th diagonal entry of S1−1 ∂i S1 is of the following form (a similar expression holds for

8

the diagonal entries of S2−1 ∂i S2 ):
pik =

1
(wi + δwi )f ′ (si ; φk )
.
2 (wi + δwi )f (si ; φk ) + (w−i + δw−i )f (s−i ; φk )

(20)

The weights in the numerator and the denominator do not cancel in this expression, as they do in the case of
the encoding of a single stimulus (Ecker et al., 2011). Unfortunately, because the random fluctuations appear in
divisive form in the above expressions (and inside a further square root nonlinearity in the expression for gki ), it
is difficult to quantify the effect of heterogeneity in mixing weights on Imean and Icov . Moreover, the asymptotic
variance of the optimal estimator, in its turn, is related to the terms of the FIM through an additional nonlinearity
(Equation 21 below).
Because of these difficulties, we resort to sampling to quantitatively account for the effect of heterogeneity in mixing
weights on Imean and Icov and on the asymptotic variance of the optimal estimator. In particular, we draw the
2
mixing weights of the neurons independently from beta distributions with mean w1 or w2 and variance σw
. The

weights are then plugged in Equation 11 for Imean and in Equation 18 for Icov . A mathematical analysis of the
effect of heterogeneity in mixing weights on the FIM is given below for the simpler case of additive noise.
Asymptotic variance and correlation of optimal unbiased estimates: After computing Imean and Icov , we
find the asymptotic variance of the optimal unbiased estimates of the individual stimuli and the correlation between
the two estimates as follows:

Var(ŝ1 |s) =
Corr(ŝ1 , ŝ2 |s) =

I11
I22
= 2
2
I11 I22 − I12 I21
I11 − I12
I12
I12
−√
=−
,
I11
I11 I22

(21)
(22)

where I11 and I12 can be written as the sum of the mean and covariance contributions:

I11

=

I11,mean + I11,cov

(23)

I12

=

I12,mean + I12,cov .

(24)

We note that because the FIM is symmetric, I12 = I21 , and for large n, I12 depends only on ∆s = |s1 − s2 | and
does not depend on the stimuli themselves. Moreover, because of the circular symmetry of the stimuli considered
in this paper, I11 = I22 , and I11 does not depend on the stimuli for large n. In the following, we will refer to the
inverse of the asymptotic variance as encoding precision or, more commonly, as encoding accuracy.
Scaling of the asymptotic variance with the mean response gain α and the Fano factor: In the expression
√
√
for Imean (Equation 11), gi ∝ α (this is because each of its entries, σk−1 ∂fk /∂si , scales as α for Poisson-like

9

noise). Similarly, the other h and g vectors also scale as

√
α. Thus, each of the summands in Equation 11 scales as

α, and therefore the whole expression scales as α. It is easy to see that Icov is independent of the gain (to see this,
note that the entries of the p and t vectors in Equation 18 are of the form shown in Equation 20 in which the gain
term cancels). Hence from Equation 21, we see that the asymptotic variance should have a weaker gain-dependence
than α−1 . This means that a doubling of the gain leads to a less than twofold increase in encoding precision. By
a similar reasoning, it is easy to show that the asymptotic variance has a dependence on the Fano factor, FF, that
is weaker than FF (because Imean scales as FF−1 and Icov is independent of FF). Again, this implies that halving
the Fano factor should lead to a less than twofold increase in encoding precision.

Additive noise: finite n
If the noise is assumed to be additive, the covariance matrix Q becomes stimulus-independent; hence, Icov = 0.
Furthermore, the expression for Imean (Equation 11) can be written in a more transparent form in this case, because
it becomes possible to separate out the effect of mixing weights in the g̃ and h̃ vectors. Each of the terms on the
right-hand side of Equation 11 can be expressed as a sum over different Fourier modes. Considering the I12,mean
term first, we have:
I12,mean

=
=

=



1
(g̃1 )T C(g̃2 )∗ + (h̃1 )T D(g̃2 )∗ + (g̃1 )T D(h̃2 )∗ + (h̃1 )T C(h̃2 )∗
n


n−1
n−1
X
X
1
∗
∗
2
2
D
m̃
(
m̃
exp(−2πikδ/n))
2w
w
C
m̃
(
m̃
exp(−2πikδ/n))
+
(w
+
w
)
1
2
kk
k
k
kk
k
k
1
2
nσ 2
k=0
k=0


n−1
n−1
X
X
1
2
2
2
2
|
m̃
|
D
cos(2πkδ/n)
,
2w
w
|
m̃
|
C
cos(2πkδ/n)
+
(w
+
w
)
1
2
k
kk
k
kk
1
2
nσ 2

(25)

k=0

k=0

where σ 2 is the variance of the additive noise, m is the vector of the derivatives of the von Mises tuning functions
(Equation 2) with respect to s and δ is an integer that expresses the difference between the two stimuli s1 and
s2 in terms of the number of tuning function centers that separate them. Because of the circular nature of the
stimuli, the Fisher information only depends on the angular distance between the stimuli. Hence, without loss of
generality, we assume s1 = 0 and thus the derivatives in m are all evaluated at s = 0. In deriving Equation 25, we
used the fact gi can be expressed as a scaled circular shift of gj by δ and similarly as a circular shift of hj . In the
Fourier domain, a circular shift corresponds to the multiplication of the k-th Fourier mode by exp(−2πikδ/n). In
Equation 25, we made use of the fact that m̃k = m̃N −k (due to the circular nature of the stimulus space) and the
identity exp(2πikδ/n) + exp(2πi(n − k)δ/n) = 2 cos(2πkδ/n).
Similarly, for the I11,mean term, we derive the following expression:

I11,mean



n−1
n−1
X
X
1
2
2
2
2
=
|m̃k | Ckk + 2w1 w2
(w1 + w2 )
|m̃k | Dkk .
nσ 2
k=0

k=0

10

(26)

In addition, I22,mean = I11,mean and I21,mean = I12,mean as usual. Finally, the asymptotic variance and correlation
of estimates can be computed using Equations 21 and 22 and recalling that for additive noise Icov = 0.
T

∂f
For the additive-noise model, if the neurons are assumed to be independent, we can also write I11 = σ −2 ∂f
∂s1 ∂s1 =
 2
T
 ∂f 
∂f
2
σ −2  ∂s
 and I12 = σ −2 ∂f
∂s1 ∂s2 where σ denotes the common noise variance. Plugging these in Equation 21, we
1

obtain the following proportionality relation for the asymptotic variance of the optimal estimator:
 2
 ∂f 
 ∂s1 
Var(ŝ1 |s) ∝  4 
T
 ∂f 
 ∂s1  − ∂f
∂s1

∂f
∂s2

2 .

(27)

Inverting this proportionality yields Equation 77 for the encoding precision, which is used below to provide a
qualitative explanation for the stimulus-dependence of encoding accuracy.
Effect of heterogeneity in mixing weights on Imean : Heterogeneity in the mixing weights can be accounted
for by writing gi = (w + δw) ◦ ḡi where we separated out the effect of mixing weights on gi (note that we can
do this for additive noise, but not for Poisson-like noise). In this expression, w denotes the mean weight that is
constant across the neurons in the group and δw represents the stochastic component of the weight vector that is
2
different from neuron to neuron. We denote the variance of the weights across the neurons by σw
. We first note

that heterogeneity in the mixing weights only affects the C terms in I11,mean in Equation 11, because the weight
fluctuations are assumed to be uncorrelated across different groups and across the same group but for different
stimuli. With this in mind, from Equation 11, we have:
het
hI11,mean
i =

=

hom
I11,mean
+ h[δw1 ◦ ḡ1 ]T U CU ∗ [δw1 ◦ ḡ1 ]i + h[δw2 ◦ h̄1 ]T U CU ∗ [δw2 ◦ h̄1 ]i
X
X
hom
∗
∗
I11,mean
+h
[δw1 ◦ ḡ1 ]Ti Uij Cjj Ujk
[δw1 ◦ ḡ1 ]k i + h
[δw2 ◦ h̄1 ]Ti Uij Cjj Ujk
[δw2 ◦ h̄1 ]k i
i,j,k

i,j,k

=

X
X
∗
∗
hom
[δw2 ◦ h̄1 ]i i
[δw1 ◦ ḡ1 ]i i + h [δw2 ◦ h̄1 ]Ti Uij Cjj Uji
I11,mean
+ h [δw1 ◦ ḡ1 ]Ti Uij Cjj Uji

=

hom
I11,mean

=

hom
I11,mean
+

i,j

i,j

X
X
1 X
1 X
1
+
Cjj h (δw1,i ḡi 1 )2 i +
Cjj h (δw2,i h̄i )2 i
n j
n
i
i
j
2 X
 X 1 2
σw
1 
(g¯i ) + (h̄i )2 ,
Cjj
n
i
j

(28)

hom
where I11,mean
is the Fisher information for a homogeneous population derived above. In terms of O(1) quantities,
het
we can express hI11,mean
i as follows:
het
hom
2
M,
hI11,mean
i = I11,mean
+ nσw

(29)

P
P
1
where M = (1/n2 )( j Cjj )( i (ḡi 1 )2 + (h̄i )2 ) ∼ O(1). Thus, the effect of heterogeneity is linear in the number of

neurons per group. This scaling is the same as the corresponding scaling relationship derived previously in Ecker
et al. (2011) for a neural population encoding a single stimulus. In Ecker et al. (2011), it is further shown that

11

2
the same scaling holds for Poisson-like noise as well, but the variance σw
should, in that case, be interpreted as the
√
variance of α, i.e. the square root of the gain rather than the variance of the gain itself.

Additive noise: large n
In this section, we give a detailed analysis of the additive noise scenario in the limit of large n. We first derive
explicit expressions for the C and D matrices (Equations 8 and 9) in the large n limit. To do this, we first need to
derive ã and b̃. It is convenient in this case to use indices ranging from −(n − 1)/2 to (n − 1)/2. For the derivations
to be presented in this section, we adopt the following notation: ω = 2π/n, λ = exp(−ω/L) and τ = exp(−iωjk).
For ã, we have:
(n−1)/2

ãk

X

=

j=−(n−1)/2

= 1 + c0

X


|ωj| 
δj + (1 − δj )c0 exp −
exp(−iωjk)
L

exp(−

j6=0

|ωj|
) exp(−iωjk)
L

(n−1)/2

= 1 + c0

exp(−

X

(λτ )j + (λτ −1 )j .

j=1

(n−1)/2

= 1 + c0


ωj 
) exp(−iωjk) + exp(iωjk)
L

X

(30)

j=1

We now take the sum of the two geometric series in the last equation. Denoting Γ =

ãk = 1 + c0



n−1
2


1 − (λτ −1 )Γ
1 − (λτ )Γ
−2 .
+
1 − λτ
1 − λτ −1

+ 1, we have:

(31)

After a little bit of algebra and rearranging, we obtain:

ãk = 1 + c0

λΓ (λ cos((Γ − 1)ωk) − cos(Γωk)) + λ cos(ωk) − λ2
,
1 − 2λ cos(ωk) + λ2

(32)

where we made repeated use of the identity exp(−iθ) + exp(iθ) = 2 cos(θ). We now consider the large n value of the
expression above by keeping only terms of leading order in ω = 2π/n. We recall that exp(x) = 1 + x + x2 /2 + O(x3 )
and cos(x) = 1 − x2 /2 + O(x4 ). The final result is:
ãk = 1 +

c0 n 1 − exp(−π/L)(−1)k
.
Lπ
k 2 + L−2

12

(33)

This expression is the same as the one derived in Sompolinsky et al. (2001) for the same type of limited-range
correlation structure. Proceeding similarly for b̃k , we find the following expression:
h
n 1 − exp(−π/L)(−1)k i
.
b̃k = βc0 1 +
Lπ
k 2 + L−2

(34)

Plugging these expressions in Equations 8 and 9 for Ckk and Dkk and considering the large n limit again, we arrive
at the following large n expressions for Ckk and Dkk :
1
c0 (1 − β 2 )Φk
β
=−
,
c0 (1 − β 2 )Φk

Ckk =
Dkk

(35)
(36)

where we denote:
Φk =

n 1 − exp(−π/L)(−1)k
.
Lπ
k 2 + L−2

(37)

By plugging these large n expressions for Ckk and Dkk in Equations 25 and 26, we obtain the following large n
expressions for I11,mean and I12,mean for the case of additive noise:

I11,mean

=

n−1
w12 + w22 − 2βw1 w2 X |m̃k |2
nσ 2 c0 (1 − β 2 )
Φk

(38)

k=0

I12,mean

=

n−1
2w1 w2 − β(w12 + w22 ) X |m̃k |2 cos(2πkδ/n)
.
nσ 2 c0 (1 − β 2 )
Φk

(39)

k=0

As in Sompolinsky et al. (2001), it can be shown that I11,mean and I12,mean saturate to finite values when c0 6= 0
and diverge for c0 = 0. To see this, write Ψk = Φk /n. Similarly, write µk = m̃k /n. Then, considering I11,mean as
an example, we have :
I11,mean =

n−1
w12 + w22 − 2βw1 w2 X |µk |2
.
σ 2 c0 (1 − β 2 )
Ψk

(40)

k=0

We note that Ψk ∼ O(k −2 ). Thus, if the power spectrum |µk |2 decays sufficiently rapidly with k, e.g. |µk |2 ∼ O(k −p )
with p > 3 (meaning that the tuning function derivatives are sufficiently smooth), the sum above remains O(1) for
c0 6= 0. An identical argument can be made for the sum in I12,mean to show that I11,mean and I12,mean are both
O(1) for c0 6= 0 assuming sufficiently smooth tuning function derivatives.
The effect of stimulus mixing on encoding accuracy
We can now ask what the effects of changing various parameters are on encoding accuracy in the case of additive
noise. We first consider the effect of stimulus mixing. Assuming w1 = w and w2 = 1 − w and ignoring a common

13

pre-factor which is always positive, we have (from Equations 25 and 26):

I11,mean

∝

G ≡ (2w2 − 2w)(X − Y ) + X

(41)

I12,mean

∝

H ≡ (2w2 − 2w)(Z − T ) + Z

(42)

Var(ŝ1 |s) ∝

G
,
G2 − H 2

(43)

where we use the following abbreviations:

X

Y

Z

T

=

=

=

=

n−1
X

k=0
n−1
X

k=0
n−1
X

k=0
n−1
X
k=0

|m̃k |2 Ckk

(44)

|m̃k |2 Dkk

(45)

|m̃k |2 Dkk cos(2πkδ/n)

(46)

|m̃k |2 Ckk cos(2πkδ/n).

(47)

We now show that for the interval 0.5 < w < 1, the variance (Equation 43) is a decreasing function of w. Therefore,
stimulus mixing always reduces encoding accuracy and the stronger the mixing, the lower the encoding accuracy.
For this purpose, it is sufficient to consider the sign of the derivative of Equation 43 with respect to 2w2 − 2w only,
because using the chain rule, the derivative with respect to w can be obtained by multiplying with 4w − 2 which is
always positive in the interval 0.5 < w < 1. Denoting the derivatives with respect to 2w2 − 2w with a prime, we
first observe the following: (i) G′ = X − Y ≥ 0; (ii) H ′ = Z − T ; (iii) G′ > |H ′ | and (iv) G > 0. These are all easy
to verify using the definitions in Equations 44-47 and the large n expressions for C and D (Equations 35-36).
Now, taking the derivative of the variance (Equation 43) and ignoring the denominator in the derivative which is
always non-negative, we have:

′
Var(ŝ1 |s)

∝

G′ (G2 − H 2 ) − (G2 − H 2 )′ G

=

G′ (G2 − H 2 ) − (2GG′ − 2HH ′ )G

=

−G′ (G2 + H 2 ) + 2GHH ′

<

−|H ′ |(G2 + H 2 ) + 2G|H||H ′ |

=

−|H ′ |(G − |H|)2 ≤ 0

Thus, the derivative is always negative and the variance is a decreasing function of w.

14

(48)

The effect of noise correlations on encoding accuracy
In this section, we separately consider the effects of the three parameters determining the shape and the magnitude
of noise correlations in the model: c0 , β, and L. Our strategy is to consider the derivative of the variance with
respect to the parameter of interest and look at the sign of the derivative for different settings of the parameters.
When the derivative is negative, the variance is a decreasing function of the parameter of interest; whereas a
positive derivative means that the variance is an increasing function of the parameter of interest. We use the large
n expressions for the matrices C and D (thus for I11,mean , I12,mean and the asymptotic variance as well) in the
analyses to be presented below.
The effect of c0 : We first consider the effect of changing c0 , the maximum correlation between any two neurons
in the population. The analysis of the effect of c0 is easier than the other two cases, because from the large n
expressions for I11,mean and I12,mean (Equations 38 and 39), we see that the effect of c0 completely factorizes in
these expressions and that they are both proportional to 1/c0 . Thus, the variance is proportional to c0 . Hence,
increasing c0 is always harmful for encoding accuracy. This result is consistent with earlier results for homogeneous
neural populations with additive noise encoding a single stimulus (Sompolinsky et al., 2001; Ecker et al., 2011).
The effect of β: We next consider the effect of β, the scaling parameter for across-group correlations. β appears in
a factorized form in I11,mean and I12,mean in the large n limit (Equations 38 and 39) and therefore the derivative of
the variance with respect to β is relatively straightforward to compute. Figure 3A shows the sign of ∂Var(ŝ1 |s)/∂β
for different values of w, ∆s and β.
The effect of L: To investigate the effect of correlation length scale L on encoding accuracy, we proceed similarly.
In Figure 3B, we plot the sign of ∂Var(ŝ1 |s)/∂L for different values of w, ∆s and L. This figure suggests that
there is a critical value of L around L ≈ 0.6 below which the derivative is always positive, suggesting that it is
beneficial to decrease L in this regime. On the other hand, above the critical value of L, the derivative is always
negative, suggesting that it is beneficial for encoding accuracy to increase L in this regime. The critical value of L
decreases with the concentration parameter of the tuning functions γ (Equation 2), but does not depend on any
other parameters of the encoding model. This type of threshold-like behavior for the effect of correlation length
scale L is again consistent with a similar behavior reported in Sompolinsky et al. (2001).

Optimal linear estimator (OLE)


For the OLE, we first map the stimuli to Cartesian coordinates, x = cos(s1 ) sin(s1 ) cos(s2 ) sin(s2 ) , and calculate
the weight matrix W that minimizes the mean squared error hkx − x̂k2 i between the actual and estimated stimuli.

T
The optimal weight matrix is given by: W = Q−1
rr Qxr where Qrr = hr ri is the covariance matrix of the responses

15

and Qxr = hrT xi is the covariance between the stimuli and neural responses (here h·i denotes an average over both
stimuli x and noisy neural responses r). These averages were computed over 8192 random samples of x (generated
from uniformly distributed s1 and s2 values) and r. The performance of the estimator was then measured by
numerically estimating the mean squared error (MSE) for stimulus pairs of the form (−s, s) with s ∈ [0, π/2].

The nonlinear mixing rule of Britten and Heuer (1999)
For the nonlinear mixing model of Britten and Heuer (1999), the mean response, fk , of a neuron to a pair of stimuli
(s1 , s2 ) is given by Equation 78. We include a factor of 2 in the denominator in Equation 78 to make the neural
gains approximately independent of ν. Britten and Heuer’s original equation does not include this factor. The
derivative of the mean response with respect to the stimulus si is given by:
 1−ν

∂fk
= a2−1/ν f (s1 ; φk )ν + f (s2 ; φk )ν ν f (si ; φk )ν−1 f ′ (si ; φk ),
∂si

(49)

where f denotes the von Mises tuning function (Equation 2) and f ′ its derivative. Given the mean responses and
their derivatives with respect to each stimulus, expressions similar to Equations 11 and 18 can be used to compute
the Fisher information matrix for the nonlinear mixing model of Britten and Heuer (1999), taking into account that
we assume an unsegregated population for this case (see Results).

A divisive form of stimulus mixing
In the divisively normalized stimulus mixing model, the response of a neuron in the first group is described by
Equation 79 below. Responses of neurons in the second group are similar, but with the roles of s1 and s2 reversed
in the right hand side. In Equation 79, f (s; φ) is the von Mises tuning function defined in Equation 2 and the
weighting profile w(φk , φk′ ) is a normalized von Mises function given by:
exp(γw [cos(φk − φk′ ) − 1])
w(φk , φk′ ) = P
.
k′ exp(γw [cos(φk − φk′ ) − 1])

(50)

The derivatives of the mean response fk with respect to s1 and s2 are given by:
∂fk
∂s1
∂fk
∂s2

2f (s1 ; φk )f ′ (s1 ; φk )
P
∆ + kw k′ w(φk , φk′ )f (s2 ; φk′ )2
P
2f (s1 ; φk )2 k′ w(φk , φk′ )f ′ (s2 ; φk′ )f (s2 ; φk′ )
,
= −
2

P
∆ + kw k′ w(φk , φk′ )f (s2 ; φk′ )2
=

where f ′ denotes the derivative of the von Mises tuning function.

16

(51)
(52)

Equations 11 and 18 for the Fisher information matrix are still valid for the divisively normalized mixing model
with the difference that the mean responses of neurons and their derivatives with respect to the two stimuli are
computed according to Equations 79, 51 and 52 respectively.
Figure 10 shows the effect of varying the divisive normalization scaling factor kw and the across-group neural
correlations β on encoding accuracy for both the divisively normalized mixing model and a model where the offdiagonal terms in the Fisher information matrix (Iij with i 6= j) were set to zero, but the diagonal terms were the
same as in the divisively normalized mixing model. As explained in the Results section, this latter model eliminates
stimulus mixing, but preserves the neuron-by-neuron response gains in the divisively normalized model.

Stimulus mixing is not always harmful for encoding accuracy
We first analyze the general stimulus mixing problem with a two-dimensional toy model. We imagine two “neurons”
mixing the two stimuli s1 , s2 according to f1 (s1 , s2 ) and f2 (s1 , s2 ) respectively. The responses of the two neurons
are given by r1 = f1 (s1 , s2 ) + ε1 and r2 = f2 (s1 , s2 ) + ε2 , where ε1 and ε2 are Gaussian random variables with
variance σ 2 and correlation ρ. Thus, in the following analysis, we assume stimulus-independent additive noise. We
denote the Jacobian matrix for the mean responses of the neurons by J:



∂f1
 ∂s1

J=

∂f2
∂s1



∂f1
∂s2 
∂f2
∂s2

.

(53)

As explained in the Results section, one can think of J as a mixing matrix describing the sensitivity of each neuron
to changes in each stimulus. The Fisher information matrix is given by IF = JT Σ−1 J where Σ is the covariance
matrix of the response noise. The inverse of IF gives the asymptotic covariance matrix of the maximum likelihood
estimator. To find the optimal mixing matrix J, we minimize the trace of IF−1 , i.e. Tr[IF−1 ] = Tr[J−1 ΣJ−T ] with
respect to J, subject to the constraint that the sum of the squares of the derivatives in J be a finite constant K,
i.e. Tr[JT J] = K.
We find the optimal J by the method of Lagrange multipliers. The objective function is given by:
L = Tr[J−1 ΣJ−T ] + λ(Tr[JT J] − K)

(54)

and the required derivatives are as follows:
∂L
∂J
∂L
∂λ

=

−2J−T J−1 ΣJ−T + 2λJ

(55)

=

Tr[JT J] − K.

(56)

17

Setting these to zero and rearranging, we get the following equations:
JJT JJT

=

Y2 = λ−1 Σ

(57)

Tr[JT J] =

Tr[Y] = K,

(58)

where we denote JJT by Y. By taking the eigendecomposition of the right-hand side of Equation 57, we obtain:

Y=

√
p p
λ−1 Pdiag( λ1 , λ2 )P−1 ,

(59)

where P is the matrix of eigenvectors of Σ and λ1 and λ2 are its eigenvalues. Because the trace of a matrix is equal
q
q
√
√
√
to the sum of its eigenvalues, from Equation 58, we get λλ1 + λλ2 = K or λ−1 = K/( λ1 + λ2 ). Plugging
this in Equation 59:

p p
K
√ Pdiag( λ1 , λ2 )P−1 .
Y= √
λ1 + λ2

(60)

The matrix of eigenvectors and the eigenvalues of the response covariance matrix Σ are given by:




1 1


P = ρ

−1 1
λ1 = σ 2 (1 − ρ),

(61)

λ2 = σ 2 (1 + ρ).

(62)

Plugging these expressions in Equation 60 and simplifying, we get:

Y=



K

2 1−

1−

1
√

1−ρ2
ρ

√



1−ρ2
ρ


1

.

(63)

Now, the Y matrix can be written as:




k∇f1 k2
∇f1 



T
T
Y=
 ∇f1 ∇f2 = 
∇f1 · ∇f2
∇f2

∇f1 · ∇f2
k∇f2 k

2




,

(64)

where ∇f1 and ∇f2 denote the gradients of f1 and f2 respectively. The cosine of the angle θ∗ between these two
gradients is given by:
p
1 − 1 − ρ2
∇f1 · ∇f2
=
.
(65)
cos θ =
k∇f1 kk∇f2 k
ρ
p
The optimal solution is thus to set the gradients to have equal norm ( K/2) and the angle between them to θ∗
∗

with cos θ∗ as given in Equation 65. Because cos is an even function, θ∗ and −θ∗ are both solutions. Figure 12A
18

plots the positive solution as a function of ρ.
The solution of the two-dimensional toy model can be readily generalized to models with more than two neurons
under certain assumptions. Consider, for instance, two groups of neurons with responses given respectively by:

f1,k (s1 , s2 ) =

w1 f (s1 ; φk ) + w2 f (s2 ; φk )

(66)

f2,k′ (s1 , s2 ) =

w3 f (s1 ; φk′ ) + w4 f (s2 ; φk′ ).

(67)

This model is very similar to the linear mixing model analyzed in detail in this paper (see Results), with the only
difference being that the restriction for the weights to be positive and symmetric between the groups is now lifted.
Assuming s1 ≈ s2 = s and an additive noise model, the problem of finding the optimal weights w1 , w2 , w3 and w4
subject to a total power constraint on the derivatives, i.e.:
(w12 + w22 + w32 + w42 )

X

f ′ (s; φk )2 = P

(68)

k

can be directly translated into the two-dimensional problem with the following transformation:

J

Σ−1

K





 w1 w2 

= 




= 
=

w3 w4

f′
0

P


f′
 −1 
Q 
′
0
f
0



(69)




P
,
′ (s; φ )2
f
k
k

0
f

′

T



(70)

(71)

where f ′ is a row vector of the derivatives df (s; φk )/ds, 0 is a row vector of zeros and Q is the covariance structure
of the neurons (e.g., Q = SRS with R as given in Equation 3). In particular, for uncorrelated neurons (diagonal




Q), we find that the optimal solution is to set the weight vectors w1 , w2 and w3 , w4 such that they have equal

norm and are orthogonal to each other.

For the more general case of n neurons encoding two stimuli, as far as we know, there is no closed-form solution for
the optimal mixing matrix subject to a constraint on the total power of the derivatives. We thus solve this more
general problem numerically. Figure 11 shows three distinct solutions for n = 16 with both a diagonal covariance
matrix (Figure 11A) and a limited-range correlation structure (Figure 11B) as in Equation 4 with c0 = 0.3 and
L = 2.
Similarly, for the linear encoding model (Equations 66-67), when s1 6≈ s2 , it does not seem possible to reduce the
problem of finding the optimal mixing weights w1 , w2 , w3 and w4 to our two-dimensional toy problem. Thus,
19

we solve this optimization problem numerically as well. Because IF−1 depends on ∆s, we minimize the following
objective function in this case:
hTr[IF−1 (∆s)]i ∝

Z

Tr[IF−1 (∆s)]d∆s,

(72)

where the integral is approximated by the average of Tr[IF−1 (∆s)] over 21 ∆s values uniformly spaced in the interval
[0, π]. The total resource K is assumed to be equal to the number of neurons and the noise is assumed to be additive
(and the noise variance identical for all neurons) both in the arbitrary encoding model and in the linear encoding
model. All numerical optimization problems were solved using the genetic algorithm routines in Matlab’s Global
Optimization Toolbox.

Parameter values for the results reported in the figures
For our main results, reported in Figures 1, 2, 4, 7, 8, 9, 10, we use a Poisson-like noise model and a limited-range
noise correlation structure with parameters c0 = 0.3 and L = 2, which is consistent with the small but broad
noise correlations typically observed in the visual cortical areas (Cohen and Kohn, 2011). For the tuning function
parameters, we again use values broadly consistent with response properties of neurons in the visual cortex: α = 20,
γ = 2, η = 0 (Ecker et al., 2011). For convenience, we assume tuning function centers that are uniformly spaced
between 0 and 2π.
In Figures 3, 11, 12, we use an additive Gaussian noise model. The additive-noise assumption is needed to establish
the analytic results regarding the effects of varying the parameters of the encoding model on encoding accuracy, as
well as for the solution of the optimal mixing model presented at the end of the Results section.
For the number of neurons, we used as large a number of neurons as possible. Specifically, in Figures 1, 2, 4, 6, 10,
we use n = 4096 (number of neurons per group). In Figure 9, since there is only a single, unsegregated population,
we use n = 8192. Due to computational costs, we had to use a smaller number of neurons per group in Figures 7
and 8: in Figure 7, n = 1024 (note this means that the total number of neurons is 2048 for set size N = 2 and 6144
for N = 6) and in Figure 8, n = 1024.
Other parameter values specific to each figure are as follows: in Figure 9, a = 1, b = 0 (parameters of the nonlinear
mixing model of Britten and Heuer). In Figure 10, ∆ = 10, γw = 2 (divisive normalization parameters). In
Figure 12, for the linear encoding model, tuning function parameters are the same as those reported above for
Figure 1.

20

Results
Linear mixing
We consider a population of neurons encoding a pair of stimuli (s1 , s2 ) where the mean responses of the neurons
are expressed as a weighted average of their responses to the individual stimuli:

fk (s1 , s2 ) = w1 f (s1 ; φk ) + w2 f (s2 ; φk ).

(73)

Here w1 and w2 are the mixing weights and φk is the prefered stimulus of neuron k. We call this type of mixed
selectivity linear mixing, although it should be noted that the mean response fk (s1 , s2 ) is linear only in the individual
responses and not in the stimuli themselves, therefore linear mixing in this sense is different from what is referred
to as linear mixing in Rigotti et al. (2013). Neural responses of this type have been observed throughout the cortex
(Recanzone et al., 1997; Britten and Heuer, 1999; Zoccolan et al., 2005; Zoccolan et al., 2007; Beck and Kastner,
2007; Busse et al., 2009; MacEvoy et al., 2009; MacEvoy and Epstein, 2009; Nandy et al., 2013). It is, thus, of
considerable interest to understand the information encoding consequences of this type of mixed selectivity. For
our analysis, we separate the neurons into two groups such that neurons in the first group have a larger weight for
the first stimulus and neurons in the second group have a larger weight for the second stimulus. For simplicity, we
assume symmetric weights for the two groups: i.e. if the mixing weights associated with s1 and s2 are w1 and w2
respectively for the first group (with w1 ≥ w2 ), they are w2 and w1 for the second group. We initially consider
the case where all neurons within the same group have the same weights for the two stimuli, but later consider the
effects of heterogeneity in mixing weights.
To model variability in neural responses, unless otherwise noted, we use a biologically realistic, Poisson-like noise
model, where the variance of the noise is equal to the mean response (Materials and Methods). We assume that
neural variability is correlated across the population. Specifically, within-group correlations between neurons decay
exponentially with the distance between their prefered stimuli, consistent with experimental measurements of noise
correlations throughout the visual cortex (Cohen and Kohn, 2011):

Rkl = δkl + (1 − δkl )c0 exp




|φk − φl |
−
,
L

(74)

where δ is the Kronecker delta function. Across-group correlations are simply scaled versions of within-group
correlations:
Rkl′ = βc0 exp



−


|φk − φl′ |
,
L

(75)

where 0 ≤ β ≤ 1 represents the scaling factor. In this paper, we only consider stimuli defined over circular spaces
21

due to their conceptual simplicity and analytical tractability. Stimuli defined over bounded spaces would introduce
edge effects where stimuli toward the edges are encoded with less accuracy than stimuli toward the center. This
can be explained entirely by a decrease in the effective number of neurons covering the stimuli toward the edges
and hence is uninteresting for our considerations. We do not expect any of our main results concerning stimulus
mixing to depend on the choice of a circular rather than a bounded stimulus space.

Consequences of linear mixing
We derived a mathematical expression for the Fisher information matrix (FIM) of the encoding model described
above. The main interest in deriving the FIM comes from the fact that, by the Cramér-Rao bound, the inverse of
the FIM provides a lower bound on the covariance matrix of any unbiased estimator of the stimuli and expresses
the asymptotic covariance matrix of the maximum-likelihood estimator. From the inverse of the FIM, we obtained
expressions for the asymptotic variance of the optimal estimates of s1 and s2 and the correlation between the
estimates (Materials and Methods).
We then asked how changes in different parameters affect encoding accuracy, i.e. the inverse of the asymptotic
variance of the estimates. Considering the effect of stimulus mixing first and assuming w1 = w and w2 = 1 − w
with 0.5 ≤ w ≤ 1, we find that increased stimulus mixing (i.e., decreasing w) reduces encoding accuracy and
that these reductions can be substantial (Figure 1A). The harmful effect of stimulus mixing for encoding accuracy
depends on the similarity between the two stimuli (Figure 1A), being more severe for more similar stimuli (smaller
∆s = |s1 −s2 |). For some stimulus pairs, increased stimulus mixing can cause several orders of magnitude reductions
in encoding accuracy (Figure 1A). It is easy to see that the total response across the whole population is independent
of the mixing weight w. Therefore, the reduction in encoding accuracy with increased stimulus mixing is due entirely
to stimulus mixing itself, rather than any reduction in the overall response level.
We next analyzed the effect of heterogeneity in mixing weights by assuming that the weights of different neurons are
2
drawn from a distribution with mean w or 1−w and variance σw
(Materials and Methods). Such heterogeneity in the

mixing weights partially alleviates the harmful effects of stimulus mixing (Figure 1B). Increasing the across-group
neural correlations, i.e. increasing β, can also counteract the effects of stimulus mixing under certain parameter
regimes (Figure 1C).
Effects of stimulus mixing, heterogeneity in mixing weights and across-group neural correlations on
encoding accuracy: The presence of Poisson-like noise makes an analytic quantification of the effects of stimulus
mixing, heterogeneity in mixing weights and across-group neural correlations on the asymptotic variance difficult.
However, for the parameter regimes reported in Figure 1, we numerically checked and confirmed the following: (i)
increasing stimulus mixing always increases the asymptotic variance (Figure 1A); (ii) increasing the heterogeneity

22

of mixing weights generally reduces the asymptotic variance, except for a small number of cases where this pattern
2
is reversed for very close σw
values due to stochasticity in sampling (Figure 1B); (iii) for all ∆s, the increase in

variance caused by halving w1 = w from 1 to 0.5 is always greater than the increase in variance caused by halving the
mean response gain or doubling the Fano factor. Figure 2 compares the effect of stimulus mixing on the asymptotic
variance with the effects of halving the mean response gain or doubling the Fano factor (FF). The results shown
in Figure 2 are for ∆s = π, for which the effect of stimulus mixing is among the weakest. For smaller ∆s values,
stimulus mixing generally has a much larger effect on the variance than the effect of changing the gain or the Fano
factor. This result could explain why attention acts primarily by stimulus selection, which in our model corresponds
to changing the mixing weights, rather than simply through noise reduction or gain increase (Pestilli et al., 2011),
because the former mechanism typically leads to a much larger improvement in encoding accuracy than the latter
mechanisms (see Discussion).
Analytical results for an additive noise model: To develop a better understanding of the effects of changing the
parameters of the encoding model on encoding accuracy, we supplement the numerical results for Poisson-like noise
with analytical results for a simpler additive noise model. In the additive noise model, the noise variance, rather
than being equal to the mean response as in the Poisson-like noise model, is assumed to be the same for all neurons
independent of their mean responses. In Materials and Methods, for the additive noise model and in the limit of a
large number of neurons, we mathematically show that increased stimulus mixing always reduces encoding accuracy,
increased heterogeneity in mixing weights always improves encoding accuracy, whereas the conditions under which
increased neural correlations between the groups, i.e. increasing β, improves encoding accuracy are slightly more
complicated. In Figure 3A, we plot the sign of ∂Var(ŝ1 |s)/∂β for different values of w, ∆s and β. A positive sign
means that the asymptotic estimation variance is an increasing function of β (i.e. it is harmful to increase β),
whereas a negative sign means that the asymptotic estimation variance is a decreasing function of the parameter.
This figure shows that the derivative is always negative for very small values of ∆s, suggesting that it is beneficial
(in terms of encoding accuracy) to increase the across-group correlations in this case. For other values of ∆s, the
sign depends on β and w, being more likely to be negative for larger β and larger w values. A detailed analysis of
the effects of changing the other parameters of the encoding model under the additive noise assumption can also
be found in the Materials and Methods section. In summary, this analysis shows that increasing the maximum
neural correlation, c0 , is always harmful for encoding accuracy, whereas for the correlation length scale L, there is a
critical threshold below which it is always harmful to increase L and above which it is always beneficial to increase
L (Figure 3B).
Correlations between the estimates: The FIM also predicts prominent stimulus-dependent correlations between
the estimates of the two stimuli. The asymptotic correlation between the optimal estimates is given by Equation 22
(Materials and Methods). In Figure 4, we show the correlations between the two estimates under different parameter

23

regimes, using the Poisson-like noise model. Figure 4A and 4B show the effects of changing w and β respectively
on the correlation between the estimates. Psychophysical tasks where subjects have to estimate multiple stimuli
simultaneously are uncommon (see Orhan and Jacobs (2013) for an exception), but Figure 4 suggests that the
pattern of correlations between the estimates obtained from such tasks can be potentially informative about the
optimality of the subjects’ decoding methods and about possible parameter regimes for their encoding models.

A reduced model to understand the effects of stimulus mixing
To develop a geometric intuition for the effects of stimulus mixing and across-group neural correlations on encoding
accuracy, we consider a simpler, reduced version of the linear mixing model. In this model, neurons in each group
are reduced to a single neuron. In addition, we model the responses of these two “reduced neurons” linearly, ignoring
the non-linearity introduced by the tuning function, f . Thus, the responses of the two neurons are modeled as:

r1 = ws1 + (1 − w)s2 + ε1 ,

r2 = (1 − w)s1 + ws2 + ε2 ,

(76)

where ε1 and ε2 are zero-mean random variables with correlation β, representing correlated noise in the responses.
A given (r1 , r2 ) pair describe two lines in the (s1 , s2 ) plane: r1 = ws1 + (1 − w)s2 and r2 = (1 − w)s1 + ws2 .
The maximum likelihood estimate of the stimuli is given by the intersection of these two lines. As r1 and r2
vary stochastically from trial to trial due to noise, the lines as well as their intersection point change. If there
is any stimulus mixing (w 6= 1), the geometry of the lines dictates that the estimates should be stretched along
the anti-diagonal direction, making them more variable than under the no mixing condition (w = 1) for the same
(r1 , r2 ) values. This is illustrated in the middle panel in Figure 5 for w = 0.8 and β = 0. Increasing the stimulus
mixing makes the slopes of the lines more similar to each other, which stretches the intersection points even further
(left panel in Figure 5) and increases their variance. Increasing the across-group neural correlation β, on the other
hand, makes the intersection points along the diagonal more probable (right panel in Figure 5), counteracting the
anti-diagonal stretching caused by stimulus mixing and decreasing the variance of the estimates.

Stimulus dependence of the variance
The dependence of the asymptotic variance of the optimal estimates on ∆s cannot be explained with the reduced
model. To gain some insight into the mechanism behind this dependence, we consider the linear mixing model with
additive noise and independent neurons. In this case, it can be shown that the encoding precision is proportional

24

0

Variance

A

0

10

∆s

β = 0.1
−2

10

10

β = 0.9

0
π/16
π/8
π/4
π/2
π

−2

10

−4

−4

10

10

0.5
−2

β = 0.1
−3

10

= 0.005
= 0.025
= 0.05

0

1

β = 0.9
−3

w

10

1

−4

−4

10
0

Variance

w

10
σ w2
σ w2
σ w2

10

C

0.5
−2

10

Variance

B

1

w

σw2

0.05

−2

0

σw2

0.05

−3

10

10

w = 0.6

−3

w=1

−4

10

10

−4

−5

10

10
0

β

1

0

β

1

Figure 1: Analysis of the linear mixing model. (A) The asymptotic variance of the optimal estimator, Var(ŝ1 |s), as
a function of the mixing weight w. The left panel shows the optimal variance for a small β value (β = 0.1) and the
right panel for a large β value (β = 0.9). (B) The effect of heterogeneity in mixing weights on encoding accuracy.
2
The weights are drawn from a beta distribution with mean 0.6 or 0.4, and variance σw
. The two panels show the
2
optimal variance for two different β values. To aid the interpretation of σw values, the inset in the left panel shows
2
the weight distributions for three different σw
values. (C) The asymptotic variance of the optimal estimator as a
function of β. The left panel shows the optimal variance for a strong stimulus mixing regime (w = 0.6) and the
right panel for the no stimulus mixing regime (w = 1). Different curves in each panel correspond to six different
∆s = |s1 − s2 | values indicated in the inset in (A), with lighter colors corresponding to smaller ∆s values. Other
parameter values are listed in Materials and Methods.

25

A

B

−4

x 10

4

Baseline
Half gain
Double FF

Variance

Variance

4

−4

x 10

3

β = 0.1
2
0.5

2

β = 0.9

1
0.5

1

w

3

1

w

Figure 2: Comparison of the effect of stimulus mixing on the asymptotic variance with the effects of halving the
mean response gain α, or doubling the Fano factor, FF. (A) Weak across-group correlations (β = 0.1). (B) Strong
across-group correlations (β = 0.9). The asymptotic variance has the same scaling with α as it does with FF−1 .
The results shown in this figure are for ∆s = π, for which the effect of stimulus mixing is among the weakest. For
smaller ∆s values, stimulus mixing generally has a much larger effect. The parameter values for the baseline results
(shown in black) are the same as those reported for Figure 1 (see Materials and Methods).

B

A

1

w

w

1
0.5

0.5

3

3

1

∆s

0 0

2

∆s

β

0 0

L

1 |s)
as a function of β, ∆s and w. Red dots indicate parameter combinations for
Figure 3: (A) The sign of dVar(ŝ
dβ
which the derivative is positive, blue dots indicate negative derivatives. w is varied from 0.51 to 0.99, β is varied
from 0 to 0.99 and ∆s is varied from 0 to π, all on a linear scale. L = 2 for the results shown in this plot. (B) The
1 |s)
as a function of L, ∆s and w. w is varied from 0.51 to 0.99, L is varied from 10−6 to 2 and ∆s is
sign of dVar(ŝ
dL
varied from 0 to π. The results shown in this figure are for the additive noise model.

26

Corr(ŝ1 , ŝ2 |s)

A

1

1

β = 0.1

β = 0.9

0

0

w
−1

0.5
0.6
0.7
0.8
0.9
1

0

Corr(ŝ1 , ŝ2 |s)

B

−1
3

0

∆s

3

∆s

1

1

w = 0.6

w=1

0

0

β
−1
0

0
0.2
0.4
0.6
0.8
1

−1
3

0

∆s

3

∆s

Figure 4: The asymptotic correlation between the estimates, Corr(ŝ1 , ŝ2 |s), as a function of ∆s. (A) The effect
of changing w. The left panel shows correlations for β = 0.1 and the right panel shows correlations for β = 0.9.
Different curves in each panel correspond to different w values indicated in the inset in the left panel. (B) The
effect of changing β. The left panel shows correlations for a strong stimulus mixing regime (w = 0.6) and the right
panel shows correlations under the no stimulus mixing regime (w = 1). Different curves in each panel correspond
to different β values indicated in the inset of the left panel.

27

w = 0.6
β=0

w decreases

←−

w = 0.8
β=0

β increases

−→

w = 0.8
β = 0.8

s2

s2

s2

r1 changes
r2 changes

s1

s1

s1

Figure 5: Geometric intuition for the effects of w and β on the variance of estimates in a simplified two-neuron
model (Equation 76). Maximum-likelihood estimates of the stimuli are represented by the blue dots, as r1 and r2
vary stochastically for a particular stimulus pair (s1 , s2 ). Dot size represents the probability of the corresponding
estimates.
to (see Equation 27 in Materials and Methods):
 ∂f 2


 −

∂s1

∂f T ∂f
∂s1 ∂s2
∂f
k
k ∂s
1

!2

,

(77)

where k · k denotes the Euclidean norm and f is a column vector of the mean responses of all neurons in both
∂f
groups. In the linear mixing model k ∂s
k does not depend on ∆s, hence the dependence of encoding precision on
1

∆s is determined solely by the magnitude of the inner product

∂f T ∂f
∂s1 ∂s2 :

when the magnitude of this inner product

is large relative to the norms of the individual vectors, i.e. when the derivative profiles with respect to s1 and s2
overlap more, the encoding precision becomes low. Figure 6 shows that

∂f
∂s1

and

∂f
∂s2

overlap more extensively for

smaller ∆s, explaining why stimulus mixing is especially harmful for stimuli with small ∆s.
Although the simple proportionality relation above does not hold in the case of Poisson-like noise (or for correlated
neurons for that matter), it qualitatively captures the stimulus-dependence of the asymptotic variance in Figure 1.

Generalization to more than two stimuli
It is straightforward to generalize the preceding linear mixing model to more than two stimuli. However, deriving
a simplified mathematical expression for the FIM, as was done for the case of two stimuli, becomes infeasible for
this case. Therefore, we present results from numerically computed FIMs for up to 6 stimuli.
To generate the mixing weights of different groups of neurons, for each set size N , we first draw a random weight
P
vector w, uniformly distributed on the (N −1)–dimensional probability simplex, i.e. the region defined by i wi = 1
28

A 15

∆s = 0

∆s = π

∆s = π/2

B2

5

x 10

∂f T ∂f
∂s1 ∂s2

1
0
0
−15

∂ fk /∂ s 1
∂ fk /∂ s 2

−1

Neuron index (k)

0

3

∆s

Figure 6: (A) Derivatives of the mean responses with respect to s1 (black) and s2 (red) plotted for three different
T
∂f
∆s values. For all three panels, w = 0.6. (B) The inner product ∂f
∂s1 ∂s2 as function of ∆s (blue line) and the points
corresponding to the ∆s values shown in (A) are indicated by the open square signs.
and wi ≥ 0. We then generate an N × N circulant matrix W from the weight vector w. The rows of this matrix,
which are all circular permutations of w, give the weight vectors of each group in the population. We generate
512 such weight matrices and, for each weight matrix, compute the asymptotic variance of the optimal estimator
from the inverse of the Fisher information matrix for the particular stimulus configuration where all the stimuli are
identical, s1 = s2 = . . . = sN . The number of neurons per group and the magnitude of noise correlations between
groups are held constant across set sizes in these simulations. Similar to the results for N = 2, the estimation
variance increases when the weight vector w becomes more uniform, i.e. when different groups become equally
responsive to all stimuli. To quantify the uniformity of the weight vectors, we use the Shannon entropy of w treated
as a discrete probability distribution. Figure 7A shows the asymptotic estimation variance as a function of the
Shannon entropy of the weight vector. When the logarithm of the estimation variance is linearly regressed on the
logarithm of the set size N , we find a highly significant effect with a positive slope of about 0.82 (p < .0001),
suggesting that the variance increases with set size (Figure 7B). This result is not sensitive to the particular way
the weight matrices W are chosen and holds as well for the case where the weight vectors of different groups in
the population, rather than being circular permutations of a single weight vector, are random samples from the
(N − 1)–dimensional probability simplex (Figure 7C). In this case, the linear regression of the logarithm of the
estimation variance on the logarithm of the set size yields a highly significant effect with a positive slope of about
2.2 (p < .0001), again suggesting an increase in the estimation variance with set size. This increase is not caused by
a reduction in gain per stimulus, as the number of neurons per group was held constant and the presented stimuli
were identical. Rather, it is due to an increase with N in the mean normalized entropy (normalized by the maximum
possible entropy, i.e. log N ) of weight vectors drawn from a probability simplex (Nemenman et al., 2002). In other
words, with increasing N , it becomes more and more difficult to find “harmless”, low-entropy weight vectors. This
result suggests a novel mechanism that might contribute to set size effects, i.e. declines in performance with set
size, observed in various psychophysical tasks (see Discussion).
29

B

A 10
Variance

1

C

∇
∇

10

∇

N =5

∇

−1

∇

N =6

N =3

∇

N =2

−3

N =4

∇

N =3

∇

N =5

∇

N =4

∇

N =6

N =2

10

0

1

2

−3

−1

10

Entropy(w)

10

Variance

1

10

−3

10

−1

10

1

10

Variance

Figure 7: Linear mixing with more than two stimuli. (A) Asymptotic variance as a function of the entropy of
the weight vector. Vertical dashed lines show log N (maximum entropy for each set size). (B) Histograms of
the asymptotic variance of the optimal estimates for different set sizes from N = 2 to N = 6 with darker colors
representing larger set sizes. The inverted triangles indicate the median variance for each set size. (C) Similar to
(B), but the weight vectors of different groups are sampled randomly from the (N − 1)–dimensional probability
simplex.

Generalization to a suboptimal decoder
The results presented so far concern the FIM which describes the asymptotic behavior of the optimal estimator.
An important question is to what extent these results generalize to empirically motivated suboptimal decoders.
Here, we show that the effects of stimulus mixing, heterogeneity of the mixing weights, and across-group noise
correlations obtained from the analysis of the FIM generalize to a particular type of suboptimal decoder called the
optimal linear estimator (OLE) (Salinas and Abbott, 1994). Because OLE is a biased estimator, we use the mean
squared error (MSE) as a measure of the estimator’s performance.
Figure 8A shows the MSE of the OLE for different degrees of stimulus mixing. Increased stimulus mixing (decreasing
w) deteriorates the estimator’s performance. This is consistent with the results presented above for the FIM. The
stimulus dependence of the estimator error, however, has a different form than for the FIM. Figure 8B shows the
MSE of the OLE for different amounts of heterogeneity in the mixing weights. Again, consistent with the results
obtained from the FIM, increased heterogeneity improves the estimator’s performance. Figure 8C shows the MSE
of the OLE for different across-group correlation values. Under strong stimulus mixing (w = 0.6), increasing β
improves the decoder’s performance by up to an order of magnitude in most cases. This effect is also consistent
with the results presented earlier for the FIM. The effect of β, however, becomes less significant under the no
stimulus mixing condition (w = 1). The results for the OLE are also, in general, less dependent on ∆s than the
results for the FIM.

30

A

0

0

MSE

10

β = 0.1

−1

∆s

10

−2

10
0
π/16
π/8
π/4
π/2
π

10

−2

10

10

0.5

B

1

w

−1

MSE

w

1

β = 0.1
−2

10

10

σw2 = 0.005

β = 0.9

σw2 = 0.025
σw2 = 0.05

0

−2

w

10

1

−3

−3

10

10
0

MSE

0.5
−1

10

C

β = 0.9

−1

σw2

0.05

0

0

σw2

0.05

−1

10

10

w = 0.6

w=1

−1

−2

10

10

−2

−3

10

10
0

1

β

0

β

1

Figure 8: Results for the OLE decoder. (A) Dependence of the MSE of the optimal linear estimator on w. The
left panel shows the MSE for a small β value (β = 0.1) and the right panel for a large β value (β = 0.9). (B)
Dependence of the MSE on the heterogeneity of mixing weights. The weights w are drawn from a beta distribution
2
with variance σw
. The two panels show the MSE for two different β values. The inset in the left panel shows the
2
weight distributions for three different σw
values. (C) Dependence of the MSE on β. The left panel shows the MSE
for a small w value (w = 0.6) and the right panel shows the MSE for a large w value (w = 1). Different curves
in each panel correspond to six different ∆s = |s1 − s2 | values indicated in the inset in (A), with lighter colors
corresponding to smaller ∆s values. Other parameter values are listed in Materials and Methods.

31

Nonlinear forms of stimulus mixing
So far, we have considered a linear stimulus mixing model where the responses of neurons to multiple stimuli are
modeled as simple linear combinations of their responses to individual stimuli alone. Do our results also hold for
other forms of stimulus mixing, or is the assumption of linearity crucial? To show that our results are not specific
to the linear mixing model, here we consider two nonlinear, experimentally motivated forms of stimulus mixing.
Nonlinear mixing model of Britten and Heuer (1999): Britten and Heuer (1999) present pairs of moving
gratings inside the receptive fields of MT neurons and show that a nonlinear mixing equation of the following form
provides a good characterization of their responses:

fk (s1 , s2 ) = a

 f (s ; φ )ν + f (s ; φ )ν 1/ν
1
k
2
k
+ b,
2

(78)

where f (s1 ; φk ) and f (s2 ; φk ) are the mean responses of neuron k to the individual gratings. This equation can
interpolate smoothly between simple averaging (a = 1, ν = 1) and max-pooling (ν → ∞) of responses to the
individual gratings. Britten and Heuer (1999) report a wide range of values for the parameters a and ν across the
population of neurons they recorded from. They further show that allowing these parameters to vary results in
significantly better fits than the simple averaging model for most of the neurons.
We assume a single unsegregated population of neurons and derive the Fisher information matrix as before, using the
mean responses described in Equation 78 above. Figure 9A shows the asymptotic variance of the optimal estimator
as a function of the exponent ν. Increasing ν reduces stimulus mixing and significantly improves the encoding
accuracy, consistent with the results from the linear mixing model. As in the linear mixing model, the effect of
stimulus mixing on encoding accuracy can be understood, at least qualitatively, by considering the magnitude of
the overlap between the profiles of the partial derivatives of the mean responses with respect to the two stimuli,
and

∂f
∂s1 .

∂f
∂s1

For small ν values, there is significant overlap between the partial derivative profiles, leading to a severe

reduction in encoding accuracy, whereas larger ν values make the neurons sensitive to changes in only one of the
stimuli and thus reduce the overlap between the derivative profiles (Figure 9B-C).
Divisive stimulus mixing: We next consider another biologically motivated form of stimulus mixing based on
divisive normalization. Specifically, as in the linear mixing model, we separate the neurons into two groups and
assume that the responses of neurons in each group are normalized by a weighted sum of the activity of neurons in
the other group. The weighting is assumed to be neuron-specific such that neurons with similar stimulus preferences
exert a larger divisive influence on each other. This type of divisive normalization has previously been motivated
by considerations of efficient coding in early visual cortical areas (Schwartz and Simoncelli, 2001) and can be used
to describe stimulus-dependent suppressive surround effects in the visual cortex (Allman et al., 1985; Cavanaugh

32

A 10

B 16

−2

4

x 10

Variance

∆s

π/160
π/16
π/8
π/4
π/2
π

−4

10

−6

10

8

−8

10

−1

10

0

1

10

10

∂f T ∂f
∂s 1 ∂s 2

0
−1
10

2

10

ν

C

0

1

10

10

2

10

ν

16

ν = 0.2

ν =1

ν = 100

0

−16

∂fk /∂s 1
∂fk /∂s 2

Neuron index (k)
Figure 9: Analysis of the nonlinear mixing model of Britten and Heuer (1999). (A) The asymptotic variance of
the optimal estimator as a function of the exponent ν (Equation 78). Different curves correspond to six different
∆s = |s1 − s2 | values indicated in the inset, with lighter colors corresponding to smaller ∆s values. Note that for
∆s = 0, the variance diverges in this model as in the linear mixing model with w = 0.5. (B) The inner product
∂f T ∂f
∂s1 ∂s2 as function of ν (for ∆s = π/160). (C) Profiles of the partial derivatives of the mean responses for three
different ν values: ν = 0.2, ν = 1 and ν = 100. These points are also indicated by the open square signs in (A) and
(B). Parameter values for the results shown here are listed in Materials and Methods.

33

et al., 2002). Mathematically, the response of a neuron in the first group is described by:

fk (s1 , s2 ) =

∆ + kw

P

f (s1 ; φk )2
,
2
k′ w(φk , φk′ )f (s2 ; φk′ )

(79)

where for the weighting profile w(φk , φk′ ), we use a normalized von Mises function (Materials and Methods). Responses of neurons in the second group are similar, but with the roles of s1 and s2 reversed.
Figure 10A-B show the effects of varying the divisive normalization scaling factor kw and the across-group neural
correlations β on encoding accuracy. Increasing kw decreases encoding accuracy. However, unlike in the linear
mixing model, this decrement in encoding accuracy does not only reflect the effect of stimulus mixing per se, but
also a stimulus-independent scaling of the response gain. To tease apart the contribution of stimulus mixing per se
from that of a simple gain change in the responses, we built a model that, neuron by neuron, had the same gain as
the divisive normalization model in Equation 79 (for all stimuli), but whose Fisher information was computed by
treating the denominator in Equation 79 as constant (this was done by setting the off-diagonal entries of the FIM
to zero). This second model thus eliminates stimulus mixing, but preserves the neuron-by-neuron response gains
in the first model. The results for this second model are shown with dashed lines in Figure 10A-B. Comparing
the dashed lines with the same colored solid lines, we see that stimulus mixing per se induces a cost to encoding
accuracy for stimulus pairs with large and small ∆s values, but not for stimulus pairs with intermediate ∆s values.
As in the linear mixing model, increasing β improves encoding accuracy in the divisively normalized model as well
(Figure 10B).
Figure 10A suggests that stimulus mixing in the divisively normalized form given in Equation 79 is not as harmful as
in the linear mixing model (or in the nonlinear mixing model of Britten and Heuer). To understand this difference
between the two forms of stimulus mixing, we consider a two-neuron version of the divisively normalized model
analogous to the two-neuron model considered earlier for the linear mixing model. We again ignore the nonlinearities
introduced by the tuning function f (s; φ) and model the responses of the two neurons as follows:

r1 =

s1
+ ε1 ,
∆ + ws2

r2 =

s2
+ ε2 .
∆ + ws1

(80)

As in the two-neuron version of the linear mixing model, for a given (r1 , r2 ) pair, the two equations above define
two lines in the (s1 , s2 ) plane whose intersection gives the maximum likelihood estimate of the stimuli. We first
note that unlike in the linear mixing model, noise in ri changes the slopes of the lines. The slopes of the two lines
described in Equation 80 are given by 1/(r1 w) and r2 w respectively. This suggests that as long as r1 and r2 are
sufficiently large, the slope of the first line will be small, and will remain small despite random variations in r1 ,
whereas the slope of the second line will be large and will remain large in the face of variations in r2 . Unless r1 and
r2 are very small, the two neurons thus encode the stimuli roughly orthogonally (Figure 10C), unlike in the case of

34

B1

1

−4

C

x 10

β =0
β = 0.5
β =1

0

s2

Variance

kw = 0
k w = 0.2
kw = 1

Variance

A

−4

x 10

0
0

3

∆s (rad)

0

3

∆s (rad)

r2 changes
r1 changes

s1

Figure 10: Analysis of the divisive mixing model. (A) The effect of varying kw . Dashed lines in (A) and (B) show
the results for a model with no stimulus mixing, but with gain matched neuron-by-neuron to that of the divisive
mixing model. (B) The effect of varying β. Other parameter values are listed in Materials and Methods. (C)
Geometric intuition for the mildly harmful effects of a divisive form of stimulus mixing in a simplified two-neuron
model (Equation 80). Maximum-likelihood estimates of the stimuli are represented by the blue dots.
linear mixing where the slopes of the lines can become arbitrarily close to each other as stimulus mixing increases.
This approximately orthogonal coding of the stimuli, in turn, causes only a relatively small amount of distortion
in the estimates (ŝ1 , ŝ2 ) as r1 and r2 vary stochastically for a particular stimulus pair (as indicated by the spread
of the blue dots in Figure 10C), explaining why stimulus mixing is less harmful in the divisive mixing model than
in the linear mixing model. As in the linear mixing model, the stimulus dependence of encoding accuracy in the
divisive mixing model can be qualitatively understood by considering the magnitude of the inner product of the
derivatives of the mean responses with respect to s1 and s2 (not shown).

Stimulus mixing is not always harmful for encoding accuracy
The examples of stimulus mixing considered thus far showed a harmful effect on encoding accuracy. This raises the
important question: is stimulus mixing always harmful for encoding accuracy? Here we show that the answer is no.
To show this, we first analyze the general stimulus mixing problem with a two-dimensional toy model similar to
the ones presented earlier for linear and divisive mixing. We imagine two “neurons” mixing the two stimuli s1 , s2
according to f1 (s1 , s2 ) and f2 (s1 , s2 ) respectively. The responses of the two neurons are given by r1 = f1 (s1 , s2 ) + ε1
and r2 = f2 (s1 , s2 ) + ε2 where ε1 and ε2 are Gaussian random variables with variance σ 2 and correlation ρ. We
denote the Jacobian matrix for the mean responses of the neurons by J. One can think of J as a mixing matrix
describing the sensitivity of each neuron to changes in each stimulus. J would be diagonal (or anti-diagonal) in the
case of no stimulus mixing. The FIM is given by IF = JT Σ−1 J where Σ is the covariance matrix of the response
noise. IF−1 gives the asymptotic covariance matrix of the maximum likelihood estimator. To find the optimal mixing

35

matrix J, we minimize the trace of IF−1 , i.e. Tr[IF−1 ] = Tr[J−1 ΣJ−T ] with respect to J. With no constraints on
J, IF−1 can be made arbitrarily small, for example by making J diagonal and its diagonal entries arbitrarily large.
Because the derivatives in the Jacobian can be negative or non-negative, a plausible constraint on J is to require the
sum of the squares of the derivatives in J to be a finite constant K. In terms of the matrix J, this means requiring
Tr[JT J] = K. The optimal J can then be found by the method of Lagrange multipliers (Materials and Methods).
The optimal solution is to set the gradients of the two neurons, ∇f1 and ∇f2 , to have equal norm and the angle
between them to θ∗ with cos θ∗ given by (Figure 12A):
∗

cos θ =

1−

p
1 − ρ2
.
ρ

(81)

The absolute orientation of the gradients in the plane, however, can be arbitrary. Thus, Equation 81 describes a
one-dimensional family of solutions. Because Fisher information is a local measure of information, this solution
holds for a given arbitrary point (s1 , s2 ) in the plane. For optimal mixing over the entire plane, the conditions
specified by the solution have to be satisfied for each point (s1 , s2 ) in the plane. This can be achieved by choosing
the mixing function of the first neuron f1 (s1 , s2 ) arbitrarily and then choosing the mixing function of the second
neuron f2 (s1 , s2 ) such that the optimality conditions on the gradients are satisfied at each point in the plane.
Three important aspects of the solution are worth emphasizing. First, the solution does not require J to be
diagonal (or anti-diagonal). Thus, stimulus mixing is not intrinsically harmful, but rather stimuli should be mixed
in complementary ways by different neurons so that the conditions on the gradients are satisfied. Second, the
optimal solution, in fact, necessitates a certain amount of stimulus mixing when ρ 6= 0, as the optimality condition
for ρ 6= 0 cannot be satisfied with ∇f1 and ∇f2 aligned with the two axes of the (s1 , s2 ) plane. Third, for ρ −→ 0,
cos θ∗ −→ 0; therefore, the gradients have to be orthogonal to each other in this case. Furthermore, the optimal
angle θ∗ between the gradients changes rather slowly as ρ moves away from 0. Thus the gradients should be close
to orthogonal for a large range of ρ values around 0. The orthogonality condition can be understood as follows.
Intuitively, ∇f1 is the first neuron’s “least uncertain” direction in the (s1 , s2 ) plane. The second neuron has to
align its least uncertain direction orthogonally to ∇f1 so that together the two neurons can encode the stimuli
with the least total uncertainty. In our original analysis of the linear mixing model, the orthogonality condition
can be satisfied only when there is no stimulus mixing, because, motivated by a consideration of consistency with
physiological data, the weights were assumed to be non-negative in that case.
The solution of the two-dimensional toy model can be readily generalized to models with more than two neurons
under certain conditions (Equations 66-71; see Materials and Methods). For the general case of n neurons encoding
two stimuli, as far as we know, there is no closed-form solution for the optimal mixing matrix, J, subject to a
constraint on the total power of the derivatives. Numerical solution of this more general problem shows that for

36

any given neural covariance structure, there is a diverse set of solutions: Figure 11A shows three example solutions
for n = 16 independent neurons and Figure 11B shows three example solutions for n = 16 correlated neurons with
a limited-range correlation structure. Moreover, random mixing of the stimuli by neurons performs remarkably well
especially for large n. Figure 12B compares the performance of the median random mixing model with that of the
optimal mixing model for different n (compare the black asterisks vs. the black open squares). For the random
mixing models, the gradients of neurons were chosen subject to the total power constraint, i.e. the sum of the
squared norms of gradients was constant, but they were otherwise random.
We also wanted to see how well linear mixing models perform compared to unconstrained encoding models where
the gradients can be set arbitrarily (subject to the resource constraint). When the encoding model is constrained to
be a linear mixing model with two groups and fixed weights for each stimulus (Equations 66-67; see Materials and
Methods), random ensembles of linear mixing models perform worse than random ensembles of arbitrary encoding
models (compare the red vs. black open squares in Figure 12B). Interestingly, however, the optimal solution for
the linear mixing model appears to have the same form as the optimal solution of the two-dimensional problem
and performs as well as the optimal solution for arbitrary encoding models (compare the red vs. black asterisks in
Figure 12B). Figure 12B shows the results for populations with independent neurons. Analogous results hold for
correlated neural populations. With correlated neurons, the improvement in the relative performance of random
ensembles with increasing n becomes somewhat slower and the optimal linear encoding model no longer achieves
the same performance as the optimal arbitrary encoding model (Figure 12C).

Discussion
Theoretical studies of population coding have traditionally focused on the encoding of single stimuli by neural
populations (Abbott and Dayan, 1999; Sompolinsky et al., 2001; Ecker et al., 2011). In this work, we extended the
neural population coding framework to the encoding of multiple stimuli, assuming encoding models with biologically
motivated properties. We examined a linear mixing rule commonly observed in cortical responses. According to this
rule, the response to the presentation of multiple objects can be described as a linear combination of the responses to
the constituent objects. We find that this rule incurs a severe cost to encoding accuracy. This cost is directly related
to the mixing of the stimuli in the neural responses, is independent of any general decrease in the overall activity
of the population, and can be larger than the cost incurred by even large reductions in the gain or large increases
in neural variability. As noted earlier, this result could explain why attention acts primarily as a stimulus selection
mechanism (Reynolds et al., 1999; Pestilli et al., 2011), rather than purely as a gain increase or noise reduction
mechanism. However, it should be emphasized that mechanistically, stimulus selection can be implemented with
a combination of gain increase and a nonlinearity that would amplify the gain differences (Reynolds and Heeger,

37

A

2

s2

1
0
−1
−2
−2

B

−1

0

1

2

0

1

2

s1

2

s2

1
0
−1
−2
−2

−1

s1

Figure 11: Numerical solution of the optimal mixing problem. (A) Three example numerical solutions to the optimal
stimulus mixing problem for n independent neurons. In each panel, the spokes represent the gradients of different
neurons in the population and the black circle shows the unit circle. (B) Three example numerical solutions to the
optimal stimulus mixing problem for n correlated neurons. Correlations between the neurons are assumed to be
limited-range, i.e. the correlation matrix R has the form given in Equation 74 (with c0 = 0.3 and L = 2). The noise
is assumed to be additive Gaussian with constant standard deviation σ = 1 for all neurons. In (B), the gradient
vectors are colored such that neurons with higher correlations have more similar colors, where the similarity of the
colors is indicated by their position on the circle shown in the inset of the left panel. The population has n = 16
neurons in the examples shown here.

38

90

0
−1

2

Arbitrary opt.
Arbitrary med.
Linear opt.
Linear med.

10

0

10

ρ

1

10

2

10

0

10

−2

−2

0

C
Total Variance

B

180

Total Variance

θ ∗ (deg.)

A

4

16

n

64

256

10

4

16

n

64

256

Figure 12: Optimal stimulus mixing. (A) Optimal angle between the gradients as a function of the correlation
ρ in a two-dimensional stimulus mixing problem. (B) Total variance, i.e. Tr[IF−1 ], for the median random model
compared with the total variance for the optimal model for both linear and arbitrary encoding models. For the
linear mixing model, results for n ≥ 16 are shown only. With smaller n, the results are highly sensitive to the
choice of tuning function centers. For random models, medians are computed over 256 random realizations of the
encoding model. (C) Similar to (B), but for correlated neurons (c0 = 0.3 for both the arbitrary encoding model
and the linear encoding model and β = 0.9 for the linear encoding model).
2009; Pestilli et al., 2011). Therefore, gain increases might be an integral part of the stimulus selection mechanism
by which attention operates.
Why does linear mixing seem to be so prevalent in cortical responses, if it indeed incurs such a large cost? It is
possible that linear, or close-to-linear, mixing is an inevitable consequence of computations carried out for other
purposes, such as achieving object representations invariant to identity-preserving transformations (Zoccolan et al.,
2005; Zoccolan et al., 2007). We emphasize that our framework evaluates an encoding model in terms of the ability
of a downstream decoder to accurately estimate the constituent stimuli in the face of neural variability. If, however,
the goal of the computation is not accurate representation of the constituent stimuli themselves, but computing a
possibly complex function of them, then linear mixing or similar models are not necessarily harmful. For example,
in the problem we considered in this paper, if the computational goal were only to estimate a weighted average of the
two stimuli s1 and s2 , linearly mixed responses would be ideally suited for such a task. Finding the optimal neural
codes for the representation of multiple stimuli that achieve the simultaneous objectives of successful performance
in behaviorally relevant tasks (e.g., see Salinas, 2006) and accurate encoding of constituent stimuli could be an
important future direction.
The harmful effects of stimulus mixing can be partially alleviated by increased across-group neural correlations
or by increased heterogeneity in the mixing weights of the neurons. Importantly, all our main results concerning
the linear mixing model, i.e. the effects of stimulus mixing, across-group neural correlations and heterogeneity in
mixing weights generalize to the suboptimal OLE decoder. This is not a trivial result, because there is, in general,
no guarantee that manipulating the properties of a neural population should affect the performance of optimal
and suboptimal decoders in similar ways. Indeed, a previous study (Ecker et al., 2011), for instance, found that

39

in the presence of diversity in neural tuning properties, limited-range correlations can be beneficial for accurately
encoding a single stimulus, but this holds only if the responses are decoded optimally, and does not generalize to
the suboptimal OLE decoder.
In the linear mixing model, increasing the number of stimuli makes stimulus mixing even costlier. This result
suggests that stimulus mixing might contribute to set size effects commonly observed in visual short-term memory
and other psychophysical tasks. Decreases in performance with set size in such tasks are typically attributed to
a capacity limitation, e.g. an upper limit on the total amount of neural activity, which might be implemented by
divisive normalization (Ma et al., 2014). However, our results demonstrate that even without any constraint on
the total amount of neural activity (indeed, in our simulations, total activity was proportional to set size), set size
effects would be expected in the linear mixing regime, as it becomes more difficult to find harmless, low (normalized)
entropy weight vectors with increasing set size. Such weight vectors can still be found through learning, but any
learning algorithm would take longer to reach these low entropy regions in the weight space and it would require
more fine-tuning to keep the weights in a low entropy region once such a region is found through learning, as
any noise in the weights, or in the learning algorithm itself, would be more likely to push the weights out of the
low-entropy region.
The property that makes linear mixing particularly harmful for encoding accuracy is not the linearity of response
mixing per se. It is rather the degree of overlap, or similarity, between the derivative profiles of the neural responses
with respect to different stimuli that, to a first approximation, determines how harmful a particular form of stimulus
mixing can be (e.g. see Figure 6 and Figure 9B-C). Indeed, our results for the non-linear mixing rule of Britten
and Heuer (1999) show that stimulus mixing can lead to a severe reduction in encoding accuracy even when mixing
takes a strongly non-linear form.
Stimulus mixing, in itself, is not always harmful for encoding accuracy. As our analytic solution to the optimal
mixing problem in a toy model and numerical solutions in more complex cases suggest, it may even be optimal
in the presence of neural correlations. Stimulus mixing has to satisfy certain conditions in order to be unharmful
for encoding accuracy. In a simple two-dimensional problem and with sufficiently low neural correlations, those
conditions can be condensed into an intuitive orthogonality constraint on the gradients of the two group’s mean
responses. In the linear mixing model, this constraint is satisfied only if either there is no stimulus mixing at all,
or negative weights are allowed. We also found that random mixing by individual neurons, assuming that there
is no restriction to non-negative weights, performs remarkably well, especially in large populations. This result is
reminiscent of other cases where random solutions have been found to perform well (Rigotti et al., 2010; Barak et
al., 2013) and calls for a more general account of the effectiveness of such random solutions in diverse computational
problems in neuroscience.

40

A stimulus mixing problem similar to the one investigated in this paper has been studied previously for temporal
signals (White et al., 2004; Ganguli et al., 2008). Stimulus mixing in this context refers to the mixing of signals
at different time points in the responses of a recurrently connected dynamical population of neurons. White et
al. (2004) show that neural networks with an orthogonal connectivity matrix can achieve optimal estimation
performance for temporal signals uncorrelated across time. We note that this is formally similar to the solution for
the optimal mixing matrix in our toy stimulus mixing problem, where we found that orthogonal mixing matrices
are optimal when neurons are independent.
Finally, our results suggest that psychophysical tasks that require the simultaneous encoding of multiple stimuli
can be informative about the brain’s decoding strategies. Although behavioral tasks with a single encoded stimulus
can already provide useful information about the brain’s decoding schemes (Hohl et al., 2013; Haefner et al., 2013),
tasks with multiple encoded stimuli can yield additional and possibly richer information about the decoder. For
example, an optimal decoder predicts specific types of correlations between the estimates of two stimuli in a task
where both stimuli have to be estimated simultaneously (Figure 4). If the pattern of correlations observed in such
an estimation task is found to be inconsistent with the predicted correlations from an optimal decoder, this may be
taken as evidence against optimal decoding. Similarly, different types of decoders make different predictions about
the stimulus dependence of encoding accuracy even in tasks that require the estimation of a single target stimulus
among multiple stimuli (e.g. compare Figure 1 vs. Figure 8 for the stimulus dependence of encoding accuracy using
the optimal and OLE decoders, respectively). Again, such differences can be used to rule in or rule out certain
decoding schemes as plausible decoding strategies the brain might be using in a given psychophysical task.

References
[1] Abbott LF, Dayan P (1999) The effect of correlated variability on the accuracy of a population code. Neural
Comput 11:91-102.
[2] Allman J, Miezin F, McGuinness E (1985) Stimulus specific responses from beyond the classical receptive field:
neurophysiological mechanisms for local-global comparisons in visual neurons. Annu Rev Neurosci 8:407-30.
[3] Barak O, Rigotti M, Fusi S (2013) The sparseness of mixed selectivity neurons controls the generalizationdiscrimination trade-off. J Neurosci 33:3844-56.
[4] Beck DM, Kastner S (2007) Stimulus similarity modulates competitive interactions in human visual cortex. J
Vis 7:1-12.
[5] Britten KH, Heuer HW (1999) Spatial summation in the receptive fields of MT neurons. J Neurosci 19:5074-84.

41

[6] Busse L, Wade AR, Carandini M (2009) Representation of concurrent stimuli by population activity in visual
cortex. Neuron 64:931-42.
[7] Cavanaugh JR, Bair W, Movshon JA (2002) Selectivity and spatial distribution of signals from the receptive
field surround in macaque V1 neurons. J Neurophysiol 88:2547-2556.
[8] Cohen MR, Kohn A (2011) Measuring and interpreting neuronal correlations. Nat Neurosci 14:811-9.
[9] Duncan J (2001) An adaptive coding model of neural function in prefrontal cortex. Nat Rev Neurosci 2:820-9.
[10] Ecker AS, Berens P, Tolias AS, Bethge M (2011) The effect of noise correlations in populations of diversely
tuned neurons. J Neurosci 31:14272-83.
[11] Ganguli S, Huh D, Sompolinsky H (2008) Memory traces in dynamical systems. PNAS 105:18970-5.
[12] Haefner RM, Gerwinn S, Macke JH, Bethge M (2013) Inferring decoding strategies from choice probabilities
in the presence of correlated variability. Nat Neurosci 16:235-42.
[13] Hohl SS, Chaisanguanthum KS, Lisberger SG (2013) Sensory population decoding for visually guided movements. Neuron 79:167-79.
[14] Ma WJ, Husain M, Bays PM (2014) Changing concepts of working memory. Nat Neurosci 17:347-56.
[15] MacEvoy SP, Epstein RA (2009) Decoding the representation of multiple simultaneous objects in human
occipitotemporal cortex. Curr Biol 19:943-947.
[16] MacEvoy SP, Tucker TR, Fitzpatrick D (2009) A precise form of divisive suppression supports population
coding in primary visual cortex. Nat Neurosci 12:637-45.
[17] Nandy AS, Sharpee TO, Reynolds JH, Mitchell JF (2013) The fine structure of shape tuning in area V4.
Neuron 78:1102-15.
[18] Nemenman I, Shafee F, Bialek W (2002) Entropy and inference, revisited. In T. G. Dietterich, S. Becker, and
Z. Ghahramani, editors, Adv Neural Inf Proc Syst 14, Cambridge, MA, MIT Press.
[19] Orhan AE, Jacobs RA (2013) A probabilistic clustering theory of the organization of visual short-term memory.
Psychol Rev 120:297-328.
[20] Pestilli F, Carrasco M, Heeger DJ, Gardner JL (2011) Attentional enhancement via selection and pooling of
early sensory responses in human visual cortex. Neuron 72:832-846.
[21] Recanzone GH, Wurtz RH, Schwarz U (1997) Responses of MT ans MST neurons to one and two moving
objects in the receptive field. J Neurophysiol 78:2904-2.

42

[22] Reynolds JH, Chelazzi L, Desimone R (1999) Competitive mechanisms subserve attention in macaque areas
V2 and V4. J Neurosci 19:1736-53.
[23] Reynolds JH, Heeger DJ (2009) The normalization model of attention. Neuron 61:168-85.
[24] Rigotti M, Ben Dayan Rubin D, Wang XJ, Fusi S (2010) Internal representation of task rules by recurrent
dynamics: the importance of the diversity of neural responses. Frontiers in Computational Neuroscience 4:24.
[25] Rigotti M, Barak O, Warden MR, Wang XJ, Daw ND, Miller EK, Fusi S (2013) The importance of mixed
selectivity in complex cognitive tasks. Nature 497:585-90.
[26] Salinas E (2006) How behavioral constraints may determine optimal sensory representations. PLoS Biology:
4(12):e387.
[27] Salinas E, Abbott LF (1994) Vector reconstruction from firing rates. J Comput Neurosci 1:89-107.
[28] Schwartz O, Simoncelli E (2001) Natural signal statistics and sensory gain control. Nat Neurosci 4:819-825.
[29] Sompolinsky H, Yoon H, Kang K, Shamir M (2001) Population coding in neuronal systems with correlated
noise. Phys Rev E 64:051904.
[30] Warden MR, Miller EK (2007) The representation of multiple objects in prefrontal neuronal delay activity.
Cereb Cortex 17:41-50.
[31] Warden MR, Miller EK (2010) Task-dependent changes in short-term memory in the prefrontal cortex. J
Neurosci 30:15801-10.
[32] White OL, Lee DD, Sompolinsky H (2004) Short-term memory in orthogonal neural networks. Phys Rev Lett
92:148102.
[33] Zoccolan D, Cox DD, DiCarlo, JJ (2005) Multiple object response normalization in monkey inferotemporal
cortex. J Neurosci 25:8150-8164.
[34] Zoccolan D, Kouh M, Poggio T, DiCarlo JJ (2007) Trade-off between object selectivity and tolerance in monkey
inferotemporal cortex. J Neurosci 27:12292-12307.

43

