Symmetry invariance for adapting biological systems
Oren Shoval1 , Uri Alon1 , and Eduardo Sontag2

arXiv:1012.2782v1 [cs.SY] 13 Dec 2010

December 14, 2010

1

Introduction

We study in this paper certain properties of the responses of dynamical systems to external inputs. Our results are purely mathematical and thus of wide applicability, but our motivation arises
from molecular systems biology. Indeed, the behavior, adaptability, and survival of organisms depends critically upon their capability to formulate appropriate responses to chemical and physical
environmental cues. In particular, signal transduction and gene regulatory networks in individual
cells mediate the processing of measured chemical concentrations and physical conditions, such
as ligand concentrations or stresses, eventually leading to regulatory changes in metabolism and
gene expression. Often, the ultimate goal of these changes is to maintain a narrow range of concentration levels of vital quantities (homeostasis, adaptation) while at the same time appropriately
reacting to changes in the environment (signal detection). Much theoretical, modeling, and analysis effort has been devoted to the understanding of these questions, traditionally in the context of
steady-state responses to constant or step-changing stimuli.
In this work, we are concerned with questions that complement the analysis of simple temporal
inputs and steady-state responses, focusing on certain properties of transient behaviors, both for
simple stimuli like step changes and for more complex time-varying input profiles. The study of
transient responses is of a central concern in cell biology, since behavior at the time-scale of signaling may have important consequences for cell survival. Moreover, typical signals encountered
by cells in their natural environments may well exhibit interesting temporal information, and thus
characterizing responses to fluctuating temporal patterns may provide new insights regarding cell
behavior. Such responses and the analysis of transient behavior, can also help rule out postulated
mechanisms when tested through modern experimental tools which allow for fine spatiotemporal
resolution in measurements. This type of model falsification is of course routine, but the broader
framework allows for testing of a richer class of “phenotypes” and may thus help guide searches
for yet-unknown molecular mechanisms.
The immediate motivation for this work is the recent discovery of an important transient property, related to Weber’s law in psychophysics: fold-change detection (FCD) in adapting systems,
the property that scale uncertainty does not affect responses. FCD appears to play an important role
1 Department of Molecular Cell Biology, Weizmann Institute of Science, Rehovot, Israel
2 Department of Mathematics, Rutgers University, Piscataway, NJ, USA

1

in key signaling transduction mechanisms in eukaryotes, including the ERK and Wnt pathways,
as well as in Escherichia ecoli and possibly other prokaryotic chemotaxis pathways [1–3]. The
mathematical analysis of FCD was started in [3, 4]. In this paper, we provide further theoretical
results regarding this property. Far more generally, we develop a necessary and sufficient characterization of adapting systems whose transient behaviors are invariant under the action of a set
(often, a group) of symmetries in their sensory field. A particular instance is FCD, which amounts
to invariance under the action of the multiplicative group of positive real numbers. Our main result is framed in terms of a notion which considerably extends equivariant actions of compact Lie
groups.
This paper is organized as follows. Section 2 introduces the main definitions and the statement of
the main result of this paper. Section 3 explains illustrates how the main result can be used to check
invariance in a number of simple examples. Section 4 has the proof of the main theorem, as well as
a self-contained review of some key concepts needed from nonlinear control theory. Section 5 fillsin a stability proof that is needed in order to justify that several of our simple examples are indeed
adapting systems. Section 6 compares feedforward and feedback architectures, in the context of
the “internal model principle” of control theory. Section 7 provides a simple result showing that
search strategies in sensory fields subject to symmetries are invariant, provided that the underlying
system itself be invariant.

2

Notations, definitions, and statement of main theorem
We study dynamical systems with inputs and outputs:
ż = F(z, u) ,

y = h(z) ,

(1)

where F, h are functions which describe respectively the dynamics and the read-out map. Equation (1) is meant as shorthand for
dz
(t) = F(z(t), u(t)) ,
dt

y(t) = h(z(t)) .

Here, u = u(t) is a generally time-dependent input (also called, depending on the context, a “stimulus” or “excitation”) function, z(t) is an n-dimensional vector of state variables, and y(t) is the
output (“response” or “reporter” variable).
The paradigm of studying systems in the form (1) is standard in in control systems theory [5].
Typically, y(t) is just a read-out of one of the components, zi (t), of z(t). However, it is also
sometimes natural to take a more complicated function y(t) = h(z(t)) of the coordinates of z than
just picking an individual zi . For instance, suppose that z1 (t) represents the concentration of the
free form of an enzyme E, that z2 (t) is the concentration of E complexed with some substrate, and
that these two species are indistinguishable by a Western blot assay measurement. Then the sum
y = h(z) = z1 + z2 might be the reporter variable of interest. More generally, the theory does not
change substantially if we allow the output variable y(t) to be a function of the current input as
well as on the current state, y = h(z, u). It is notationally and technically more convenient to take
y = h(z), so we do that here. In any event, one could add a new variable zn+1 to z = (z1 , . . . , zn )
2

and a differential equation εżn+1 = h(z, u) − zn+1 which (provided 0 < ε  1) quickly equilibrates
to y = zn+1 = h(z, u), and with this small modification, now y depends only on a single coordinate,
zn+1 , of an extended state vector (z1 , . . . , zn+1 ), and does not directly depend on the current input.
In order to describe positivity of variables as well and other constraints, we introduce the following additional notations. States, inputs, and outputs are constrained to lie in particular subsets,
which we call Z, U, and Y respectively, of Euclidean spaces Rn , Rm , Rq . For example, U = R>0
means that the input values must be scalar (m = 1, U ⊂ R1 ) and positive. The functions f, h are
differentiable. We will assume that for each piecewise-continuous input u : [0, ∞) → U, and each
initial state ξ ∈ Z, there is a (unique) solution z : [0, ∞) → Z of (1) with initial condition z(0) = ξ,
which we write as
ϕ(t, ξ, u) ,
and we denote the corresponding output y : [0, ∞) → Y, given by h(ϕ(t, ξ, u)), as
ψ(t, ξ, u)
(see [5] for more discussion, properties of ODE’s, global existence of solutions, etc.).
Since we are interested in adapting systems, will assume that for each constant input u(t) ≡ ū,
there is a unique steady state of the system (which depends on the particular input). We denote
this steady state by σ(ū). In other words, z̄ = σ(ū) the unique solution of the algebraic equation
F(z̄, ū) = 0. Finally, we will assume that this steady state is globally asymptotically stable (GAS).
This means that σ(ū) is Lyapunov stable and globally attracting for the system when the input is
u(t) ≡ ū:
lim ϕ(t, ξ, u) = σ(ū)
t→∞

for every initial condition ξ ∈ Z. Multi-stable systems may be considered as well, of course, but
the definitions to follow become very cumbersome.
We will illustrate our results using the two sets of examples that are shown in Figs. 1 and 2. In

ẋ = α(y − y0 )
ẏ = βu − µx − γy
(a) linear

ẋ = αx(y − y0 )
u
ẏ = β − γy
x

ẋ = α(y − y0 )
ẏ = β ln u − µx − γy
(b) loglinear

(c) nonlinear I

ẋ = αx(y0 − y)
ẏ = βux − γy
(d) nonlinear II

Figure 1: Integral feedback systems (assuming u > 0 in (b), and u, x > 0 in (c,d))
the equations shown in these figures, the vector z = (z1 , z2 ) = (x, y) has dimension n = 2 and the
output is the second component, h(z) = y. In other words, in these examples, F(z, u) = F(x, y, u) is
a vector function with two components, which for n = 2 we write as ( f (x, y, u), g(x, y, u)). We have
that f (x, y, u) is α(y − y0 ) in Fig. 1(a,b), αx(y − y0 ) in Fig. 1(c,d), and αu − δx, in Fig. 2, and g(x, y, u)
3

ẋ = αu − δx
u
ẏ = β − γy
x

ẋ = αu − δx
ẏ = βu − γxy

(a) intermediate inhibits formation

(b) intermediate promotes degradation

Figure 2: Incoherent feedforward loops (IFFL) (assuming u, x > 0 in (a))
is βu − µx − γy, in Fig. 1(a), β ln u − µx − γy, in Fig. 1(b), β ux − γy in Fig. 1(c) and Fig. 2(a), βux − γy
in Fig. 1(d), and βu − γxy in Fig. 2(b). The constants α, β, . . . are positive numbers. We emphasize
that our theory is valid for any dimension n; these examples are only picked for illustration.

2.1

Perfect adaptation

Definition 2.1 The system (1) perfectly adapts to constant inputs provided that the steady-state
output h(σ(ū)) equals some fixed y0 ∈ Y, independently of the particular input value ū ∈ U.

Adaptation (we will just say “adapt” instead of “perfectly adapt to constant inputs” in what follows) means that the steady-state output value is independent of the actual value of the input, if
the input is constant. This property may be achieved by differentiating the input signal before it is
further processed by the system. However, differentiation is very sensitive to high-frequency noise,
and in fact there is no need for differentiation to be explicitly performed: there are several alternative mechanisms, such as those represented in Figs. 1 and 2, integral feedback and incoherent
feedforward loops respectively, that are also capable of achieving adaptation.
In integral feedback systems, an internal “memory” variable keeps track of the accumulated
(i.e., integrated) difference between the current value y(t) of the response variable and its desired
steady-state value y0 . A difference, or a nonlinear comparison such as a ratio, is performed between
this memory variable and the current input, thus providing an “error” signal that is used to drive
the feedback mechanism that brings the system back to its default value. Integral feedback is
recognized as a key feature of perfectly adapting biological systems, both at the physiological and
cellular level, such as, for example, in blood calcium homeostasis [6], in neuronal control of the
prefrontal cortex [7], in the regulation of tryptophan in E. coli [8], and in E. coli chemotaxis [9].
Fig. 1(a) shows the linear integral feedback configuration (PI, or proportional-integral, control)
that is classically treated in control theory. When u is constant, the unique steady state of this
system is given by x̄ = (βu − γy0 )/µ and ȳ = y0 . Thus, this system adapts: the steady state value of
y is independent of the input u. Moreover, since the eigenvalues of any matrix of the form
!
0 α
−µ −γ
are negative, it is clear that this system is globally asymptotically stable: x(t) → x̄ and y(t) → ȳ =
y0 as t → ∞. (Taking α and µ both negative instead of positive gives the same conclusions.) Two
other integral feedback configurations, also perfectly adapting, are shown in Fig. 1. In (b), a “loglinear system,” the only difference with (a) being that the input is logarithmically pre-processed;
4

this does not change the conclusions of adaptation and stability. In system (c), the memory variable
feeds upon itself, and the ratio u/x, instead of a difference, is used to compare the current input
and memory values. In system (d), the memory variable also feeds upon itself, and the product
ux is used in the feedback term to y. Both (c) and (d) adapt (to ȳ = y0 ), and x̄ = βµ/(γȳ) in (c),
x̄ = γȳ/(βµ) in (d). Stability is a bit more subtle, and is based on a control-Lyapunov approach [5]
that recasts (c) as a Hamiltonian system with added damping, see Section 5. (The ratio “u/x”
in (c) is not a natural choice for biological models; however, one may think of this term as an
approximation of a Michaelis-Menten inhibition term u/(Km + x), with Km  1.)
A different type of architecture is based on feedforward as opposed to feedback interconnections. Feedforward circuits are ubiquitous in biology, as emphasized in [10], where they were
shown to be over-represented in E. coli gene transcription networks, compared to other “motifs”
involving three nodes. Similar conclusions apply to certain control mechanisms in mammalian
cells [11]. A large number of papers have been devoted to the signal-processing capabilities of the
feedforward motif, notably [12] which looked into its properties as a “change detector” (essentially,
sensitivity to changes in the magnitude of the input signal), and [13] which studied its optimality
with respect to periodic inputs. Comparisons with other “three node” architectures with respect to
the trade-off of sensitivity versus noise filtering are given in [14]. Other references on feedforward
circuits include [15] (showing their over-representation at the interface of genetic and metabolic
networks), [16] (classification of different subtypes of such circuits), and [17] (classification into
“time-dependent” versus “dose-dependent” biphasic responses, which are in a sense the opposite
of adapted responses).
In particular, in incoherent feedforward loops (IFFL), as in Fig. 2, the input u directly helps
promote formation of the reporter y and also acts as a delayed inhibitor, through an intermediate
variable x. This “incoherent” counterbalance between a positive and a negative effect gives rise,
under appropriate conditions, to adaptation. The reference [17] provides a large number of incoherent feedforward input-to-response circuits, which participate in EGF to ERK activation [18,19],
glucose to insulin release [20,21], ATP to intracellular calcium release [22,23], nitric oxide to NFκB activation [24], microRNA regulation [25], and many others. Several varieties of IFFL circuits
have been often proposed for perfect adaptation to constant signals in biological systems. Notably,
the IFFL shown in Fig. 2(b), often called the “sniffer” [26, 27], appears in slightly modified forms
in models for Dictyostelium chemotaxis and neutrophils [28, 29], microRNA-mediated loops [30],
and E. coli carbohydrate uptake via the carbohydrate phosphotransferase system [31] and other
metabolic systems [32]. The work [33] shows experimentally and analytically that IFFL’s are
especially well-suited to controlling protein expression under DNA copy variability. For both systems in Fig. 2, the unique steady state, when the input u is constant, has coordinates x̄ = αu/δ and
ȳ = y0 = βδ/(αγ). Since ȳ is independent of u, the system adapts. Global asymptotic stability for
(a) follows from the fact that the x-subsystem is linear and stable, and the y-subsystem is a stable
linear system driven by the converging signal u/x. For (b) (and several variations of this system),
the GAS property is studied in [27].

5

2.2

Invariance

As mentioned in the introduction, we wish to study the invariance of outputs under input transformations. The original motivation was the study of the particular case of scale invariance, which
is described through the following thought experiment.
2.2.1

A special case: scale-invariance (FCD)

Suppose that a system that adapts has had a chance to “pre-adapt” to a certain constant (“background”) level ū of the input, for t < 0, and that now we present the system with the new input
u(t), t ≥ 0. Let y1 (t) be the output function that results. Next imagine that we allowed the same
system to pre-adapt, instead, to pū for t < 0, and we now present the system with pu(t) for t ≥ 0,
where p is some positive scalar. Let y2 (t) be the resulting output. Scale invariance means that the
outputs of the two experiments should be the same: y1 (t) = y2 (t) for all t > 0. In other words,
for any two inputs u(t) and pu(t), as in Fig. 3, and no matter what positive number or “scale” p

Figure 3: Two scaled inputs
we picked, the entire shape of the response, including amplitude and duration, is identical. As an
example, a step change in input from, say, a constant level 2 to a constant level 4, should result in
precisely the same output as a step from constant level 5 to constant level 10 (we scaled everything
by p = 2), Fig. 4(d). On the other hand, a change from, say, level 5 to 25 (which has a fold-change
of 5, instead of 2) will typically lead to a different response. Thus, another way to describe this
invariance property is to say that the only potentially detectable differences in response are due to
fold changes, and this motivates the name fold-change detection (FCD) [3, 4]. FCD represents a
particular type of adaptation, one in which there is with robustness to scale uncertainty, and it can
be found at many levels of biological organization. A weak version is present in the Weber-Fechner
law logarithmic sensing feature in psychophysics: many sensory systems (for weight, vision, hearing) produce responses whose maximal amplitude only depends on the ratio between the stimulus
and a background or starting value, Fig. 4(c). It was also recently discovered that the transient
responses of several biological cellular signaling systems [1, 2] display FCD features.
We can formally define the FCD property as follows. Denoting by “πu” the input t 7→ pu(t), and
by “πū” the constant pū, the equality of outputs
ψ(t, σ(ū), u) = ψ(t, σ(πū), πu) ∀ t ≥ 0

(2)

should hold for all possible p > 0, as well as all constant inputs ū ∈ U and inputs u = u(t)
with values in U. (Observe that the requirement that ū and u take values in U serves to impose
constraints such as, for example, positivity.)
6

(a) scaled inputs

(b) adapting system

(c) Weber-like

(d) FCD

Figure 4: In an adapting system, the response to scaled steps (a) returns to the same adapted
value, but may differ in time (b), or may produce the same maximal response (c) or even identical
transients (d))
Equipped with this formalism, we may study a far more general question, namely invariance
with respect to any set of transformations, not merely scalings but also far more general invariances
to sensory field changes: translational, rotational or reflection symmetries, anisotropic dilations,
projective transformations, and so forth.
2.2.2

General case

Suppose given a set P of continuous and onto input transformations π : U → U:
{π : U → U , π ∈ P} .
For an input u(t), we denote by “πu” the function of time that equals π(u(t)) at time t. (For notational simplicity, we write “πu” rather than “π(u)” when there is no risk of confusion, but we
emphasize that there is no requirement that the mapping π be linear.) The continuity assumption is
only made in order to insure that πu is a piecewise continuous function of time if u is, as technically required when using it as an input to a differential equation. The ontoness assumption, that
is, πU = U, is made for technical reasons in the proof of the main theorem.
Definition 2.2 The system (1) has response invariance to symmetries in P or, for short, is Pinvariant if (2) holds for all t > 0, all inputs u = u(t), all constants ū, and all transformations π ∈ P.

Under the assumption that that the action of P is transitive, i.e., for any two ū, v̄ ∈ U, there is
some π such that v̄ = πū, P-invariance implies perfect adaptation, because the outputs in (2) must
coincide at time zero, and any two inputs can be mapped to each other.
Examples: In the special case of the transitive action with P = U = R>0 and πu = pu, we obtain
the FCD property. Another example of a set of symmetries P is as follows. Suppose that U = Rn
and P consists of all the transformations πR (u) := Ru, for R ∈ SO (n), the special orthogonal group.
That is, we transform inputs by multiplication with a rotation matrix R. In dimensions 2 or 3, this
can be used to impose the requirement that a visual sensing system should react the same if the
visual field is rotated. Another slightly different example would be that in which R ∈ O(n), the
orthogonal group, meaning that we want invariance with respect to reflections as well as rotations.

7

2.3

Equivariances

We now introduce a concept that leads to an effective criterion for checking FCD, and more
generally arbitrary P-invariance, without having to compute the solutions ψ(t, σ(πū), πu). We make
the definition for arbitrary systems as in (1), ż = F(z, u), y = h(z).
Definition 2.3 Given a system (1) and a set of input transformations P, a parametrized set of
differentiable mappings {ρπ : Z → Z}π∈P is a P-equivariance family provided that, for each π:
F(ρπ (z), πu) = ρ0π (z)F(z, u) and

h(ρπ (z)) = h(z) for all z ∈ Z, u ∈ U

(3)

(ρ0π = Jacobian matrix of ρπ ). If (3) holds, we say that the system is ρπ -equivariant under the input
transformation π.

The first part of Equation (3) is a first order quasilinear partial differential equation on the n
components of the vector function ρπ , and the second part is an additional algebraic constraint on
these components. for all u ∈ U. We omit the subscript π when clear from the context. Quasi-linear
first-order PDE’s appear in related questions in control theory, for instance [5] in Hamilton-JacobiBellman’s approach to optimal control and in feedback linearization based on Frobenius’ Theorem.
They may be in principle solved using the method of characteristics [34].
Our notion of equivariance generalizes a mathematical concept fundamental in group theory,
and specifically in the symmetry analysis of nonlinear dynamical systems [35, 36] with the same
name. A parametrized vector field F is said to be ρ-equivariant, or ρ is a symmetry of F, if, for
each solution z(t) of ż = F(z, u), ρ(z(t)) is also a solution. This property is equivalent to the PDE
F(ρ(z), u) = ρ0 (z) f (z, u) [35] that is part of the equivariance definition, when π = identity. We
are generalizing this concept in several ways. First of all, we must consider the far more general
case π , identity; in fact, in our context, π = identity is not of any interest whatsoever, since
we are precisely interested in the effect of the input transformations. Second, it is essential to our
definition that we include the algebraic “boundary condition” h(ρπ (z)) = h(z), Finally, in dynamical
systems one typically studies equivariances only with respect to Lie group actions. Moreover, since
compact Lie groups acting on Euclidean space can be identified with subgroups of the orthogonal
group O(n), one finds in the classical definitions [35] only linear maps ρ0 (z) = γz with γ ∈ O(n)
being considered (so ρ0 (z) = γ).

2.4

Statement of main theorem

Our main result is as follows. It is proved in Section 4.
Theorem 1 An analytic and irreducible system is P-invariant if and only if there exists a Pequivariance family.
An analytic system is one for which all the functions defining the system are real-analytic on
the state coordinates. This means that they can be expanded into locally convergent power series
around each point. Every function made up of elementary algebraic compositions of elementary
8

functions is analytic; this includes all expressions involving polynomials and, more generally,
any well-defined (no poles) rational functions (so mass-action kinetics and Hill-type models with
any integer Hill coefficients are allowed) as well as trigonometric functions, exponentials, and
logarithms in any combinations. All our examples are analytic, as long as we restrict expressions
such as u/x to x > 0 (or x < 0), so that there are no poles.
An irreducible system is one for which no conservation laws restrict motions to proper submanifolds (accessibility property) and no pairs of distinct states give rise to the same input/output
behavior (observability property). We define precisely accessibility and observability in Section 4.
Irreducibility is a weak technical assumption; we will show that these two properties hold for all
the systems in Figs. 1 and 2. Irreducible systems, which are also called “minimal” or “canonical”
in the control theory literature, are minimal, in the sense that no lower-dimensional subsystem has
an identical input/output behavior [5,37–39]. Analogous, but simpler, notions of irreducibility also
appear in areas such as group representations (Schur’s Lemma).

3

Examples of finding symmetries using the main theorem

We show here how Theorem 1 allows one to immediately determine invariance properties for
large classes of two-dimensional systems, including the integral feedback and feedforward examples shown in Figs. 1 and 2. In all these cases, the PDE for equivariances, if there is one, can be
easily solved for in closed form. We consider two-dimensional systems with output equal to one
of the coordinates. We write z = (x, y), F(z, u) = ( f (x, y, u), g(x, y, u)):
ẋ = f (x, y, u)
ẏ = g(x, y, u)
h(x, y) = y
and wish to determine for which possible input set mappings π : U → U there is an associated
equivariance ρ = ρπ . We drop the subscript and write ρ = (ρ x , ρy ). Since h(x, y) = y, the condition
h(ρ(x)) = h(x) says that ρy (x, y) = y. Thus finding ρ is equivalent to finding its x-component, a
function ρ x that satisfies:
∂ρ x
(x, y) f (x, y, u)
∂x
g(ρ x (x, y), y, πu) = g(x, y, u)
f (ρ x (x, y), y, πu) =

(no derivative in the second equation because, ∂ρy /∂y = 1). This is a scalar first order quasi-linear
PDE subject to a side algebraic “boundary condition”.
We specialize next to special two cases that cover many examples of interest. Particular instances
of the first case are the systems in Figs. 1(c,d) and 2(a). A particular instance of the second case
is the system in Fig. 1(a). We assume that the systems in both of the next Lemmas are irreducible.
For all our examples, irreducibility is checked in Section 4.
Lemma 3.1 Suppose that:

g(x, y, u) = G(uβ xµ , y)
9

and G(·, y) is one-to-one for each fixed y. (Assuming x > 0 if µ < 0 or u > 0 if β < 0.) Then, the
only possible symmetries are fold-changes πu = pu. Furthermore, the system is invariant under a
set P of such symmetries if and only
p−β/µ f (x, y, u) = f (p−β/µ x, y, pu) for all x, y, u
and each p in the set.
In the special case in which β=1 and µ=−1, that is, if g depends on the ratio u/x, this means that
f must satisfy:
p f (x, y, u) = f (px, y, pu)
and in the special case β=µ=1, f must satisfy p−1 f (x, y, u) = f (p−1 x, y, pu). In either special
case, if f is independent of u, then response invariance to all scaling transformations (P = R>0 ) is
equivalent to the requirement that f be be homogeneous of degree 1 in x.
Proof. Since G is one to one on y,




G (πu)β (ρ x (x, y))µ , y = g(ρ x (x, y), y, πu) = g(x, y, u) = G uβ xµ , y for all x, y, u
implies that:

(πu)β (ρ x (x, y))µ = uβ xµ for all x, y, u

or, equivalently:
 πu β
u

=

ρ x (x, y)
x

!−µ
for all x, y, u .

Define
p :=

πu0
u0

! β1

for any fixed but arbitrary element u0 ∈ U. It follows that
πu = pu and ρ x (x, y) = p−β/µ x for all x, y, u ,
from which all the conclusions are immediate.
Lemma 3.2 Suppose that:
g(x, y, u) = G(µx + βu, y)
and G(·, y) is one-to-one for each fixed y. Then, the only possible symmetries are translations
πu = p + u. Furthermore, the system is invariant under a set P of such symmetries if and only
f (x, y, u) = f (−(β/µ)p + x, y, p + u) for all x, y, u .
and each p in the set.

10

Proof. Since G is one to one on y,
G (µρ x (x, y) + βπu, y) = g(ρ x (x, y), y, πu) = g(x, y, u) = G (µx + βu, y)
implies that:
µρ x (x, y) + βπu = µx + βu for all x, y, u
or, equivalently:

µ
πu − u = − (ρ x (x, y) − x) for all x, y, u .
β

Define
p := πu0 − u0
for any fixed but arbitrary element u0 ∈ U. It follows that
πu = p + u and ρ x (x, y) = −

βp
+ x for all x, y, u ,
µ

from which all the conclusions are immediate.
We can now quickly classify the examples shown in Figs. 1 and 2.
The linear integral feedback system in Fig. 1(a) fits the form in Lemma 3.2, so it can only
be P-invariant with respect to transformations u 7→ p + u, and the only possible equivariance is
ρ x (x, y) = x + βp/µ. Since f (x, y, u) is independent of x and u, this is indeed an equivariance. This
this system is P-invariant with respect to translations.
The systems in Fig. 1(c,d) and Fig. 2(a) all fit the form in Lemma 3.1, so they can only be Pinvariant with respect to scaling transformations u 7→ pu, and the only invariance is equivalent to
the condition
pε f (x, y, u) = f (pε x, y, pu)
where ε is +1 and −1 for the systems in Fig. 1(c,d) respectively, and is +1 for the system in
Fig. 2(a). In Fig. 1(c,d), the value of ε is irrelevant, because f (x, y, u) is independent of u and is
homogeneous of degree 1 in x, so the property holds. In Fig. 2(a), f is homogeneous of degree 1 in
x and u simultaneously, so again the property holds. In summary, all three systems are P-invariant
with respect to scalings P.
The log-linear system in Fig. 1(b) is also P-invariant for the set of scalings. This may be shown
with the equivariance ρ x (x, y) = x + β ln p/µ.
We remark that, generalizing Fig. 1(a) and Fig. 2(a), any n-dimensional linear system ẋ = Ax+bu
with a stable A and h(x) = cx such that cA−1 b = 0 (i.e., its DC gain is zero) is P-invariant for
u 7→ p + u, with ρ(x) = x − A−1 bp. The corresponding log-linear system, in which ẋ = Ax + b ln u,
is invariant with respect to scalings.
Finally, we study the “sniffer” IFFL shown in Fig. 2(b). The equation “g(ρ x (x, y), y, πu) =
g(x, y, u)” means that βπu − γρ x (x, y)y = βu − γxy for all x, y, u, and thus evaluating at y = 0 it
follows that πu = u (assuming β , 0). So no nontrivial P-equivariance exists. By the necessity
part of Theorem 1, we conclude that this system is not P-invariant for any possible P.

11

Remark 3.3 Interestingly, although not invariant, the system in Fig. 2(b), as well as many other
examples from [27], satisfy an “approximate” invariance property, in the following sense. Suppose
that the y variable varies faster than the x variable, so that one may make a quasi-steady state
approximation βu − γxy = 0. This allows one to reduce to a one-dimensional system ẋ = αu − δx
with output y = (β/γ)(u/x). Scalings u 7→ pu and x 7→ px preserve the equations, thus showing
that the reduced system is P-invariant with respect to scaling. (More generally, given a linear
system with an output that depends on the ratio xi /x j of two variables xi and x j , one obtains scaleinvariance.) This means that the original system is “close” to having the scale invariance property.
A precise statement can be made using singular perturbation theory. This observation was made
several years ago in the context of models of Dictyostelium chemotaxis and neutrophils [40]. 
Remark 3.4 An example with vector inputs is as follows. Suppose that we consider a vector
integral feedback system of the following form:
ẋ = (y − y0 )x
ẏ = G(hu, xi, y)
with output y. The state-space Z is Rn+1 : x and u are n-dimensional real vectors and y is scalar,
and hu, xi denotes the inner (dot) product of x and u. We claim that this system is P-invariant
with respect to the rotation group P = SO (n). Indeed, for each R ∈ SO (n) we may define the
equivariance ρR (x, y) by mapping (x, y) to the n + 1-vector with first n components Rx and last
component y. Since ρR is linear, and its partial derivative with respect to y is 1 and its Jacobian
with respect to the x variables is R, we only need to check that R(y − y0 )x = (y − y0 )Rx, which
is true because y − y0 is a scalar, and that G(hRu, Rxi, y) = G(hu, xi, y), which is true because
R is an special orthogonal matrix. The exact same proof works for the larger orthogonal group
(reflection/rotations) O(n). We can also generalize to the case when P consists of completely
arbitrary nonsingular transformations (R ∈ GL(n)). In that case, we would take ρR (x) = (RT )−1 x
(inverse transpose) and use that hRu, (RT )−1 xi = hu, xi.


4

Details and proof of main theorem

The main theorem is stated for systems that are irreducible, meaning that the both the accessibility and the observability properties must hold. We define precisely and discuss these two properties
next.

4.1

The accessibility property and the accessibility Lie algebra

In order to define accessibility, we will need to employ the notion of accessibility Lie algebra
associated to a system (1), which we briefly review here; see [5] for further details and basic
properties. Recall first the notion of Lie bracket of two vector fields on a manifold, specialized
here to open subsets of Euclidean space Rn : for any two differentiable vector fields f, g : Z → Rn
defined in an open set Z ⊆ Rn , their Lie bracket is the new vector field
[ f, g] : Z → Rn
12

defined by the formula

∂g
∂f
f−
g.
∂x
∂x
When f, g are twice differentiable, [ f, g] is again differentiable, and one can take further brackets
such as [[ f, g], g]. More generally, if the vector fields are smooth (that is, infinitely differentiable),
one may take any number of iterated brackets.
[ f, g] :=

For any subset of smooth vector fields F on Z, the Lie algebra generated by F , denoted as FLA ,
is defined as the intersection of all the Lie algebras of vector fields which contain F . (The set of
all such algebras is nonempty, since it includes the algebra of all vector fields on Z.) Since the
intersection of any family of Lie algebras is also a Lie algebra, it follows that FLA is the smallest
Lie algebra of vector fields which contains the set F , and it coincides with the set obtained as
follows. Denote F0 := F , and, recursively, Fk+1 := {[ f, g] | f ∈ Fk , g ∈ F }, k = 0, 1, 2, . . ., as well
S
as F∞ := k≥0 Fk . Then, FLA is equal to the linear span of F∞ . (See [5], Lemma 4.1.4 for a
proof of this equality.) In summary, every element in the Lie algebra generated by the set F can
be expressed as a linear combination of iterated brackets of the form
[ f` , . . . , [ f3 , [ f2 , f1 ] . . .]] ,
for some fi ∈ F .
If the system (1) is such that the state-space Z is an open subset of Rn and all the vector fields
F = { f (·, u), u ∈ U} are smooth, we define its accessibility Lie algebra as FLA . For each z0 ∈
Z, one may consider the subspace FLA (z0 ) := {X(z0 ), X ∈ FLA } of Rn . The accessibility rank
condition is said to hold for the system if
FLA (z0 ) = Rn

for every z0 ∈ Z .

For analytic systems, the accessibility condition is equivalent to the property that the set of points
reachable from any given state z has a nonempty interior, as follows by a result of Sussmann [37]
that generalizes Nagano’s theorem [41] on integrability of involutive families of vector fields; see
a proof and more details in the textbook [5].
In the special case of input-affine systems, i.e. those defined by differential equations
ẋ = g0 (x) + u1 g1 (x) + . . . + um gm (x)

(4)

where gi , i = 0, . . . , m are m + 1 vector fields, it is not necessary to use all the vector fields in F
when generating FLA , since FLA (z0 ) = {g0 , . . . , gm }LA (z0 ) for all z0 . See [5], Lemma 4.3.3 for a
proof. (The technical condition in that Lemma that zero must belong to the input set U fails when
inputs are asked to be strictly positive, as in many of our applications. However, the Lemma also
holds when U consists of positive inputs, since a continuity argument makes clear that one obtains
the same spans if 0 is added to U.)
For example, we show next that all the systems in Figs. 1 and 2, which are all input-affine (with
m = 1), satisfy the accessibility rank condition.
ẋ = α(y − y0 ), ẏ = βu − µx − γy. Here:
!
α(y − y0 )
g0 =
,
µx − γy

0
β

g1 =
13

!
,

[g0 , g1 ] =

−αβ
γβ

!
.

Since the determinant of (g1 , [g0 , g1 ]) equals αβ2 , 0 at every x, the accessibility rank condition
holds.
ẋ = α(y − y0 ), ẏ = β ln u − µx − γy. This is the same as the previous case, in so far as the accessibility condition is concerned, because the set of vector fields F is identical to the previous one.
ẋ = αx(y − y0 ), ẏ = β ux − γy. Here:
g0 =

αx(y − y0 )
−γy

!
,

0

g1 =

!
,

β
x

−αβ
αβ
− x (y − y0 ) +

[g0 , g1 ] =

!
βγ
x

.

Since the determinant of (g1 , [g0 , g1 ]) equals αβ2 /x , 0 at every x, the accessibility rank condition
holds.
ẋ = αx(y0 − y), ẏ = βux − γy. Here:
!
αx(y − y0 )
g0 =
,
g1 =
−γy

0
βx

!
,

−αβx2
αβx(y − y0 ) + βγx

[g0 , g1 ] =

!
.

Since the determinant of (g1 , [g0 , g1 ]) equals αβ2 x3 , 0 at every x, the accessibility rank condition
holds.
ẋ = αu − δx, ẏ = β ux − γy. Here:
g0 =

−δx
−γy

!
,

g1 =

Since the determinant of (g1 , [g0 , g1 ]) equals
holds.
ẋ = αu − δx, ẏ = βu − γxy. Here:
!
!
−δx
α
g0 =
,
g1 =
,
−γxy
β

α

,

β
x

αβγ
x

αδ
βδ
+ βγx
x

!
[g0 , g1 ] =

!
.

, 0 at every x, the accessibility rank condition

[g0 , g1 ] =

αδ
αγy + βγx

!
,

[g1 , [g0 , g1 ] =

0
2αβγ

!
.

Since the determinant of (g1 , [g1 , [g0 , g1 ]]) equals 2α2 βγ , 0 at every x, the accessibility rank
condition holds.

4.2

The observability property

A system (1) is said to be observable, or to have the observability property, if no two distinct
states can give rise to an identical temporal response to all possible inputs. Formally:
ψ(t, z0 , u) = ψ(t, ze0 , u) ∀ u, t

⇒

z0 = ze0 .

For analytic input-affine systems (4) with output h = (h1 , . . . , h p ), one can restate the observability
property as follows. The observation space O associated to the system is the vector space spanned
by the set of all functions of the type:
Lgi1 . . . Lgik h j
(5)
14

(called the elementary observables of the system) over all possible sequences i1 , . . . , ik , k ≥ 0, out
of {0, . . . , m} and all j = 1, . . . , p, where LX H = ∇H · X is the directional or Lie derivative of the
function H with respect to the vector field X and one understands LY LX H as the iteration LY (LX H).
We include the case in which k = 0, in which case the expression in (5) is simply h j . Two states
z1 and z2 are said to be separated by O if there exists some H ∈ O such that H(z1 ) , H(z2 ).
Observability is equivalent to the property that any distinct two states can be separated by the
observation space. See [5], Remark 6.4.2 for a proof and discussion.
For example, it is very easy to see that all the systems in Figs. 1 and 2, which are all input-affine
(with m = 1 and p = 1), satisfy the observability condition. We must prove that if H(z1 ) = H(z2 )
for every elementary observable (5), then z1 = z2 . Since already with k = 0 we have that y1 =
h(z1 ) = h(z2 ) = y2 , it only remains to show that some linear combination of observables gives a
one-to-one function of x. Note that ∇h = (0, 1), so the dot product Lg H = ∇H.g simply picks out
the second coordinate of H.
ẋ = α(y − y0 ), ẏ = βu − µx − γy. Here:
!
α(y − y0 )
g0 =
,
µx − γy

g1 =

0
β

!
,

Lg0 h = µx − γy .

Since Lg0 h + γh = µx is one-to-one on x, the observability condition holds.
ẋ = α(y − y0 ), ẏ = β ln u − µx − γy. This is the same as the previous case, in so far as the observability condition is concerned, because ψ(t, z0 , u) is the same as ψ(t, z0 , log u) for the previous
system.
ẋ = αx(y − y0 ), ẏ = β ux − γy. Here:
αx(y − y0 )
−γy

g0 =

!
,

0

g1 =

!
,

β
x

Lg1 h =

β
.
x

Since Lg1 h is one-to-one on x, the observability condition holds.
ẋ = αx(y0 − y), ẏ = βux − γy. Here:
g0 =

αx(y − y0 )
−γy

!
,

0
βx

g1 =

!
,

Lg1 h = βx .

Since Lg1 h is one-to-one on x, the observability condition holds.
ẋ = αu − δx, ẏ = β ux − γy. Here:
g0 =

−δx
−γy

!
,

g1 =

α
β
x

!
,

Lg1 h =

β
.
x

Since Lg1 h is one-to-one on x, the observability condition holds.
ẋ = αu − δx, ẏ = βu − γxy. Here:
!
−δx
g0 =
,
g1 =
−γxy

α
β

!
,

Lg0 h = −γxy ,
15

Lg1 Lg0 h = −αγy − βγx .

Since Lg1 Lg0 h + αγhy = −βγx is one-to-one on x, the observability condition holds. (Observe that
we cannot argue simply with Lg0 h, because the function −γxy is not one-to-one on x in the special
case y = 0.)

4.3

Proof of Theorem 1

We must prove that, for analytic and irreducible systems, existence of a P-equivariance family
is sufficient as well as necessary for P-invariance.
Sufficiency.
Suppose given a π ∈ P and an associated equivariance ρ = ρ p . We claim that the steady-state
mapping σ interlaces π and its associated ρ = ρ p , in the sense that
ρ(σ(ū)) = σ(π(ū))
for every ū ∈ U. Indeed, from the property F(ρ(z), πu) = ρ0 (z)F(z, u), applied with any constant
input u(t) ≡ ū, and z = σ(ū), it follows in particular that F(ρ(σ(ū)), πū) = ρ0 (σ(ū))F(σ(ū), ū).
Now, F(σ(ū), ū) = 0, by definition of σ(ū), so also F(ρ(σ(ū)), πū) = 0, which this means that
ρ(σ(ū)) is the steady state σ(πū) corresponding to the constant input πū, as we claimed.
Now suppose that z(t) = ϕ(t, σ(ū), u) solves ż = F(z, u) with initial condition z(0) = σ(ū).
Consider z∗ (t) = ρπ (z(t)). Computing the derivative z˙∗ (t), and using the chain rule:
z˙∗ (t) = ρ0π (z(t))ż(t) = ρ0π (z(t))F(z(t), u(t)) = F(ρπ (z(t)), πu(t)) = F(z∗ (t), πu(t))
Moreover, z∗ (0) = ρπ (σ(ū)) = σ(πū), by the interlacing property. It follows that z∗ is the solution
with initial condition correcponding to the “preadapted” value σ(πū) and the input π(u(t)), i.e.
z∗ (t) = ϕ(t, σ(πū), πu). We conclude that
ψ(t, σ(ū), u) = h(z(t)) = h(ρπ (z(t))) = ψ(t, σ(πū), πu).
Necessity.
Suppose given an analytic and irreducible system that is P-invariant. Fix any π ∈ P. We must
find a differentiable mapping ρ = ρπ : Z → Z such that (3) holds: F(ρ(z), πu) = ρ0 (z)F(z, u) and
h(ρ(z)) = h(z). Let us consider a modified system in which G(z, u) = F(z, πu) and same output
k(z) = h(z). Then (3) asks that
ρ0 (z)F(z, u) = G(ρ(z), u) ,

k(ρ(z)) = h(z)

∀ z ∈ Z, u ∈ U.

(6)

In the language of [37], property (6) says that ρ should be an isomorphism between the two systems:
ż = F(z, u) , y = h(z)
(7)
and
ż = G(z, u) ,

y = k(z)

(8)

or, equivalently, that the diagram in Fig. 5 should be commutative, where we write “z · ut ” and
“z · πut ” to denote the states φ(t, z, u) and φ(t, z, πu) respectively.
16

z · ut

Z

-

Z
H

ρπ
?

Z

HH h
H

ρπ

Y

?

z · πut


-

HH
j
H


 h

*



Z

Figure 5: Equivariance as commutative diagram
Paraphrased in our language, Theorem 5 in [37] asserts the following. Suppose given any two
analytic and irreducible systems (7) and (8), and two initial states z0 and e
z0 respectively, with the
property that the input/output behaviors are the same:
e(t,e
ψ(t, zo , u) = ψ
zo , u)
for all t and all inputs u (where we use tildes for the ψ map of the second system), Then, there exists
an isomorphism ρ (that is, Equation (6) holds) such that also ρ(zo ) = e
z0 . To apply this theorem
to our case, we need to show that the modified system, with G(z, u) = F(z, πu), is irreducible (it
is clearly analytic, since the original system is) and we must find, for our original system and for
G(z, u) = F(z, πu), two respective initial states z0 ande
z0 that lead to the same input/output behavior.
This latter requirement is achieved by taking z0 = σ(u) for any fixed u, ande
z0 = σ(πu) for the same
u. The definition of P-symmetry is precisely the statement that the two input/output behaviors are
identical. We next verify that the modified system with G(z, u) = F(z, πu) is irreducible.
Accessibility: Since π is onto, the set of vector fields {F(·, u), u ∈ U} is included in the set of vector
fields {F(·, πu), u ∈ U}. Thus, the accessibility Lie algebra of the modified system can only be
larger, and hence the accessibility property for the original system guarantees that of the modified
system.
Observability: Suppose that two different states z1 , z2 give rise to different outputs in the original
system, when the input is u(t). Since π is onto, there is an input v such that u(t) = π(v(t)) for all
t. This means that the outputs corresponding to the initial states z1 and z2 in the modified system,
under the input v, will be different. This argument can be applied for any two distinct states. Thus,
the observability property for the original system guarantees that of the modified system.
This completes the proof of the main theorem.
Remark 4.1 The intuitive idea of the construction of ρ is not difficult to understand, and is standard in control theory [5, 37–39]: given an initial state x0 = σ(ū), input u, and time t > 0, we
look at the state z = ϕ(t, x0 , u) and the state e
z = ϕ(t, x0 , πu), and define ρ(z) = e
z. The accessibility
property says that this map is defined on an open subset of the state space. The interlacing property
represented by Fig. 5 is checked by seeing that the states ρ(z) · ut and z · πut are not distinguishable by any input/output experiments, and hence by observability must be the same. The technical
difficulties arise in proving the differentiability of ρ and its definition on the entire state space, not
merely a subset, and this was one of the main contributions of [37].

17

Remark 4.2 In the proof of the theorem, we only needed to know the existence of some two
initial states z0 and e
z0 that lead to identical behaviors. This means that, if we define a “weakly
invariant” system as one for which there exists some constant ū such that (2) holds: ψ(t, σ(ū), u) =
ψ(t, σ(πū), πu) for all inputs u and all t ≥ 0 (instead of asking that this holds for every ū), then weak
invariance implies the existence of an equivariance, and hence also invariance. (The irreducibility
property plays a subtle role in this argument.)


5

Stability result for nonlinear integral feedback systems

We wish to show the global asymptotic stability (GAS) of the unique steady-states (γȳ/(βµ), y0 )
and (γȳ/(βµ), y0 ) respectively, of the two nonlinear integral feedback systems in Fig. 1:
ẋ = αx(y − y0 )
ẋ = αx(y0 − y)
u
ẏ = β − γy
ẏ = βux − γy
x
for any constant input u, where we assume that x(t) > 0 and u > 0.
Lemma 5.1 Consider any two-dimensional system evolving on R2 having the following “nonlinear damping” form:
ẋ = g(y)
ẏ = − f (x) − k(y) ,
where f, g, k are functions that have positive derivatives. Suppose that (x0 , y0 ) is a steady state.
Then this is the unique steady state of the system, and it is globally asymptotically stable.
Corollary 5.2 Consider any two-dimensional system evolving on R>0 ×R and having the following
form:
ẋ = xg(y)
ẏ = − f (x) − k(y)
where k has positive derivatives and either (a) both f and g have positive derivatives or (b) both
f and g have negative derivatives. Suppose that (x0 , y0 ) is a steady state. Then this is the unique
steady state of the system, and it is globally asymptotically stable.
Proof. Suppose first that both f and g have positive derivatives. We transform the system into one
in R2 , using variables y and z = ln x:
ż = e
g(y)
ẏ = − e
f (z) − k(y)
where e
g(y) = g(y) and e
f (z) := f (ez ). This functions defining this system have positive derivatives,
as required in Lemma 5.1. Moreover, the transformed system has the steady state (ln x0 , y0 ), which
18

is therefore globally asymptotically stable (and unique). Transforming back to the original coordinates, we proved our claim. Now suppose that case (b) holds instead: both f and g have negative
derivatives. We pick z := − ln x. The equations transform as above, except that now e
g(y) = −g(y)
−z
e
and f (z) := f (e ). Since these now have positive derivatives, the same argument as in case (a)
applies.
The nonlinear integral feedback systems discussed earlier are particular cases of the above form.
The linear function k(y) = γy is increasing. When g(y) = αx(y − y0 ), g is increasing, and when
g(y) = αx(y0 − y), g is decreasing. The function f (x) = −βu/x (where u is any positive constant) is
increasing, while f (x) = −βux is decreasing.
Proof of Lemma 5.1. Uniqueness: since g is strictly increasing, y0 is uniquely determined, and
it then follows that x0 is also uniquely determined (because f is increasing) as the solution of
f (x) = −k(y0 ).
At the steady state (x0 , y0 ), we may assume, without loss of generality that f (x0 ) = 0 and k(y0 ) =
0 (as well as g(y0 ) = 0). Indeed, let c := f (x0 ) = −k(y0 ). Redefining e
f (x) := f (x) − c and
e
e
e
k(y) := k(y) + c, we have that f (x0 ) = 0 and k(y0 ) = 0, and the differential equations have not
changed, since − e
f (x) − e
k(y) = −( f (x) − c) − (k(y) + c) = − f (x) − k(y). Since f is strictly increasing,
its values are positive when x > x0 and negative otherwise, and similarly for g, k with respect to y0 .
Let us define
Z x
Z y
V(x, y) :=
f (r) dr +
g(r) dr .
x0

y0

By definition, V(x0 , y0 ) = 0 and V(x, y) > 0 for all (x, y) , (x0 , y0 ) As ∂∂2Vx = f 0 (x) > 0, ∂∂2Vy =
g0 (y) > 0, and mixed second derivatives are zero, it follows that the Hessian matrix of V is positive
definite everywhere. Thus, V is strictly convex, and it follows that V is a proper function: V(x, y) →
∞ as k(x, y)k → ∞. In conclusion, V is a Lyapunov function candidate function. To conclude global
stability based on the LaSalle Invariance Principle [5], we must show that the derivative of V along
trajectories:


∂V
∂V
(x, y)g(y) +
(x, y) − f (x) − k(y)
V̇(x, y) :=
∂x
∂y
2

2

has the properties that V̇(x, y) ≤ 0 for all (x, y) and that (x(t), y(t)) ≡ (x0 , y0 ) is the only solution
with V̇(x(t), y(t)) ≡ 0. Now,


V̇(x, y) = f (x)g(y) + g(y) − f (x) − k(y) = −g(y)k(y) ≤ 0
because g and k have everywhere the same sign (positive if y > y0 , negative if y < y0 ). Suppose if
a solution satisfies that V̇(x(t), y(t)) ≡ 0, then y(t) ≡ y0 , so that also ẏ(t) ≡ 0, which substituted into
the second equation gives 0 = − f (x(t)) − 0, which implies that x(t) ≡ x0 .
Observe that the only place that k appears in the proof is in the statement that g and k have
everywhere the same sign; thus the same proof works if instead of assuming that k has everywhere
positive derivatives, we assume that f (x0 ) = 0 and (y − y0 )k(y) > 0 for all y , y0 . Note that V
would be a Hamiltonian for the system if k ≡ 0, so the system can be thought simply as adding
damping to a conservative system.

19

6

Comparing feedforward and feedback structures

We make several remarks in this section concerning the relations and comparisons between
adapting feedforward and feedback architectures.

6.1

Internal model principle

The “internal model principle” (IMP, for short) in control theory states that one should be able
to recast any system which adapts to steps as a system which integrates an “adaptation error”
signal (integral feedback). For example, it should be possible to rewrite the feedforward system in
Fig. 2(a) in such a manner. In this section, we review one precise statement of the IMP, and apply
it to this example.
Adaptation is called “disturbance rejection” in control theory [5] (not to be confused with a
different topic, “adaptive control”). A key mathematical idea, the internal model principle (IMP),
states that, to be able to adapt to all signals in a given class of inputs “U”, the system must include
an “internal model”: a subsystem which is driven by the “error” in adaptation, and whose solutions
when the error is zero (that is, when the system has perfectly adapted) are the possible signals
in U. Intuitively, an internal representation of the external signal is memorized, and adaptation
performance is constantly evaluated; any error in adaptation is used to form a better estimate of
this external signal. For example, for adaptation to constant signals, the IMP requires integral
feedback, as in Fig. 1: U = all constant signals, the error is y − y0 , and solutions of ẋ = 0
are precisely the constant signals. In systems biology, the IMP suggests biochemical structures,
thus guiding modeling and experiments as well as interpretation of the role of various regulatory
and signal processing motifs. For instance, the relevance of the IMP to E. coli chemotaxis was
remarked in [42]: the methylation state can be viewed as a memory (integrator) and the “error” is
the average kinase activity relative to its basal value. The IMP encompasses adaptation also with
respect to richer classes of signals U, not just constant ones. For example, one might speculate [43]
that circadian rhythms might have evolved as an IMP mechanism to allow adaptation to day/night
light and temperature cycles: a harmonic oscillator with period T is predicted by the IMP when
a system adapts to U = all signals of the form A sin(2πt/T + ϕ). The IMP was proved as a
theorem for linear dynamics by Francis and Wonham in the mid 1970s [44,45]. It remains an open
problem to find ultimate nonlinear generalizations, but there are some partial extensions known.
For example, using “zero dynamics” ideas from [46], a theorem was given in [43] that shows,
under appropriate technical assumptions, the existence of coordinate changes, generally nonlinear,
that exhibit an internal model. Using this theorem, one should expect to find coordinate changes
transforming IFFL circuits into integral feedback form, and this is indeed true. We first describe
the comparatively trivial case of linear systems and then discuss how to obtain an analogous result
for nonlinear IFFL systems.
As a simple first illustration, consider the following feedforward linear system:
ẋ = −x + u ,

ẏ = −x − y + u ,

which perfectly adapts to y = y0 = 0 but is not in integral feedback form. We may perform a simple
change of coordinates, representing the system using the state variables (x = x − y, y) instead of
20

the original (x, y). In this new set of coordinates, we have:
ẋ = y ,

ẏ = −x − 2y + u ,

(9)

which is now an integral feedback system (the variable z integrates the “error” y). We next review
the main theorem from [43], and then work out the application to the nonlinear feedforward system
in Fig. 2(a).
The general setup in [43] is as follows. The systems studied are scalar-input scalar-output ndimensional systems for which the input appears to first order:
ż = f (z) + ug(z) ,

y = h(z) .

(10)

The vector fields f and g are smooth, and h is a smooth function. We assume that z = 0 is a steady
state when u = 0, f (0, 0) = 0.
We will say that the system (10) adapts to inputs in a class U if for each u ∈ U and each
initial state x0 ∈ Rn , the solution of (10) with initial condition x(0) = x0 exists for all t ≥ 0 and
is bounded, and the corresponding output y(t) = h(x(t)) converges to a fixed value y0 ∈ Y (which
does not depend on the particular input u ∈ U) as t → ∞.
In control theory, it is standard to describe the class of inputs U with respect to which adaptation
holds through the specification of an “exosystem” that produces these inputs. An exosystem is
simply any autonomous system Γ:
ẇ = Q(w) ,

u = θ(w)

(11)

with the following property: the input class U consists exactly of the functions u(t) = θ(w(t)),
t ≥ 0, for each possible initial condition w(0). For example, if we are interested in step responses,
we pick ẇ = 0, u = w. This means that the possible signals are the solutions of ẇ = 0, i.e. the
constant functions of time; that is, U is the set of functions u(t) for which u(t) = ū for all t for
some ū ∈ U. On the other hand, if we are interested in sinusoidals with frequency ω then we use
ẋ1 = x2 , ẋ2 = −ω2 x1 , u = x1 . A technical assumption is that the signals in U do not grow without
bound. Specifically, one assumes that the exosystem is Poisson-stable, meaning that for every state
w0 , the solution w(·) of ẇ = Q(w), w(0) = w0 is defined for all t > 0 and it satisfies that w0 is in
the omega-limit set of w (recall that this means that there is a sequence of times ti → ∞ such that
the sequence w(ti ) converges to w0 as t → ∞). In other words, the exosystem is almost-periodic
in the sense that trajectories keep returning to neighborhoods of the initial state. Both the constant
and sinusoidal examples mentioned above are generated by Poisson-stable systems. In contrast,
ramps (linearly growing signals) are not generated by Poisson-stable systems, since they require an
unstable second-order system ẇ1 = 0, ẇ1 = w2 , u = w1 to generate them. Thus, the phenomenon
of adaptation to ramps is not included in the scope of the theorem to be stated. The exosystem is
assumed to have states that evolve on some differentiable manifold, Q is a smooth vector field, and
θ is a real-valued smooth function.
The IMP claims that a copy of this exosystem must be embedded in the system (10). More
precisely, one says that the system contains an output-driven internal model of U if there is a
change of coordinates which brings the equations (10) into the following block form:
ż1 = f1 (z1 , z2 ) + ug1 (z1 , z2 )
ż2 = f2 (y, z2 )
y = κ(z1 )
21

(12)

so that the subsystem with state variables z2 is capable of generating all the possible functions in
U: for some some function ϕ(z2 ), and for each possible u ∈ U, there is some solution of
ż2 = f2 (y0 , z2 )

(13)

which satisfies ϕ(z2 (t)) ≡ u(t). “Change of coordinates” means that there is some integer r ≤ n and
two differentiable manifolds Z1 and Z2 of dimensions r and n − r respectively, as well as a smooth
function κ : Z1 → R and two vector fields F and G on Z1 × Z2 which take the partitioned form
!
!
f1 (z1 , z2 )
g1 (z1 , z2 )
F=
, G=
f2 (κ(z1 ), z2 )
0
and a diffeomorphism Φ : Rn → Z1 × Z2 , such that
Φ0 (x) f (x) = F(Φ(x)) ,

Φ0 (x)g(x) = G(Φ(x)) ,

κ(Φ1 (x)) = h(x)

for all x ∈ Rn , where Φ1 is the Z1 -component of Φ and prime indicates Jacobian. Intuitively, the
signal z2 computes an integral of a function of the output y(t), and when y(t) ≡ y0 , z2 is (up to
the mapping ϕ, which may be interpreted as a sort of rescaling) a signal in U. For example, if U
consists of constant functions (adaptation to steps), then for y ≡ y0 one obtains (for different initial
conditions) the possible constant signals.
In order to prove a theorem justifying the IMP, several technical conditions are imposed in [43].
The first is a signal detection or “sensitivity” property: (1) for some positive integer r, called in
control theory a finite uniform relative degree,
Lg Lkf h ≡ 0 , k = 0, . . . , r − 2

and

Lg Lr−1
f h(x) , 0 ∀ x ∈ Z .

As in the section on observability, generally, LX H denotes the directional or Lie derivative of a
function H along the direction of a vector field X: (LX H)(x) = ∇H(x) · X(x), and one understands
LY LX H as the iteration LY (L x H). (In the special case that Lg h(x) , 0 for all x, the relative degree
is r = 1, since the condition for k < r − 1 is vacuous.) Given that the relative degree is r, one may
consider the following vector fields:
e
g(x) =

1
Lg Lr−1
f h(x)

g(x) ,



e
f (x) = f (x) − Lrf h(x) e
g(x) ,

e
τi := adi−1
g, i = 1, . . . r ,
e
f

where adX is the operator adX Y = [X, Y] = Lie bracket of the vector fields X and Y, and adi−1
is the
e
f
iteration of this operator i − 1 times (when i = 1, τi = e
g). One says that a vector field X is complete
if the solution of the initial value problem ẋ = X(x), x(0) = x0 is defined for all t and for any initial
state x0 . Two vector fields X and Y are said to commute if [X, Y] = 0. The final assumptions, then,
are that (2) τi is complete, for i = 1, . . . , r and (3) the vector fields τi commute with each other. (In
the special case r = 1, condition (3) is automatic, since every vector field commutes with itself.)
These assumptions are satisfied for linear systems. They are also satisfied, for example, for the
feedforward system in Fig. 2(a):
u
ẏ = β − γy
x

ẋ = αu − δx ,
22

(14)

with h(x, y) = y. In vector form, this is ż = f (z) + ug(z), where the vector fields are:
!
!
−δx
α
f (x, y) =
and
g(x, y) =
.
−γy
β/x

(15)

Since Lg h = (0, 1) · (α, β/x)T = β/x is everywhere nonzero, we have that r = 1. Thus we only need
to check that
!
α
x
1
x
β
τ1 = e
g =
g(x) = g(x) =
1
Lg h(x)
β
is complete, which is true because e
g is a linear vector field.
The main theorem in [43] says: Suppose that assumptions (1)-(3) hold for the system (10).
If (10) adapts to inputs in a class U generated by a Poisson-stable exosystem, then it contains an
output-driven internal model of U.
The proof of the theorem consists of showing that there is, under the stated conditions, a change
of variables as claimed. The map producing the change of variables is obtained by solving a firstorder partial differential equation.

6.2

Illustration of IMP for the feedforward system in Fig. 2(a)

We consider the system (14), or (15) in vector form. We already checked properties (1)-(3), and
the system adapts to steps (constant inputs), so the theorem says that it should be possible to to
recast it integral feedback form. The proof in [43] asserts the existence of a mapping ϕ(x, y) whose
Lie-derivative along g solves the following first-order linear PDE:
β
Lg ϕ = ∇ϕ · g = αϕ x (x, y) + ϕy (x, y) = 0 .
x
Generally, such an equation may be solved using the method of characteristics. However, in our
example the solution is immediate: ϕ(x, y) = αy − β log x. The map
(x, y) 7→ (z1 , z2 ) = (y, ϕ(x, y)) = (y, αy − β log x)
is a diffeomorphism whose inverse is y = z1 and x = e(αz1 −z2 )/β . We obtain the following equations
in the new coordinates (z1 , z2 ):
ż1 = βue(z2 −αz1 )/β − γz1
ż2 = βδ − αγz1
with output y = z1 . This has the desired internal model form ż1 = f1 (z1 , z2 ) + ug1 (z1 , z2 ), ż2 =
f2 (y, z2 ), y = κ(z1 ), if we define: f1 (z1 , z2 ) = −γz1 , g1 (z1 , z2 ) = βe(z2 −αz1 )/β , f2 (y, z2 ) = f2 (y) =
βδ − αγy, and κ = identity. Thus z2 is the variable that integrates the error: when y = y0 = 1, the
equation for z2 becomes z2 = 0, whose solutions are all the possible constant signals. We can also
write this system in terms of the coordinates x = ez2 /β , y = z1 as follows:
!
α
αγ
y ,
ẏ = βuxe− β y − γy
(16)
ẋ = x δ −
β
which has the generic form ẋ = xF(y), ẏ = G(ux, y) of nonlinear integral feedback systems considered in Lemma 3.1.
23

6.3

What are the relative advantages of different architectures?

Both integral feedback and IFFL circuits allow adaptation, as well as, for appropriate models,
symmetry invariance to scalings. Thus, it is natural to ask what are the relative merits of each of
these architectures: what fitness-conferring signal processing and control properties are special for
them? We view this as a question for further research, and limit ourselves here to a few remarks.
In a certain sense, the question is meaningless, since feedforward networks can often be simulated by feedback ones, as just shown. Nonetheless, the variable e
x = x − y may well be merely a
mathematical construct with no biological meaning. In addition, any (linearized) system obtained
by such a transformation from an IFFL is special: it can have only real eigenvalues, while the more
general integral feedback form may have damped oscillatory behavior (depending on parameters).
This means that the feedback systems have a wider range of possible dynamical behaviors, and
thus might be selected when it is desirable to meet specific performance objectives. In addition,
feedback confers a certain robustness to uncertainty. We illustrate this point by comparing the
feedforward system in Fig. 2(a):
u
ẋ = αu − δx ,
ẏ = β − γy
x
with its recasting in feedback form:
!
α
αγ
y ,
ẏ = βuxe− β y − γy .
ẋ = x δ −
β
βδ
). However, while in the feedback form, any perturbation of the
Both systems adapt (y(t) → αγ
− αβ y
right-hand side: ẏ = βuxe
− γy + ∆(x, y) does not alter the property that the steady state must
βδ
have y = αγ
, no analogous simple statement can be made for the feedforward form.

Conversely, one may also speculate that biological or evolutionary could constraint the value
of feedback structures. As an example, the stability of feedback (but not feedforward) systems is
fragile to delays. Delays could arise from slower time scales for processes such as transcription and
translation, compared to protein modifications. As an illustration, Fig. 6 show oscillations arising
from a delay from y to x in the linear integral feedback system (9) and Fig. 7 show oscillations
arising from a delay from y to x in the nonlinear integral feedback system (16).

7

Invariant steering

As remarked in [3], motile systems that measure a field in order to determine their velocity of
movement have the property that their entire search patterns, as a function of time, are invariant
to scale, if their sensory systems have the FCD property. We now discuss a precise formulation of
this fact for arbitrary systems and symmetries.
We think of a system that, through its output y(t), drives a steering mechanism (“motor complex”), resulting in a new position r(t). We model this by a system with inputs, which is a way of
saying that the position is computed by a dynamical system that keeps track of the past history of
y:
q̇ = Q(q, y) , r = R(q)
24

Figure 6: Oscillations in ẋ(t) = y(t − h), ẏ(t) = −x(t) − 2y(t) + u(t). Using u ≡ 0 and h = 5.

Figure 7: Oscillations in ẋ(t) = x(1 − y(t − h)), ẏ(t) = x(t)u(t)e−y(t) − y(t). Using u ≡ 0 and h = 5.
where q(t) is the internal state of the steering mechanism. At position r(t) in space, an “intensity”
(e.g., light or nutrient concentration) is queried, and the result is a sensed input I(t, r(t)). The
intensity field I(t, r) could well be time as well as space-dependent. Finally, the loop is closed
by the system measuring I(t, r(t)), except that we are interested in understanding how the system
behaves if it measures πI(t, r(t)) instead of I(t, r(t)), where π ∈ P is a symmetry. See Fig. 8 for
an illustration. We want to study the invariance of behavior of this system, and in particular of
its position r(t) as a function of time, under the assumption that the system had pre-adapted to a
constant environment before I(t, r) ≡ I0 when t < 0 before being placed in the current environment.
Formally, we start with a system that adapts and is invariant with respect to a set of symmetries
P, and consider the following extended system:
ż = F(z, u)
q̇ = Q(q, y)

u = I(t, r)
y = h(z), r = R(q) .

We let (z, q) be the solution with initial conditions z(0) = σ(I0 ) and q(0) = q0 , where Q(q0 , y0 ) = 0,
that is, q0 is a steady state that corresponds to the adaptation value y0 of the original system.
25

Figure 8: Closed-loop diagram for search under symmetry uncertainty for inputs
Also, for any given π ∈ P, we consider the solution (e
z, e
q) of the system with initial conditions
z(0) = σ(πI0 ) and q(0) = q0 and intensity field πI(t, r):
e
z˙ = F(z,e
u)
˙e
q = Q(e
q,e
y)

e
u = πI(t,e
r)
e
y = h(e
z), e
r = R(e
q) .

We claim that e
y(t) = y(t), e
u(t) = πu(t), e
r(t) = r(t), and e
q(t) = q(t) for all t ≥ 0.
To prove the claim, we consider the solution of the system
ẋ = F(x, πu(t))
ṡ = Q(s, h(x))
with initial conditions x(0) = σ(πI0 ) and s(0) = q0 . By definition of P-invariance, we know that
h(x(t)) = h(z(t)) for all t ≥ 0. Thus, since the initial conditions on q and s are the same, it follows
that also s(t) = q(t) for all t ≥ 0. Therefore, u(t) = I(t, r(t)) = I(t, R(q(t))) = I(t, R(s(t))) for all
t ≥ 0. It follows that ẋ = F(x, πI(t, r)). We conclude that (x, s) and (e
z, e
q) solve the same initialvalue problem, and thus x(t) = e
z(t) and s(t) = e
q(t) for t ≥ 0, from which y(t) = h(z(t)) = h(x(t)) =
h(e
z(t)) = e
y(t), and the claim is proved.
Remark 7.1 A converse of this result holds as well. Suppose that, for every possible field I,
the above system results in the same r(t) when started from z(0) = σ(I0 ) as when starting from
z(0) = σ(πI0 ) but using input πI(t, r). This property then holds, in particular, when the field I(t, r)
is independent of position r, that is, I is merely an arbitrary open-loop input. We would like to
conclude that y(t) = e
y(t) from r(t) = ρ(t), which means, since the input is arbitrary, that the original
system must be invariant under π. This conclusion will hold provided that the steering system has
the following property: if we solve q̇ = Q(q, y) with initial condition q(0) = q0 and two inputs y1
and y2 , and the resulting solutions satisfy R(q1 (t)) = R(q2 (T )) for t ≥ 0, then y1 (t) = y2 (t) for t ≥ 0.
In control-theoretic terms, this input reconstruction property is stated as the requirement that the
system q̇ = Q(q, y) with output R(q) be input-observable, which is a property closely related to
right-invertibility, inverse dynamics, and “output to input observability” [46, 47]. Observability is
almost enough to guarantee input-observability: if the system is observable, then q1 (t) = q2 (t) for
t ≥ 0, so it is only needed that Q(q(t), y1 (t)) = Q(q(t), y2 (t)) imply y1 (t) = y2 (t), which is a weak
nondegeneracy property.

Remark 7.2 In many applications, the system output y(t) drives a stochastic steering mechanism:
the system producing the location r(t) is subject to randomness, For example, in bacterial chemotaxis, y(t) may represent a signal, such as the level of phosphorylated protein CheY, which serves
26

to bias the random switches between tumbling and swimming behavior. One way to represent
this probabilistic behavior is to model the dynamical system that computes the position from the
history of y(t) as:
q̇ = Q(q, y, X) , r = R(q)
where X is a random process: X = {Xt (ω), t ≥ 0} is defined on a probability space (Ω, F , P), where
Ω is a sample space, F is a σ-algebra of events, and P is a probability measure; P(X=v(t)) is
the probability of a given outcome (sample path) of this process. Under such a formalization,
and assuming appropriate technical conditions, all the variables (z(t), u(t), r(t), q(t), y(t)) are themselves stochastic processes defined on the same probability space Ω. (Technical conditions need
to be imposed to insure the existence of solutions of the differential equations. We proceed intuitively, assuming that X has piecewise continuous sample paths, thus avoiding complications of
Itô calculus.) Now, given any fixed ω ∈ Ω, we may view q̇ = Q(q, y, X) as a time-varying system
q̇(t) = Q(q(t), y(t), t), where we have substituted X = Xt (ω) along this sample path. The previous
proof extends, with no changes, to such a time-varying system. It follows that identical r(t) (as
well as y(t) and q(t)) are obtained, whether using the field I or using the changed field πI, so long
as the initial condition z(0) had also been modified by π. This holds for each ω ∈ Ω. Thus, as a
random variable, r(t) is invariant under P. In particular, all statistics of r remain invariant.

Acknowledgements
E.S. acknowledges support from grants from AFOSR FA9550-08, and NIH 1R01GM086881.
Contributions: O.S, E.S. and U.A. participated in defining the questions, the examples, and their
analysis; E.S. wrote the proofs.

27

References
[1] L. Goentoro and M. W. Kirschner. Evidence that fold-change, and not absolute level, of β
-catenin dictates Wnt signaling. Molecular Cell, 36:872–884, 2009.
[2] C. Cohen-Saidon, A. A. Cohen, A. Sigal, Y. Liron, and U. Alon. Dynamics and variability of
ERK2 response to EGF in individual living cells. Molecular Cell, pages 885–893, 2009.
[3] O. Shoval, L. Goentoro, Y. Hart, A. Mayo, E.D. Sontag, and U. Alon. Fold change detection
and scalar symmetry of sensory input fields. Proc Natl Acad Sci USA, 107:15995–16000,
2010. Online before print doi: 10.1073/pnas.1002352107.
[4] L. Goentoro, O. Shoval, M. W. Kirschner, and U. Alon. The incoherent feedforward loop can
provide fold-change detection in gene regulation. Mol. Cell, 36:894–899, 2009.
[5] E.D. Sontag. Mathematical Control Theory. Deterministic Finite-Dimensional Systems, volume 6 of Texts in Applied Mathematics. Springer-Verlag, New York, second edition, 1998.
[6] H. El-Samad, J. P. Goff, and M. Khammash. Calcium homeostasis and parturient hypocalcemia: An integral feedback perspective. J. Theor. Biol., 214:17–29, 2002.
[7] P. Miller and X. J. Wang. Inhibitory control by an integral feedback signal in prefrontal cortex: A model of discrimination between sequential stimuli. Proc. Natl. Acad. Sci., 103:201–
206, 2006.
[8] K. V. Venkatesh, S. Bhartiya, and A. Ruhela. Mulitple feedback loops are key to a robust
dynamic performance of tryptophan regulation in Escherichia coli. FEBS Letters, 563:234–
240, 2004.
[9] T.-M. Yi, Y. Huang, M.I. Simon, and J. Doyle. Robust perfect adaptation in bacterial chemotaxis through integral feedback control. Proc. Natl. Acad. Sci. U.S.A., 97:4649–4653, 2000.
[10] U. Alon. An Introduction to Systems Biology: Design Principles of Biological Circuits.
Chapman & Hall, 2006.
[11] A. Ma’ayan, S. L. Jenkins, S. Neves, A. Hasseldine, E. Grace, B. Dubin-Thaler, N. J. Eungdamrong, G. Weng, P. T. Ram, J. J. Rice, A. Kershenbaum, G. A. Stolovitzky, R. D. Blitzer,
and R. Iyengar. Formation of regulatory patterns during signal propagation in a Mammalian
cellular network. Science, 309:1078–1083, Aug 2005.
[12] S. Mangan, S. Itzkovitz, A. Zaslaver, and U. Alon. The incoherent feed-forward loop accelerates the response-time of the gal system of Escherichia coli. J. Mol. Biol., 356:1073–1081,
Mar 2006.
[13] A. Cournac and J. A. Sepulchre. Simple molecular networks that respond optimally to timeperiodic stimulation. BMC Syst Biol, 3:29, 2009.
[14] G. Hornung and N. Barkai. Noise propagation and signaling sensitivity in biological networks: a role for positive feedback. PLoS Comput. Biol., 4:e8, Jan 2008.
28

[15] S. Semsey, S. Krishna, K. Sneppen, and S. Adhya. Signal integration in the galactose network
of Escherichia coli. Mol. Microbiol., 65:465–476, Jul 2007.
[16] M. E. Wall, M. J. Dunlop, and W. S. Hlavacek. Multiple functions of a feed-forward-loop
gene circuit. J. Mol. Biol., 349:501–514, Jun 2005.
[17] D. Kim, Y. K. Kwon, and K. H. Cho. The biphasic behavior of incoherent feed-forward loops
in biomolecular regulatory networks. Bioessays, 30:1204–1211, Nov 2008.
[18] S. Sasagawa, Y. Ozaki, K. Fujita, and S. Kuroda. Prediction and validation of the distinct
dynamics of transient and sustained ERK activation. Nat. Cell Biol., 7:365–373, Apr 2005.
[19] T. Nagashima, H. Shimodaira, K. Ide, T. Nakakuki, Y. Tani, K. Takahashi, N. Yumoto, and
M. Hatakeyama. Quantitative transcriptional control of ErbB receptor signaling undergoes
graded to biphasic response for cell differentiation. J. Biol. Chem., 282:4045–4056, Feb
2007.
[20] P. Menè, G. Pugliese, F. Pricci, U. Di Mario, G. A. Cinotti, and F. Pugliese. High glucose
level inhibits capacitative Ca2+ influx in cultured rat mesangial cells by a protein kinase
C-dependent mechanism. Diabetologia, 40:521–527, May 1997.
[21] R. Nesher and E. Cerasi. Modeling phasic insulin release: immediate and time-dependent
effects of glucose. Diabetes, 51 Suppl 1:S53–59, Feb 2002.
[22] M. P. Mahaut-Smith, S. J. Ennion, M. G. Rolf, and R. J. Evans. ADP is not an agonist at
P2X(1) receptors: evidence for separate receptors stimulated by ATP and ADP on human
platelets. Br. J. Pharmacol., 131:108–114, Sep 2000.
[23] S. Marsigliante, M. G. Elia, B. Di Jeso, S. Greco, A. Muscella, and C. Storelli. Increase
of [Ca(2+)](i) via activation of ATP receptors in PC-Cl3 rat thyroid cell line. Cell. Signal.,
14:61–67, Jan 2002.
[24] L. A. Ridnour, A. N. Windhausen, J. S. Isenberg, N. Yeung, D. D. Thomas, M. P. Vitek,
D. D. Roberts, and D. A. Wink. Nitric oxide regulates matrix metalloproteinase-9 activity
by guanylyl-cyclase-dependent and -independent pathways. Proc. Natl. Acad. Sci. U.S.A.,
104:16898–16903, Oct 2007.
[25] J. Tsang, J. Zhu, and A. van Oudenaarden. MicroRNA-mediated feedback and feedforward
loops are recurrent network motifs in mammals. Mol. Cell, 26:753–767, Jun 2007.
[26] J.J. Tyson, K. Chen, and B. Novak. Sniffers, buzzers, toggles, and blinkers: dynamics of
regulatory and signaling pathways in the cell. Curr. Opin. Cell. Biol., 15:221–231, 2003.
[27] E.D. Sontag. Remarks on feedforward circuits, adaptation, and pulse memory. IET Systems
Biology, 4:39–51, 2010.
[28] L. Yang and P.A. Iglesias. Positive feedback may cause the biphasic response observed in the
chemoattractant-induced response of dictyostelium cells. Systems Control Lett., 55(4):329–
337, 2006.
29

[29] A. Levchenko and P.A. Iglesias. Models of eukaryotic gradient sensing: Application to
chemotaxis of amoebae and neutrophils. Biophys J., 82:50–63, 2002.
[30] F.-D. Xu, Z.-R. Liu, Z.-Y. Zhang, and J.-W. Shen. Robust and adaptive microRNA-mediated
incoherent feedforward motifs. Chinese Physics Letters, 26(2):028701–3, February 2009.
[31] A. Kremling, K. Bettenbrock, and E. D. Gilles. A feed-forward loop guarantees robust behavior in escherichia coli carbohydrate uptake. Bioinformatics, 24:704–710, 2008.
[32] E. Voit, A. R. Neves, and H. Santos. The intricate side of systems biology. Proc. Natl. Acad.
Sci. U.S.A., 103:9452–9457, Jun 2006.
[33] L. Bleris, Z. Xie, D. Glass, A. Adadey, E.D. Sontag, and Y. Benenson. Synthetic incoherent feed-forward circuits show adaptation to the amount of their genetic template. 2010.
Submitted.
[34] L. Evans. Partial Differential Equations. American Mathematical Society, Providence, 1998.
[35] M. Golubitsky and I. Stewart. The Symmetry Perspective. Birkhüser Verlag, Basel, 2002.
[36] J. Guckenheimer and P. Holmes. Nonlinear Oscillations, Dynamical Systems, and Bifurcations of Vector Fields. Springer-Verlag, New York, 1983.
[37] H.J. Sussmann. Existence and uniqueness of minimal realizations of nonlinear systems.
Math. Systems Theory, 10:263–284, 1977.
[38] E.D. Sontag. Polynomial Response Maps, volume 13 of Lecture Notes in Control and Information Sciences. Springer-Verlag, Berlin, 1979.
[39] E.D. Sontag. Spaces of observables in nonlinear control. In Proceedings of the International Congress of Mathematicians, Vol. 1, 2 (Zürich, 1994), pages 1532–1545, Basel, 1995.
Birkhäuser.
[40] Pablo Iglesias, 2010. Personal communication.
[41] T. Nagano. Linear differential systems with singularities and an application to transitive lie
algebras. J. Math. Soc. Japan, 18:398–404, 1966.
[42] T. M. Yi, Y. Huang, M. I. Simon, and J. Doyle. Robust perfect adaptation in bacterial chemotaxis through integral feedback control. Proc. Natl. Acad. Sci. U.S.A., 97:4649–4653, 2000.
[43] E.D. Sontag. Adaptation and regulation with signal detection implies internal model. Systems
Control Lett., 50(2):119–126, 2003.
[44] B.A. Francis and W.M. Wonham. The internal model principle for linear multivariable regulators. Appl. Math. Optim., 2:170–194, 1975.
[45] W.M. Wonham. Linear Multivariable Control: A Geometric Approach, 3rd ed. SpringerVerlag, New York, 1985.

30

[46] A. Isidori. Nonlinear Control Systems. Springer, London, 3rd edition, 1995.
[47] D. Liberzon, A. S. Morse, and E.D. Sontag. Output-input stability and minimum-phase nonlinear systems. IEEE Trans. Automat. Control, 47(3):422–436, 2002.

31

