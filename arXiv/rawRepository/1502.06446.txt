arXiv:1502.06446v1 [q-bio.NC] 23 Feb 2015

Modeling networks of spiking neurons as interacting processes
with memory of variable length
A. Galves

E. Löcherbach

February 21, 2015
Abstract
We consider a new class of non Markovian processes with a countable number of
interacting components, both in discrete and continuous time. Each component is
represented by a point process indicating if it has a spike or not at a given time. The
system evolves as follows. For each component, the rate (in continuous time) or the
probability (in discrete time) of having a spike depends on the entire time evolution
of the system since the last spike time of the component. In discrete time this class
of systems extends in a non trivial way both Spitzer’s interacting particle systems,
which are Markovian, and Rissanen’s stochastic chains with memory of variable length
which have finite state space. In continuous time they can be seen as a kind of
Rissanen’s variable length memory version of the class of self-exciting point processes
which are also called “Hawkes processes”, however with infinitely many components.
These features make this class a good candidate to describe the time evolution of
networks of spiking neurons. In this article we present a critical reader’s guide to
recent papers dealing with this class of models, both in discrete and in continuous
time. We briefly sketch results concerning perfect simulation and existence issues,
de-correlation between successive interspike intervals, the longtime behavior of finite
non-excited systems and propagation of chaos in mean field systems.

Key words : Biological neural nets, chains of variable length memory, Hawkes processes, interacting particle systems, mean field interaction, perfect simulation, propagation of chaos.
AMS Classification : 60K35, 60J25, 60J55

1

Introduction

A biological neural system has the following characteristics. It is a system with a huge
(about 1011 ) number of interacting components, the neurons. The activity of each neuron
is represented by a point process, namely, the successive times at which the neurons emit
an action potential or a so-called spike. It is generally considered that the spiking activity
is the way the system encodes and transmits information.
The spiking probability or rate of a given neuron depends on its membrane potential.
The membrane potential of a given neuron is affected by the actions of all other neurons
interacting with it. Neurons interact either by chemical or by electrical synapses. Chemical
synapses can be described as follows. Each neuron spikes randomly following a point
process with rate depending on the membrane potential of the neuron. At its spiking
1

time, the membrane potential of the spiking neuron is reset to an equilibrium potential 0.
At the same time, simultaneously, the neurons affected by it receive an additional amount
of potential which is added to their membrane potential.
Electrical synapses occur through gap-junctions which allow neurons in the brain to communicate with one another. This induces an attraction between the values of the membrane
potentials of each other and, as a consequence, a drift of the system towards its center of
mass. Finally, leakage channels may induce a loss of membrane potential for each neuron.
The fact that the membrane potential of each neuron is reset to 0 when it spikes makes its
time evolution to be dependent of a variable length of the past. More precisely, it depends
on the influence received from its presynaptic neurons since its last spiking time. In other
terms, the time evolution of such a system is obviously not described by a Markov process
(Cessac 2011). In particular, if we consider a time continuous description of the system,
the waiting times between two successive spikes of a single neuron are not exponentially
distributed (see, for instance, Brillinger 1988).
Such a system can be described in discrete time in the following way. Consider a small
interval of time, typically of the order of 10ms which is more or less the time it takes for
a neuron to emit a spike, followed by a refractory period. We indicate the presence or
absence of a spiking activity for each neuron within each such time window. Then the
process we obtain is a system of interacting chains with memory of variable length and
a large number of components. This class of systems extends in a non trivial way both
the interacting particle systems, which are Markov, see Spitzer (1970), and the stochastic
chains with memory of variable length which have finite state space, see Rissanen (1983)
or Galves and Löcherbach (2008).
A continuous time description of the process seems however more convenient both from a
modeling and a mathematical point of view. This is the point of view adopted in De Masi
et al. (2015), Duarte and Ost (2014) and Fournier and Löcherbach (2014).
The present article is considered as a critical reader’s guide to the papers mentioned above.
We will briefly sketch the main results of these papers as well as challenges and next steps
to be addressed.
This paper is organized as follows. In Section 2 we introduce a model of an infinite network
of spiking neurons in discrete time; in Section 3 an analogous continuous time model using
point processes is introduced. In Section 4, we show that under appropriate conditions,
such a system of interacting point processes can be represented via an associated interacting particle system which is Markovian. In Section 5 we give an existence and perfect
simulation result for the process defined in Sections 2 and 3. Section 6 considers a finite
system composed of N neurons where the graph of synaptic weights is a realization of a
(slightly) supercritical directed Erdös-Rényi random graph. In this case, the correlations
of two neighboring interspike intervals are shown to be asymptotically de-correlated, as
the system size tends to infinity. Sections 7 to 9 are devoted to a study of the associated
interacting particle system introduced in Section 4 where we deal successively with the
longtime behavior of finite particle systems, with the hydrodynamical limit within a mean
field system and finally with asymptotic properties of the limit process. In Section 10 we
mention challenges and next steps to be addressed. We close our paper with a discussion
in Section 11.

2

2

Infinite systems of interacting processes with memory of
variable length in discrete time

We introduce a new class of stochastic processes, both in discrete and in continuous time,
which are models of networks of spiking neurons. The processes we consider are infinite
systems of interacting processes with memory of variable length.
Let I be a countable set of neurons and introduce a family of synaptic weights Wj→i ∈ R,
for j 6= i, Wj→j = 0 for all j. We interpret Wj→i as the synaptic weight of neuron j on
neuron i. We suppose that the synaptic weights have the following property of uniform
summability
X
|Wj→i | < ∞.
(2.1)
sup
i∈I

j

Moreover, we shall use a family of spiking rate functions φi : R → [0, 1], i ∈ I, and a family
of leak functions gi : N → R+ , i ∈ I. All functions φi and gi are measurable functions. We
assume that φi is increasing and uniformly Lipschitz continuous, i.e. there exists a positive
constant γ such that for all s, s′ ∈ R,
|φi (s) − φi (s′ )| ≤ γ|s − s′ |.

(2.2)

We start by considering a model in discrete time which is partly inspired by Cessac (2011)
who proposes a finite dimensional system. We consider a stochastic chain (Xt )t∈Z taking
values in {0, 1}I , where I is the countable set of neurons, defined on a suitable probability
space (Ω, A, P ). For each neuron i at each time t ∈ Z, Xt (i) = 1 reports if neuron i has a
spike at that time t. Otherwise we put Xt (i) = 0. The global configuration of neurons at
time t is denoted Xt = (Xt (i), i ∈ I). For each neuron i ∈ I and each time t ∈ Z let
Lit = sup{s < t : Xs (i) = 1}

(2.3)

be the last spike time of neuron i strictly before time t. At each time t, conditionally on
the whole past, sites update independently. This means that for any finite subset J ⊂ I,
ai ∈ {0, 1}, i ∈ J, if we introduce the filtration
Ft = σ(Xs , s ∈ Z, s ≤ t), t ∈ Z,
then we have
P (Xt (i) = ai , i ∈ J|Ft−1 ) =

Y

P (Xt (i) = ai |Ft−1 ),

(2.4)

i∈J

where



P (Xt (i) = 1|Ft−1 ) = φi 

X
j

Wj→i

t−1
X

s=Lit



gj (t − s)Xs (j) .

(2.5)

Here φi is the spiking rate function of neuron i, introduced above, and gj the leak function
associated to the spiking events of the j−th neuron. Observe that, since φi is increasing,
the contribution of components j is either excitatory or inhibitory, depending on the sign
of Wj→i .

3

3

Infinite systems of interacting processes with memory of
variable length in continuous time

In continuous time, the activity of each neuron i ∈ I is described by a counting process
Z i recording for any −∞ < s < t < ∞ the number Z i (]s, t]) of spikes of neuron i during
the interval ]s, t]. The sequence of counting processes (Z i , i ∈ I) is characterized by its
intensity process (λit , i ∈ I) defined through the relation
P (Z t has a jump in [t , t + dt ]|Ft ) = λit dt, i ∈ I,
where Ft = σ(Z i (]s, u]), s ≤ u ≤ t, i ∈ I) and where


Z
X
λit = Mi φi 
Wj→i
gj (t − s)dZsj  .

(3.6)

[Lit ,t[

j∈I

Here, Mi , i ∈ I, is a collection of positive numbers giving the maximal intensity of spiking
per neuron (recall that φi takes values in [0, 1]), and Lit = sup{s < t : Z i ([s] > 0}.
This form of an intensity process is close to the typical form of the intensity of a multivariate nonlinear Hawkes process as it has been considered since Hawkes (1971). We refer the
interested reader to Brémaud and Massoulié (1996) for an extensive and comprehensible
study of stability properties of nonlinear Hawkes process, and to Hansen, Reynaud-Bouret
and Rivoirard for the use of Hawkes processes as models of spike trains in neuroscience.
See also Delattre, Fournier and Hoffmann (2014) for a study of infinite systems of nonlinear Hawkes processes. Our form of the intensity (3.6) differs from the classical Hawkes
setting by its variable memory structure introduced through the term Lit . Hence the spiking intensity of a neuron only depends on its history up to its last spike time which is a
biologically very plausible assumption on the memory structure of the process. Therefore,
our model can be seen as a nonlinear multivariate Hawkes process where the number of
components is infinite with a variable memory structure.

4

Associated Markov interacting particle system

The choice of a leak function gj ≡ 1 in (3.6) gives rise to an intensity process which is
Markov. In this case, write
Z
X
dZsj .
(4.7)
Ui (t) =
Wj→i
[Lit ,t[

j∈I

We can interpret Ui (t) as value of the membrane potential of neuron i at time t. Then it
is straightforward to see that (Ui (t))i∈I is a Markov process taking values in RI , whose
generator is given for any smooth test function f : RI → R by
X
Lf (x) =
Mi φi (xi ) [f (x + ∆i (x)) − f (x)] ,
(4.8)
i∈I

where
(∆i (x))j =



Wi→j j 6= i
−xi
j=i
4



.

(4.9)

In such a system of interacting processes Ui (t), each neuron is represented by the height
xi of its membrane potential. It spikes at a rate depending on this height. When spiking,
it goes back to the value 0 which can be interpreted as resting potential. At the same
time, neurons influenced by i, i.e. the postsynaptic neurons, receive an additional amount
of potential Wi→j , independently of the former value xi of the membrane potential of the
spiking neuron. In particular, there is no conservation of mass (i.e. of potential), since
the spiking neuron does not re-distribute its own potential (this is for instance a main
difference with the Potlatch process or with sandpile processes).
Notice that from a mathematical point of view the existence of such a process in infinite
dimension, i.e. in the case when I is infinite, is not evident, since the interactions might
come down from infinity. We do not go into the details, but for a general discussion of
existence issues in infinite dimension, we invite the interested reader to consult for example
Chapter 1 of Liggett (1985).
Sometimes, we will concentrate on the finite case and take I = {1, . . . , N }, for some fixed
N > 0. In this case, we might add to the above dynamics (4.8) two terms. The first one
is a leak term modeling the fact that throughout its evolution, the membrane potential
looses potential due to leakage channels. The second is a drift term modeling the effects
of gap-junctions to the system. Whereas the leakage channels tend to push the membrane
potential of each neuron towards zero, the gap junctions, on the contrary, tend to push the
whole system towards its average membrane potential value. We are thus led to consider
a continuous time Markov process U (t) = (U1 (t), . . . , UN (t)) taking values in RN , whose
infinitesimal generator is given for any smooth test function f : RN → R by
Lf (x) =

N
X

Mi φi (xi ) [f (x + ∆i (x)) − f (x)]−λ

N
N
X
X
∂f
∂f
(x)[xi −x̄]−α
(x)xi , (4.10)
∂xi
∂xi

where λ, α ≥ 0 are positive parameters and where x̄ =
strength of electrical synapses, and α the leakage effect.

5

i=1

i=1

i=1

1
N

PN

i=1 xi .

Here, λ models the

Existence results and perfect simulation

It is natural to ask if there exists at least (and at most) one stationary process which is
consistent with the dynamics defined through (2.4) and (2.5) in discrete time or through
(3.6) in continuous time. The answer to this question is intimately related to the structure
of interactions given by the synaptic weights. These interactions can be represented as a
directed weighted graph where the directed link i → j is present if and only if Wi→j 6= 0,
and where each directed link is weighted by Wi→j . For each neuron i, we introduce
V·→i = {j ∈ I, j 6= i : Wj→i 6= 0},
the set of all neurons that have a direct influence on neuron i. Notice that in our model,
V·→i can be both finite or infinite. We fix a growing sequence (Vi (k))k≥−1 of subsets of I
suchS
that Vi (−1) = ∅, Vi (0) = {i}, Vi (k) ⊂ Vi (k + 1), Vi (k) 6= Vi (k + 1) if Vi (k) 6= V·→i ∪ {i}
and k Vi (k) = V·→i ∪ {i}.

We now state our existence and uniqueness result. We formulate it for discrete time
systems incorporating spontaneous spike times, see Condition (5.11) below. These spontaneous spikes can be interpreted as external stimulus or, alternatively, as autonomous
5

activity of the brain. In order to state our result, let us introduce, for all s < t ∈ Z, the
process Xst (i) = (Xs (i), Xs+1 (i), . . . , Xt (i)), which is the trajectory of X(i) between times
s and t.
Theorem 1 [Theorem 1 of Galves and Löcherbach (2013)]
Grant conditions (2.1) and (2.2). Assume that the functions φi and gj satisfy moreover
the following assumptions:
i) There exists δ > 0 such that for all i ∈ I, s ∈ R,
φi (s) ≥ δ.
ii) We have that
G(1) +

∞
X

(1 − δ)n−2 n2 G(n) < ∞,

(5.11)

(5.12)

n=2

where G(n) = supi

Pn

m=1 gi (m)

and where δ is as in condition 1.

iii) We have fast decay of the synaptic weights, i.e.


X
X
|Wj→i | < ∞.
|Vi (k)| 
sup
i

k≥1

(5.13)

j ∈V
/ i (k−1)

Then the following assertions hold true.
1) There exists a critical parameter δ∗ ∈]0, 1[ such that for any δ > δ∗ , there exists a
unique probability measure P under which (Xt )t∈Z satisfies (2.4) and (2.5).
2) There exists a non increasing function ℓ : N → R+ , such that for any 0 < s < t ∈ N the
following holds. For all i ∈ I, for all bounded measurable functions f : {0, 1}[s,t] → R+ ,


E[f (Xst (i))|F0 ] − E[f (Xst (i))] ≤ (t − s + 1) kf k∞ ℓ(s).
(5.14)
1
for some fixed constant C.
Moreover, ℓ(n) ≤ C n−1

Example 1 Take I = Zd , gj (s) = 1 for all j, s, and
Wi→j =

1
kj − ik2d+α
1

for some fixed α > 1, where k · k1 is the L1 −norm on Zd . In this case, if we choose
Vi (k) = {j ∈ Zd = kj − ik1 ≤ k}, we have |Vi (k)| = (2k + 1)d , and it is easy to see that
condition (5.13) is satisfied.
The proof of Theorem 1 implies the existence of a perfect simulation algorithm of the
stochastic chain (Xt )t∈Z . By a perfect simulation algorithm we mean a simulation which
samples in a finite space-time window precisely from the stationary law P. We refer the
interested reader to Galves and Löcherbach (2013) . In continuous time, the existence of
a unique stationary version of a process (Z i )i∈I having intensity (3.6) can be shown by
following similar same ideas. Details can be found in Hodara and Löcherbach (2014). In
particular, here again, we obtain a perfect simulation algorithm for the stationary law.
6

6

The interaction graph and de-correlation of neighboring
interspike intervals

Throughout this chapter, we work within the discrete time model of Section 2.
One central question in theoretical neuroscience is the distribution of consecutive interspike
intervals (ISI) and in particular their dependence or independence. In order to answer to
this question, we have to specify our choice of an interaction graph. This is related to
the second central question in theoretical neuroscience: what kind of graph should be
considered? Beggs and Plenz (2003) argue that networks of living neurons should behave
in a slightly supercritical state. Therefore we consider a slightly supercritical directed
Erdös-Rényi random graph.
More precisely, for a large but finite system of N neurons, let Wi→j , i 6= j, 1 ≤ i, j ≤ N,
be random synaptic weights. The sequence Wi→j , i 6= j, is a sequence of i.i.d. Bernoulli
random variables defined on some probability space (Ω̃, Ã, P̃ ) with parameter p = pN , i.e.
P̃ (Wi→j = 1) = 1 − P̃ (Wi→j = 0) = pN ,
where
pN = λ/N and λ = 1 + ϑ/N for some 0 < ϑ < ∞.

(6.15)

We put Wj→j ≡ 0 for all j. Conditionally on the choice of the connectivities W =
(Wi→j , i 6= j), the dynamics of the chain are then given by
t−1
X
X
gj (t − s)Xs (j))
Wj→i
P W (Xt (i) = 1|Ft−1 ) = φi (
j

s=Lit

as before. We denote P W the conditional law of the process, conditioned on the choice of
W.
Fix a neuron i and consider its associated sequence of successive spike times
i
. . . < S−n
< . . . < S0i ≤ 0 < S1i < S2i < . . . < Sni < . . . ,

(6.16)

where
i
S1i = inf{t ≥ 1 : Xt (i) = 1}, . . . , Sni = inf{t > Sn−1
: Xt (i) = 1}, n ≥ 2,

and
i
i
S0i = sup{t ≤ 0 : Xt (i) = 1}, . . . , S−n
= sup{t < S−n+1
: Xt (i) = 1}, n ≥ 1.

Let us fix W. We are interested in the covariance between successive inter-spike intervals
i
i
i
i
i
i
Cov W (Sk+1
−Ski , Ski −Sk−1
) = E W [(Sk+1
−Ski )(Ski −Sk−1
)]−E W (Sk+1
−Ski )E W (Ski −Sk−1
),

for any k 6= 0, 1. Being in stationary regime, the above covariance does not depend on the
particular choice of k. The next theorem shows that neighboring inter-spike intervals are
asymptotically uncorrelated as the number of neurons N tends to infinity.

7

Theorem 2 [Theorem 3 of Galves and Löcherbach 2013] Assume that (2.2), (5.11) and
(5.12) are satisfied. Then there exists a measurable subset A ∈ Ã, such that on A,
|Cov W (S3i − S2i , S2i − S1i )| ≤

√
3
N
N
(1
−
δ)
,
δ2

where δ is the lower bound appearing in Condition (5.11). Moreover,
P̃ (Ac ) ≤ e2ϑ N −1/2 .
For large N, if the graph of synaptic weights belongs to the “good” set A, the above
result is compatible with the discussion in Gerstner and Kistler (2002) arguing that two
consecutive interspike intervals can be considered as independent.

7

Longtime behavior for the associated Markov interacting
particle system

We briefly discuss the longtime behavior of the Markov interacting particle system having
generator (4.10). Notice that such a process is a piecewise deterministic Markov process
(PDMP). We concentrate on the case when all synapses are excitatory, i.e. Wi→j ≥ 0 for
all i, j. Moreover we consider a homogenous population where all neurons have the same
spiking behavior, i.e. Mi φi ≡ φ for all i. We do not suppose φ to be bounded nor to be
globally Lipschitz continuous any more. All we have to assume is
Assumption 1 φ : R+
R 2r→ R+ is non-decreasing, φ(0) = 0, φ(x) > 0 for all x > 0, there
exists r > 0 such that 0 φ(x)/xdx < ∞, lim∞ φ = ∞ and φ ∈ C 1 (R+ ).
In this case, the existence of the process is deduced by a simple coupling argument going
back to Fournier and Löcherbach (2014) proving that the total number of jumps during
any finite time interval is finite almost surely. Moreover, interestingly enough, Duarte and
Ost (2014) show that, if the parameter r of Assumption 1 satisfies
X
Wj→i ,
r > max
i

j

and if α > 0, i.e. there is some leakage phenomenon, then
P (∃T such that no spikes occur in [T, ∞[) = 1.
(Theorem 2.3 of Duarte and Ost (2014), compare also to Theorem 1 of Robert and Touboul
(2014)). In particular, in this case, the process goes extinct almost surely. However, if
α = 0 and there is no leak effect, then it is straightforward to show that the process does
not go extinct, but will converge to a non trivial invariant measure. Even more, in this
case the process is recurrent in the sense of Harris on RN
+ \ {0}, see Theorem 2.4 of Duarte
and Ost (2014). In particular, the trivial invariant measure δ0 is non attractive.

8

8

Mean field limits

Suppose we observe a large homogeneous population of N neurons in continuous time
evolving according to (4.10). Then we can assume that we are in an idealized situation
where all neurons have identical properties, leading to a mean field description. The mean
field assumption appears through the assumption that Wi→j = N1 for all i 6= j. Moreover,
we suppose from now on, that there is no leakage effect, i.e. α = 0. In order to keep track
of the size of the system, we denote the process by
N
U N (t) = (U1N (t), . . . , UN
(t)), t ≥ 0,

and identify the state of the system at time t with its empirical measure
µN
t =

N
1 X
δU N (t) .
i
N

(8.17)

i=1

In Theorem 2 of De Masi et al. (2015) it has been shown that, in the limit as N → ∞, this
membrane potential distribution becomes
R deterministic and it is described by a density
ρt (r), where for any interval I ⊂ R+ , I ρt (r)dr is the limit fraction of neurons whose
membrane potentials are in I at time t. The limit density ρt (r) is proved to obey a non
linear PDE which is a conservation law of hyperbolic type
∂
∂
ρt +
(V ρt ) = −φρt ,
∂t
∂x

x > 0, t > 0,

(8.18)

where
V (x, ρt ) := −λ(x − ρ̄t ) + pt

(8.19)

is the velocity field, where the first term describes the attraction to the average membrane
potential of the system, due to the gap junction effect, the second one the drift produced
by the other neurons spiking. Here, the limit total firing rate per unit time pt and the
limit average membrane potential ρ̄t are
Z ∞
Z ∞
xρt (x)dx.
(8.20)
φ(x)ρt (x)dx, ρ̄t =
pt =
0

0

In (8.18), the expression −φ(x)ρt (x) is a loss of mass term due to spiking. Finally, since
(8.18) is only defined for x, t > 0, we have to complete the PDE with boundary conditions
which read as follows.
pt
pt
=
,
(8.21)
ρ0 (x) = u0 (x), ρt (0) =
V (0, ρt )
pt + λρ̄t
where u0 is some initial density of neurons at time t = 0.
It can be shown that under suitable assumptions on the initial distribution of neurons at
time 0, there exists a unique weak solution ρt (x) of (8.18)–(8.21). This is e.g. the case if
the distribution of neurons at time 0 is of compact support. For further details, we refer
the reader to Theorem 4 of Fournier and Löcherbach (2014).
Theorem 3 (Theorem 2 of De Masi et al. (2015)) Grant Assumption 1 and suppose
that UiN (0), 1 ≤ i ≤ N are i.i.d. random variables having smooth density u0 (x). Let ρt (x)
be the unique weak solution of (8.18)–(8.21). Then for any fixed T > 0,
w

L(µU N ) → P[0,T ]
[0,T ]

9

(8.22)

(weak convergence in D([0, T ], S ′ )) as N → ∞, where P[0,T ] is the law on D([0, T ], S ′ )
supported by the distribution valued trajectory ωt given by
Z ∞
φ(x)ρt (x)dx, t ∈ [0, T ],
ωt (φ) =
0

for all φ ∈ S.
Remark 1 The equivalence between the “chaoticity” of the system and a weak law of
large numbers for the empirical measures, as proven in Theorem 3, is well-known (see
for instance Sznitman (1999)).This means that in the large population limit, the neurons
converge in law to independent and identically distributed copies of the same limit law.
This property is usually called “propagation of chaos” in the literature.
In case λ = 0 and u0 (0) = 1, (8.18) reads as follows.

∂t ρt (x) = −pt ∂x ρt (x) − φ(x)ρt (x), x > 0,
ρt (0)
= 1
for all t ≥ 0.
This equation is different from the so-called “population density equations” which are
obtained for integrate-and-fire neurons as considered e.g. in Chapter 6.2.1 of Gerstner and
Kistler (2002), see in particular their formula (6.14). As in integrate-and-fire models, also
in our model spiking neurons are reset to a reversal potential (which equals 0); but spiking
does not create Dirac-masses at the reset value. This is due to the Poissonian mechanism
giving rise to spiking in our model. The loss of mass at time t due to spiking of neurons
having potential height x is therefore described by the term −φ(x)ρt (x).
At the same time, spiking induces a deterministic drift pt dt for those neurons that are not
spiking. Finally, conservation of total mass implies that the initial density of neurons at
the border x = 0 is of height 1. This initial condition is different from the usual initial
condition obtained in integrate-and-fire models.
Remark 2 The mean field approach intending to replace individual behavior in large homogeneous systems of interacting neurons by the mean behavior of the neuronal population
has a long tradition in the frame of neural networks, see e.g. Chapter 6 of Gerstner and
Kistler (2002) or Faugeras et al. (2009) and the references therein. Most of the models
used in the literature are either based on rate models where randomness comes in through
random synaptic weights (see e.g. Cessac et al. (1994) or Moynot and Samuelides (2002));
or they are based on populations of integrate and fire neurons which are diffusion models
in either finite or infinite dimension, see for instance Delarue et al. (2012) or Touboul
(2014). The model we consider is reminiscent of integrate-and-fire models but firing does
not occur when reaching a fixed threshold, and the membrane potential is not described by
a diffusion process. The only noise which comes in is the Poissonian noise given by the
spiking features, compare also to Robert and Touboul (2014).

9

Further results

The limit density ρt (x) which is solution of (8.18)–(8.21) can be interpreted as density of a
typical single neuron U (t), evolving within an infinite system of neurons according to (4.8).
10

Its dynamics can be described as follows. Let U (0) be a u0 -distributed random variable,
independent of a Poisson measure N (ds, dz) on R+ × R+ having intensity measure dsdz.
Then
Z t
U (t) = U (0) − λ (U (s) − E[U (s)])ds
0
Z t
Z tZ ∞
E[φ(U (s))]ds. (9.23)
U (s−)1{z≤φ(U (s−))} N (ds, dz) +
−
0

0

0

Notice that the above dynamics is the dynamics of a nonlinear Markov process (in particular, non homogenous in time), since the law of the process itself – representing the state
of the other neurons within the infinitely large system – is involved in its dynamics.
As in the case of finite systems of neurons, also the limit process possesses exactly two
invariant measures supported in R+ . The first one is δ0 . The second one is of the form
g(dx) = g(x)dx, with g : [0, ∞) 7→ [0, ∞) defined by
 Z x

φ(y)
p
exp −
dy 1{0≤x<m+p/λ} ,
g(x) =
p + λm − λx
0 p + λ(m − y)
R∞
where
p > 0 and m > 0 are uniquely Rdetermined by the constraints 0 g(dx) = 1,
R∞
∞
0 xg(dx) = m. Furthermore, we have 0 φ(x)g(dx) = p and m + p/λ > 1. Note that
for λ = 0, this reads as
 1Z x

g(x) = exp −
φ(y)dy .
(9.24)
p 0
Contrary to the case of a finite system of neurons, it is surprisingly difficult to show that
the limit system does not go extinct, i.e. ρt (x)dx does not tend to δ0 , weakly - at least in
the case of presence of gap junctions λ > 0. The whole picture is only known in the case
λ = 0.
Proposition 1 (Prop. 9 of Fournier and Löcherbach (2014)) Grant Assumption 1
and suppose moreover that φ ∈ C 2 (R+ ) is convex increasing and supx≥1 [φ′ (x)/φ(x) +
φ′′ (x)/φ′ (x)]
< ∞. Let λ = 0 and suppose
U (0) ∼ u0 ∈ Cb1 ([0, ∞)) where u0 satisfies
R∞
R ∞ that
2
′
u0 (0) = 1, 0 φ (x)u0 (x)dx < ∞ and 0 |u0 (x)|dx < ∞. Denote by ρ(t) the law of U (t)
and write g(dx) = g(x)dx for the invariant probability measure defined in (9.24). Then
we have limt→∞ kρ(t) − gkT V = 0, where k · kT V denotes the total variation distance. In
particular, the process does not go extinct, almost surely.
The case λ 6= 0 is more subtle, and we only know that the process does not go extinct,
under minimal conditions on the spiking rate function, cf. Proposition 11 of Fournier and
Löcherbach (2014).

10

Questions and challenges

In this section we raise several natural questions in the context of the models considered
in this paper.
To which extend is a mean field description as adopted in Sections 8 and 9 above relevant from a neurobiological point of view? The mean field approach intending to replace
11

individual behavior in large homogeneous systems of interacting neurons by the mean behavior of the neuronal population has a long tradition in the frame of neural networks.
Bressloff (2009) argues that considering homogeneous populations “is motivated by the
observation that neurons in cortex with similar response properties tend to be arranged
in vertical columns. A classical example is the columnar-like arrangement of neurons in
primary visual cortex that prefer stimuli of similar orientation”. Therefore it is reasonable
to consider that such systems of neurons are governed by interactions of mean-field type.
Moreover, Bojak et al. (2010) claim “that a mean field model of brain activity can simultaneously predict EEG and fMRI BOLD ...”. For a recent review paper we refer the
reader to Pinotsis et al. (2014).
Description of the system and propagation of chaos. EEG as well as fMRI data describe
the collective behavior of huge subpopulations of neurons. This makes it reasonable to
consider a space-time rescaling of the “microscopic system” reminiscent of what is usually
done for interacting particle systems under the name of “hydrodynamical limits”, see De
Masi and Presutti (1991) and Kipnis and Landim (1999). The difficulty for neurobiological
models is that contrarily to the case of thermodynamical systems considered in statistical
physics, we have no macroscopic qualitative results available. In a nutshell, as far as
we know, in neurobiology there is presently nothing that plays the role that the Fourier
law plays in thermodynamics. Therefore, the first problem is to understand what kind
of limiting behavior should be obtained when rescaling a stochastic model describing a
system of spiking neurons.
Chaos propagation is an issue which is directly associated to the above discussion. The
concept of propagation of chaos has been introduced by Mark Kac in his seminal paper
of 1956, see Kac (1956).. His goal was to show that within a system of a huge number
of interacting components, in a suitable limit, any fixed number of components behave as
independent stochastic processes having the same distribution (for more details, we refer
the reader to the classical reference Sznitman (1991)).
The recent literature in neuromathematics presents many results concerning the propagation of chaos in stochastic models of neuronal networks. However, it is far from being
clear what is the neurobiological meaning of these results. Moreover, it is difficult to find
neurobiological experimental results clearly related with chaos propagation. At the same
time, in a large majority of the mathematical papers, including ours, there is no discussion
about the neurobiological relevance of this issue.
Exceptions are recent papers and lectures of Olivier Faugeras and members of his team.
For instance, in Baladron et al. (2012) the authors write the following.“We prove a propagation of chaos property which shows that in the mean-field limit, the neurons become
independent, in agreement with some recent experimental work [13] and with the idea that
the brain processes information in a somewhat optimal way.” In the above, [13] refers to
Ecker et al. (2010) which present experimental evidence concerning the de-correlation of
neuronal firing in the visual cortex. Here, the authors argue that “the de-correlated state
of the neocortex [...] offers substantial advantages for information processing.”
We believe that a more systematical discussion of the relation between mathematical
results on chaos propagation and qualitative experimental results of neurobiology should
be done by the neuromathematical community.
What about inhibitions? Inhibitions are considered in Sections 2, 3 and 5, but the theoretical results proved there do not take advantage of the balance between excitatory and
12

inhibitory neurons. As far as we know, no rigorous neuromathematical result relies on this
balance. This is clearly a step which must be achieved in a near future.
On the other hand, in many articles of the neurobiological literature it is very often suggested that inhibition should play a crucial role to explain many qualitative aspects found
in the data. For instance, Benayoun et al. (2010) suggest that the balance between inhibition and excitation is an important ingredient in the explanation of avalanche phenomena.
However, it is quite simple to conceive mathematical toy models without inhibition for systems of spiking neurons in which avalanches are produced. Another example is the belief
which seems to be widespread in the neuroscientific community concerning the role played
by inhibition in phenomena like chaos propagation (cf. for instance, van Vreeswijk and
Sompolinsky (1996) and Huntsman et al. (1999)). However, our Theorem 3 concerning
propagation of chaos does not rely at all on the presence of inhibitory synapses. We believe
that it is important to better understand the importance of inhibition in the qualitative
behavior of stochastic models like those presented in this paper.
What about statistical model selection? We have presented in this paper what we believe
to be a good class of candidate models describing spiking neuronal behavior. Therefore,
if we were able to do statistical model selection for this class of models we would be
able to clarify issues like the way different neurons and regions of the cortex interact. In
mathematical terms, this amounts of estimating the underlying interaction graph. There
are two obvious difficulties.
First of all, our data concerns only an extremely tiny part of the cortex (between 102
to 103 neurons). What does this very small view tell us about the entire region? In
former papers, one of the authors show that for models coming from statistical physics
like the Ising model, under a Dobrushin type condition, it is possible to make inference of
the entire system by just observing a small part of it. We refer the interested reader to
Galves, Orlandi and Takahashi (2015). Is this type of result still valid when we look at a
system like the brain?
The second problem is the computational complexity of the selection procedure among all
possible interaction graphs supporting models like ours. In the case of chains with memory
of variable length, the famous Context algorithm introduced by Rissanen (1983) and more
recent versions of the BIC approach to the same problem by Cszisar and Talata (2006) as
well as its application to random Markov fields of variable neighborhoods in Löcherbach
and Orlandi (2011), this selection problem can be reduced to a linear complexity problem.
Is a solution like this available for the class of models considered here?

11

Discussion

We close with a discussion of the particular aspects of the models we propose in this paper.
We start by discussing the “ variable length memory dependence” on the past which is
incorporated in our models via (2.5) or (3.6). From a theoretical point of view, Cessac
(2011) suggested the same kind of dependence from the past in a discrete time model.
In the framework of leaky integrate and fire models, he considers a system with a finite
number of membrane potential processes. The image of this process in which only the
spike times are recorded is a stochastic chain of infinite order where each neuron has to
look back into the past until its last spike time. Cessac’s process is a finite dimensional
version of the model considered in Section 2.
13

A second important feature of the class of processes introduced in our paper is the fact that
we are able to deal with infinite systems of neurons in a rigorous mathematical way. Finite
systems of point processes in discrete or continuous time aiming to describe biological
neural systems have a long history whose starting points are probably Hawkes (1971) from
a probabilistic point of view and Brillinger (1988) from a statistical point of view, see also
the interesting paper by Krumin et al. (2010) for a review of the statistical aspects. For
non-linear Hawkes processes, but in the frame of a finite number of components, Brémaud
and Massoulié (1994) address the problem of existence, uniqueness and stability. Møller
and coauthors propose a perfect simulation algorithm in the linear case, see Møller and
Rasmussen (2005). In spite of the great attention that Hawkes processes received recently,
especially in association with modeling problems in finance and biology, all the studies
are reduced to the case of systems with a finite number of components. Only recently,
Delattre, Fournier and Hoffmann (2014) studied an infinite system of nonlinear Hawkes
processes but did not address the perfect simulation issue. Notice also that in none of the
above articles, a variable length memory dependence on the past is incorporated although
this kind of dependence is completely natural in the frame of neural nets.

Acknowledgments
We thank Pierre Hodara and Daniel Takahashi for careful reading of this manuscript and
valuable comments.
This article was produced as part of the activities of FAPESP Research, Dissemination
and Innovation Center for Neuromathematics (grant 2013/07699-0, S. Paulo Research
Foundation). This work is part of USP project “Mathematics, computation, language
and the brain” and CNPq project “Stochastic modeling of the brain activity” (grant
480108/2012-9). AG is partially supported by a CNPq fellowship (grant 309501/2011-3).

References
[1] J. Baladron, D. Fasoli, O. Faugeras, and J. Touboul. Mean-field description and
propagation of chaos in networks of Hodgkin-Huxley and Fitzhugh-Nagumo neurons.
2012.
[2] J.M. Beggs and D. Plenz. Neuronal avalanches in neocortical circuits. J. Neurosc,
23:11167–11177, 2003.
[3] M. Benayoun, J.D. Cowan, W. van Drongelen, and E. Wallace. Avalanches in a
Stochastic Model of Spiking Neurons. PLOS Comput Biol., 6(7), 2010.
[4] I. Bojak, T.F. Oostendorp, A.T. Reid, and Rolf R. Kötter. Connecting mean field
models of neural activity to eeg and fmri data. Brain Topography, 23(2):139–149,
2010.
[5] P. Brémaud and L. Massoulié. Stability of nonlinear Hawkes processes. Ann. Probab.,
24(3):1563–1588, 1996.
[6] P. C. Bressloff. Stochastic neural field theory and the system-size expansion. SIAM
J. Appl. Math., 70(5):1488–1521, 2009.
14

[7] D. Brillinger. Maximum likelihood analysis of spike trains of interacting nerve cells.
Biol. Cybern., 59(3):189–200, 1988.
[8] B. Cessac. A discrete time neural network model with spiking neurons: II: Dynamics
with noise. Journal of Mathematical Biology, 62:863–900, 2011.
[9] B. Cessac, B. Doyon, M. Quoy, and M. Samuelides. Mean-field equations, bifurcation
map and route to chaos in discrete time neural networks. Physica D: Nonlinear
Phenomena, 74:24 – 44, 1994.
[10] I. Csiszár and Z. Talata. Context tree estimation for not necessarily finite memory
processes, via BIC and MDL. IEEE Trans. Inf. Theory, 52(3):1007–1016, 2006.
[11] F. Delarue, J. Inglis, S. Rubenthaler, and E. Tanré. Global solvability of a networked
integrate-and-fire model of McKean-Vlasov type, 2012.
[12] S. Delattre, N. Fournier, and M. Hoffmann. High dimensional Hawkes processes,
2014.
[13] A. Duarte and G. Ost. A model for neural activity in the absence of external stimuli,
2014.
[14] A. S. Ecker, P. Berens, G. A. Keliris, M. Bethge, N.K. Logothetis, and A.S. Tolias.
Decorrelated neuronal firing in cortical microcircuits. Science, 327(5965):584–587,
2010.
[15] O. Faugeras, J. Touboul, and B. Cessac. A constructive mean-field analysis of multipopulation neural networks with random synaptic weights and stochastic inputs.
Frontiers in Comp. Neuroscience, 3:1–28, 2009.
[16] N. Fournier and E. Löcherbach. On a toy model of interacting neurons, 2014.
[17] A. Galves and E. Löcherbach. Stochastic chains with memory of variable length.,
pages 117–133. TICSP Series vol. 38, 2008.
[18] A. Galves and E. Löcherbach. Infinite systems of interacting chains with memory
of variable length - a stochastic model for biological neural nets. J. Stat. Phys.,
151:896–921, 2013.
[19] A. Galves, E. Orlandi, and D.Y. Takahashi. Identifying interacting pairs of sites in
ising models on a countable set., 2015.
[20] W. Gerstner and W. M. Kistler. Spiking neuron models. Single neurons, populations,
plasticity. Cambridge: Cambridge University Press., 2002.
[21] N. Hansen, P. Reynaud-Bouret, and V. Rivoirard. Lasso and probabilistic inequalities
for multivariate point processes, 2012.
[22] A. G. Hawkes. Point spectra of some mutually exciting point processes. J. R. Stat.
Soc., Ser. B, 33:438–443, 1971.
[23] P. Hodara and E. Löcherbach. Hawkes processes with variable length memory and
an infinite number of components, 2014.

15

[24] M.M. Huntsman, D.M. Porcello, G.E. Homanics, T.M. DeLorey, and J.R. Huguenard. Reciprocal inhibitory connections and network synchrony in the mammalian
thalamus. Science, 283:541–543, 1999.
[25] M. Kac. Foundations of kinetic theory., pages 171–197. Proc. 3rd Berkeley Sympos.
Math. Statist. Probability 3. 1956.
[26] C. Kipnis and C. Landim. Scaling limits of interacting particle systems. Grundlehren
der Mathematischen Wissenschaften. 320. Berlin: Springer., 1999.
[27] M. Krumin, I. Reutsky, and S. Shoham. Correlation-based analysis and generation of
multiple spike trains using Hawkes models with an exogenous input. Front Comput
Neurosci., 4:147, 2010.
[28] T.M. Liggett. Interacting Particle Systems. Springer Berlin Heidelberg, 1985.
[29] E. Löcherbach and E. Orlandi. Neighborhood radius estimation for variableneighborhood random fields. Stochastic Processes Appl., 121(9):2151–2185, 2011.
[30] A. De Masi, A. Galves, E. Löcherbach, and E. Presutti. Hydrodynamic limit for
interacting neurons. J. Stat. Physics, 0-3, 2015.
[31] A. De Masi and E. Presutti. Mathematical methods for hydrodynamic limits. Lecture
Notes in Mathematics. 1501. Berlin: Springer-Verlag, 1991.
[32] J. Møller and J. G. Rasmussen. Perfect simulation of Hawkes processes. Adv. Appl.
Probab., 37(3):629–646, 2005.
[33] O. Moynot and M. Samuelides. Large deviations and mean-field theory for asymmetric
random recurrent neural networks. Probability Theory and Related Fields, 123(1):41–
75, 2002.
[34] D. Pinotsis, P. Robinsons, P. beim Graben, and K. Friston. Neural masses and fields:
modeling the dynamics of brain activity. Front Comput Neurosci, 8:149, 2014.
[35] J. Rissanen. A universal data compression system. IEEE Trans. Inform. Theory,
29(5):656–664, 1983.
[36] P. Robert and J. Touboul. On the dynamics of random neuronal networks, 2014.
[37] F. Spitzer. Interaction of Markov Processes. Adv. Math., 5(2):246–290, 1970.
[38] A-S. Sznitman. Topics in propagation of chaos. Calcul des probabilités, Ec. d’Été,
Saint-Flour/Fr. 1989, Lect. Notes Math. 1464, 165-251 (1991)., 1991.
[39] J. Touboul. Propagation of chaos in neural fields. The Annals of Applied Probability,
24(3):1298–1328, 06 2014.
[40] C. van Vreeswijk and H. Sompolinsky. Chaos in neuronal networks with balanced
excitatory and inhibitory activity. Science, 274:1724–1726, 1996.

16

