epl draft

Dynamics of Neural Networks with Continuous Attractors
C. C. Alan Fung1 , K. Y. Michael Wong1 and Si Wu2
1

arXiv:0801.4461v2 [cond-mat.dis-nn] 31 Jan 2015

2

Department of Physics, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, China
Department of Informatics, University of Sussex, Brighton, United Kingdom

PACS
PACS

87.10.-e – General theory and mathematical aspects in biological and medical physics
05.45.-a – Nonlinear dynamics and chaos

Abstract. - We investigate the dynamics of continuous attractor neural networks (CANNs). Due
to the translational invariance of their neuronal interactions, CANNs can hold a continuous family
of stationary states. We systematically explore how their neutral stability facilitates the tracking
performance of a CANN, which is believed to have wide applications in brain functions. We
develop a perturbative approach that utilizes the dominant movement of the network stationary
states in the state space. We quantify the distortions of the bump shape during tracking, and
study their effects on the tracking performance. Results are obtained on the maximum speed for a
moving stimulus to be trackable, and the reaction time to catch up an abrupt change in stimulus.

Understanding how the dynamics of a neural network is
shaped by the network structure, and consequently facilitates the functions implemented by the neural system, is at
the core of using physical models to elucidate brain functions [1]. Traditional models of attractor neural networks
(ANNs), such as those based on the Hopfield model [2], are
powerful for elucidating the computation and the memory
retrieving processes in the brain. When external noisy
inputs are presented, the dynamics of ANNs will reach
an attractor that is highly correlated with the memories
stored in the network. Each attractor has its own basin of
attraction well separated from the others. These models
successfully describe the behavior of associative memories,
and stimulated an upsurge of studies on the retrieval behavior of the models [3]. However, other than memory
retrieval, many other computational issues of the brain
have not been analyzed to the same extent.
Recently, a new type of attractor networks, called continuous attractor neural networks (CANNs), has received
considerable attention (see, e.g., [4–20]). These networks
possess a translational invariance of the neuronal interactions. As a result, they can hold a family of stationary
states which can be translated into each other without
the need to overcome any barriers. Thus, in the continuum limit, they form a continuous manifold in which
the system is neutrally stable, and the network state can
translate easily when the external stimulus changes continuously. Beyond pure memory retrieval, this endows the
neural system with a tracking capability.

The tracking dynamics of a CANN has been investigated by several authors in the literature (see, e.g.,
[5,6,11,12,14,20]) These studies have shown that a CANN
has the capacity of tracking a moving stimulus continuously and that this tracking property can well justify
some brain functions. Despite these successes, however,
a detailed analysis of the tracking behaviors of a CANN
is still lacking. These include, for instance, 1) the conditions under which a CANN can successfully track a moving stimulus, 2) the distortion of the shape of the network
state during the tracking, and 3) the effects of these distortions on the tracking speed. In this Letter we will report,
as far as we know, the first systematic study on these issues.
We will use a simple, analytically-solvable, CANN
model as the working example. The form of the network
state is reminiscent of those of solitons in nonlinear dynamics. We display clearly how the dynamics of a CANN
is decomposed into different distortion modes, corresponding to, respectively, changes in the height, position, width
and skewness of the network state. We then demonstrate
which of them dominates the tracking behaviors of the
network. In order to solve the dynamics which is otherwise extremely complicated for a large recurrent network,
we develop a time-dependent perturbation method to approximate the tracking performance of the network. The
solution is expressed in a simple closed-form, and we can
approximate the network dynamics up to an arbitory accuracy depending on the order of perturbation used. We

p-1

Fung et al.

r(x, t) =

U (x, t)2
R
,
1 + kρ dx′ U (x′ , t)2

(1)

where ρ is the neural density, and k is a small positive
constant controlling the strength of global inhibition. The
dynamics of the synaptic input U (x, t) is determined by
the external input Iext (x, t), the network input from other
neurons, and its own relaxation. It is given by
Z
dU (x, t)
= Iext (x, t) + ρ dx′ J(x, x′ )r(x′ , t) − U (x, t),
τ
dt
(2)
where τ is the time constant, which is typically of the order 1 ms, and J(x, x′ ) is the neural interaction from x′
to x. The key characteristic of CANNs is the translational invariance of their neural interactions. In our solvable model, we choose Gaussian interactions with a range
a, namely,
√
(3)
J(x, x′ ) = exp[−(x − x′ )2 /(2a2 )]J/ 2πa2 .
CANN models with other neural interactions and inhibition mechanisms have been studied [4–6, 8, 13]. However,
our model has the advantage of permitting a systematic
perturbative improvement. Nevertheless, the final conclusions of our model are qualitatively applicable to general
cases (to be further discussed at the end of the paper).
We first consider the intrinsic dynamics of the CANN
model in the√ absence of external stimuli. For 0 < k <
kc ≡ ρJ 2 /(8 2πa), the network holds a continuous family
of stationary states, which are


(x − z)2
Ũ (x|z) = U0 exp −
,
(4)
4a2
√
where U0 = [1 + (1 − k/kc )1/2 ]J/(4 πak). These stationary states are translationally invariant among themselves
and have the Gaussian bumped shape peaked at arbitrary
positions z.

2
1.5

U(x)

expect that our method will provide a useful tool for the
theoretical studies of CANNs. Our work generates new
predictions on the tracking behaviors of CANNs, namely,
the maximum tracking speed to moving stimuli, and the
reaction time to sudden changes in external stimuli, both
are testable by experiments.
Specifically, we consider a one-dimensional continuous
stimulus being encoded by an ensemble of neurons. For
example, the stimulus may represent the moving direction,
the orientation, or a general continuous feature of an external object. Let U (x, t) be the synaptic input at time t to
the neurons with preferred stimulus of real-valued x. We
will consider stimuli and responses with correlation length
a much less than the range of x, so that the range can be
effectively taken to be (−∞, ∞). The firing rate r(x, t)
of these neurons increases with the synaptic input, but
saturates in the presence of a global activity-dependent
inhibition. A solvable model that captures these features
is given by

1
0.5
0

-2

0
x

2

Fig. 1: The canyon formed by the stationary states projected
onto the subspace formed by b1 |0 and b0 |0 . Motion along the
canyon corresponds to the displacement of the bump (inset).

The stability of the Gaussian bumps can be studied by
considering the dynamics of fluctuations. Consider the
network state U (x, t) = Ũ (x|z)+δU (x, t). Then we obtain
Z
d
τ δU (x, t) = dx′ F (x, x′ )δU (x′ , t) − δU (x, t), (5)
dt
where the interaction kernel is given by Eq. (19) in the
Appendix. To compute the eigenfunctions and eigenvalues
of the kernel F (x, x′ ), we choose the wave functions of the
quantum harmonic oscillators as the basis, namely,
exp(−ξ 2 /2)Hn (ξ)
,
(6)
vn (x|z) = p
(2π)1/2 an!2n
√
where ξ ≡ (x − z)/( 2a) and Hn (ξ) is the nth order Hermite polynomial function. Indeed, the ground state of the
quantum harmonic oscillator corresponds to the Gaussian
bump, and the first, second, and third excited states correspond to fluctuations in the peak position, width, and
skewness of the bump respectively (Fig.2). As described
in the Appendix, the eigenvalues of the kernel F are calculated to be
λ0
λn

= 1 − (1 − k/kc )1/2 ,

= 1/2

n−1

,

for n ≥ 1.

(7)
(8)

and the first four eigenfunctions of F are
v0 (x|z),
(9)
v1 (x|z),
(10)
p
√
1 − 2 1 − k/kc
2
v0 (x|z) +
v2 (x|z),
(11)
u2 (x|z) =
2D0
D0
r
r
1
6
u3 (x|z) =
v1 (x, z) +
v3 (x, z),
(12)
7
7
p
where D0 = [(1 − 2 1 − k/kc )2 + 1/2]1/2 . The eigenfunctions of F correspond to the various distortion modes of
the bump. Since λ1 = 1 and all other eigenvalues are less
than 1, the stationary state is neutrally stable in one component, and stable in all other components. The first two
eigenfunctions are particularly important. (1) The eigenfunction for the eigenvalue λ0 is u0 (x|z), and represents a
distortion of the amplitude of the bump. As we shall see,

p-2

u0 (x|z) =
u1 (x|z) =

Dynamics of Neural Networks with Continuous Attractors
Height
v0

v1

0.5
0
-1
0
-0.5

-2

Position

1

1

2

-1

-2

v3

0.5

-1

U (x, t) = Ũ (x|z(t)) +

2

1

2

-2

1

0.5
0
-1
0
-0.5

∞
X

an (t)vn (x|z(t)).

(13)

n=0

Skew

1

0
-1
0
-0.5

1

-0.5

Width
v2

0.25
0
-1
0
-0.25

-2

basis, and consider perturbations in increasing orders of
n. This is done by considering solutions of the form

0.5

1

2

-1

Fig. 2: The first four basis functions of the quantum harmonic
oscillators, which represent four distortion modes of the network dynamics, namely, changes in the height, position, width
and skewness of a bump state.

amplitude changes of the bump affect its tracking performance. (2) Central to the tracking capability of CANNs,
the eigenfunction for the eigenvalue 1 is u1 (x|z) and is
neutrally stable. We note that u1 (x|z) ∝ ∂v0 (x|z)/∂z,
corresponding to the shift of the bump position among
the stationary states. This neutral stability is the consequence of the translational invariance of the network.
It implies that when there are external inputs, however
small, the bump will move continuously. This is a unique
property associated with the special structure of a CANN,
not shared by other attractor models. Other eigenfunctions correspond to distortions of the shape of the bump.
For example, the eigenfunction u3 (x|z) corresponds to a
skewed distortion of the bump.
It is instructive to consider the energy landscape in the
state space of a CANN. Since F (x, x′ ) is not symmetric, a
Lyapunov function cannot be derived for Eq. (5). Nevertheless, for each peak position
z, one can define an effective
P
energy function E|z = n (1−λn )bn |2z /2, where bn |z is the
overlap between U (x) − Ũ (x|z) and the nth eigenfunction
of F centered at z. Then the dynamics in Eq. (5) can
be locally described by the gradient descent of E|z in the
space of bn |z . Since the set of points bn |z = 0 for n 6= 1
traces out a line with E|z = 0 in the state space when z
varies, one can envisage a canyon surrounding the line and
facilitating the local gradient descent dynamics, as shown
in Fig. 1. A small force along the tangent of the canyon
can move the network state easily. This illustrates how the
landscape of the state space of a CANN is shaped by the
network structure, leading to the neutral stability of the
system, and how this neutral stability shapes the network
dynamics.
Next, we consider the network dynamics in the presence
of a weak external stimulus. Suppose the neural response
at time t is peaked at z(t). Since the dynamics is primarily
dominated by the translational motion of the bump, with
secondary distortions in shape, we may develop a timedependent perturbation analysis using {vn (x|z(t))} as the

Furthermore, since the Gaussian bump is the steady-state
solution of the dynamical equation in the absence of external stimuli, the neuronal interaction term in Eq. (2)
can be linearized for weak stimuli. Making use of the orthonormality and completeness of {vn (x|z(t))}, we obtain
for each order n of perturbation (see Appendix),
!
" q
In
1 − λn
d
an =
+
− U0 (2π)1/2 aδn1
dt
τ
τ
#
√
√
1 dz
nan−1 − n + 1an+1
+
2a dt
r
∞
1 X (n + 2r)! (−1)r
an+2r ,
(14)
+
τ r=1
n!
2n+3r−1 r!
where In (t) is the projection of the external input Iext (x, t)
on the nth eigenfunction.
Determining z(t) by the center of mass of U (x, t), we
obtain the self-consistent condition (see Appendix)
!
p
P∞
I1 + n=3,odd n!!/(n − 1)!!In + a1
2a
dz
p
.
p
=
P
dt
τ U0 (2π)1/2 a + ∞
(n − 1)!!/n!!an
n=0,even
(15)
Eqs.(14) and (15) are the master equations of the perturbation method. We can approximate the network dynamics up to an arbitary accuracy depending on the choice
of the order of perturbation. In practice, low order perturbations already yield very accurate results. Below, we
consider the network dynamics for several kinds of external stimuli.
1) Tracking a stationary stimulus: Consider the external stimulus consisting of a Gaussian bump, namely,
Iext (x, t) = αU0 exp[−(x − z0 )2 /4a2 ]. Perturbation up to
the order
n = 1 yields a1 (t) = 0, [d/dt + (1 − λ0 )/τ ]a0 =
p
αU0 (2π)1/2 a exp[−(z0 − z)2 /8a2 ]/τ , and


dz
α
(z0 − z)2
R(t)−1 ,
= (z0 − z) exp −
dt
τ
8a2

(16)

Rt
where R(t) = 1 + α −∞ (dt′ /τ ) exp[−(1 − λ0 )(t − t′ )/τ −
(z0 − z(t′ ))2 /8a2 ], representing the ratio of the bump
height relative to that in the absence of the external stimulus (α = 0). Hence, the dynamics is driven by a pull of the
bump position towards the stimulus position z0 . When
compared with the limiting expression for weak stimuli,
which was considered in the dynamics of the moving bump
in [18], the pull is reduced by the factor R(t). This is due
to the increase in amplitude of the bump, which slows
down its response.

p-3

Fung et al.
0.04
0.03

4

(a)
vmax

1

(b)

3

v

0.6
2

0.02

s

s

g(s)

(c)

0.8

0.4
1

0.01
0
0

s2

s1

0.5

1

2

1.5
s

2.5

3

0.2

0
0

50

100

t

150

200

250

0
0

vmax
0.01

0.02
v

0.03

0.04

Fig. 3: (a) The function g(s) and the stable and unstable fixed points of Eq. (17). (b) The time dependence of the separation
s starting from different initial values. Symbols: simulations with N = 200. Lines: n = 5 perturbation. Dashed lines: s1
(bottom) and s2 (top). (c) The dependence of the terminal separation s on the stimulus speed v. Symbols: simulations with
N = 200. Solid line:
√ prediction of [18]. Dashed line: n = 1 perturbation. Parameters: α = 0.05, a = 0.5, τ = 1, k = 0.5,
ρ = N/(2π), J = 2πa2 .

Suppose, in addition to the bumped signal, there is also
the I1 component consisting of a white noise of temperature Tn . Then the noises shift the bump position randomly. In the absence of the bumped signal in the external stimulus (α = 0), the peak position of the bumped
response undergoes a random walk with hz(t)2 i = 2Dt in
the low noise limit, where D ≡ 4aTn /[(2π)1/2 U02 τ 2 ] is the
diffusion constant. In the presence of a weak bumped signal (α > 0), the peak of the bumped response experiences
a pull which can be written as the gradient of a potential
V (z) = −4a2 α exp[−(z − z0 )2 /8a2 ]/τ . Thus, the probability distribution of z can be approximated by a Boltzmann
distribution P (z) ∝ exp[−V (z)/D] in the low noise limit.
2) Tracking a moving stimulus: The tracking performance of a CANN is a key property that is believed to
have wide applications in neural systems. Suppose the
stimulus is moving at a constant velocity v, and the noise
is negligible. The dynamical equation becomes identical
to Eq. (16), with z0 replaced by vt. Denoting the lag of
the bump behind the stimulus by s = z0 − z we have, after
the transients,
2

αse−s
ds
= v − g(s); g(s) ≡
dt
τ

/8a2

"

2

αe−s /8a
1+
1 − λ0

2

#−1

.

(17)
The value of s is determined by two competing factors:
the first term represents the movement of the stimulus,
which tends to enlarge the separation, and the second term
represents the collective effects of the neuronal recurrent
interactions, which tends to reduce the lag. Tracking is
maintained when these two factors match each other, i.e.,
v = g(s); otherwise, s diverges.
Fig.3(a) shows that the function g(s)√is concave, and has
the maximum value of gmax = 2αa/(τ e) at s = 2a. This
means that if v > gmax , the network is unable to track
the stimulus. Thus, gmax defines the maximum trackable
speed of a moving stimulus. Notably, gmax increases with
the strength of the external signal and the range of neuronal recurrent interactions. This is reasonable since it
is the neuronal interactions that induce the movement of
the bump. gmax decreases with the time constant of the

network, as this reflects the responsiveness of the network
to external inputs.
On the other hand, for v < gmax , there is a stable and
unstable fixed point, respectively denoted by s1 and s2 in
Fig. 3(a). When the initial distance is less than s2 , it will
converge to s1 . Otherwise, the tracking of the stimulus
will be lost. Figs. 3(b) and (c) show that the analytical
results of Eq. (17) well agree with the simulation results.
3) Tracking an abrupt change of the stimulus: Suppose
the network has reached a steady state with an external
stimulus stationary at t < 0, and the stimulus position
jumps from 0 to z0 suddenly at t = 0. This is a typical scenario in experiments studying mental rotation behaviors. We first consider the case that the jump size
z0 is small compared with the range a of neuronal interactions. In the limit of weak stimulus, the dynamics
is described by Eq. (16) with R(t) = 1. We are interested in estimating the reaction time T , which is the time
taken by the bump to move to a small distance θ from the
stimulus position. In the limit of low noise, the reaction
time increases logarithmically with the jump size, namely,
T ≈ (τ /α) ln(|z0 |/θ).
When the strength α of the external stimulus is larger,
improvement using a perturbation analysis up to n = 1 is
required when the jump size z0 is large. This amounts to
taking into account the change of the bump height during
its movement from the old to new position. The result is
identical to Eq. (16), with R(t) replaced by


Z t ′
(1 − λ0 )
dt
α
exp −
t +α
R(t) = 1 +
1 − λ0
τ
0 τ


′ 2
(z0 − z(t ))
(1 − λ0 )
.
(18)
(t − t′ ) −
× exp −
τ
8a2
Indeed, R(t) represents the change in height during the
movement of the bump. Contributions from the second
and third terms show that it is highest at the initial and
final positions respectively, and lowest at some point in between, agreeing with simulation results shown in Fig. 4(b).
Fig. 4(a) shows that the n = 1 perturbation overcomes the
insufficiency of the logarithmic estimate, and has an excellent agreement with simulation results for z0 up to the

p-4

Dynamics of Neural Networks with Continuous Attractors

400

2

(a)

T

200

1.5
U(x)

300

(b)
Simulation
Prediction of [16]
"n=1" perturbation
"n=2" perturbation
"n=3" perturbation
"n=4" perturbation
"n=5" perturbation

100
0
0

1
0.5

0.5

1

1.5
z0

2

2.5

0

3

-2

0
x

2

Fig. 4: (a) The dependence of the reaction time T on the new stimulus position z0 . (b) Profiles of the bump between the old
and new positions at z0 = π/2 in the simulation. Parameters: as in Fig.3.

order of 2a. We also compute the reaction time up to
the n = 5 perturbation, and the agreement with simulations remains excellent even when z0 goes beyond 2a.
This implies that beyond the range of neuronal interaction, tracking is influenced by the distortion of the width
and the skewed shape of the bump.
To conclude, we have systematically investigated how
the neutral stability of a CANN facilitates the tracking
performance of the network, a capability which is believed
to have wide applications in brain functions. Two interesting behaviors are observed, namely, the maximum trackable speed for a moving stimulus and the reaction time
for catching up an abrupt change of a stimulus, logarithmic for small changes and increasing rapidly beyond the
neuronal range. These two properties are associated with
the unique dynamics of a CANN. They are testable in
practice and can serve as general clues in for checking the
existence of a CANN in neural systems. In order to solve
the dynamics which is otherwise extremely complicated
for a large recurrent network, we have developed a perturbative analysis to simplify the dynamics of a CANN.
Geometrically, it is equivalent to projecting the network
state on its dominant directions of the state space. This
method works efficiently and may be widely used in the
study of CANNs. Furthermore, the special structure of
a CANN may have other applications in brain functions,
for instance, the highly structured state space of a CANN
may provide a neural basis for encoding the topological
relationship of objects in a feature space, as suggested by
recent psychophysical experiments [21,22]. We expect the
mathematical framework developed in this study will be
very valuable to explore these issues.
The tracking dynamics of a CANN has also been studied
by other authors. In particular, Zhang proposed a mechanism of using asymmetrical recurrent interactions to generate the bump movement without the bump shape being
distorted [6]. Xie et al.further proposed a double ring network model to implement these asymmetrical interactions
based on the external inputs [11]. These models work well
for the head-direction system. Different from their model

setting, here we consider the movement of the bump directly driven by external inputs, and hence the shape of
the bump is unavoidably distorted. This setting has also
been used by many other authors in modeling brain functions (see, e.g., [25,26]). We quantify how the distortion of
the bump shape affects the network tracking performance,
and obtain a new finding on the maximum trackable speed
of the network. We expect these results will enrich our
knowledge on the tracking dynamics of CANNs.
Finally, we would like to remark on the generality of the
results in this work and their relationships to other studies in the literature. In order to pursue an analytical solution, we have used a divisive normalization to represent
the inhibition effect. This is different from the Mexicanhat type of recurrent interactions used by many authors.
For the latter, it is often difficult to get a closed-form
of the network stationary state. Amari used a Heaviside
function to simplify the neural response, and obtained the
box-shaped network stationary state [4]. However, since
the Heaviside function is not differentiable, it is difficult
to describe the tracking dynamics in the Amari model.
Truncated sinusoidal functions have been used, but it is
difficult to use them to describe general distortions of the
bumps [5]. Here, by using divisive normalization and the
Gaussian-shaped recurrent interactions, we solve the network stationary states and the tracking dynamics analytically.
One may be concerned about the feasibility of the divisive normalization. Firstly, we argue that neural systems
can have resources to implement this mechanism [8,23,24].
Let us consider, for instance, a neural network, in which
all excitatory neurons are connected to a pool of inhibitory
neurons. Those inhibitory neurons have a time constant
much shorter than that of excitatory neurons, and they
inhibit the activities of excitatory neurons in a uniform
shunting way [23], thus achieving the effect of divisive normalization.
Secondly, and more importantly, the main conclusions
of our work are qualitatively indpendent of the choice of
the model. This is because our calculation is based on the

p-5

Fung et al.
fact that the dynamics of a CANN is dominated by the After integrations, we have
motion mode of position shift of the network state, and
s
∞
X
this property is due to the translational invariance of the
n!!
an = 0.
(25)
neuronal recurrent interactions, rather than the inhibition
(n − 1)!!
n=1,odd
mechanism. We have formally proved that for a CANN
model, once the recurrent interactions are translationally Eq. (15) follows from combining with Eq. (14).
invariant, the interaction kernel has a unit eigenvalue with
Further details can be found in [27].
respect to the position shift mode no matter what the
inhibition mechanism is (we will report this result in a
REFERENCES
separate publication).
This work is partially supported by the Research Grant
[1] P. Dayan and L. Abbott, Theoretical Neuroscience: ComCouncil of Hong Kong (Grant No. HKUST 603606 and
putational and Mathematical Modelling of Neural SysHKUST 603607), BBSRC (BB/E017436/1) and the Royal
tems, (MIT Press, Cambridge MA, 2001).
Society.
Appendix: Mathematical Derivations. – The interaction kernel involved in Eq. (5), F (x, x′ ), can be derived as
(x−x′ )2
(x′ −z)2
2
F (x, x′ ) = √ e− 2a2 e− 4a2
a π
p
2
(x′ −z)2
1 + 1 − k/kc − (x−z)
√
e 4a2 e− 4a2 .
−
2πa

(19)

In the basis of {vn (x|z)}, the matrix elements Fmn are
defined by
Z Z
Fmn =
dxdx′ vm (x|z)F (x, x′ |z)vn (x′ |z).
(20)
By doing some mathematics, we have
F
mn p
m = n = 0,
1 − 1 − k/kc ,


q

n−m
2
(−1)
n!
=
, (n − m)/2 > 0, (21)
21−n m!
n−m

2 2 ( n−m

2 )!

0,
otherwise.

The result from Eq. (7) to Eq. (12) follows.
For Eq. (14), since

 1 dz
√
√
d
n + 1vn+1 (x|z) − nvn−1 (x|z)
vn (x|z) =
,
dt
2a dt
(22)
and
Z
ρ dx′ J(x, x′ )r(x′ |z) − U (x|z)
X X
X
≈
an
vm (x|z)Fmn −
an vn (x|z). (23)
τ

n

m

n

Substituting into Eq. (2) and collecting terms, we get
Eq. (14).
To derive Eq. (15), we take the first moments about the
centre of mass,
∞
X

an
√
n!2n
n=1,odd

Z

[2] J. Hopfield, Proc. Natl. Acad. Sci. USA, 79 2554 (1982).
[3] E. Domany, J. van Hemmen and K. Schulten eds., Models
of Neural Networks III, (Springer, Berlin, 1995).
[4] S. Amari, Biological Cybernetics 27, 77 (1977).
[5] R. Ben-Yishai, R. Lev Bar-Or and H. Sompolinsky, Proc.
Natl. Acad. Sci. USA, 92 3844 (1995).
[6] K.-C. Zhang, J. Neurosicence 16, 2112 (1996).
[7] B. Ermentrout, Reports on Progress in Physics 61, 353
(1998).
[8] S. Deneve, P. Latham and A. Pouget, Nature Neuroscience, 2, 740 (1999).
[9] D. Pinto and B. Ermentrout, SIAM J. Applied Maths 62,
206 (2001).
[10] C. Laing and C. Chow, Neural Computation 13, 1473
(2001).
[11] X. Xie, R. H. R. Hahnloser and S. Seung, Phys. Rev. E
66, 041902 (2002).
[12] S. Stringer, T. Trappenberg, E. Rolls and I. de Araujo,
Network: Comput. Neural Syst. 13, 217 (2002).
[13] A. Renart, P. Song and X. Wang, Neuron 38, 473 (2003).
[14] S. Wu and S. Amari, Neural Computation 17, 2215
(2005).
[15] S. Coombes and M. R. Owen, Phys. Rev. Lett. 94, 148102
(2005).
[16] B. Blumenfeld, S. Preminger, D. Sagi and M. Tsodyks,
Neuron 52, 383 (2006).
[17] S. Coombes, Scholarpedia 1, 1373 (2006).
[18] S. Wu, K. Hamaguchi and S. Amari, Neural Computation,
in press (2008).
[19] A. Samsonovich and B. L. McNaughton, J. Neurosci. 17,
5900 (1997).
[20] S. Folias and P. Bressloff, SIAM J. Appl. Dyn. Syst. 3,
378 (2004).
[21] J. Jastorff, Z. Kourtzi and M. Giese, J. Vision 6, 791
(2006).
[22] A. B. A. Graf, F. A. Wichmann, H. H. Bülthoff, and B.
Schölkopf, Neural Computation 18, 143 (2006).
[23] S. Grossberg, Neural Network 1, 17 (1988).
[24] D. Heeger, J. Neurophysiology 70, 1885 (1993).
[25] Y. Fu, Y. Shen and Y. Dan, J. Neuroscience 21, 1 (2001).
[26] W. Erlhagen, Biol. Cybern. 88, 409 (2003).
[27] C. C. A. Fung, K. Y. M. Wong, and S. Wu,
arXiv:0808.1930 (2008).





x
x2
= 0.
dx exp − 2 xHn √
4a
2a

(24)
p-6

