ON THE DYNAMICS OF RANDOM NEURONAL NETWORKS

arXiv:1410.4072v3 [math.PR] 16 Mar 2015

PHILIPPE ROBERT AND JONATHAN TOUBOUL

Abstract. We study the mean-field limit and stationary distributions of a
pulse-coupled network modeling the dynamics of a large neuronal assemblies.
Our model takes into account explicitly the intrinsic randomness of firing times,
contrasting with the classical integrate-and-fire model. The ergodicity properties of the Markov process associated to finite networks are investigated. We
derive the limit in distribution of the sample path of the state of a neuron
of the network when its size gets large. The invariant distributions of this
limiting stochastic process are analyzed as well as their stability properties.
We show that the system undergoes transitions as a function of the averaged
connectivity parameter, and can support trivial states (where the network activity dies out, which is also the unique stationary state of finite networks in
some cases) and self-sustained activity when connectivity level is sufficiently
large, both being possibly stable.

Work in progress - Preliminary version of March 17, 2015
Contents
1. Introduction
2. Biological background
3. Stochastic Model
4. Finite Networks
5. Analysis of the McKean Vlasov process
6. Mean-Field Asymptotics
7. Analysis of Invariant distributions
Appendix
References

1
5
6
8
14
17
25
35
39

1. Introduction
1.1. Leaky Integrate and Fire Neuron Models. We investigate a model of
neuronal network in which the state of a neuron i at time t is given by a jump process
describing the membrane potential of a cell. Transitions occur either when a cell
receives an action potential from another cell in the network, or when the cell fires
a spike and is reset to its resting potential. This event occurs randomly at a statedependent rate. The inspiration for the development of this model is the celebrated
leaky integrate-and-fire neuron (see biological motivation below in section 2). The
original integrate-and-fire model has been studied under the assumptions that the
membrane potential of each cell is noisy, and that spikes are fired as soon as a fixed
threshold VF is exceeded. It can be described as an hybrid ODE with jumps:
Date: March 17, 2015.
1

2

PH. ROBERT AND J. TOUBOUL

â€” if Xi (tâˆ’) < VF
dXi (t) = âˆ’Xi (t) dt +

X

wji (t) dDj (t) + dIie (t),

j6=i

â€” if Xi (tâˆ’) = VF
dDi (t) = 1, Xi (t) = VR ,
with f (tâˆ’) the left-limit of f (Â·) at t and Dj (t) the number of spikes of cell j up to
time t and wji (t) is the input of cell j sent to cell i when it fires at time t. The
quantity Iie (t) is the external input to cell i.
Caceres, Carrillo and Perthame in [6] have demonstrated the interesting property
that in the mean-field limit, the solutions blow-up in finite time. While this may
attributed to the instantaneity of spikes firings and their immediate effects on the
firing of other cells, it remains a non-trivial issue since this blow-up also occurs when
considering propagation delays [7], which prevents from avalanche phenomena and
instantaneous self-excitation.
It should be noted that in most of works of the literature the stochastic component of this dynamical system is generally represented by the external input
(Iie (t)), see Section 1.2. It is classically considered to be a Gaussian process describing the effect of the neurons belonging to other populations. In these models
the firing mechanism of a given neuron is a deterministic function of the input
received. Experimental data suggest nevertheless that the firing process has a significant random component. A number of theoretician and experimentalists have
discussed this issue [8, 26], and we briefly summarize their findings in section 2. All
in all, a more accurate description incorporates the intrinsic spike time variability
by considering that neurons fire according to a non-homogeneous Poisson process,
with a rate being an increasing function of their voltage.
The stochastic model investigated in this paper describes the occurrences of
spikes as follows. A cell with membrane potential x fires at rate b(x) where x 7â†’ b(x)
is a non-decreasing function. Note that the threshold model corresponds to the case
b(x)=0 if x<VF and b(x)=+âˆ otherwise. In this context, the analogue of the above
differential equations are given by
X
j
i
(1)
dXi (t) = âˆ’Xi (t) dt+
Wji (tâˆ’)Nb(X
(dt) âˆ’ Xi (tâˆ’)Nb(X
(dt)
i (tâˆ’))
j (tâˆ’))
j6=i

where
â€” Nyj (dt) denotes a Poisson point process with rate y â‰¥ 0,
â€” For s â‰¥ 0, (Wij (s)) is an i.i.d. sequence of non-negative integrable random
variables,
and f (tâˆ’) denotes the left limit of the function f at t. In particular, VR is set
to be 0, after a spike the state of a neuron is 0. A more formal formulation of
these equations is given in Section 3. Note that intrinsic randomness occurs in this
model through the firing mechanism with the Poisson processes but also through the
fluctuations of synaptic efficacy Wji (t) representing the effect of a spike of neuron
j on the membrane potential of neuron i at time t.
The quantity b(0) is the rate at which a neuron fires when its membrane potential
is at rest, i.e. when it is not driven by spikes from other neurons. It can be
interpreted as the firing rate of the cell due to external noise. In this way the
firing rate can be decomposed as b(x)=b(0)+[b(x)âˆ’b(0)], the sum of external and

RANDOM INTEGRATE-AND-FIRE NEURONS

3

intrinsic firing rates. In other words, our network incorporates input from cells not
belonging to the network considered as a Poisson Process with constant rate: this
is the classical interpretation behind the Gaussian input authors generally consider.
1.2. Brief overview of the literature in neural mean-field dynamics. As
we have already remarked, networks of integrate and fire neurons have been mainly
studied mathematically with the assumption that the randomness is essentially
contained in the external input (Iie (t)) and that it is a continuous process driven
by a Brownian motion. In this setting the classical tools of stochastic calculus, ItoÌ‚
Formula in particular for probabilistic approaches, can be used to investigate the
behaviour of these networks.
There are several works using PDEs, see Caceres et al. [6, 7], Pakdaman et
al. [23, 25, 24]. Recently, these were complemented by probabilistic approaches
Delarue et al. [13, 12] and Inglis and Talay [16] with Gaussian processes. At the
other end of the spectrum of mathematical models for neurosciences are the detailed
conductance-based models. These systems represent accurately the different ionic
exchanges at play during the firing of an action potential. This has been the subject
of a number of recent works in the domain, see [35, 36] in the context of spatially
extended networks. Although these models lead to complex equations, some simple
models for the firing rate of neurons, are tractable. For these networks, it should
be noted that there is an interesting behavior as a function of noise levels in a
mean-field context, see [34].
To the best of our knowledge mathematical models of neural networks using
Poisson processes to describe the occurrences of firing events are more rare compared to models with Brownian random components. The main reason is, probably,
the fact that the corresponding stochastic calculus is more awkward to deal with.
But there has been a renewed interest recently. De Masi et al. [11] investigated a
model with firing rates depending on the state of the cell but without the leaking
term of the integrate-and-fire model. They proved an hydrodynamic limit behavior
by using coupling methods on a discrete-time version of the process. The same
model has been investigated by Fournier and LoÌˆcherbach [21] where a quite general
mean-field convergence result is proved with an explicit upper bound on the rate of
convergence to the mean-field model. In this paper [21], appeared independently
at the same time as the first version of the present paper, the analysis of Section 3
and 4 parallel, with different methods, Sections 5 and 6 below.
1.3. Summary of the results. The main goal of the present paper is to investigate the stability properties of these networks whose firing mechanisms are driven
by Poisson processes. The state of the network is described in terms of the solutions of Equation (1). We first consider finite-sized networks and then the limit of
the system as the network size tends to infinity; in both case, we analyze of the
corresponding invariant distributions.
Finite Networks
It is shown that if there is an external noise, i.e. b(0)>0, then there exists a
unique non-trivial invariant distribution for the Markov process (Xi (t)). An explicit
representation of the invariant distribution is given when b(Â·) is a constant function.
When b(0)=0 it is shown that, almost surely no spike occurs after some time. The
network dies with probability 1 in this case, and the Dirac mass at 0 is the unique,
trivial, invariant distribution.

4

PH. ROBERT AND J. TOUBOUL

The Harris recurrence properties of the associated Markov process are proved in
Section 4. They rely on a regeneration argument and the invariant distribution is
obtained via a backward coupling argument.
Large Networks
For large size networks, the output of cell j to cell i when cell j fires at time t, the
variable (Wji (t)), is taken as (Vji (t)/N ) where (Vji (t)) are i.i.d. integrable random
variable so that Equation (1) of the evolution of (XiN (t)) becomes
1 X
j
i
(dt)
(dt)âˆ’XiN (tâˆ’)Nb(X
(2) dXiN (t) = âˆ’XiN (t) dt+
Vji (tâˆ’)Nb(X
N
N
i (tâˆ’))
j (tâˆ’))
N
j6=i

It is shown that provided that b(Â·) satisfies some growth condition and other technical conditions, a mean-field result holds. In particular, the distribution of the
sample path of the state of a cell of the network converges in distribution to the
solution of the non-linear stochastic differential equation, the associated McKeanVlasov process (Z(t))
(3)

dZ(t) = âˆ’Z(t) dt + E(V )E(b(Z(t))) dt âˆ’ Z(tâˆ’)Nb(Z(tâˆ’)) (dt).

One of the problems here is to cope with the fact that the function (b(x)) is not
necessarily Lipschitz like b(x) = Î»xÎ± with Î± > 1 which will be considered. This is
generally a source of trouble to get mean-field results. See Sznitman [33] page 176
and Scheutzow [31] for example. As it will be seen, a control of the sample paths of
the firing events is the main technical ingredient to overcome this problem. Another
difficulty is to deal with the discontinuities due to Poisson events and the associated
ItoÌ‚ calculus.
Invariant States
The last part of the paper is devoted to the properties of the invariant distributions
of Equation (3). When b(0)=0 the Dirac mass at 0, Î´0 , is always a possible invariant
distribution, its stability properties are investigated.
In the linear case, b(x)=Î»x, it is shown that if Î»E(V )<1 then Î´0 is the unique
invariant distribution but if Î»E(V )>1 there exists a non-trivial invariant distribution for (Z(t)). The existence of non-trivial distribution suggests a quasi-stationary
phenomenon. It is known that, in this case, the state of the network of finite size is
dying with probability 1 absorbed at 0. Nevertheless if Î»E(V )>1, starting from a
large initial state, the time before absorption at 0 is quite large. Before absorption,
the state of the network can be, for a while, in a local equilibrium.
More striking, it is shown that if b(x)=Î»xÎ± with Î±>1, then there exists some
Ïc >0 such that if Î»E(V )>Ïc then, in addition to the trivial invariant distribution,
there exist at least two non-trivial invariant distributions for (Z(t)). Simulations
presented at the end seem to suggest that one of them is stable but not the other
one and that Î´0 has a non-empty basin of attraction.
Outline of the paper
The paper is organized as follows. In section 3 we present our model and the main
results of the paper, before addressing in section 4 the properties of finite networks.
The following sections are devoted to the analysis of the limit as the system size
diverges: we analyze the properties of the McKean-Vlasov limit in section 5, and
show the mean-field convergence of the network towards this limit in section 6. We

RANDOM INTEGRATE-AND-FIRE NEURONS

5

conclude with the analysis of invariant distributions of the McKean-Vlasov equation
in section 6.
Acknowledgments
The authors are grateful to Nicolas Fournier for his remarks and for pointing out a
mistake in the proof of Theorem 3 in the first version of the paper.
2. Biological background
Neurons are intrinsically noisy electrically excitable cells, that transmit information through stereotyped electrical impulses, called action potentials, or spikes.
Spikes are transmitted through synapses to all connected neurons, which have the
effect of either increasing (excitatory) or decreasing (inhibitory) their membrane
potential. In the cortex, neurons tends to form large populations of statistically
identical cells, receiving the same input and strongly interconnected. These cortical
areas (or cortical columns) are of the order of a few millimeters and contain hundreds to hundred of thousands of neurons. In such structures, neurons fire spikes
when the difference of electrical potential between the intra- and extra-cellular domains is sufficiently large (hyperpolarization), therefore occurring in response to a
sufficient amount of excitatory spikes received by the neuron.
The spiking nature of the neuronal activity motivated the introduction of a simple heuristic model of single cell, the integrate-and-fire neuron, which makes the
assumption that the membrane potential linearly integrates the inputs it receives
and fires a spike as soon as a fixed voltage threshold is reached. This model, introduced in the beginning of last century by Louis Lapique [18], has given rise to
an extensive literature (see e.g. Burkitt [4, 5] for reviews). The introduction of
this model was instrumental in the advances made in the mathematical and computational understanding neuronsâ€™ activity. Though overly simplified, this model
has allowed to take advantage of the stereotyped, fast nature of spikes to focus
on the important problem of spike timings. Moreover, this model, for its relative
simplicity, has allowed interesting mathematical developments and yielded better
understanding of isolated cells, see Brunel [2].
In biological conditions, it has been observed that neurons display a noisy activity. The first model integrating randomness is due to Gerstein and Mandelbrot [14]
who incorporated to the model random spike arrivals as a random walk. This model
did not considered the intrinsic nature of the nerve cells firing times, but rather
considered that this randomness was due to incoming spikes from cells outside of
the network considered, i.e. disregarded the fact that isolated cells actually display
an intrinsically noisy activity, i.e. fire irregularly even when disconnected from their
network. This seminal model, interesting in many regards, lead to many developments. In particular, diffusion limits of the incoming spike strain using stochastic
differential equations, see Knight [17] and Stein [32] lead to the introduction of the
celebrated leaky integrate-and-fire model. This type of model has been paramount
in the study of noisy integrate-and-fire since then and lead to develop new technique
to analyze their singular behavior. See e.g. Brunel [2, 3], Delarue et al. [12] and
Caceres et al. [6] to cite a few.
At the level of one cell, classical models assume random arrivals of spikes. In
a diffusion limit, it is represented as an external additive Brownian noise. In the
context of a large set of cells, randomness of spike times due to intrinsic variability
is nevertheless an important phenomenon which cannot be ignored, see e.g. Rolls

6

PH. ROBERT AND J. TOUBOUL

and Deco [30]. Another drawback of the classical noisy integrate-and-fire neuron is
the presence of a fixed threshold. This feature, abstraction of the actual dynamics
of neurons, induces a number of artifacts, among which the so-called avalanche
effects. It occurs in excitatory networks, corresponding to an explosion of the spike
rate: all neurons fire instantaneously at the same time inducing again all neurons to
fire again. And these phenomena do occur, generically, in the mathematical models
of integrate-and-fire networks, as shown in Caceres et al. [6]. All this context
points to the fact that the integrate-and-fire neuron model, interesting in order to
understand single isolated cells in a non-noisy context, may not be the best model
when considering the behavior of very large networks of noisy neurons as appearing
in physiological conditions in the brain.
Accounting for the intrinsic randomness of firing times has been a longstanding
issue in computational neurosciences. The fact that nerve cells integrate the input
received and fire spikes at random times with an intensity depending on their membrane potential. Instead of continuous Gaussian processes, a natural model is to
consider that the membrane potential is driven by inhomogeneous Poisson jumps
whose intensity is a function of the voltage of the cells. This model has been introduced one century later than the integrate-and-fire model in Chichilnisky [8] and is
called linear-nonlinear Poisson model. This model seems particularly well suited in
order to represent the neuronal firing, and displayed a good fit with experimental
data, allowing precise prediction of spike trains Pillow et al. [26, 27]. The present
study use this model as the building block of our networks.
The model we shall study in the present paper has the interest of conserving the
discrete nature of spikes, allowing to characterize the statistics of spike trains in the
limit of large networks. Moreover, taking into account intrinsic noise, beyond its
biological interest, ensures well-posedness of the system, allowing to describe the
limit without resorting to additional processes. Eventually, an interesting property is that the dynamics of the limit equation can be partially characterized, and
stationary solutions can be described. As it will be seen, depending on the sharpness of the spiking intensity function as well as the average coupling strength,
self-sustained spontaneous activity may arises in infinite networks. This original
phenomenon opens interesting questions on these models.
3. Stochastic Model
We consider a network composed of N neurons, whose state is described by a
scalar variable representing its membrane potential XiN (t) (i.e., the difference of
electric potential between the intra- and extra-cellular domains). This quantity
decays exponentially fast towards zero in the absence of input due to the leak
currents and ions flowing across the cellular membrane, leading the voltage to its
equilibrium value, assumed here to be 0. The timescale of this process is our time
unit, i.e., for 1 â‰¤ i â‰¤ N , if the neuron does not spike and does not receive any
spike in the interval of time [T, T 0 ], the membrane potential of neuron i satisfies
the ordinary differential equation:
dXiN (t)
= âˆ’XiN (t).
dt
Neurons fire at random times, according to a voltage-dependent Poisson process
with rate b(x) where x denotes the voltage of the neuron. After spiking, the neuronâ€™s
voltage is instantaneously reset to its rest potential Xi (t) = 0, and the voltage of

RANDOM INTEGRATE-AND-FIRE NEURONS

7

neurons j 6= i are instantaneously updated: their voltage is added the synaptic
coefficient Wij , which are considered to be i.i.d. random variables with law F :
Xj (t) = Xj (tâˆ’) + Wij .
For simplicity, it is assumed that the random variables (Wij , j 6= i) are positive
i.i.d. and integrable and that their distribution does not depend on i. In this way,
it is easily seen that the process (X(t)) is Markov process. If x = (xi ) âˆˆ RN
+ , kxk
denotes the l1 norm: kxk = |x1 | + Â· Â· Â· + |xN |.
Evolution Equations. An equivalent description of (X(t)) can be provided in
terms of the solution of the following Stochastic Differential Equation (SDE),
XZ
(4) dXi (t) = âˆ’Xi (t) dt +
zi 1{0â‰¤uâ‰¤b(Xj (tâˆ’))} Nj (du, dz, dt)
R2+

j6=i

âˆ’

XiN (tâˆ’)

Z
R2+

1{0â‰¤uâ‰¤b(Xj (tâˆ’))} Nj (du, dz, dt),

where (Nj ) are independent Poisson processes, for 1 â‰¤ i â‰¤ N , Ni has the intensity
measure given by du âŠ— Wi (dz) âŠ— dt where
N
Wi (dz) = âŠ—iâˆ’1
j=1 W (dzj ) âŠ— Î´0 (dzi ) âŠ—j=i+1 W (dzj )

is the measure corresponding to the result of the emission of a spike by neuron i
on the voltage of all neurons. In the latter expression, Î´0 is the Dirac distribution
at 0 and W (dx) is the common distribution of the random variable (Wij , j 6= i) on
R+ associated with the amount of excitation received by a neuron after a spike of
another neuron.
Equivalently, it can be written as
Z t
XZ t
(5) Xi (t) = âˆ’
Xi (s) ds + E(W1 )
b(Xj (s)) ds
0

j6=i

0
t

Z

XiN (s)b(Xj (s)) ds + Mi (t),

âˆ’
0

where
(6)

Mi (t)=

XZ
j6=i

t

s=0

Z

b(Xj (sâˆ’))

u=0

Z

+âˆ


zi Nj (du, dz, ds)âˆ’E(W1 ) du ds

z=0

Z

t

âˆ’

Z

b(Xi (sâˆ’))

Z

+âˆ

Xi (sâˆ’)
s=0

u=0


Ni (du, dz, ds)âˆ’ du ds ,

z=0

is the associated local martingale. See Proposition 13 of the appendix and Rogers
and Williams [29] for example.
Extinction properties of the network. From the biological viewpoint, it is
natural to assume that x 7â†’ b(x) is a non-decreasing positive function since the
higher the potential, the more likely a spike will occur. If the initial state of a neuron
is x and if no spike occurs in the network (no neuron fires) in the time interval [0, t],
its state at time t is equal to x exp(âˆ’t), and in particular its instantaneous firing
rate at this time, b(x exp(âˆ’t)) decreases to b(0) if t diverges. If this later quantity
is 0, it may happen that the neuron will not spike with positive probability. In this
case, if the components xi (0), 1 â‰¤ i â‰¤ N of the initial value of the state of the

8

PH. ROBERT AND J. TOUBOUL

network are too small, there would be an event of positive probability for which
no spike occurs at all. The following lemma provides a sufficient condition on the
behavior of the map b(x) at 0 under which extinction of the network activity does
not occur. Theorem 1 below completes this result.
Lemma 1. If the condition
Z
(7)
[0,1]

b(s)
ds = +âˆ
s

holds, then a node with a non-zero initial value spikes with probability 1.
Proof. The function x 7â†’ b(x) being non-decreasing, Relation (7) also holds when
[0, 1] is replaced by [0, a] with a > 0. Let 1 â‰¤ i â‰¤ N and Xi (0) = x > 0 then, if Ï„i
denote the instant (possibly infinite) of the first spike of neuron i, one has

 Z t

P(Ï„i > t) = E exp âˆ’
b(Xi (s) ds
0

 Z t

 Z x

b(s)
ds
â‰¤ exp âˆ’
b xeâˆ’s ds = exp âˆ’
0
xeâˆ’t s
since the relation Xi (t) â‰¥ x exp(âˆ’t) holds for t â‰¤ Ï„ (the other neurons may only
increase the state of neuron i) and that the function x 7â†’ b(x) is non-decreasing.
By letting t go to infinity, one gets that P(Ï„i = +âˆ) = 0 by Condition (7).

Condition (7) together with the monotonicity and a convenient regularity property imply in fact that b(0) is positive. The quantity b(0) is the firing rate of a
neuron with a flat potential, it can be see as a representation of the external noise.
We now investigate the stability of the Markov process (Xi (t)). In the absence
of spikes, each of the components decreases exponentially to 0 and it is reset to
0 when the corresponding node fires. The ergodicity property seems to be quite
likely provided that it is proved that the nodes do not fire too quickly as in the
PDE description of Caceres et al. [6]. The analysis of these properties for finite-size
networks are now investigated in order to ensure that these properties hold.
4. Finite Networks
In this section the number of neurons will be kept fixed, so we drop the upper
index N throughout the section for simplicity of the notations.
4.1. Recurrence, Ergodicity and Invariant Measures. We start with a technical result related to an estimation of the mean return time in a specific compact
set.
Proposition 1. There exists C0 such that if

	
T0 = inf u > 0 : X(u) âˆˆ [0, C0 ]N
then, for X(0) = x = (xi ) 6âˆˆ [0, C0 ]N ,
Ex (T0 ) â‰¤ kxk = x1 + Â· Â· Â· + xN .

(8)
Proof. Let
(
F =

xâˆˆ

RN
+

:

N
X
i=1

xi [1 + b(xi )] â‰¤ (N âˆ’1)E(W1 )

N
X
i=1

)
b(xi ) + 1

RANDOM INTEGRATE-AND-FIRE NEURONS

9

due to the monotonicity property of the function x 7â†’ b(x), F is a compact subset
of [0, C0 ]N , with C0 = N 2 E(W1 ) + 1. Denote
TF = inf {u > 0 : X(u) âˆˆ F } ,
then clearly T0 â‰¤ TF .
If X(0) = x 6âˆˆ F and t â‰¥ 0, define S(t)=kX(t)k=X1 (t) + X2 (t) + Â· Â· Â· + XN (t),
then Relation (5) gives the identity
Z t"
N
X
S(t) = S(0) +
(N âˆ’1)E(W1 )
b(Xj (u))
0

i=1

âˆ’

N
X

#
Xi (u)(1 + b(Xi (u))) du +

i=1

X

Mi (t),

i=1

where (Mi (t)) are the local martingales defined by Equation (6).
Assume that X(0) 6âˆˆ F , since TF is a stopping time, one has
0 â‰¤ E(S(TF âˆ§ t)) = S(0)
# !
Z tâˆ§TF "
N
N
X
X
+E
(N âˆ’1)E(W1 )
b(Xj (u))âˆ’
Xi (u)[1+b(Xi (u))] du
0

i=1

i=1

â‰¤ S(0) âˆ’ E(t âˆ§ TF ).
One gets E(t âˆ§ TF ) â‰¤ S(0) and consequently the desired relation.



To state the stability properties of the Markov process (X(t)), the framework of
Harris Markov processes now is used. See Nummelin [22] and Asmussen [1] for a
general introduction.
Theorem 1 (Stability).
(1) If the condition
Z
(9)
0

1

b(s)
ds < +âˆ,
s

holds then, almost surely, no spike occurs after some finite time, in particular
lim (Xi (t), 1 â‰¤ i â‰¤ N ) = 0
tâ†’+âˆ

and the Dirac mass at 0 is the unique invariant distribution.
(2) If b(0) > 0 and if there exists K > 0 such that W1 â‰¤ K a.s., then the
Markov process (Xi (t), 1 â‰¤ i â‰¤ N ) is Harris ergodic.
Proof. Assume that Condition (9) holds. The notations of Proposition 1 are used.
N
Let X(0) = x âˆˆ RN
+ , x 6= 0. If X(0) = x âˆˆ [0, C0 ] , in the proof of Proposition 1 it
has been seen that if Ï„i is the first time neuron i spikes, 1 â‰¤ i â‰¤ N , then in absence
of spikes of the other nodes,
!
 Z xi

Z C0
b(s)
b(s)
def.
P(Ï„i = +âˆ) = exp âˆ’
ds â‰¥ Î· = exp âˆ’
ds > 0.
s
s
0
0
Consequently if X(0) âˆˆ [0, C0 ]N there is a positive probability lower-bounded by
Î· N that none of the nodes spike. From Proposition 1 one gets that if (X(t)) leaves

10

PH. ROBERT AND J. TOUBOUL

F then it returns with probability 1, hence almost surely the process (X(t)) will
stop having upward jumps after some time, (1) is proved.
Now it is assumed that b(0) > 0. The strategy of the proof is as follows, in
absence of spikes of other nodes, the duration of time for the next spike of a node
with value y > 0 can be decomposed as the minimum of two random variables
with one of them not depending of y. This property provides a way of having a
regeneration mechanism (i.e. forgetting the value y). If all nodes proceed along the
same line, then the initial value of the Markov process is forgotten after some time
on some event of positive probability, which gives the key regenerative structure of
a Harris Markov process.
If X(0) = x = (xi ), one denotes by E1i an exponential random variable with
parameter b(0) and E1xi ,i a random distribution such that
 Z t




P(E1xi ,i â‰¥ t) = exp âˆ’
b xi eâˆ’u âˆ’ b(0) du .
0

Hence, starting from the initial state x, if no other spike occurs before, the first
instant when node i spikes has the same distribution as E1i âˆ§ E1xi ,i . The variable E1
is independent of the variables (E1x,i , x > 0) which can be chosen so that E1x,i â‰¤ E1y,i
for x â‰¥ y.
For n â‰¥ 1, one denotes by Yn = (yi,n ) the state of the Markov process (X(t))
just after the nth jump/spike. The sequence (Yn ) is the embedded Markov chain,
for n â‰¥ 1 Yn is the state of (X(t)) at the instant of the nth jump. Let f be some
non-negative Borelian function on RN
+ . If X(0) = x âˆˆ F , then, since xi â‰¤ C0 ,


Ex (f (Y1 )) â‰¥ E f (Y1 ) 1{E 1 â‰¤mini (E i âˆ§E xi ,i )} â‰¥ E (f (Y1 ) 1A1 ) ,
1

where


A1 =

1

1

E11 â‰¤ min E1i âˆ§ E1C0 ,i



1â‰¤iâ‰¤N

and, on A1 ,
1

1

Y1 = (y1,1 , y2,1 , . . . , yN,1 ) = (0, x2 eâˆ’E1 +W1,2 , . . . , xN eâˆ’E1 +W1,N ).
This inequality is associated to the event that node 1 spikes first on its â€œb(0)componentâ€ E11 . As a result the lower bound of the above relation does not depend
on x1 anymore. We proceed in the same way for the second step, with node 2
spiking this time, and since yi,1 â‰¤ C0 + K for 1 â‰¤ i â‰¤ N ,
(10)

Ex (f (Y2 )) â‰¥ E (f (Y2 ) 1A1 âˆ©A2 ) ,

with


A2 =

E22

â‰¤ min

1â‰¤iâ‰¤N

E2i

âˆ§

E2C0 +K,i



and
2

2

Y2 = (W2,1 , 0, y3,1 eâˆ’E2 +W2,3 , . . . , yN,1 eâˆ’E2 +W2,N ).
on the event A1 âˆ© A2 . This time the lower bound (10) does not depend on x1 and
x2 . We can proceed recursively, and finally get the relation
Ex (f (YN )) â‰¥ E(f (Z)1A ) ,

âˆ€x âˆˆ F,

where the random variable Z and the set A do not depend on x âˆˆ F and that
P (A) > 0. Consequently F is a regeneration set of the Markov chain (Yn ), see

RANDOM INTEGRATE-AND-FIRE NEURONS

11

Asmussen [1, page 198] for example. The Harris property of the Markov chain has
been established. To prove the ergodicity, it is enough to prove that if

	
T0+ = inf u > 0 : X(u) âˆˆ [0, C0 ]N and âˆƒv â‰¤ u, X(v) 6âˆˆ [0, C0 ]N ,
is the first return time to F after an exit, then
sup Ex (T0+ ) < +âˆ.

xâˆˆF

See, for example, Asmussen [1, Theorem 3.2, page 200] and Robert [28, Proposition 8.12, page 221]. For x âˆˆ F , the first time (X(t)) is in y = (yi ) outside F ,
necessarily yi â‰¤ C0 + K, consequently, by the strong Markov property,
Ex (T0+ ) â‰¤

Ey (T0 ) â‰¤ C0 + K,

sup
y=(yi )6âˆˆF,
maxi yi â‰¤C0 +K

by Proposition 1. The theorem is proved.



4.2. The State-Independent Process. The case when the firing rate function
b is constant is investigated. In this setting, the neurons spike independently of
their state. This is one of the very rare cases where one can get some substantial
information on the distribution of the equilibrium of the network.
Proposition 2. If the firing rate is constant and equal to Î» > 0, then the invariant
distribution of the Markov process (X(t)) is the law of the vector (X1 , . . . , XN ) with
XX j
(11)
Xi =
Wik eâˆ’tjk 1{tjk â‰¤ti1 } , 1 â‰¤ i â‰¤ N,
j6=i kâ‰¥1

where, for 1 â‰¤ j â‰¤ N , (tjk , k â‰¥ 1) are N i.i.d. Poisson point processes on R+ with
j
rate Î» and, for i, k âˆˆ N, the random variables (Wik
, 1 â‰¤ j â‰¤ N ) are i.i.d. with the
same distribution as W1 .
Proof. The proof relies on a backward coupling argument, see Levin et al. [19] for
a general presentation of so called coupling from the past methods and Loynes [20]
for one of its early uses. Let (Nj , 1 â‰¤ j â‰¤ N ) be N i.i.d. Poisson Processes on R
with rate Î», for 1 â‰¤ j â‰¤ N , Nj is the sequence of instants when the jth node spikes.
Note that the time interval considered is (âˆ’âˆ, +âˆ). Assume that for some fixed
T > 0, Xi (âˆ’T ) = 0 for all 1 â‰¤ j â‰¤ N , then, by using the invariance properties of
Poisson processes, it is not difficult to see that (Xi (0), 1 â‰¤ i â‰¤ N ) has the same
distribution of the state of the network at time T when it starts empty at time 0.
For 1 â‰¤ i â‰¤ N , if ti,âˆ’1 is the last instant of spike of node i before time 0, then
if âˆ’T â‰¤ ti,âˆ’1 state of this node at time 0 is determined by the spikes of the other
nodes after time ti,âˆ’1 . If node j 6= i spikes at time s, ti,âˆ’1 â‰¤ s â‰¤ 0, then the
contribution at time 0 for node i is the value of the spike multiplied by exp(s).
Consequently, if T is sufficiently large the value of X(0) does not depend on T and
its distribution is the law of the vector given by Relation (11). The proposition is
proved.

Proposition 3. When the firing rate is constant and equal to Î», the Laplace transform of the state of a node at equilibrium is given by, for Î¾ â‰¥ 0,


Z +âˆ
Z x


âˆ’Î¾X1
âˆ’u
f
E e
=
exp âˆ’Î»(N âˆ’ 1)
1 âˆ’ W Î¾e
du Î»eâˆ’Î»x dx.
0

0

f (Î¾) = E(exp(âˆ’Î¾W )) is the Laplace transform of W at Î¾.
where W

12

PH. ROBERT AND J. TOUBOUL

This formula gives P(X1 =0)=1/N , which is simply the probability that the node
is the last one which spiked. Similarly, the expected value at equilibrium is given
by E(X1 ) = (N âˆ’ 1)E(W )Î»/(Î» + 1)
Proof. With the same notations as before, M = N2 + Â· Â· Â· + NN = (sn ) is a Poisson
process with rate Î»(N âˆ’ 1), and the above proposition gives that
ï£¶ï£¶
ï£«
ï£«
X

Wn eâˆ’sn 1{sn â‰¤t11 } ï£¸ï£¸
E eâˆ’Î¾X1 = E ï£­exp ï£­âˆ’Î¾
nâ‰¥1

ï£¶ï£¶
ï£¶


= E ï£­E ï£­exp ï£­âˆ’Î¾
Wn eâˆ’sn 1{sn â‰¤t11 } ï£¸ (sn ), t11 ï£¸ï£¸

nâ‰¥1
ï£« ï£«

ï£«

X

where (W1 (s), s âˆˆ M) are i.i.d. with the same distribution as W . Consequently,
one obtains that
ï£«
ï£¶ï£¶
Y


E exp âˆ’Î¾W1 eâˆ’sn  sn , t11 ï£¸ï£¸
E eâˆ’Î¾X1 = E ï£­
sn â‰¤t11

ï£¶

ï£«
Y

= Eï£­

f Î¾e
W


âˆ’sn


 Z
ï£¸ = E exp âˆ’

t11


g(u) M(du)

0

sn â‰¤t11

with


def.
f Î¾eâˆ’u .
g(u) = âˆ’ log W
This gives the relation
E e

âˆ’Î¾X1



Z
=
0

+âˆ



 Z
E exp âˆ’

x


g(u) M(du)
Î»eâˆ’Î»x dx,

0

since t11 is exponentially distributed with parameter Î».
The point process M being Poisson with rate Î»(N âˆ’ 1), from a classical formula
for its Laplace transform, see Proposition 1.5 of Robert [28] for example, one gets

 Z x


Z x
 
âˆ’g(u)
E exp âˆ’
g(u) M(du)
= exp âˆ’Î»(N âˆ’ 1)
1âˆ’e
du .
0

0

The Laplace transform of X1 can thus be expressed as


Z +âˆ
Z x


âˆ’Î¾X1
âˆ’u
f
E e
=
exp âˆ’Î»(N âˆ’ 1)
1 âˆ’ W Î¾e
du Î»eâˆ’Î»x dx.
0

The proposition is proved.

0



One concludes with a limiting regime which will be analyzed in a more general
framework in the following. With little effort, it gives an idea of the results which
can be obtained when the size of the network gets large, for example that the
states of the nodes become independent in the limit. For this limiting regime, the
size N of the network goes to infinity and the rescaling is achieved through the
values of spikes which are of the order of 1/N . The proposition shows in fact the
mean-field convergence of the invariant distribution of the state of the network. See
Sznitman [33] for an introduction on this topic.

RANDOM INTEGRATE-AND-FIRE NEURONS

13

Proposition 4 (A large network at equilibrium for constant firing rate). When
the firing rate is constant and equal to Î» and the value of a spike is V1 /N for some
integrable random variable V1 and if (XiN ) is the vector whose distribution is the
equilibrium distribution of the state of the network then
(1) the sequence of random variables (X1N ) converges in distribution to a random variable X1âˆ whose distribution has the density
1
E(V1 )


1âˆ’

u
Î»E(V1 )

Î»âˆ’1
for u âˆˆ [0, Î»E(V1 )].

(2) For fixed i and j, 1 â‰¤ i < j â‰¤ N , the random variables XiN and XjN are
asymptotically independent when N gets large.
Proof. By symmetry, one can take i = 1 and j = 2, for ` = 1, 2, if
def

Y`N =

N X
X
Vj

ik âˆ’tjk

j=3 kâ‰¥1

N

e

1{tjk â‰¤t`1 } , 1 â‰¤ i â‰¤ N,

then, as N gets large, the distribution of (X1N , X2N ) is arbitrarily close to the
distribution of (Y1N , Y2N ), just because the contribution of the spikes of node 1 to
the state of node 2 are of the order of 1/N and vice versa. For Î¾1 , Î¾2 â‰¥ 0, by using
the same method as in the proof of the above proposition, one gets that


N
N
E eâˆ’Î¾1 Y1 âˆ’Î¾2 Y2 =
 
 



Z +âˆ 
Î¾2 âˆ’u
Î¾1 âˆ’u
e
e
e 1{uâ‰¤t11 } V
e 1{uâ‰¤t21 }
du ,
E exp âˆ’Î»(N âˆ’2)
1âˆ’V
N
N
0
where Ve denotes the Laplace transform of V1 . The equivalence 1 âˆ’ Ve (x) âˆ¼ xE(V1 )
when x goes to 0 gives the relation


N
N
lim E eâˆ’Î¾1 Y1 âˆ’Î¾2 Y2 = H(Î¾1 )H(Î¾2 ),
N â†’+âˆ

with


H(Î¾) = E exp âˆ’Î¾Î»E(V1 ) 1 âˆ’ eâˆ’t1
.
This gives the asymptotic independence and the identification of the limit. The
proposition is proved.

The analysis of the constant firing-rate model is instructive in many regards: it
provides a completely solvable model for which no explosion of the firing rate is
found. By comparison with the constant firing-rate case, coupling methods may
allow to show that there is no explosion of the total firing-rate in the network
(see [11, Appendix A.1]), i.e. that the probability of occurrence of a large number
of spikes in a fixed interval is small. We will come back to this property in the
forthcoming section. Let us just state that this is an important from the biological
viewpoint: consistently with the actual firing of neurons and in contrast with what
happens for the stochastic integrate-and-fire neuron, there is no explosion of the
firing rate and non-explosion of the membrane potential.

14

PH. ROBERT AND J. TOUBOUL

5. Analysis of the McKean Vlasov process
We shall prove in particular in section 6 that if the i.i.d. sequence (Wij ) has the
same distribution as (Vij /N ) where (Vij ) are i.i.d. with the same distribution as
V , then (X1N (t)) converges in law towards the distribution of the stochastic process
(X(t)) such that, for all t â‰¥ 0, b(X(t)) is integrable and it satisfies the SDE:
(12)



dX(t) = E(V )E(b(X(t))) âˆ’ X(t) dt
Z
âˆ’ X(tâˆ’)

1{0â‰¤uâ‰¤b(X(tâˆ’))} N (du, dt, dz),

The object of this section is to show that this McKean-Vlasov equation defines
a unique process. Section 7 will characterize its stationary solutions. Throughout
this section, we denote for (U (t)) a locally bounded process and T > 0 the quantity
kU kT defined as
kU kT = sup{|U (t)|, 0 â‰¤ t â‰¤ T }.
Lemma 2. If b is a non-decreasing C 1 -function on R+ and P is a Poisson process
on R2+ with rate 1, for any non-negative locally bounded Borelian function (u(t))
there exists a unique solution (Zu (x, t)) of the SDE
(13)

dZu (x, t) = âˆ’Zu (x, t) dt + u(t) dt âˆ’ Zu (x, tâˆ’)P([0, b(Zu (x, tâˆ’))], dt)

with initial condition x > 0. For any couple of non-negative locally bounded Borelian
functions u and v on R+ , for t â‰¤ T , the relation
E (kZu âˆ’ Zv kt ) â‰¤ eDT t

(14)

Z

t

ku âˆ’ vks ds
0

holds with DT = 1 + (x + T kukT )(1 + kb0 kx+T ku+vkT ).
Proof. For a non-negative Borelian function (u(t)), the existence and uniqueness of
a solution to the SDE (13) is straightforward. Let u and v be non-negative locally
bounded Borelian functions on R+ . Note that, almost surely,
Z
kZu kT â‰¤ x +

T

u(s) ds â‰¤ x + T kukT .
0

One has to estimate, for 0 â‰¤ t â‰¤ T ,
Z t

Z t

def. 
âˆ†(t) = 
Zu (sâˆ’)P([0, b(Zu (sâˆ’))], ds) âˆ’
Zv (sâˆ’)P([0, b(Zv (sâˆ’))], ds)
0

0

then
Z
âˆ†(t) â‰¤
0

t

|Zu (sâˆ’) âˆ’ Zv (sâˆ’)| P([0, b(Zu (sâˆ’))], ds)
Z t




Zv (uâˆ’)P([b(Zu (sâˆ’)), b(Zv (sâˆ’))], ds)
+
0

RANDOM INTEGRATE-AND-FIRE NEURONS

15

hence
t


kZu âˆ’ Zv ks P([0, b(Zu (sâˆ’))] ds)
0

Z t


Zv (uâˆ’)P([b(Zu (sâˆ’)) b(Zv (sâˆ’))] ds)
+ E 
0
Z t
â‰¤ b(x + T kukT )
E(kZu âˆ’ Zv ks ) ds
0
Z t
+ (x + T kukT )
E(|b(Zv (s)) âˆ’ b(Zu (s))|) ds.
Z

E(kâˆ†kT ) â‰¤ E

0

The SDE associated to u and v give, for 0 â‰¤ t â‰¤ T and a convenient constant DT ,
Z t
Z t
E(kZu âˆ’ Zv kt ) â‰¤ DT
E(kZu âˆ’ Zv ks ) ds +
ku âˆ’ vks ds,
0

0

with DT as defined above. Gronwallâ€™s Lemma completes the proof of the lemma. 
Lemma 3 (Integrability). If b is a non-decreasing unbounded C 1 function such
that there exist Î³ > 0 with 3Î³E(V ) < 1 and c > 0 and, for all x â‰¥ 0Â¡
b0 (x) â‰¤ Î³b(x) + c
and, for any 0 â‰¤ C â‰¤ +âˆ, if (Z C (t)) is a solution of the following SDE


(15) dZ C (t) = E(V )[C âˆ§ E(b(Z C (t)))] âˆ’ Z C (t) dt
Z
âˆ’ Z C (tâˆ’) 1{0â‰¤uâ‰¤b(Z C (tâˆ’))} N (du, dt, dz),
with an initial condition Z(0) independent of C such that Z(0) and b3 (Z(0)) are
integrable then, for p âˆˆ {1, 2, 3},

p 
< +âˆ.
sup E b Z C (t)
sup E(Z C (t)) < +âˆ and
tâ‰¥0
0â‰¤Câ‰¤+âˆ

tâ‰¥0
0â‰¤Câ‰¤+âˆ

Proof. For a fixed t, it is easily seen that the non-negative random variable Z C (t)
is integrable. Define Âµ(t) = E(Z C (t)). From Equation (15), one gets
Z th
i
Âµ(t) â‰¤ Âµ(0) +
âˆ’ Âµ(s) + E(V )E(b(Z C (s))) âˆ’ E(Z C (s)b(Z C (s))) ds
0
Z t
= Âµ(0) +
âˆ’Âµ(s) + E(Î¦(Z C (s))) ds.
0

with Î¦(x) = (E(V ) âˆ’ x)b(x). The equivalence Î¦(x) âˆ¼ âˆ’xb(x) as x gets large and
Gronwallâ€™s lemma give the boundedness of the first moment. The proof of lemma 8
in the Appendix shows that, there exists some Îµ > 0 such that, for 1 â‰¤ p â‰¤ 3 + Îµ,
the derivative of bp is also upper-bounded by Î³1 bp (x) + c with Î³1 < 1/E(V ). It
is thus enough to prove the boundedness of the first moment B(t) = E(b(Z C (t))
assuming Î³ < 1/E(V ). Equation (15) gives
Z th
B(t) â‰¤ B0 +
E(b0 (Z C (s))(âˆ’Z C (s) + E(V )B(s))
0
i
+ (b(0) âˆ’ b(Z C (s)))b(Z C (s))) ds.

16

PH. ROBERT AND J. TOUBOUL

and by Cauchy-Schwarzâ€™ inequality using the fact that Î³E(V ) < 1,
Z t h
i
E âˆ’b0 (Z C (s))Z C (s)+(b(0) âˆ’ b(Z C (s)))b(Z C (s))
B(t) â‰¤ B(0) +
0

+E(b0 (Z C (s)))E(V )B(s) ds
Z t
â‰¤ B(0) +
E((b(0) âˆ’ b(Z C (s)) + Î³E(V ))b(Z C (s)) + Î³E(V )b(Z C (s))2 ) ds
0
Z t
â‰¤ B(0) +
(b(0) + E(V )Î³)B(s) + (Î³E(V ) âˆ’ 1)B(s)2 ) ds.
0

We conclude using lemma 12, clearly the associated upper bound for B(t)) does
not depend on C.

We can now state our main result on the existence and uniqueness of a solution
to mean-field equations (12).
Theorem 2 (Existence and Uniqueness of the McKean-Vlasov Process). If b is a
non-decreasing unbounded C 1 function and if there exist Î³ â‰¥ 0 with 3Î³E(V ) < 1
and c > 0 such that, for all x â‰¥ 0,
b0 (x) â‰¤ Î³b(x) + c,
then for any T > 0, there exists a unique caÌ€dlaÌ€g process (Z(t)) satisfying the
stochastic differential equation
(16)

dZ(t) = âˆ’Z(t) dt + E(V )E(b(Z(t))) dt âˆ’ Z(tâˆ’)P([0, b(Z(tâˆ’))], dt),

and with initial condition x > 0.
Proof. For C > 0 we define by induction the sequence of processes (Zn (t)) by
Z0 (t) = x exp(âˆ’t) and for t > 0 and n â‰¥ 1,


dZn (t) = âˆ’Zn (t) dt + E(V ) E(b(Znâˆ’1 (t))) âˆ§ C dt âˆ’ Zn (tâˆ’)P([0, b(Zn (tâˆ’))], dt),
with Zn (0) = x. It is easy to show that we have:
Zn (t) â‰¤ xeâˆ’t + C(1 âˆ’ eâˆ’t ) â‰¤ x + C.
For T > 0, Lemma 2 and the above relation show that there exists a constant DT
independent of n such for 0 â‰¤ t â‰¤ T ,
Z t
E (kZn+1 âˆ’ Zn kt ) â‰¤ DT
kun âˆ’ unâˆ’1 ks ds,
0

with un (t) = E(b(Zn (t))). This implies that:
Z t
E (kZn+1 âˆ’ Zn kt ) â‰¤ DT
E (kb(Zn ) âˆ’ b(Znâˆ’1 )ks ) ds,
0

and thanks to the deterministic bound on the sequence of processes (Zn ), we have:
Z t
E (kZn+1 âˆ’ Zn kt ) â‰¤ KT
E (kZn âˆ’ Znâˆ’1 ks ) ds,
0

with KT = DT kb0 kx+C .
We hence have:
E (kZn+1 âˆ’ Zn kt ) â‰¤

(KT t)n
kz(x, Â·)kT
n!

RANDOM INTEGRATE-AND-FIRE NEURONS

17

From this relation one gets that 1) the sequence of processes (ZN (t), 0 â‰¤ t â‰¤ T ) is
converging almost surely uniformly on compact sets to a solution (Z(t), 0 â‰¤ t â‰¤ T )
e
of SDE (16). If (Z(t),
0 â‰¤ t â‰¤ T ) is another solution of this SDE starting from x,
e
then necessarily Z(t) â‰¤ z(t) for all 0 â‰¤ t < T , consequently the relation
Z t 



e
e s ds,
E kZn+1 âˆ’ Zkt â‰¤ KT
E kZn âˆ’ Zk
0

holds, hence
Z t 



e t â‰¤ KT
e s ds,
E kZ âˆ’ Zk
E kZ âˆ’ Zk
0

so Z and Ze are identical. We have therefore shown that there exists a unique
solution to the equation:


(17) dZ C (t) = âˆ’Z C (t) dt + E(V ) E(b(Z C (t))) âˆ§ C dt
âˆ’ Z C (tâˆ’)P([0, b(Z C (tâˆ’))], dt)
with Z(0) = x. Lemma 3 actually ensures that there exists some constant C0 such
that E(b(Z C (t))) < C0 for all t â‰¥ 0 and C > 0. Hence, for C â‰¥ C0 , b(Z C (t)) is
integrable and (Z C (t)) is a solution of (16). Conversely, by using again Lemma 3
with C = +âˆ, any solution of Equation (12) is also solution of Equation (17). The
theorem is proved.

6. Mean-Field Asymptotics
In this section, one considers the asymptotic regime of these networks when the
number N of nodes of the network goes to infinity. The interaction between nodes
is as follows: for 1 â‰¤ i 6= j â‰¤ N , when node i fires the value of the state of node
j is increased by WijN = Vij /N . The variables (Vij ) are i.i.d. integrable random
variables with distribution V (dx)), with a slight abuse of notation V denotes in
the following a random variable with such a distribution. The associated Markov
process is denoted by (XiN (t)). We recall the SDE equations (5) in this context,
for 1 â‰¤ i â‰¤ N , one has
Z
1 X
N
N
zi 1{0â‰¤uâ‰¤b(XjN (tâˆ’))} Nj (du, dz, dt)
(18) dXi (t) = âˆ’Xi (t) dt +
2
N
j6=i R+
Z
N
âˆ’ Xi (tâˆ’)
1{0â‰¤uâ‰¤b(XiN (tâˆ’))} Ni (du, dz, dt),
R2+

where Ni is a Poisson processes with intensity measure given by du âŠ— V (dz) âŠ— dt.
The Poisson processes (Nj , 1 â‰¤ j â‰¤ N ) are independent
The empirical distribution is denoted by (Î›N (t)), for any continuous function Ï†
on R+ ,
N
1 X
hÎ›N (t), Ï†i =
Ï†(XiN (t)).
N i=1
The main result of this section is that under appropriate conditions a mean-field
convergence holds: The sequence (Î›N (t)) of random measures valued processes
converges in distribution to the distribution of the McKean-Vlasov process analyzed
in Section 5.

18

PH. ROBERT AND J. TOUBOUL

The strategy of the proof is the following: first it is shown that the scaled moment
of the total firing rate of the network
N
1 X
b(XiN (t))
N i=1

is, with high probability, bounded uniformly on any finite time interval. Then,
by using the stochastic evolution equations of (Î›N (t)), it is then proved that for
any continuous function Ï† on R+ with compact support, the sequence of processes
(hÎ›N (t), Ï†i) is tight for the topology of the uniform norm, in particular any of
its limiting points is a continuous process. One concludes by a uniqueness result
proved in the appendix.
Concerning the main parameters and the initial state of the network, the assumptions are given below.
Assumptions MF
(a) Growth Condition.
The firing rate function x 7â†’ b(x) is assumed to be C 1 , non-decreasing and
such that there exist 3E(V )Î³ < 1 and c > 0 such that
(19)

b0 (x) â‰¤ Î³b(x) + c

holds for any x â‰¥ 0.
(b) Bounded Support.
The distribution V (dx)) has a bounded support, there exists some SV > 0
such that V ([0, SV ]) = 1.
(c) Initial Conditions.
The random variables (XiN (0), 1 â‰¤ i â‰¤ N ) are i.i.d. with law m0 having a
bounded support.
Assumption (MF-a) implies that, for any a > 0, the ratio b(x + a)/b(x) is bounded
as x gets large, i.e. a slow growth at infinity. Note that polynomial functions
satisfy this assumption. See the proof of Lemma 8 in the Appendix. Additionally,
for convenience, it will be assumed that b(0) = 0 in the following. It turns out that
the case b(0) > 0 is easier from the point of view of the mean-field analysis of this
section. Indeed, in this case, the nodes are â€œrefreshedâ€ at a minimal positive rate,
in this way there is a maximal, state independent, interval between two spikes of a
given node.
6.1. Stochastic Evolution Equations for the Empirical Distribution. Let f
a C 1 -function on R+ then, from Equation (18) and Proposition 13 of the Appendix,
one gets that, for 1 â‰¤ i â‰¤ N ,
Z t
(20) f (XiN (t)) = f (XiN (0)) âˆ’
XiN (u)f 0 (XiN (u)) du
0

XZ t Z  
v
âˆ’ f (XiN (u)) b(XjN (u)) duV (dv)
+
f XiN (u) +
N
R+
j6=i 0
Z t


N
+
f (0) âˆ’ f (XiN (u)) b(XiN (u)) du + Mf,i
(t),
0

RANDOM INTEGRATE-AND-FIRE NEURONS

19

N
where (Mf,i
(t)) is the local martingale defined by

Z

t

Z h

s=0 R2+

i
ih
f (0)âˆ’f XiN (sâˆ’) 1{0â‰¤uâ‰¤b(XjN (sâˆ’))} Nj (du, dz, ds)âˆ’b(XjN (s)) ds
+

XZ

t

s=0

j6=i

h

Z

 

zi 
f XiN (sâˆ’) +
âˆ’ f (XiN (sâˆ’))
N
R2+
i

1{0â‰¤uâ‰¤b(XjN (sâˆ’))} Nj (du, dz, ds) âˆ’ b(XjN (s)) dsV (dzi ) .

N
Provided that the local martingales (Mf,i
), i = 1, . . . ,N are locally square
integrable, the associated previsible increasing processes are given by

(21)

Z

t


2
f (0) âˆ’ f (XiN (s) b(XiN (s)) ds
0
Z h 
XZ t
i 2
v
+
b(XjN (s)) ds
f XiN (s) +
âˆ’ f XiN (s)
V (dv),
N
0
R+


 N
Mf,i (t) =

j6=i

and, for 1 â‰¤ i < j â‰¤ N ,

 N

N
(22)
Mf,i , Mf,j
(t) =
Z t
i


h 
v
âˆ’ f XjN (s) b XiN (s) ds
f (0) âˆ’ f (XiN (s) f XjN (s) +
N
0
Z t
h 
i



v
âˆ’ f XiN (s) b XjN (s) ds
f (0) âˆ’ f (XjN (s) f XiN (s) +
N
0
X Z tZ h 
i
v
+
f XiN (s) +
âˆ’ f XiN (s) V (dv)
N
R+
k6âˆˆ{i,j} 0
Z h 
i

v
âˆ’ f XjN (s) V (dv) b XkN (s) ds
Ã—
f XjN (s) +
N
R+
by Proposition 13 of the Appendix. Equation (20) gives therefore the following
relation for the empirical measure
Z tZ
(23)

hÎ›N (u), f 0 (Â·)i du

hÎ›N (t), f i = hÎ›N (0), f i âˆ’
0

R+

Z tZ

E
D
 
v
âˆ’ f (Â·) hÎ›N (u), bi duV (dv)
+N
Î›N (u), f Â· +
N
0
R+
Z tD
 

E
v
âˆ’
Î›N (u), f Â· +
âˆ’ f (Â·) b(Â·) duV (dv)
N
0
Z t
âˆ’
hÎ›N (u), (f (0) âˆ’ f (Â·))b(Â·)i du + MfN (t),
0

where
(24)

(MfN (t))

is the martingale
MfN (t) =

N
1 X N
M (t),
N i=1 f,i

20

PH. ROBERT AND J. TOUBOUL

The corresponding previsible increasing process is given by
ï£«
ï£¶
N
X
X

 N






1
N
N
N
(25)
Mf (t) = 2 ï£­
Mf,i
(t) + 2
Mf,i
, Mf,j
(t)ï£¸ .
N
i=1
i<j
6.2. Estimates for the scaled firing rate. In this section it is proved that the
scaled moment of the firing rate
hÎ›N (t), bi =

N

1 X
b XiN (t) ,
N i=1

remains with high probability within a finite interval. One starts with a result on
the boundedness of some of its moments.
Lemma 4. Under Assumptions (MF), there exists Î´ > 3 such that relation



sup sup E( Î›N (t), bÎ´ ) < +âˆ,
N â‰¥1 tâ‰¥0

holds.
Proof. By Lemma 8 of the Appendix and Assumptions (MF) there exists Î´ > 3
such that

(26)
bÎ´ (x + a) âˆ’ bÎ´ (x) â‰¤ a Î³1 bÎ´ (x) + c1 , âˆ€a âˆˆ (0, Î·b ) and âˆ€x â‰¥ 0,
for some Î·b > 0 and with Î³1 < 1/E(V ). 


For K > 0. let Ï„K = inf{t â‰¥ 0 : Î›N (t), bÎ´+1 â‰¥ K}. Holderâ€™s Inequality
shows that for all t â‰¥ 0 the random variables hÎ›N (t âˆ§ Ï„K ), bp i, 1 â‰¤ p â‰¤ Î´ + 1, are
integrable. By taking f = bÎ´ in Equation (23), the optional stopping theorem gives
the relation


 


E Î›N (t âˆ§ Ï„K ), bÎ´ â‰¤ Î›N (0), bÎ´
Z tZ
E

D

v
âˆ’ bÎ´ (Â·) hÎ›N (u âˆ§ Ï„K ), bi duV (dv)
+N
E Î›N (u âˆ§ Ï„K ), bÎ´ Â· +
N
0
R+
Z t



âˆ’
E( Î›N (u âˆ§ Ï„K ), b1+Î´ ) du,
0

Recall that SV is an upper bound for the support of the distribution V , N is chosen
sufficiently large so that SV /N â‰¤ Î·b , from Inequality (26), one gets


 


E Î›N (t âˆ§ Ï„K ), bÎ´ â‰¤ Î›N (0), bÎ´
Z t




+ Î³1 E(V )
E Î›N (u âˆ§ Ï„K ), bÎ´ hÎ›N (u âˆ§ Ï„K ), bi du
0
Z t
Z t



+ c1 E(V )
E (hÎ›N (u âˆ§ Ï„K ), bi) du âˆ’
E( Î›N (u âˆ§ Ï„K ), b1+Î´ ) du.
0

0

Holderâ€™s Inequality and the fact that Î›N is a probability distribution give


1/Î´
hÎ›N (uâˆ§Ï„K ), bi â‰¤ Î›N (uâˆ§Ï„K ), bÎ´


 

(1+Î´)/Î´
Î›N (uâˆ§Ï„K ), b1+Î´ â‰¥ Î›N (uâˆ§Ï„K ), bÎ´

RANDOM INTEGRATE-AND-FIRE NEURONS

21

hence
E




Î›N (t âˆ§ Ï„K ), bÎ´



Z

t

â‰¤ C0 âˆ’ (1 âˆ’ Î³1 E(V ))
0

E
Z




+ c1 E(V )

Î›N (u âˆ§ Ï„K ), bÎ´

t

E




(1+Î´)/Î´ 

du

1/Î´ 

du,

Î›N (u âˆ§ Ï„K ), bÎ´

0

where



def.
C0 = sup E( Î›N (0), bÎ´ ).
N â‰¥1

With again Holderâ€™s Inequality and the fact that Î³1 E(V ) < 1, one gets finally
Z t



 

Î´/(1+Î´)
Î´
E Î›N (t âˆ§ Ï„K ), b
â‰¤ C0 âˆ’ (1 âˆ’ Î³1 E(V ))
E Î›N (u âˆ§ Ï„K ), bÎ´
du
0
Z t
 

1/Î´
E Î›N (u âˆ§ Ï„K ), bÎ´
du,
+ c1 E(V )
0

By using the inequality Î³1 E(V ) < 1 and, since Î´/(1 + Î´) > 1/Î´, Proposition 12
of the Appendix, one gets that there exists a finite constant C0 independent of K
such that



sup sup E Î›N (t âˆ§ Ï„K ), bÎ´ â‰¤ C0 ,
N â‰¥1 tâ‰¥0

on concludes the proof by letting K go to infinity.



Proposition 5 (Control of the scaled firing rate). Under Assumptions (MF), for
any T > 0 there exists Îº > 1 and some constant C0 such that the


Îº
(27)
lim P sup hÎ›N (t), b i â‰¥ C0 = 0.
N â†’+âˆ

0â‰¤tâ‰¤T

Proof. For simplicity, the proof is done for Îº = 1. The case Îº > 1 follows the same
lines together with the same method as in the proof of the previous lemma with
Î´ > 3. One first shows that there exists some constant CT such that,



  CT
.
(28)
sup E MbN (t)2 = E MbN (t) â‰¤
N
0â‰¤tâ‰¤T
The relations (21), (22) and (21) are used in the case f = b. The first term of the
right hand side of Relation (21) for E(hMbN i(t)) gives the contribution
Z
N Z




1 X t
1 t
N
3
(29)
E
b(X
(u))
du
=
E Î›N (u), b3 du
i
2
N i=1 0
N 0
and, for the second term,

2 
 
X Z tZ
1
v
N
N
N
E b(Xj (u)) b Xi (u) +
âˆ’ b(Xi (u))
duV (dv).
N2
N
0
R+
1â‰¤i6=jâ‰¤N

As before, if N is sufficiently large so that SV /N â‰¤ Î·b , Equation (43) of the
Appendix gives that this last term is upper bounded by
Z






E(V 2 ) t
Î³1 E Î›N (u), b3 + 2Î³1 c1 E Î›N (u), b2 + c1 E (hÎ›N (u), bi) du.
(30)
N
0
With similar arguments, analogous bounds can be obtained for the second term of
N
N
Relation (22) for E(hMbN i(t)) involving the E(hMb,i
, Mb,i
i(t)). Holderâ€™s Inequality

22

PH. ROBERT AND J. TOUBOUL

and Lemma 4 show that there exists a constant CT such that the upper bound (28)
holds. Define
def.

MbN,âˆ— (T ) =

sup MbN (s),

0â‰¤sâ‰¤T

Doobâ€™s Inequality shows therefore that, for any Îµ > 0, there exists N0 such that if
N â‰¥ N0 then P(MbN,âˆ—
(T ) > 1) â‰¤ Îµ. With f = b in Equation (23), one gets that,
2
for 0 â‰¤ t â‰¤ T ,
(31)

hÎ›N (t), bi â‰¤ hÎ›N (0), bi + MbN,âˆ— (T )
Z tZ D
E

v
âˆ’ b(Â·) hÎ›N (u), bi duV (dv)
+N
Î›N (u), b Â· +
N
0
R+
Z t



Î›N (u), b2 ) du.
âˆ’
0

By the integrability condition of b with respect to m0 in Assumptions (MF), if N
is sufficiently large then, by the law of large numbers,



P hÎ›N (0), bi > 1 + E b(X11 ) < Îµ.
def.

Let C0 = 2 + E(b(X11 (0)))}, On the event {MbN,âˆ—
(t) â‰¤ 1}, by using again Equa2
tion (43) of the Appendix and Relation (28), Relation (31) gives the inequality
Z tZ
(32)

hÎ›N (t), bi â‰¤ C0 + (Î³1 E(V ) âˆ’ 1)

2

hÎ›N (u), bi duV (dv)
0

R+

Z

t

hÎ›N (u), bi du.

+ c1
0

Proposition 12 allows to conclude that, on the event {MbN,âˆ—
(t) â‰¤ 1}, the random
2
variable sup0â‰¤tâ‰¤T hÎ›N (t), bi is uniformly bounded for all N â‰¥ 0 (i.e., by a constant
independent of N ), proving (27).

6.3. Tightness of (Î›N (t)). To prove this result, Theorem 3.7.1 of Dawson [10]
shows that it is enough to prove that, for any continuous function Ï† on R+ with
compact support, the sequence of processes (hÎ›N (t), Ï†i) is tight for the topology of
the uniform norm on compact sets.
Proposition 6. The sequence of measure-valued processes (hÎ›N (t)i) is tight for
the convergence in distribution with continuous limits.
Proof. Take Ï† a C1 -function on R+ with compact support.
By using
the same


N
2
method as in the proof of Proposition 5, one has that E MÏ† (t) is a O(1/N ),
in particular MÏ†N (t) converges to 0 in distribution for the uniform norm on finite
time interval.
The modulus of continuity of hÎ›N , Ï†i is defined as
wÎ›N ,Ï† (Î´) =

sup
0â‰¤sâ‰¤s0 â‰¤t
|sâˆ’s0 |â‰¤Î´

|hÎ›N (s), Ï†i âˆ’ hÎ›N (s0 ), Ï†i| .

RANDOM INTEGRATE-AND-FIRE NEURONS

23

For s â‰¤ s0 ,
hÎ›N (s), Ï†i âˆ’ hÎ›N (s0 ), Ï†i = âˆ’

s0

Z

hÎ›N (u), Â· Ï†0 (Â·)i du

s
s0

Z
+N
s

Z

E
D
 
v
âˆ’ Ï†(Â·) hÎ›N (u), bi duV (dv)
Î›N (u), Ï† Â· +
N
R+
Z s0
âˆ’
(hÎ›N (u), Ï†i) du + MÏ†N (s) âˆ’ MÏ†N (s0 ),
s

take Î· > 0 and Îµ > 0, by Equation (27), there exists some C1 such that, for all
N â‰¥ 1,


(33)
P (wÎ›N ,Ï† (Î´) > Î·) â‰¤ Îµ + P wÎ›N ,Ï† (Î´) > Î·, sup hÎ›N (t), bi â‰¤ C1 .
tâ‰¥0
âˆ’1

Now, choose Î´ such that Î´ < Î· min(1, (E(W )C1 ) )/(4K), then
ï£¶
ï£«




ï£·
ï£¬
P wÎ›N ,Ï† (Î´) > Î·, sup hÎ›N (t), bi â‰¤ C1 â‰¤ P ï£­ sup MÏ†N (s) âˆ’ MÏ†N (s0 ) â‰¥ Î·/4ï£¸ ,
0â‰¤sâ‰¤s0 â‰¤t
|sâˆ’s0 |â‰¤Î´

tâ‰¥0

due to the convergence in distribution to 0 of the martingale, this term can be made
arbitrarily small for N large.
Hence, one has shown that the sequence of processes (hÎ›N (t), Ï†i) is tight for
convergence in distribution for the uniform norm on compact sets.

Theorem 3.7.1 of Dawson [10] shows that the sequence of measure-valued processes (Î›N (t)) is tight, let Î›(t) be one the limit of a given convergent subsequence
(Î›Nk (t)). In particular, for any continuous function with compact support, one has
the convergence of the processes
lim (hÎ›Nk (t), Ï†i) = (hÎ›(t), Ï†i) .

N â†’+âˆ

The next result shows that this convergence also for the function (b(x)).
Lemma 5. If ((Î›Nk (t))) is a converging subsequence with (Î›(t)) as a limit, then
lim (hÎ›Nk (t), bi) = hÎ›(t), bi .

kâ†’+âˆ

for the convergence in distribution.
Proof. We first prove that the sequence of processes (hÎ›N (t), bi) is tight. By a
similar argument as before, for 0 â‰¤ s â‰¤ s0 â‰¤ t,
hÎ›N (s), bi âˆ’ hÎ›N (s0 ), bi = âˆ’

Z

s0

hÎ›N (u), Â· b0 (Â·)i du

s

Z

s0

Z

+N
s

R+

D

 
E
v
Î›N (u), b Â· +
âˆ’ b(Â·) hÎ›N (u), bi du V (dv)
N
Z s0
âˆ’
(hÎ›N (u), bi) du + MÏ†N (s) âˆ’ MÏ†N (s0 ).
s

24

PH. ROBERT AND J. TOUBOUL

From the estimate (27) of Proposition 5 one concludes that one can choose Î´ so
that the left hand side of the above relation is arbitrarily small. The tightness has
been proved. One has to identify the limit.
For K > 0 there exists a function Ï†K âˆˆ Cc (R+ ) which coincides with b on the
set SK = {x : b(x) â‰¤ K} and such that Ï†K â‰¤ b and K 7â†’ Ï†K is increasing. We
know that, for t > 0 one has, for the convergence in distribution,
lim hÎ›Nk (t), Ï†K i = hÎ›(t), Ï†K i ,

kâ†’+âˆ

and



|hÎ›Nk (t), bi âˆ’ hÎ›Nk (t), Ï†K i| â‰¤ Î›Nk (t), |b âˆ’ Ï†K |1{bâ‰¥K}



1
â‰¤ Î›Nk (t), b1{bâ‰¥K} â‰¤ Îºâˆ’1 hÎ›Nk (t), bÎº i .
K
Proposition 5 shows that the sequences of variables (hÎ›Nk (t), bÎº i) is uniformly
bounded on finite time intervals with high probability. Consequently, for K sufficiently large, the left hand side of Relation 34 is arbitrarily close to 0 in distribution
for all k â‰¥ 1.
For t â‰¥ 0, the monotone convergence theorem gives that

(34)

lim hÎ›(t), Ï†K i = hÎ›(t), bi .

Kâ†’+âˆ

The convergence in distribution of hÎ›Nk (t), bi proved for a fixed t is clearly valid
for a vector of finite time marginals. By tightness one concludes that the process
(hÎ›Nk (t), bi) converges in distribution to (hÎ›(t), bi) for the topology of the uniform
convergence on compact sets.

Theorem 3 (Mean-Field Convergence). Under Conditions (MF) the sequence of
processes (X1N (t)) converges in distribution to the law of the unique process (Z(t))
with initial distribution m0 and solution of the SDE (16) with Î± = E(V ).
Proof. The SDE (23) and Lemma 5 show that any possible limit (Î›(t)) of the
sequence (Î›N (t)) satisfies the relation
Z t
(35) hÎ›(t), f i = hÎ›(0), f i âˆ’
hÎ›(u), f 0 (Â·)i du
0
Z t
Z t
0
+ E(V )
hÎ›(u), f i hÎ›(u), bi du âˆ’
hÎ›(u), f bi du.
0

0

Proposition 14 of the Appendix gives that such a process (Î›(t)) is unique, in particular it is deterministic. The convergence in distribution of (Î›N (t)) holds.
The process (X1N (t)) is the solution (X(t)) of the SDE
Z
Z t
E(V ) t
N
X(u)b(X(u) du
(36) X(t) = X(0) âˆ’
X(u) du du + Î¦ (t) âˆ’
N
0
0
Z t
Z
âˆ’
X(uâˆ’)
1{0â‰¤vâ‰¤b(X(uâˆ’))} N1 (dv, dz, du),
R2+

0

where
N

Z

Î¦ (t) = E(V )
0

t





Î›N (u), b du + MIN (t),

RANDOM INTEGRATE-AND-FIRE NEURONS

25

where (M1N (t)) is the martingale (6). The process (Î¦N (t)) is converging in distribution for the uniform norm to
Z t
Î¦(t) = E(V )
hÎ›(u), bi du,
0

and the martingale vanishes with a usual argument, it is then not difficult to get
that (X1N (t)) converges in distribution in the Skorohodâ€™s space D(R+ , R+ ) to the
solution (X(t)) of the SDE
Z t
Z

0
X(t) = âˆ’X(t) + Î¦ (t) dt âˆ’
X(tâˆ’)
1{0â‰¤vâ‰¤b(X(tâˆ’))} N1 (dv, dz, dt).
R2+

0

By using the fact that, by exchangeability, E(b(X1N (t))) = E(hÎ›N (t), bi) and that
Î›(t) is deterministic, one gets that
Z t

Î¦(t) = E(Î¦(t)) = E(V )
E X(u) du,
0

one concludes the proof of the theorem by using the uniqueness result of Theorem 2.

7. Analysis of Invariant distributions
This section is devoted to the analysis of the invariant distributions of the
McKean-Vlasov process (12). We shall denote in this section Ï€ a distribution
on R+ which is invariant along the McKean-Vlasov evolution. In particular the
map t 7â†’ E(b(X(t)) is constant. If we denote Î± = E(V )E(b(X(0))), the stationary
process (X(t)) can then be seen as the solution (Y (t)) of the SDE
Z
(37)
dY (t) = (Î± âˆ’ Y (t)) dt âˆ’ Y (tâˆ’) 1{0â‰¤uâ‰¤b(Y (tâˆ’))} N (du, dt, dz).
Define
Ï„ = inf{t > 0 : Y (t) = 0}
and x(t) = Î±(1 âˆ’ exp(âˆ’t)).
The variable Ï„ is the instant of the first spike of the neuron. If Y (0) = 0, before
time Ï„ the evolution of (Y (t)) is deterministic, one has in fact Y (t) = x(t) for t < Ï„ .
The Poisson property gives that
 Z t

P(Ï„ â‰¥ t) = exp âˆ’
b(x(u)) du
0

) The invariant distribution Ï€ of (Y (t)) can then be expressed as
Z Ï„

1
E
f (x(u)) du ,
Ï€(f ) =
E(Ï„ )
0
for f a continuous function with compact support on [0, Î±). By Fubiniâ€™s Theorem
Z Ï„
 Z +âˆ
E
f (x(u)) du =
f (x(u))P(Ï„ â‰¥ u) du
0
0
 Z u

Z +âˆ
f (x(u)) exp âˆ’
b(x(v)) dv du
=
0
 Z u 0

Z Î±
f (u)
b(v)
=
exp âˆ’
dv du.
0 Î±âˆ’u
0 Î±âˆ’v

26

PH. ROBERT AND J. TOUBOUL

The measure has a compact support [0, Î±), it has finite mass if and only if
 Z u

Z Î±
1
b(v)
E(Ï„ ) =
exp âˆ’
dv du < +âˆ.
0 Î±âˆ’u
0 Î±âˆ’v
Theorem 4. The invariant distribution of the solution of SDE (12) has density
 Z u

b(v)
1
(38)
u 7â†’
exp âˆ’
dv
C(Î²)(Î²E(V ) âˆ’ u)
0 Î²E(V ) âˆ’ v
on [0, Î²E(V )), where
Z
C(Î²) =
0

Î²E(V )

 Z u

b(v)
1
exp âˆ’
dv du
Î²E(V ) âˆ’ u
0 Î²E(V ) âˆ’ v

and Î² is the solution of the equation
Î²E(V )

Z
(39)

Î²C(Î²) = 1 âˆ’ exp âˆ’
0

!
b(v)
dv .
Î²E(V ) âˆ’ v

In the case where b(Î²E(V )) > 0, the term
!
Z Î²E(V )
b(v)
dv
exp âˆ’
Î²E(V ) âˆ’ v
0
vanishes, the fixed point equation reduces to Î²C(Î²) = 1.
Moreover, the change of variable x = u/Î²E(V ) and y = v/Î²E(V ) yields the
simplified formulation of C(Î²):
 Z x

Z 1
1
b(Î²E(V )y)
C(Î²) =
exp âˆ’
dy dx.
1âˆ’y
0 1âˆ’x
0
We shall now analyze the behavior of the map Î² 7â†’ Î²C(Î²) in order to characterize
the number of possible stationary distributions.
Lemma 6. The function Î² â†’ Î²C(Î²) is converging to infinity as Î² gets large, and
if
b(x)
= Î» âˆˆ [0, +âˆ]
lim
xâ†’0 x
then
1
lim Î²C(Î²) =
.
Î²â†’0
Î»E(V )
Proof. Let us start by the behavior at infinity, by monotonicity of b, the quantity
Z x
B(x) =
b(y) dy
0

is upperbounded by xb(x). Let us fix Î´ âˆˆ (0, 1), we have:


Z Î´
1
1 B(Î²E(V )Î´)
Î²C(Î²) â‰¥ Î²
exp âˆ’
dx
1 âˆ’ Î´ Î²E(V )
0 1âˆ’x


Î´
â‰¥ âˆ’Î² log(1 âˆ’ Î´) exp âˆ’
b(Î²E(V )Î´)
1âˆ’Î´
and simply taking, for arbitrary C > 0, Î´ = C/Î² (for Î² > C), we obtain


C
C
Î²C(Î²) â‰¥ âˆ’Î² log(1 âˆ’ ) exp âˆ’
b(CE(V )) .
Î²
Î²âˆ’C

RANDOM INTEGRATE-AND-FIRE NEURONS

27

It is then easy to see that from this formula that:
lim Î²C(Î²) â‰¥ C

Î²â†’âˆ

and since C is arbitrary, this precisely means that Î²C(Î²) â†’ âˆ.
Let us now analyze the behavior of Î²C(Î²) at Î² = 0 as a function of the limit Î».
â€” Î» = 0.
For any Î´ > 0, there exists Î²(Î´) such that b(Î²E(V ))/Î²E(V ) â‰¤ Î´ for all
Î² â‰¤ Î²(Î´), and therefore for such Î² â‰¤ Î²(Î´) we have:


Z 1
Z x
1
y
1
Î²C(Î²) â‰¥ Î²
exp âˆ’Î±Î²E(V )
dy dx â‰¥
1
âˆ’
x
1
âˆ’
y
Î´E(V
)
0
0
which proves that Î²C(Î²) â†’ âˆ at Î² = 0.
â€” Î» = âˆ.
For any Î´ > 0 we have for Î² small enough b(Î²E(V )y) â‰¥ Î´Î²E(V )y, and
therefore:
Z 1
Î²C(Î²) â‰¤ Î²
(1 âˆ’ x)Î´E(V )Î²âˆ’1 exp(Î´Î²E(V )x) dx
0

1
+Î²
=
Î´E(V )

Z

1

(1 âˆ’ x)Î´E(V )Î² eÎ´Î²E(V )x dx

0

and therefore we have limÎ²â†’0 Î²C(Î²) â‰¤ 1/Î´E(V ) for arbitrarily large Î´,
showing that Î²C(Î²) â†’ 0 at Î² = 0.
â€” 0 < Î» < +âˆ.
It is easy to show using the same estimates as in the two previous cases
that for any Î´ > 0 small enough we have
1
1
â‰¤ lim Î²C(Î²) â‰¤
,
(Î» + Î´)E(V ) Î²â†’0
(Î» âˆ’ Î´)E(V )
which ends the proof.

From this lemma, one gets directly the following proposition.
Proposition 7 (Number of Stationary Solutions of Mean-Field Equations). Let
b(x)
= Î» âˆˆ [0, +âˆ]
x
then, for the McKean-Vlasov process (12),
â€” if Î» âˆˆ [1/E(V ), âˆ], then there always exists at least one non-trivial stationary invariant distribution.
â€” If Î» âˆˆ (0, 1/E(V )), the Dirac mass at 0 is the unique invariant distribution.
â€” If Î» = 0, then the Dirac mass at 0 is always an invariant distribution.
Non-trivial invariant distribution exist if the minimal value of Î² â†’ Î²C(Î²)
is smaller than 1.
lim

xâ†’0

The mean-field equations can therefore present several stationary distributions.
The question of the stability of these solutions (in a sense to be made more precise)
is then natural. We have seen in theorem 1 that in finite-sized networks, the only
stationary solution is the trivial state in which no neuron spikes, and that the
network converges towards this solution. With additional solutions arising in the

28

PH. ROBERT AND J. TOUBOUL

mean-field limit, a deep question concern the emergence of new stationary and their
significance if they are attractive.
7.1. Stability of the trivial solution. In the cases where b(0) = 0, the Dirac
mass at 0 is invariant for the mean-field equations. We investigate here the stability
of this invariant distribution as a function of the local behavior of b(x) at zero. The
main result of the section is the following:
Proposition 8. Let
Î» = lim

xâ†’0

b(x)
âˆˆ [0, âˆ]
x

and Ï = Î»E(V ),

then
â€” if Ï âˆˆ [0, 1), the trivial solution Î´0 is almost surely exponentially stable.
More precisely, there exists Î´ > 0 and AÎ´ > 0 sufficiently small such that
for any initial condition with support included in [0, AÎ´ ],
lim sup
tâ†’âˆ

log(Xt )
< âˆ’Î´
t

a.s.

â€” if Ï âˆˆ (1, âˆ], the trivial solution is unstable in probability.
That is, there exists A > 0 such that for any initial condition X0 with
support included an interval [0, A] and mean Âµ0 > 0, there exists a t0 > 0
such that
!
P

sup Xt > A

> 0.

tâˆˆ[0,t0 ]

The quantity Ï can be seen as the excitation rate of the network. The result for
Ï < 1 is quite strong: almost any trajectory converge exponentially fast towards 0
provided that the initial condition is chosen sufficiently close from Î´0 , in the sense
that its support is included in a small interval around 0.
However, the instability result part of the proposition for Ï > 1 is weaker: whatever the initial condition, the probability of reaching in finite time a specified level
away from 0 is strictly positive. From a pathwise viewpoint, the result is indeed less
strong than the exponential stability result. But from the distribution viewpoint,
this corresponds to an instability of the distribution Î´0 in the sense of Khasminskii [15].
Proof. Let us first deal with the case Î»E(V ) < 1. Since jumps are all negative, any
solution of the McKean-Vlasov equation has the upperbound:
Z t
Xt â‰¤ X0 +
(âˆ’Xs + E(V )E(b(Xs )) ds.
0

Moreover, if Î»E(V ) < 1, there exists Î´ > 0 and AÎ´ > 0 such that for any x < AÎ´ ,
âˆ’x + E(V )b(x) â‰¤ âˆ’Î´x
Xtâˆ—

We introduce
= ess supXt = inf{u > 0; P[Xt > u] = 0}, and assume that
X0âˆ— < AÎ´ . We show that along the evolution, the essential support of (Xt ) never
exceeds AÎ´ and actually shrinks to 0, ensuring stability of the solution Î´0 . Indeed,
using Gronwallâ€™s lemma and the monotonicity of the map b, we have:
Z t
Z t
âˆ’t
tâˆ’s
âˆ— âˆ’t
Xt â‰¤ X0 e + E(V )
e E(b(Xs )) ds â‰¤ X0 e + E(V )
etâˆ’s b(Xsâˆ— ) ds
0

0

RANDOM INTEGRATE-AND-FIRE NEURONS

29

readily implying that:
Xtâˆ—

â‰¤ X0 e

âˆ’t

Z

t

eâˆ’(tâˆ’s) b(Xsâˆ— ) ds.

+ E(V )
0

Let us now introduce the deterministic time:
Ï„ = inf{t > 0 ; Xtâˆ— > AÎ´ }
On the interval [0, Ï„ ), we have:
Xtâˆ— â‰¤ X0 eâˆ’t + (1 âˆ’ Î´)

Z

t

eâˆ’(tâˆ’s) Xsâˆ— ds

0

and by Gronwallâ€™s lemma again, we obtain for any t âˆˆ [0, Ï„ ):
Xtâˆ— â‰¤ X0âˆ— eâˆ’Î´t â‰¤ AÎ´ eâˆ’Î´t .
This implies that (i) Ï„ = âˆ and (ii) Xtâˆ— converges exponentially fast towards
0. We have therefore proved that for any initial condition with support sufficiently
concentrated around 0, the process converges almost surely towards 0 when t â†’ âˆ,
hence the solution Î´0 is stable.
If Î»E(V ) > 1, we can find Î´ > 0 and AÎ´ > 0 sufficiently small so that:
âˆ’x + E(V )b(x) âˆ’ xb(x) â‰¥ Î´x.
Let Xt be the solution of the McKean-Vlasov equation with initial condition X0
such that X0âˆ— < AÎ´ (we recall that Xtâˆ— denotes in this proof the essential supremum
of Xt ). Denoting Âµt = E(Xt ), we have:
Z t
Âµt = Âµ0 +
âˆ’Âµs + E(V )E(b(Xs )) âˆ’ E(Xs b(Xs )) ds
0
Z tZ
= Âµ0 +
(âˆ’x + E(V )b(x) âˆ’ x b(x)) ps (dx) ds
0

R

with ps is the distribution of Xs . Similarly to the previous case, let us denote Ï„ the
deterministic time:
Ï„ = inf{t > 0 ; Xtâˆ— < AÎ´ }.
On the interval [0, Ï„ ), we have:
Z t
Âµt â‰¥ Âµ0 + Î´
Âµs ds
0
Î´t

i.e. Âµt â‰¥ Âµ0 e . This implies that necessarily
 
1
AÎ´ def.
= t0 .
t â‰¤ log
Î´
Âµ0
We therefore conclude that the essential supremum of the solution exceeds AÎ´
whatever the initial condition, which means that
!
P

sup Xs > AÎ´

> 0.

tâˆˆ[0,t0 ]


7.2. Power firing functions. We now provide some specific examples, for power
functions of the form b(x) = Î»xÎ± + Î´. We distinguish the affine (Î± = 1), superlinear
(Î± > 1) and sublinear (Î± < 1) cases.

30

PH. ROBERT AND J. TOUBOUL

7.2.1. Affine firing functions. We start by considering affine firing functions, and
apply Proposition 7 and the characterization of the stability of Proposition 8 to
study the number of invariant distributions and their stability:
Proposition 9 (Linear Firing-Rate). If b(x) = Î» x + Î´ with Î» > 0 and Ï = Î»E(V ),
then if Î´ = 0
â€” For Ï < 1, Î´0 is the unique stationary solution and it is almost surely
exponentially stable
â€” For Ï > 1, Î´0 is unstable in probability, and there exists an additional
solution to the mean-field equations.
For Î´ > 0, there exists a unique, non trivial, invariant distribution.
Proof. Affine firing functions allow analytical calculations for all quantities. Basic
algebra yields
Z 1
ÏÎ²
1
+
(1 âˆ’ x)ÏÎ²+Î´ eÏÎ² x dx.
C(Î²) =
ÏÎ² + Î´
ÏÎ² + Î´ 0
We change variables and define x = âˆ’1 + y âˆ’ log(y). The map y 7â†’ âˆ’1 + y âˆ’ log(y)
is strictly decreasing on (0, 1) and its inverse is Ï†(x) = exp(âˆ’W (âˆ’eâˆ’1âˆ’x ) âˆ’ 1 âˆ’ x)
where W is the first real branch of the Lambert function W , see Corless et al. [9].
One gets that

  
Î²
E1
Î²C(Î²) =
1âˆ’E Î¦
ÏÎ² + Î´
ÏÎ²
Î´
0
where Î¦(x) = Ï† (x)Ï† (x). It is then easy to see that:

 

  
E1 0 E1
d
Î´
E1
1
Î²C(Î²) =
+
E
Î¦
.
1
âˆ’
E
Î¦
dÎ²
(ÏÎ² + Î´)2
ÏÎ²
ÏÎ² + Î´
ÏÎ²
ÏÎ²
The map Ï† satisfies:
ï£±
W (âˆ’eâˆ’xâˆ’1 )
ï£´
0
ï£´
Ï†
(x)
=
ï£´
ï£´
1 + W (âˆ’eâˆ’xâˆ’1 )
ï£²
ï£´
ï£´
ï£´
ï£´
ï£³Ï†00 (x) = âˆ’

W (âˆ’eâˆ’xâˆ’1 )
,
(1 + W (âˆ’eâˆ’xâˆ’1 ))3

and therefore,
Î¦0 (x) =

(âˆ’W (âˆ’eâˆ’xâˆ’1 ))Î´+1 (Î´(1 + W (âˆ’eâˆ’xâˆ’1 )) + 1)
.
(1 + W (âˆ’eâˆ’xâˆ’1 ))3

For x â‰¥ 0, âˆ’eâˆ’1 â‰¤ âˆ’eâˆ’xâˆ’1 â‰¤ 0 and hence W (âˆ’eâˆ’xâˆ’1 ) â‰¤ 0, ensuring that Ï•0 (x) <
0, Ï•00 (x) > 0 and eventually Î¦0 (x) > 0. All these estimates put together prove
that Î² 7â†’ Î²C(Î²) is strictly increasing, and therefore that there exists a unique
non-trivial solution to the fixed point equation Î²C(Î²) = 1 when Ï > 1 or when
Î´ > 0.

We therefore conclude that in the case b(0) > 0, there exists a unique stationary
solution, which is non-trivial, as was also the case in the finite-sized networks.
In the case b(0) = 0, we have shown that the only stationary distribution of
finite-sized networks is the trivial solution Î´0 . In the mean-field limit, this solution
persists whatever the value of the parameters. However, we showed that for Ï > 1,
this solution is no more stable, and non-trivial solution appears when Ï > 1. This

RANDOM INTEGRATE-AND-FIRE NEURONS

0.8

31

120 100 80

0.8

theoretical
90

0.6

extinction time

80

0.4

0.2

60

140
40

70
60
50
40
30

20

20
10

0
0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

(a) Averaged activity

0.5

0.6

0.7

0.8

0.9

1

1.1

1.2

1.3

1.4

(b) Extinction time

Figure 1. Affine firing-rate functions. (a) Empirical averaged
activity Î²Ì‚ of a 2 000 neurons network for different values of E(V ),
at time T = 100, averaged across 30 initial conditions uniformly
drawn in [0, 1], with Î» = 1. The value of Î² corresponding to
the non-trivial invariant measure of the mean-field equation was
computed numerically and plotted in red. (b) Extinction Time
of the network as a function of E(V ) for different values of the
network size N .

is what we observe in the simulations of the network (see Fig. 1) in the linear firing
rate case b(x) = x for varying values of E(V ): for E(V ) > 1, the trivial solution no
more attracts the network, and a new solution with a non-zero value of the firing
rate emerges. The value of Î² at this equilibrium can be computed numerically, and
shows a very good agreement with the simulations of the finite-sized network, even
if this finite-sized network will eventually extinct.
This phenomenon suggests the presence of a phase transition in the system. For
small coupling (Ï < 1), both finite-sized networks and their mean-field limit have a
trivial stationary solution. In that case, the time of extinction remains small and
do not dramatically depend on the network size. However, for Ï > 1, the trivial
solution is no more stable for the mean-field limit and, in that limit, a sustained
activity appears. The time of extinction shows a dramatic dependence on the
network size. A pseudo-stationary solution emerges, which is meta-stable for any
finite system in the sense that even though the system will eventually stop firing,
the time during which the system supports this non-trivial stationary firing rate
diverges as the network size increases (see Fig. 1(b)). See the end of Appendix on
the simulation algorithm used to obtain the figures.
From the biological viewpoint, the non-trivial solution found corresponds to a
self-sustained activity. In this regime, neurons fire independently as a Poisson
processes with a common intensity. This regime is a natural regime of activity of
large neuronal networks. It is a typical regime of the awake brain often referred to
as the asynchronous irregular state, see Brunel [2].
7.2.2. Sub-linear power firing functions.

32

PH. ROBERT AND J. TOUBOUL

Proposition 10. If b(x) = Î»xÎ± + Î³ with 0 < Î± < 1 and Î³ â‰¥ 0, then there exists
a unique non-trivial invariant distribution to the McKean-Vlasov equations. For
Î³ = 0, the trivial solution is unstable.
Proof. For Î± < 1, proposition 6 implies that the map Î² 7â†’ Î²C(Î²) tends to 0 at
Î² = 0 and to infinity when Î² â†’ âˆ, ensuring the existence of a non-trivial invariant
distribution. Moreover, proposition 8 shows that the trivial solution is unstable.
The only result that remains to be proved is the uniqueness of the invariant distribution. To this end, we show that the map Î²C(Î²) is strictly increasing. This is done
by rewriting the expression of Î²C(Î²) noting Ï = Î»Î² Î± and using the expression:
Z 1
1 âˆ’ xÎ± âˆ’ÏÎ² Î± Ï†(x)
1
+
Î²
(1 âˆ’ x)Î³
e
dx
Î²C(Î²) =
Î±âˆ’1
ÏÎ²
1âˆ’x
0
with
Z
Ï†(x) =
0

x

yÎ± âˆ’ 1
dy âˆ’ log(1 âˆ’ x).
1âˆ’y

This map is strictly increasing, tends to 0 when x â†’ 0 and to âˆ at x = 1. It is
therefore invertible, and we denote Ï• = Ï†âˆ’1 . Using the variable z = Ï†(x), we can
express our equation as:

 

1
E1
1
+
E
Ïˆ
Î²C(Î²) =
Î³
ÏÎ² Î±âˆ’1
ÏÎ² Î±
where E1 is an exponential random variable with parameter 1 and
ÏˆÎ³ = (1 âˆ’ Ï•)Î³

1 âˆ’ Ï•Î± 0
Ï•.
1âˆ’Ï•

Hence, we have:


 



d
1
Î±
E1
E1
0
Î²C(Î²) = (1 âˆ’ Î±) Î± 1 + E ÏˆÎ³
âˆ’ 2 2Î± E E1 ÏˆÎ³
.
dÎ²
ÏÎ²
ÏÎ² Î±
Ï Î²
ÏÎ² Î±
The first term of this expression is clearly positive. The second term is handled by
expressing the differential ÏˆÎ³0 and showing that it is strictly negative. In details,
we have:
ÏˆÎ³0 = âˆ’Î³(1 âˆ’ Ï•)Î³âˆ’1 Ïˆ0 Ï•0 + (1 âˆ’ Ï•)Î³ Ïˆ00
and therefore we only need to show that Ïˆ00 < 0. Straightforward calculations yield:
 0 2
Ï•
1 âˆ’ Ï•Î± 00
Ïˆ00 =
(1 âˆ’ (Î±Ï•Î±âˆ’1 + (1 âˆ’ Î±)Ï•Î± )) +
Ï• .
1âˆ’Ï•
1âˆ’Ï•
The first term is clearly positive, and the second term has the sign of Ï•00 . Since we
have:
ï£±
1
Ï•0
ï£´
ï£´
,
Ï•00 = âˆ’ 02
Ï†00 â—¦ Ï†
ï£²Ï•0 = 0
Ï† â—¦Ï†
Ï† â—¦Ï†
Î±
Î±xÎ±âˆ’1
xÎ±
ï£´
ï£´Ï†0 (x) = x
> 0,
Ï†00 (x) =
+
> 0,
ï£³
1âˆ’x
1âˆ’x
(1 âˆ’ x)2
we conclude that Ï•00 < 0. This ensures that Î² 7â†’ Î²C(Î²) is strictly increasing, and
therefore there exists a unique invariant distribution for the mean-field equations.


RANDOM INTEGRATE-AND-FIRE NEURONS

33

7.2.3. Super-linear power firing functions. The case b(x) = Î»xÎ± with Î» > 0 and
Î± > 1 shows a more intricate behavior. Proposition 6 shows that the map Î² 7â†’
Î²C(Î²) diverges to infinity when Î² â†’ 0 or Î² â†’ âˆ, which allowed to conclude that
apart from the trivial invariant distribution, there either exist no other invariant
distribution or generically an even number of non-trivial invariant distributions.
We analyze the dependence of the number of non-trivial invariant distributions
as a function of the parameters. The following lemma investigates the fixed point
equation Î²C(Î²) = 1 as a function of Î», in fact of Ï = Î»E(V )Î± .
Lemma 7. Denoting by Ï = Î»E(V )Î± . For any Î² > 0, there exists a unique Ï(Î²)
such that Î² satisfies the fixed point equation Î²C(Î²) = 1 and there exists Î²c âˆˆ Râˆ—+
such that
Ïc = min Ï(Î²) = Ï(Î²c ) > 0.
Î²âˆˆR+

Proof. Simple algebraic manipulations allow to rewrite the fixed point equation as:


Z 1
Z x Î±
Î±
y âˆ’1
1
ÏÎ² Î± 1 âˆ’ x
Î±
+Î²
(1 âˆ’ x)
exp âˆ’ÏÎ²
dy dx
(40) Î²C(Î²) =
ÏÎ² Î±âˆ’1
1âˆ’x
1âˆ’y
0
0
We define
Z u Î±
Z 1
v
1
g(u) =
dv and Î¨(x) =
exp(âˆ’xg(u)) du.
1
âˆ’
v
1
âˆ’
u
0
0
Our fixed point equation simply reads
(41)

Î²Î¨(ÏÎ² Î± ) = 1.

With this expression, it is now relatively easy to show that for any Î² > 0 fixed,
there exists a unique Ï(Î²) such that equation (41) is satisfied. Indeed, it is clear
from the expression (41) that Ï 7â†’ Î²Î¨(ÏÎ² Î± ) is strictly decreasing, tends to infinity
at Ï = 0 and to 0 when Ï â†’ âˆ.
Moreover, the map Î² 7â†’ Ï(Î²) has the following properties.
â€” Using equation (40), we observe that Î² Î±âˆ’1 Ï(Î²) â†’ 1 when Î² â†’ 0 hence
Ï(Î²) â†’ âˆ
â€” Ï(Î²) â†’ âˆ when Î² â†’ âˆ. Indeed, using equation (40) and the series
representation of 1/(1 âˆ’ y), we can show that
1

 Î±+1
Z âˆ
Î±+1
Î²
lim
=
eâˆ’u /Î±+1 du.
Î²â†’âˆ Ï(Î²)
0

The quantity Ïc constitute a transition point in the system, and governs the
number of invariant distributions. The following proposition is a simple consequence
of lemma 7.
Proposition 11 (Number of Stationary Solutions of Mean-Field Equations). For
b(x) = Î»xÎ± with Î» > 0 and Î± > 1, then with Ïc and Î²c defined in Lemma 7,
â€” If Ï < Ïc , there is no non-trivial invariant distribution
â€” If Ï > Ïc , there exist at least two non-trivial invariant distributions with
density defined by Relation (38) for Î² âˆˆ {Î²âˆ’ , Î²+ } with Î²âˆ’ < Î²c < Î²+ which
are solutions of the equation Ï(Î²Â± ) = Ï.
â€” If Ï = Ïc , there exists a unique non-trivial invariant distribution corresponding to Î² = Î²c .

34

PH. ROBERT AND J. TOUBOUL

Numerical computations of the fixed point equation (41) show that when Ï > Ïc ,
there exists exactly two non-trivial invariant distributions. In order to prove this
fact, we would need to show that the function Î² 7â†’ Ï(Î²) has a unique minimum on
R+ , i.e. that it is strictly decreasing on [0, Î²c ] and increasing on [Î²c , âˆ], or in other
words that there exists a unique Î² âˆˆ R+ such that Ï0 (Î²) = 0. These conditions
yield the implicit equation:
Î¨(xâˆ— ) = âˆ’2xâˆ— Î¨(xâˆ— )
with xâˆ— = (Î² âˆ— )2 Ï(Î² âˆ— ). Showing analytically uniqueness of the solutions of this
implicit equations is actually very complicated even for simple firing functions such
as b(x) = x2 . Extensive numerical simulations tend to show however that this is
the case.
averaged voltage
separatrix
theory

4
3.5
3
2.5
2
1.5
1
0.5
0
0

0.5

1

1.5

2

2.5

3

Figure 2. Quadratic firing function b(x) = x2 : numerical simulations of a 2 000 neurons network for different values of E(V ), 30
initial conditions and 1 000 realizations. Each blue dot corresponds
an average firing rate for a given initial condition (see text). The
stationary solution Î´0 persists for large values of E(V ) and the additional non-trivial invariant distribution appears when increasing
E(V ). Red dots correspond to the separatrix between initial conditions converging to the trivial solution and those going to the
sustained state. The purple line is the numerical solution of the
mean-field equation (39), and shows a good agreement with the
non-trivial and separatrix points.
Let us for instance discuss in more detail the case b(x) = x2 (see Fig. 2). We
have shown that, depending of E(V ), either the trivial distribution is the unique
stationary distribution, or there exists two additional non-trivial equilibrium solutions. These solutions can be found numerically and are depicted in Fig. 2 (purple
line). A phase transition arises, at a specific value of E(V ), in which two additional

RANDOM INTEGRATE-AND-FIRE NEURONS

35

solutions emerge. Similarly to what we did for the linear network, we extensively
simulated the network in order to characterize equilibria of the system. In contrast to the linear case, all trajectories do not go to the same state, and we do
expect to find certain initial conditions converging towards the trivial solution and
some towards the non-trivial equilibrium. In one-dimensional dynamical systems
presenting multi-stability, one typically has, between two stable equilibria, one unstable equilibrium, which acts as a separatrix, in the sense that trajectories with
an initial condition on one side of the unstable equilibrium converge to the stable
equilibrium on that same side. Here, the system is much more complex, and in
particular it is a priori infinite-dimensional. However, one may conjecture that the
limiting dynamics collapses on a smaller dimensional system. In our simulations,
we considered a network made of 2 000 neurons, in the simple case in which the
initial conditions of neurons are uniformly distributed around a value v0 , with a
fixed standard deviation 0.2. For 30 fixed values of v0 , we simulated 1 000 times the
network. We observed that, except for values of E(V ) close from the phase transition, that trajectories converge either towards the trivial equilibrium or towards
a state with non-zero voltage. The value of v0 at which a switch occurs between
those trajectories going to the trivial state and those going to the non-zero state has
been recorded. The average value is depicted in red in Fig 2 and shows very good
agreement with the middle stationary solution of the mean-field equation. The end
state is characterized by one quantity per initial condition, which corresponds to
the average in time (in the time interval [90, 100]), over all neurons and over the
different simulations, of the voltage variable. One point is therefore obtained for
each of the 30 initial conditions, and is depicted as a blue circle in Fig 2. The
dynamics of the system, constrained to these precise initial conditions, is therefore
highly similar to a one-dimensional dynamical system. Note that it is not rare
that solutions of McKean-Vlasov systems reduce to low-dimensional systems. For
instance in a model arising in neuroscience, it was shown in Touboul [34, 37] that
a specific, rate-based neuron model reduces exactly, in the mean-field limit, to a
one-dimensional dynamical system. Although dynamics of the spiking neuron is
much more complex, we conjecture that the dynamics of the firing rate is much
simpler and characterized deterministically by a few statistical quantities.
Appendix
Some Technical Results.
Lemma 8. We assume that b is such that there exists Î³ > 0 and c > 0 such that
(42)

b0 (x) â‰¤ Î³b(x) + c

Then, for any Îµ > 0 and p âˆˆ [1, 3 + Îµ], there exist a constant Î³1 < (3 + 2Îµ)Î³, c1 > 0
and a value Î·b > 0 such for any a âˆˆ (0, Î·b ) and x â‰¥ 0,
(43)

bp (x + a) âˆ’ bp (x) â‰¤ a (Î³1 bp (x) + c1 ) .

Proof. Let us start by noting that the inequality is trivial for b bounded. We will
therefore assume in the rest of the proof that b diverges at infinity. We also remark
that for any 1 â‰¤ p < 3 + Îµ, the map bp satisfies an inequality of type (42) where Î³
is multiplied by p. Indeed, for any Î´ > 0, we can find cÎ´ > 0 such that:
dbp (x)
â‰¤ pÎ³bp (x) + pcbpâˆ’1 (x) â‰¤ (pÎ³ + Î´)bp (x) + cÎ´ .
dx

36

PH. ROBERT AND J. TOUBOUL

We will therefore demonstrate without loss of generality the proposition for p = 1,
and i.e. control the modulus of continuity of b under assumption (42). For an
arbitrary x0 > 0 and any x â‰¥ x0 , we have:
Z x+a 0

b (y)
b(x + a)
= exp
dy â‰¤ eaÎ³Ìƒ ,
b(x)
b(y)
x
with Î³Ìƒ = Î³ + c/b(x0 ). We conclude that for x â‰¥ x0 ,
b(x + a) âˆ’ b(x) â‰¤ (eaÎ³Ìƒ âˆ’ 1)b(x).
The map a 7â†’ (eaÎ³Ìƒ âˆ’ 1)/a is smooth, non-decreasing and tends to Î³Ìƒ at a = 0, which
can be made arbitrarily close from Î³ for sufficiently large x0 (since b is unbounded).
Therefore, there exists x0 > 0 and Î· > 0 such that for any x â‰¥ x0 and a âˆˆ [0, Î·],
b(x + a) âˆ’ b(x) â‰¤ a(1 + Îµ)Î³b(x).
Denoting c1 the Lipschitz constant of b over the interval [0, x0 + Î·], we readily
obtain (43) with Î³1 = Î³(1 + Îµ).

Another elementary property that is useful in our developments is the following:
Proposition 12. If x(t) is a non-negative caÌ€dlaÌ€g function on R+ and Îº > Î´ > 0
such that, for A, B âˆˆ R,
Z t
Z t
x(t) â‰¤ B + x(s) âˆ’
x(u)Îº du + A
x(u)Î´ du,
s

s

holds for any 0 â‰¤ s â‰¤ t, then x(t) is uniformly bounded on any bounded time
intervals.
Moreover, if (x(t)) is C 1 on R+ and B = 0, we have a uniform bound for all
times:
sup x(t) â‰¤ C0 < +âˆ,
tâ‰¥0

where C0 = x(0) âˆ§ AÎºâˆ’Î´ .
The proof is elementary once noted that the map x 7â†’ âˆ’xÎº + AxÎ´ is upperbounded by a finite value M > 0 and is strictly negative for any x > AÎºâˆ’Î´ . The
upperbound readily implies that x(t) â‰¤ x0 + B + M t. For x continuously differentiable and B = 0, the negativity of the integrand for x > AÎºâˆ’Î´ ensures that no
trajectory exceeds C0 = x(0) âˆ§ AÎºâˆ’Î´ .
Poisson Processes. The third elementary technical result used is related to the
martingales associated to marked Poisson processes.
Proposition 13. If N is a Poisson process on R3+ with intensity measure du âŠ—
V (dz) âŠ— dt, f is a continuous function on R3+ and (Y (t) = (Y1 (t), Y2 (t)) is a caÌ€dlaÌ€g
adapted processes then the process (M (t)) defined by
!
Z t Z


f (Y (sâˆ’), z) 1{0â‰¤uâ‰¤Y1 (sâˆ’)} N (du, dz, ds) âˆ’ Y1 (s) ds V (dz)
s=0

R2+

is a local martingale whose previsible increasing process is given by
!
Z t Z
2
(hM i (t)) =
ds
dV (dz)f (Y (s), z) Y1 (s) .
0

R+

See Rogers and Williams [29] and Appendix B of Robert [28] for example.

RANDOM INTEGRATE-AND-FIRE NEURONS

37

Uniqueness. In the main text, we have shown tightness of the sequence of empirical measures, ensuring that the sequence is relatively compact. Moreover, we
showed that the possible limits are time-dependent measures Î›(t) that satisfy, for
all f âˆˆ C 1 (R), Equation (35) that we write here as:
Z th
hÎ›(u), âˆ’xf 0 (x) + E(V ) hÎ›(u), bi f 0 (x)i
(44) hÎ›(t), f i = hÎ›(0), f i +
0
i
âˆ’ hÎ›(u), (f (x) âˆ’ f (0)) b(x)i du.
In the above notations, x is a generic symbolic variable, which we use for simplicity
of notations, with the convention hÎ›(t), f (x)i = hÎ›(t), f i. We know that the law of
the solution of the mean-field McKean-Vlasov equation (3) satisfies this system. We
aim at showing there is a unique positive Radon measure such that the nonlinear
equation (44) holds. We first remark that the differential equation conserves the
total mass hÎ›(t), 1i = hÎ›(0), 1i. We are therefore searching for Î› a probability
measure satisfying the nonlinear equation (44). The proof of uniqueness uses the
following properties:
Lemma 9. For any initial probability measure Î›(0) of R+ with bounded support,
if Î›(t) is a solution of Equation (44), there exists C and K such that
(1) sup hÎ›(t), bi â‰¤ C < âˆ.
tâ‰¥0

(2) Î›(t) has its support in [0, K] for all t â‰¥ 0.
Proof. The proof of (i) is similar to the analogous property shown on the possible
solutions of the McKean-Vlasov equation. Denoting B(t) = hÎ›(t), bi and using the
inequality b0 (x) < Î³b(x) + c, we have:
Z t
B(t) â‰¤ B(0) +
hÎ›(u), E(V )B(u)b0 (x) âˆ’ b(x)(b(x) âˆ’ b(0))i du
0
Z t



â‰¤ B(0) +
Î³E(V )B(u)2 + (c + b(0))B(u) âˆ’ Î›(u), b(x)2 du
0
Z t
â‰¤ B(0) +
(Î³E(V ) âˆ’ 1)B(u)2 + (c + b(0))B(u) du
0

and we conclude using proposition 12.
We now prove that any solution Î›(t) to Equation (44) has a uniformly bounded
support. Let us assume that the support of Î›(0) is contained in the interval [0, K0 ]
and pick f a continuously differentiable and non-decreasing function such that
(
f (x) = 0 x < K
f (x) > 0 x > K
with K = max(K0 , C E(V )) with C an upperbound of suptâ‰¥0 hÎ›(t), bi. Applying
equation (44) to f and using the fact that hÎ›(t), bi < C, f b â‰¥ 0 and f 0 â‰¥ 0, we
obtain the inequality:
Z t
0 â‰¤ hÎ›(t), f i â‰¤
hÎ›(u), (âˆ’x + E(V )C)f 0 (x)i du â‰¤ 0,
0

hence hÎ›(t), f i = 0 for all t â‰¥ 0, implying that the support of Î›(t) is contained
within the compact set [0, K].


38

PH. ROBERT AND J. TOUBOUL

With these a priori estimates on Î› in hand, we can now show the uniqueness of
possible solutions to the mean-field equation. For two probability measures Î»1 and
Î»2 , we define the distance:
kÎ»1 âˆ’ Î»2 kS = sup {hÎ»1 âˆ’ Î»2 , f i : f âˆˆ S} ,
with

n
o
S = f âˆˆ C 1 (R+ ) : kf kâˆ âˆ¨ kf 0 kâˆ â‰¤ 1

and note that the subset of functions of S with bounded support is dense in the set
of continuous functions with bounded support.
Proposition 14. Let Î›(0) be a probability measure with bounded support, then
Equation (44) has a unique solution with initial condition Î›(0).
Proof. We show that kÎ›1 (t) âˆ’ Î›2 (t)kS = 0 for all times. Indeed, for any f âˆˆ S, we
have, denoting âˆ†(t) = Î›1 (t) âˆ’ Î›2 (t),
Z t
hâˆ†(t), f i =
hâˆ†(u), âˆ’xf 0 (x)âˆ’(f (x)âˆ’f (0))b(x)+ hÎ›1 (u),f 0 i b+ hÎ›2 (u),bi f 0 i du
0

and therefore using the fact that âˆ† has a support included in the compact [0, K],
we have:
Z t
(45)
| hâˆ†(t), f i | â‰¤ Î“(f, K)
kâˆ†(u)kS du
0

with
Î“(f, K) =

sup
xâ‰¤K,uâ‰¥0





âˆ’xf 0 (x)âˆ’(f (x)âˆ’f (0))b(x)+ hÎ›1 (u),f 0 i b(x)+ hÎ›2 (u),bi f 0 (x)

â‰¤ K := K + C + 3b(K)
We therefore have for all f âˆˆ S the inequality:
Z t
| hâˆ†(t), f i | â‰¤ K
kâˆ†(u)kS du,
0

which is therefore also valid for the norm of âˆ†:
Z t
kâˆ†(t)kS â‰¤ K
kâˆ†(u)kS du.
0

We conclude, by immediate recursion, that:
Z t
K n tn
sup kâˆ†(s)kS ,
sup kâˆ†(s)kS â‰¤ K
sup kâˆ†(s)kS du â‰¤
n! sâ‰¤t
sâ‰¤t
0 sâ‰¤u
hence kâˆ†(t)kS = 0 for all t â‰¥ 0.



Simulation Algorithms. This appendix describes the simulation algorithms used
to obtain our plots for linear and quadratic rate functions in section 7. We used two
distinct algorithms: an exact simulation algorithm for the simulation of the extinction time, and for the sake of computational efficiency an approximate algorithm
for large networks.
The algorithm we used in order to perform efficient simulations for large networks
implements the evolution of the process at discrete times tk = kÎ´t with Î´t a small
time step. In each time interval, we compute the probability that a spike occurs
within the interval. We then draw a Bernoulli random variable with this probability,
and update the network state accordingly.

RANDOM INTEGRATE-AND-FIRE NEURONS

39

This approximate dynamics allows to perform fast simulations and therefore
to reach very large network size. However, computing the extinction time of the
network is much more delicate. To this end, we performed, for small network
sizes, exact simulations of the jump process in the case of the linear firing function
b(x) = Î» x. In the specific model we treat here, the particularly simple form of the
dynamics of the variables Xi (t) between spikes and the simplicity of the firing map
b allows to derive the cumulative density function of the spikes:
P(Ï„i â‰¥ t) = exp(âˆ’Î»Xi (1 âˆ’ exp(âˆ’t)))
provided that Xi (0) = Xi . From this expression, one obtains the probability that
neuron i stops firing pi = exp(âˆ’Î»Xi ), and also the probability of firing at time t
provided that the neuron does not stop firing. Therefore, although we deal with
state-dependent Poisson processes, these formulae allow to simulate exactly the
process and the extinction time, reached when all neurons stop firing.
References
1. SÃ¸ren Asmussen, Applied probability and queues, John Wiley & Sons Ltd., Chichester, 1987.
2. N. Brunel, Dynamics of sparsely connected networks of excitatory and inhibitory spiking
neurons, Journal of Computational Neuroscience 8 (2000), 183â€“208.
3. Nicolas Brunel, Dynamics of networks of randomly connected excitatory and inhibitory spiking
neurons, Journal of Physiology-Paris 94 (2000), no. 5â€“6, 445 â€“ 463.
4. AN Burkitt, A review of the integrate-and-fire neuron model: I. homogeneous synaptic input,
Biological cybernetics 95 (2006), no. 1, 1â€“19.
, A review of the integrate-and-fire neuron model: Ii. inhomogeneous synaptic input
5.
and network properties, Biological cybernetics 95 (2006), no. 2, 97â€“112.
6. Maria CaÌ€ceres, JoseÌ Carrillo, and BenoÄ±Ì‚t Perthame, Analysis of nonlinear noisy integrate &
fire neuron models: blow-up and steady states, The Journal of Mathematical Neuroscience
(JMN) 1 (2011), no. 1 (English).
7. Maria J Caceres and BenoÄ±Ì‚t Perthame, Beyond blow-up in excitatory integrate and fire neuronal networks: refractory period and spontaneous activity, Journal of theoretical biology 350
(2014), 81â€“89.
8. EJ Chichilnisky, A simple white noise analysis of neuronal light responses, Network: Computation in Neural Systems 12 (2001), no. 2, 199â€“213.
9. R. M. Corless, G. H. Gonnet, D. E. G. Hare, D. J. Jeffrey, and D. E. Knuth, On the Lambert
W function, Advances in Computational Mathematics 5 (1996), no. 4, 329â€“359. MR 1414285
(98j:33015)
10. Donald A. Dawson, Measure-valued Markov processes, EÌcole dâ€™EÌteÌ de ProbabiliteÌs de SaintFlour XXIâ€”1991, Lecture Notes in Math., vol. 1541, Springer, Berlin, 1993, pp. 1â€“260.
11. A. De Masi, A. Galves, E. LoÌˆcherbach, and E. Presutti, Hydrodynamic limit for interacting
neurons, Arxiv preprint arXiv:1401.4264, February 2014.
12. F. Delarue, J. Inglis, S. Rubenthaler, and E. TanreÌ, Global solvability of a networked integrateand-fire model of mckean-vlasov type, Annals of Applied Probability (2015), To Appear.
13. FrancÌ§ois Delarue, James Inglis, Sylvain Rubenthaler, and Etienne TanreÌ, Particle systems with
a singular mean-field self-excitation. Application to neuronal networks., Stochastic Processes
and Applications (2015), 40.
14. Georges L. Gerstein and Benoit Mandelbrot, Random walk models for the spike activity of a
single neuron, Biophysical Journal 4 (1964), 41â€“68.
15. RZ Hasâ€™minskii, Stochastic stability of differential equations, Kluwer Academic Pub, 1980.
16. James Inglis and Denis Talay, Mean-field limit of a stochastic particle system smoothly interacting through threshold hitting-times and applications to neural networks with dendritic
component, arXiv preprint arXiv:1409.8221 (2014).
17. B. W. Knight, Dynamics of encoding in a population of neurons, J. Gen. Physiol. 59 (1972),
734â€“766.
18. L Lapicque, Recherches quantitatifs sur lâ€™excitation des nerfs traitee comme une polarisation,
J. Physiol. Paris 9 (1907), 620â€“635.

40

PH. ROBERT AND J. TOUBOUL

19. David A. Levin, Yuval Peres, and Elizabeth L. Wilmer, Markov chains and mixing times,
American Mathematical Society, Providence, RI, 2009.
20. R.M. Loynes, The stability of queues with non independent inter-arrival and service times,
Proc. Cambridge Ph. Soc. 58 (1962), 497â€“520.
21. Fournier N. and E. LoÌˆcherbach, On a toy model of interacting neurons, Arxiv preprint
arXiv:1410.3263, October 2014.
22. Esa Nummelin, General irreducible Markov chains and nonnegative operators, Cambridge
University Press, Cambridge, 1984.
23. Khashayar Pakdaman, BenoÄ±Ì‚t Perthame, and Delphine Salort, Dynamics of a structured neuron population, Nonlinearity 23 (2010), no. 1, 55.
24.
, Relaxation and self-sustained oscillations in the time elapsed neuron network model,
SIAM Journal on Applied Mathematics 73 (2013), no. 3, 1260â€“1279.
25. Khashayar Pakdaman, BenoÄ±Ì‚t Perthame, Delphine Salort, et al., Adaptation and fatigue
model for neuron networks and large time asymptotics in a nonlinear fragmentation equation,
(2012).
26. Jonathan W Pillow, Liam Paninski, Valerie J Uzzell, Eero P Simoncelli, and EJ Chichilnisky,
Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model,
The Journal of Neuroscience 25 (2005), no. 47, 11003â€“11013.
27. Jonathan W Pillow, Jonathon Shlens, Liam Paninski, Alexander Sher, Alan M Litke,
EJ Chichilnisky, and Eero P Simoncelli, Spatio-temporal correlations and visual signalling
in a complete neuronal population, Nature 454 (2008), no. 7207, 995â€“999.
28. Philippe Robert, Stochastic networks and queues, Stochastic Modelling and Applied Probability Series, vol. 52, Springer, New-York, June 2003.
29. L. C. G. Rogers and David Williams, Diffusions, Markov processes, and martingales. Vol. 2:
ItoÌ‚ calculus, John Wiley & Sons Inc., New York, 1987.
30. ET Rolls and G Deco, The noisy brain: stochastic dynamics as a principle of brain function,
Oxford university press, 2010.
31. M. Scheutzow, Periodic behavior of the stochastic brusselator in the mean-field limit, Probability Theory and Related Fields 72 (1986), 425â€“462.
32. R. B. Stein, A theoretical analysis of neuronal variability, Biophysics Journal 5 (1965), 173â€“
194.
33. A.S. Sznitman, Topics in propagation of chaos, EÌcole dâ€™EÌteÌ de ProbabiliteÌs de Saint-Flour
XIX â€” 1989, Lecture Notes in Maths, vol. 1464, Springer-Verlag, 1991, pp. 167â€“243.
34. Jonathan Touboul, Mean-field equations for stochastic firing-rate neural fields with delays:
derivation and noise-induced transitions, Physica D: Nonlinear Phenomena 241 (2012),
no. 15, 1223â€”1244.
35. Jonathan Touboul, The propagation of chaos in neural fields, Annals of Applied Probability
24 (2014), no. 3, 1298â€“1328.
, Spatially extended networks with singular multi-scale connectivity patterns, Journal
36.
of Statistical Physics 156 (2014), no. 3, 546â€“573 (English).
37. Jonathan Touboul, Geoffroy Hermann, and Olivier Faugeras, Noise-induced behaviors in neural mean field dynamics, SIAM Journal on Applied Dynamical Systems 11 (2012), no. 1,
49â€“81.
E-mail address: Philippe.Robert@inria.fr
(Ph. Robert) INRIA Parisâ€”Rocquencourt, Domaine de Voluceau, 78153 Le Chesnay,
France.
URL: http://team.inria.fr/rap/robert
E-mail address: jonathan.touboul@college-de-france.fr
(J. Touboul) Mathematical Neuroscience Team, CIRB - Collge de France and INRIA
Paris-Rocquencourt, 11, Place Marcelin Berthelot 75005 Paris, FRANCE
URL: http://mathematical-neuroscience.net/team/jonathan/

