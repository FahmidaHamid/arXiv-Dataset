Adaptive Observers and Parameter Estimation for a Class
of Systems Nonlinear in the Parameters

arXiv:0903.2361v7 [math.OC] 21 Jun 2013

Ivan Y. Tyukin a Erik Steur b Henk Nijmeijer c Cees van Leeuwen b
a
b

Dept. of Mathematics, University of Leicester, Leicester, LE1 7RH, UK (Tel: +44-116-252-5106; I.Tyukin@le.ac.uk)

Laboratory for Perceptual Dynamics, KU Leuven, Tiensestraat 102, 3000 Leuven, Belgium (erik.steur@ppw.kuleuven.be,
cees.vanleeuwen@ppw.kuleuven.be)
c

Dept. of Mechanical Engineering, Eindhoven University of Technology, P.O. Box 513 5600 MB, Eindhoven, The
Netherlands, (h.nijmeijer@tue.nl)

Abstract
We consider the problem of asymptotic reconstruction of the state and parameter values in systems of ordinary differential
equations. A solution to this problem is proposed for a class of systems of which the unknowns are allowed to be nonlinearly
parameterized functions of state and time. Reconstruction of state and parameter values is based on the concepts of weakly
attracting sets and non-uniform convergence and is subjected to persistency of excitation conditions. In the absence of
nonlinear parametrization the resulting observers reduce to standard estimation schemes. In this respect, the proposed method
constitutes a generalization of the conventional canonical adaptive observer design.
Key words: Adaptive observers, nonlinear parametrization, weakly attracting sets

1

Introduction

We consider observer-based methods for state and parameter estimation in nonlinear dynamical systems.
These methods are effective as long as the original system has, or can be transformed into, one of the canonical adaptive observer forms [5], [25], [6]. Their common
characteristic is linearity in the unknown parameters.
For this class of systems, subject to persistency of excitation conditions, reconstruction of state and parameter
vectors can be achieved exponentially fast.
There are systems, however, in which the unknown parameters enter the model nonlinearly. These systems
constitute a remarkably wide class including models in
chemical kinetics [13], [4], biology and neuroscience [18].
Whereas the problem of state estimation can be solved
for a large class of nonlinearly parameterized systems
[28], observer-based parameter reconstruction is often
confined to systems with monotone [39], [22] or one-toone parameterizations [14], [15], [16], [12].
⋆ This paper was not presented at any IFAC meeting. Cees
van Leeuwen is supported by an Odysseus grant from the
Flemish Science Organization FWO. Corresponding author
I. Yu. Tyukin. Tel. +44-116-2525106.

Preprint submitted to Automatica

Several authors have recently advanced strategies for
overcoming these limitations. In [19], combining interval
analysis with multiple shooting methods is proposed to
tackle the state and parameter reconstruction problem.
Another interesting approach is presented in [1]: the original continuous-time model is replaced with a discretetime approximation. Measured variables are then considered as known functions of unknown parameters and
initial conditions, of which the estimates can be found
by off-line nonlinear optimization routines (see also [36],
[2] where optimization techniques with moving horizon
are discussed). These approaches offer obvious advantages, e.g. the availability of a vast library of numerical methods for solving general nonlinear optimization
problems. Nevertheless, these methods run into restrictions too. Exhaustive search for a global minimum may
become intractable for dimensions higher than 1 or 2. On
the other hand, if conventional polynomial-complexity
algorithms are used then the possibility arises that the
algorithm will converge to a local minimum.
In this paper we explore further possibilities of developing adaptive observers for systems which are both linearly and nonlinearly parameterized. The parametrization is not required to be invertible or monotone. Our approach combines the advantages of the existing schemes,

June 24, 2013

• A solution of ẋ = f (t, x, θ, u(t)), f : R × Rn × Rm ×
R → Rn , θ ∈ Rm , u : R → R passing through x0 ∈
Rn at t = t0 is denoted by x(t, t0 , x0 , θ, [u]). In cases
when u and/or θ, x0 , t0 are clearly determined by the
context a more compact notation, x(t, t0 , x0 , θ) (or
x(t, t0 , x0 ), x(t), x respectively), is used.
• The symbol K denotes the class of all strictly increasing continuous functions κ : R≥0 → R≥0 such that
κ(0) = 0; the symbol K∞ denotes the class of all functions κ ∈ K such that lims→∞ κ(s) = ∞.
• Let ǫ ∈ R≥0 , then kxkǫ stands for: kxk − ǫ if kxk > ǫ,
and 0 otherwise.
• Finally, for λ ∈ Rp and θ ∈ Rm , the notation (λ, θ)
stands for col(λ1 , . . . , λp , θ1 , . . . , θm ).

in being capable of ensuring exponentially fast convergence with the flexibility of explorative behavior, a behavior inherent to algorithms for solving genuine nonlinear optimization problems. Inference of the values of
state and a part of the parameter vector of these systems is achieved by employing exponentially fast converging estimators. Estimation of the values of the remaining parameters is based on an explorative search
procedure. Since exploration is restricted to a subset of
the unknown parameters, the proposed strategy reduces
the overall computational costs, as compared to when
full-scale search-based optimization had been invoked.
The resulting observer can be imagined as a system comprising of an exponentially stable part coupled with an
explorative one. Systems of this type have previously
been used in adaptive control [34], [17], [29], [30]. Here
we demonstrate that these classical ideas can be applied to the problem of adaptive observer design for systems which are nonlinearly dependent on parameters.
We show that, subject to a condition of persistent excitation, it is possible to reconstruct state and parameters
of a reasonably broad subclass of these systems.

3
3.1

Adaptive observer canonical form

Throughout the paper we will focus exclusively on the
class of systems that are forward-complete:
Definition 1 Let Lu [t0 , ∞] be a subspace of L∞ [t0 , ∞].
A single-input single-output system described by ẋ =
f (t, x, u(t)), f : R × Rn × R → Rn , y = h(t, x), h :
R × Rn → R, u ∈ Lu [t0 , ∞], where u is the input, and y
is the output, is called forward-complete (with respect to
Lu ) iff for any t0 ∈ R, x0 ∈ Rn , and u ∈ Lu [t0 , ∞] the
solution x(t, t0 , x0 , [u]) exists and is defined for all t ≥ t0 .

The paper is organized as follows. Notational agreements
are introduced in Section 2. Section 3 provides the formal statement of the problem, Sections 4, 5 contain main
results of the article, Section 6 discusses possible generalizations, Section 7 contains illustrative examples, and
Section 8 concludes the paper. Proofs of auxiliary results
are presented in the Appendix.
2

Preliminaries and Problem formulation

Let Lu [t0 , ∞] = L∞ [t0 , ∞] ∩ C 0 [t0 , ∞], and consider a
forward-complete single-input single-output system; let
x ∈ Rn be its state, y : R → R be the measured output,
and u : R → R, u ∈ Lu be the input. We recall that a
system is in the adaptive observer canonical form if it is
governed by the following set of equations

Notation

The following notational conventions are used throughout the paper:
• R denotes the set of real numbers, R>a = {x ∈ R | x >
a}, and R≥a = {x ∈ R | x ≥ a};
• Z denotes the set of integers, and N stands for the set
of positive integers;
• the Euclidean norm of x ∈ Rn is denoted by kxk,
kxk2 = xT x, where T stands for transposition;
• the space of n× n matrices with real entries is denoted
by Rn×n ; let P ∈ Rn×n , then P > 0 (P ≥ 0) indicates
that P is symmetric and positive (semi-)definite; In
denotes the n × n identity matrix.
• by Ln∞ [t0 , T ], t0 ∈ R, T ∈ R, T ≥ t0 we denote
the space of all functions f : [t0 , T ] → Rn such
that kf k∞,[t0 ,T ] = ess sup{kf (t)k, t ∈ [t0 , T ]} < ∞;
kf k∞,[t0 ,T ] stands for the Ln∞ [t0 , T ] norm of f (t); if
the function f is defined on a set larger than [t0 , T ]
then notation kf k∞,[t0 ,T ] applies to the restriction of
f on [t0 , T ];
• C r denotes the space of continuous functions that are
at least r times differentiable;
• Let A be a subset of Rn , then for all x ∈ Rn , we define
dist(A, x) = inf q∈A kx − qk;

ẋ = Ax + BφT (t, y)θ + g(t, y, u(t))
y = CT x, x(t0 ) = x0 , x0 ∈ Rn ,
In−1

(1)

!

, a ∈ Rn , B ∈ Rn , C ∈ Rn , B =
0
col(1, b1 , . . . , bn−1 ), C = col(1, 0, . . . , 0), the functions
φ : R × R → Rm , g : R × R × R → Rn , φ, g ∈ C 0
are known, and θ ∈ Rm is the vector of the unknown
parameters. The triplet A, B, C is supposed to satisfy
where A =



a

P(A + ℓ CT ) + (A + ℓ CT )T P ≤ −Q
PB = C.

(2)

for some ℓ ∈ Rn and P > 0, Q > 0. Although condition
(2) may appear restrictive, it has been shown in [26]
that subject to the very natural constraint that the pair
A, C is observable, there is a time-varying parameterdependent coordinate transformation such that in new

2

3.2

coordinates the system is still of the form (1) and satisfies
condition (2). If requirement (2) holds then the system

We begin with the following class of forward-complete
single-input-single-output nonlinear systems:

x̂˙ =Ax̂ + ℓ (CT x̂ − y(t)) + BφT (t, y(t))θ̂
+ g(t, y(t), u(t))
˙
θ̂ = − γ(CT x̂ − y(t))φ(t, y(t)), γ ∈ R>0 ,

(3)

ẋ = Ax + BϕT (t, λ, y)θ + g(t, λ, y, u(t)) + ξ(t),
y = CT x, x(t0 ) = x0 , x0 ∈ Rn ,

where x̂(t) ∈ Rn , θ̂(t) ∈ Rm , is an adaptive observer for
(1) (cf. [25], [5]) provided that the restriction of φ(·, y(·))
on R≥t0 is persistently exciting:

t

β(τ )β T (τ )dτ ≥ µIm , ∀ t ≥ t0 .

(4)

In (7), x = col (x1 , x2 , . . . , xn ) ∈ Rn is the state vector, y
is the measured output, the input u is a known function,
and ξ ∈ C 0 : R → Rn is an unknown yet bounded
continuous function:

The fact that (3) is an adaptive observer for (1) is based
on a well-known result on the exponential stability of
the following class of linear time-varying systems 1
ė = A(t)e, A(t) =

A + ℓ CT Bβ T (t)
−β(t)CT

0

!

.

(7)

where A ∈ Rn×n , and B, C ∈ Rn are defined as in (1);
ϕ : R × Rp × R → Rm , g : R × Rp × R × R → Rn , are
known continuous functions, λ = col(λ1 , . . . , λp ) ∈ Rp ,
θ = col(θ1 , . . . , θm ) ∈ Rm are unknown parameters, and
u ∈ Lu ∩ C 1 , is the input. We assume that the values of
λ, θ belong to the hypercubes Ωλ ⊂ Rp , Ωθ ⊂ Rm with
known bounds: θi ∈ [θi,min , θi,max ], λj ∈ [λj,min , λj,max ],
and that y(t) ∈ Dy , u(t) ∈ Du , Dy , Du ⊂ R for t ≥ t0 .

Definition 2 A function β : R≥t0 → Rm is said to be
persistently exciting if there exist L, µ ∈ R>0 :
R t+L

Systems considered in this article

∃ ∆ξ ∈ R≥0 : kξ(t)k ≤ ∆ξ ∀ t,

(5)

(8)

representing some unmodeled dynamics (e.g. noise). The
system’s state x is not measured; only the values of the
input u(t) and the output y(t) = x1 (t), t ≥ t0 in (7) are
accessible over any time interval [t0 , t] that belongs to
the history of the system.

The result is provided in Theorem 3 below (see e.g. [23]
for a proof).
Theorem 3 Consider system (5). Suppose that condition (2) holds for certain ℓ ∈ Rn , P > 0, Q > 0, the
function β(t) is persistently exciting, and

For the time being we suppose that matrix A and vectors
B, C in (7) satisfy Assumption 3.1 below.

∃ M ∈ R>0 : max{kβ(t)k, kβ̇(t)k} ≤ M ∀ t ≥ t0 . (6)

Assumption 3.1 The triple A, B, C is known, and
there exist (and are known) a vector ℓ and matrices
P, Q > 0 such that condition (2) holds.

Let Φ(t, t0 ), Φ(t0 , t0 ) = In+m , be the fundamental solution matrix of (5). Then there exist ρ, D ∈ R>0 such that
kΦ(t2 , t1 )pk ≤ De−ρ(t2 −t1 ) kpk for all t2 ≥ t1 ≥ t0 and
p ∈ Rn+m .

Note that Assumption 3.1 implies that the vector B =
col(1, b1 , . . . , bn−1 ) in (7) is such that the polynomial
sn−1 + b1 sn−2 + · · · + bn−1 is Hurwitz. At first, Assumption 3.1 may seem restrictive. In Section 6 we lift this
restriction by showing that the results presented for (7)
can be generalized to systems

The parameters ρ and D can be expressed explicitly as
functions of M , µ, L, and A, B, C, ℓ [23]. By letting e =
col(x̂ − x, θ̂ − θ) and taking (1), (3) into account one can
confirm that the system-observer equations are of form
(5). Thus, subject to persistency of excitation of the restriction of φT (·, y(·)) on R≥t0 , limt→∞ x̂(t, t0 , x̂0 , θ̂0 ) −
x(t, t0 , x0 , θ) = 0, limt→∞ θ̂(t, t0 , x̂0 , θ̂0 ) = θ along the
solutions of (1), (3), and the convergence is exponential.
The problem, however, is that if some parameters enter
the equations nonlinearly then this creates an obstacle
for the explicit use of Theorem 3 and, consequently, observer (3). In the next sections we present and analyze a
class of systems nonlinear in the parameters which can
be thought of as an immediate generalization of (1).

ẋ = Ax + Ψ(t, λ, y)θ + g(t, λ, y, u(t)) + ξ(t),
y = CT x, C = col (1, 0, . . . , 0) ,

(9)

in which the matrix A ∈ Rn×n may be unknown but it
is known that the pair A, C is observable, the function
Ψ : R × Rp × R → Rn×m , Ψ ∈ C 1 , is Lipschitz in
λ, and g(·, λ, y(·), u(·)), ġ(·, λ, y(·), u(·)), Ψ(·, λ, y(·)),
Ψ̇(·, λ, y(·)) are bounded for all λ ∈ Ωλ on [t0 , ∞).
With regards to the functions ϕ and g in (7) the following additional technical assumptions are made:

1

In the context of adaptive observer design for (1) the function β in (5) is defined as β(t) = φ(t, y(t)), t ≥ t0 .

3

Table 1
Examples of physical systems in the form of (7), (9)
Physical system
1. Reactant dynamics in a reactor [35]
2. Magnetic bearings [21]

Model
ẋ =

F
V

(x0 − x) − kref e

−E
R



1 − 1
Tref
T (t)



x,

y=x
ẍ + J −1 d(t) = b



2
q2
(I2 ,a,x)
(x+a)2

−

2
q1
(I1 ,a,x)
(x−a)2



y = x, |x| < a − ε, a, ε ∈ R>0 ,
d, d˙ are bounded

3. Action potentials in a cell [37]

ẋ1 = − τxm1 +
ẋ2 = − xτs2 +

Af
τm

tanh



σf
Af


x1 −

x2
τm

σs
x
τs 1

y = x1

bounded. In the second model the pair A, C is observable, and ϕ, ϕ̇ are bounded if y, magnetic fluxes, expressed by q1 ,q2 , and q̇1 , q̇2 are bounded. This is achievable via external controls [21], at least for small parametric mismatches and d. In the third model the pair
A, C is observable, and boundedness of y, ẏ, Ψ, Ψ̇ is
consistent with the physics of the system.

Assumption 3.2 The functions ϕ(·, λ, ·), g(·, λ, ·, ·) in
(7) are bounded and differentiable in R≥t0 × Dy and
R≥t0 × Dy × Du respectively, and Lipschitz in λ. That
is, there exist Dϕ , Dg , Bϕ , Bg ∈ R≥0 such that for all
t ∈ R≥t0 , y ∈ Dy , u ∈ Du , λ′ , λ′′ ∈ Ωλ
kϕ(t, λ′ , y) − ϕ(t, λ′′ , y)k ≤ Dϕ kλ′ − λ′′ k,
kg(t, λ′ , y, u) − g(t, λ′′ , y, u)k ≤ Dg kλ′ − λ′′ k,

(10)

3.3

kϕ(t, λ, y)k ≤ Bϕ , kg(t, λ, y, u)k ≤ Bg .
(11)
Furthermore, there exist Mϕ , Mg ∈ R≥0 such that


 ∂ϕ(t,λ,y)
∂ϕ(t,λ,y) 
 ∂y ẏ +
 ≤ Mϕ ,
∂t


 ∂g(t,λ,y,u)
∂g(t,λ,y,u)
∂g(t,λ,y,u) 
ẏ
+
u̇
+
 ≤ Mg

∂y
∂u
∂t

Possible parametrization: θ, λ, A, B, ϕ, Ψ, g


E
System (7): θ = col VF x0 , − VF , −kref e RTref , λ = − E
,
R


λ T1
(t)
y , g=0
A = 0, B = 1, ϕ(t, λ, y) = col 1, y, e

 


0
0
1
, B =  ,
System (7): θ = col −J −1 , b , A = 
1
0 0


2
2
q1
q2
(I1 ,λ,y)
(I2 ,λ,y)
λ = a, ϕ(t, λ, y) = col d(t), (y+λ)2 − (y−λ)2 , g = 0


A
σ
System (9): θ = col − τ1m , τmf , στss , λ = Aff , A =




0 − τ1m
y tanh(λy) 0

, Ψ(t, λ, y) = 
, g = 0
0 − τ1s
0
0
y

Problem formulation

Before we proceed with a formal problem statement, several points related to parametrization of (7), (9) need to
be discussed. First, note that different definitions of systems (7), (9) may correspond the same physical model.
For example, functions ϕ and g and parameters θ, λ for
the first model in Table 1 may also be defined as:


1
ϕ(t, λ, y) = col 1, y, eλ1 T (t) +λ2 y , g = 0,
(13)

(12)

for all λ ∈ Ωλ , t ≥ t0 along the solutions of (7).

θ = col

Conditions (10), (11) often hold naturally in the context
of modeling and identification; they may, however, impose limitations in the framework of controller design.
As for condition (12), the first inequality is a version of
(6) that is essential for uniform exponential convergence
of solutions to the origin of (5) [23]. The second inequality in (12) is a technical condition. Although this latter
condition may look somewhat restrictive, it may be relaxed if g(t, λ, y, u(t)) is expressed as g(t, λ, y, u(t)) =
g1 (t, y, u(t)) + g2 (t, λ, y, u(t)). In this case we would require that (11), (12) hold for g2 .

F
V




E
x0 , − VF , −kref , λ = col − E
R , RTref , or
1

ϕ(t, λ, y) = col (1, y) , g(t, λ, y, u(t)) = −eλ1 T (t) +λ2 y,
(14)



ln(kref )E ln(kref )E
F
F
with θ = col V x0 , − V , λ = col − R , RTref .

It is clear that if parametrization (13) is chosen then
identical outputs y(t) will be observed for infinitely
many combinations of parameters θ3 , λ2 . Models of
this type are referred to as unidentifiable [11] (see also
[9], [10]). Dealing with unidentifiable models imposes
technical difficulties. We will therefore assume that
parametrizations which are obviously unidentifiable are
avoided, if possible. As for the remaining alternative
parametrizations, we assume that preference is given to
those in which the dimension of λ is minimal, i.e. the
parametrization in the first row in Table 1 is preferable
to (14).

A non-exhaustive list of systems that are relevant in engineering applications and are governed by (7) or (9)
includes bio-/ chemical reactors [7,35], nonlinear saturated magnetic circuits [32], magnetic bearings [21], tireroad interaction, and dynamics of live cells [37]. A few
examples from this list are provided in Table 1. The first
model, if described by (7), trivially satisfies Assumption 3.1; it also satisfies Assumption 3.2 if T is bounded,
differentiable, separated away from zero, and y, Ṫ are

Second, as far as identifiability is concerned, inferring
true values of θ, λ from output observations, y(t), is not

4

and q(t, λ, λ′ ) = C̃T z(t, λ, λ′ ):

always possible, even if the system is linearly parameterized and no unmodeled dynamics are present. Consider
1

ẋ = Ax +

1

!

θ+

0
1

!

λ, A =

y = x1 , a1 , a2 ∈ R>0 .

−a1 1

−a2 0

!

ż = Λz + G(g(t, λ, y(t), u(t)) − g(t, λ′ , y(t), u(t))),
!


In−2
Λ = −b
, G = −b In−1
(20)
0

(15)

z(t0 ) = 0, b = col (b1 , . . . , bn−1 ) .

If ξ(t) ≡ 0 then the set E(λ, θ) contains all indistinguishable parametrizations of (7) for the given y(·), u(·), θ,
λ (see Lemma 12 in Section 5), and E0 (λ, θ) ⊆ E(λ, θ).
Note that if dim(x) = 2 then Λ = −b1 , G = (−b1 1); if
dim(x) = 1 we set E(λ, θ) = E0 (λ, θ). For system (15),
E(λ, θ) = {(λ′ , θ′ ), λ ∈ R, θ ∈ R| θ − θ′ + λ − λ′ = 0}.
Note that if g(t, λ, y(t), u(t)) = Bg(t, λ, y(t), u(t)), B =
col(1, b1 , . . . , bn−1 ), then q(t, λ, λ′ ) ≡ 0 for all t ≥ t0
in (19). The introduction of sets E0 (λ, θ), E(λ, θ) does
not, of course, resolve identifiability issues. It helps, however, to specify constraints on the nonlinearities in (7)
for which the parameter reconstruction, up to E0 , E, can
be achieved.

Let x(t, θ, λ, x0 ) and x(t, θ′ , λ′ , x′0 ) be two solutions of
(15) corresponding to different parameter values and initial conditions, and let e = col(e1 , e2 ) = x(t, θ, λ, x0 ) −
x(t, θ′ , λ′ , x′0 ). Picking e2 (t0 ) = −θ + θ′ , e1 (t0 ) = 0 ensures that e1 (t) = 0 for all t ≥ t0 if θ − θ′ + λ − λ′ = 0.
Another, albeit nonlinearly parameterized, example is
ẋ = −x + θ + [sin2 (λ + t) + x2 + 1]−1 , y = x.

(16)

In this case x(t, θ, λ, x0 ) = x(t, θ, λ′ , x0 ) for all t ≥ t0 if
λ′ = λ + kπ, k ∈ Z, θ′ = θ.

Since this is desirable from an implementation point of
view, we will seek for a recursive procedure taking the
values y(t), u(t) as inputs and producing the estimates
x̂(t), θ̂(t), λ̂(t) of x(t, λ, θ, x0 , [u]), θ, and λ, respectively, as outputs. Note that since we allow for nonidentifiable configurations, estimation of parameters θ,
λ is possible only up to the set E(λ, θ). We will, therefore, be looking for an auxiliary system, i.e. an adaptive
observer: q̇ = f (t, y, u(t), q), f : R × R × R × Rq → Rq ,
q(t0 ) = q0 , q ∈ Rq , and functions hθ : Rq → Rm ,
hλ : Rq → Rp , hx : Rq → Rn such that for the given
t0 , appropriately chosen q0 , known functions r1 , r2 ∈ K,
and all admissible λ, θ, x0 the following requirements
hold for the observer:

In order to account for possible non-unique parametrization, for each pair θ, λ we introduce two sets: E0 (λ, θ)
and E(λ, θ). The set E0 (λ, θ):
E0 (λ, θ) = {(λ′ , θ′ ), λ′ ∈ Rp , θ′ ∈ Rm |

(17)

η 0 (t, λ, θ, λ′ , θ′ ) = 0, ∀ t ≥ t0 },

η 0 (t, λ, θ, λ′ , θ′ ) =
B(ϕT (t, λ, y(t))θ − ϕT (t, λ′ , y(t))θ ′ ) +
g(t, λ′ , y(t), u(t)) − g(t, λ, y(t), u(t))
contains all parametrizations of (7) that are indistinguishable from observations of x(t) for the given
y(·), u(·), θ, λ at ξ(t) ≡ 0. That is, if x(t, θ, λ, x0 ) −
x(t, θ ′ , λ′ , x0 ) = 0 for all t ≥ t0 then (λ′ , θ ′ ) ∈ E0 (λ, θ).
For system (15) the set E0 (λ, θ) contains just one element, (λ, θ). The set E0 (λ, θ), however, is not finite for
system (16) and for parametrization (13) of the first
model in Table 1.

lim sup khx (q(t, q0 )) − x(t, λ, θ, x0 )k ≤ r1 (∆ξ )
(21)
t→∞
!
!
hλ (q(t, q0 ))
, E(λ, θ) ≤ r2 (∆ξ ),
lim sup dist
t→∞
hθ (q(t, q0 ))
where ∆ξ is defined in (8).

The second set, E(λ, θ), is defined as:
′

′

′

′

4
′

′

E(λ, θ) = {(λ , θ ), λ ∈ Rp , θ ∈ Rm | ∃ p(θ, λ, θ , λ )
∈ Rn−1 : C̃T eΛ(t−t0 ) p + η(t, θ, λ, θ′ , λ′ ) = 0 ∀ t ≥ t0 },
(18)

Main result

In this section we introduce an observer for (7) and show
that asymptotic reconstruction of state and parameters
of (7) is achievable (up to the set E(λ, θ)), subject to
some persistency of excitation conditions.

where C̃ ∈ Rn−1 , C̃ = col(1, 0, . . . , 0),

4.1

η(t, λ, θ, λ′ , θ′ ) = ϕ(t, λ, y(t))T θ − ϕ(t, λ′ , y(t))T θ′ +
g1 (t, λ, y(t), u(t)) − g1 (t, λ′ , y(t), u(t)) + q(t, λ, λ′ ) (19)

Observer definition

Following the general ideas of [40] with regards to the
treatment of uncertain systems with general nonlinear

5

where f : Rnp → Rnp , β : Rnp → Rp are continuous,
and let Ωs be the ω-limit set 2 of s0 . In addition, suppose
that the following properties hold:

s4

ax

l 2, m
s3

P1) the functions f , β in (23) are Lipschitz;
P2) the vector s0 is such that the solution s(t, s0 ) is
bounded for all t ≥ t0 ;
P3) the image of Ωs under β contains Ωλ : for every
λ ∈ Ωλ there is an s ∈ Ωs such that β(s) = λ.

s2

l1,
l(t )

in

Properties P1 and P2 are technical requirements ensuring that the derivative of λ̂, as a function of t, is bounded
and has a bounded growth rate. Property P3, however,
is essential. It implies that the projection β(s(·, s0 )) of
the trajectory s(·, s0 ) onto Ωλ is dense and recurring in
Ωλ :

min

s1

l1,

l 2, m

max

∀ λ ∈ Ωλ , ∀ ε ∈ R>0 , ∀ t ≥ t0
∃ t′ > t : kλ − β(s(t′ , s0 ))k < ε.

Figure 1. Top panel: general structure of the observer. Bottom
panel: phase curves of system (25).

Indeed, let λ′ be an element from Ωλ . Then according
to P3 there is an s′ ∈ Ωs : β(s′ ) = λ′ . Since Ωs is the
ω-limit set of s0 , we can conclude that there is a sequence {ti }, i = 1, 2, . . . , limi→∞ ti = ∞, such that
limi→∞ s(ti , s0 ) = s′ . Finally, using the continuity of β
we arrive at limi→∞ β(s(ti , s0 )) = λ′ . In other words, for
any λ′ ∈ Ωλ and ε > 0 there will exist a sequence of time
instances ti : limi→∞ ti = ∞ such that kλ̂(ti )−λ′ k < ε,
and hence (24) follows. An example of a very simple system possessing a solution s(t, s0 ) and an output function
β satisfying properties P1–P3 for Ωλ = [−1, 1]2 is
√
√
ṡ1 = − 2s2 , ṡ2 = 2s1 , ṡ3 = −s4 , ṡ4 = s3 ,
(25)
β(s(t, s0 )) = col(s1 (t), s3 (t)), s0 = col (1, 0, 1, 0) .

parametrization, we propose that an asymptotically converging observer for (7) consists of two coupled subsystems, Sa and Sw (see Fig. 1). The role of subsystem Sa
is to provide estimates of state and parameters θ of (7),
and the role of subsystem Sw is to search the values of
parameters λ.
The dynamics of subsystem Sa is defined as follows:

T
T
˙

 x̂ =Ax̂ + ℓ (C x̂ − y(t)) + Bϕ (t, λ̂(t), y(t))θ̂



+ g(t, λ̂(t), y(t), u(t))
(22)
Sa : ˙


θ̂ = − γθ (CT x̂ − y(t))ϕ(t, λ̂(t), y(t)),



ŷ =CT x̂, x̂(t0 ) ∈ Rn , θ̂(t0 ) ∈ Rm , γθ ∈ R>0

Phase curves of (25) are shown in Fig. 1, bottom panel.
Projections of the initial segment of the trajectory are
shown by thick lines. After evolving beyond the initial
segment, the values of β(s(t, s0 )) will densely fill the set
[λ1,min , λ1,max ] × [λ2,min , λ2,max ] = [−1, 1]2 , cf. [33].

The variable θ̂ = col(θ̂1 , · · · , θ̂m ) in (22) is an estimate
of θ, and λ̂ = col(λ̂1 , . . . , λ̂p ) is an estimate of λ. For the
time being we suppose that λ̂ is a continuous function of
t. The matrix A and vectors B, C in (22) are identical to
those in (7), and the vector ℓ in (22) satisfies Assumption
3.1. If the values of λ would be known then substitution
λ̂ = λ reduces system (22) to (3), and conditions for
asymptotic reconstruction of state and parameter values
of (7) follow from Theorem 3. The values of λ, however,
are unknown and therefore a procedure for estimating
the values of λ is needed.

The problem with using (23) directly as an estimator for
λ is that exploration of the set Ωλ continues indefinitely.
For the purposes of observer design we need to ensure
that exploration of Ωλ stops once a sufficiently small
neighborhood of the set E(λ, θ) has been reached. To
enable this, the explorative subsystem must be supplied
with an error measure. A function of ky(t) − ŷ(t)kε is a
possible candidate for such a measure. Thus we replace
the earlier definition (23) for λ̂ with the following:

Regarding the definition of Sw , we propose that the values of λ̂(t) result from an explorative search in the domain Ωλ of the admissible values for λ. The exploration
can be realized by movements along the solutions of a
certain class of dynamical systems. Let us, for instance,
consider systems governed by the following equations
ṡ = f (s), s(t0 ) = s0 , λ̂ = β(s)

(24)

ṡ = γσ(ky(t) − ŷ(t)kε )f (s), ε ∈ R≥0 , γ ∈ R>0 ,

λ̂ = β(s), s(t0 ) = s0 ,
2

(26)

Recall that a point z ∈ Rnp is an ω-limit point of z0 ∈ Rnp
if there is a sequence {ti }, i = 1, 2, . . . , limi→∞ ti = ∞, such
that limi→∞ s(ti , z0 ) = z. The set of all ω-limit points of z0
is the ω-limit set of z0 .

(23)

6

The search trajectory itself does not need to be dependent on the properties of g, ϕ, and neither is the structure of Sw . This potential advantage of the approach,
however, comes at a cost. According to (24), small neighborhoods of sets to which the solutions of the combined
system (22), (28) converge are not necessarily forward
invariant. Hence these sets are not guaranteed to be
asymptotically stable. Nevertheless, albeit in a weaker
sense [31], they are still attracting. We illustrate this
point in Section 7.

where σ : R≥0 → R≥0 is a bounded Lipschitz function:
∃ Dσ , Mσ ∈ R>0 :
σ(υ) ≤ Mσ , σ(υ) ≤ Dσ υ ∀ υ ≥ 0

(27)

such that σ(υ) > 0 for υ > 0, and σ(0) = 0.
For the sake of simplicity and without loss of generality, instead of dealing with general systems (26), we will
focus on a specific system of equations:

Sw :

4.2



ṡ
= γσ(ky(t) − ŷ(t)kε ) · ωj · (s2j−1 − s2j

 2j−1



−s2j−1 (s22j−1 + s22j ))


ṡ2j =










λ̂j

Let us now proceed with specifying those properties of
(7) that can be useful for state and parameter reconstruction. Recall that (7) is a generalization of the standard canonic observer form (1). According to Theorem
3, one of the conditions for (3) to be an adaptive observer
for (1) is persistency of excitation of the restriction of
φ(·, y(·)) on R≥t0 . It is therefore natural to expect that
some kind of persistency of excitation conditions might
be needed for reconstruction of parameters θ, λ in (7)
too. Two versions of these conditions will be considered,
namely the notions of uniform persistency of excitation
[24] and nonlinear persistency of excitation [8].

γσ(ky(t) − ŷ(t)kε ) · ωj · (s2j−1 + s2j
−s2j (s22j−1 + s22j ))

= βj (s), j = {1, . . . , p},
λj,max − λj,min
βj (s) = λj,min +
(s2j−1 + 1)
2
s0 = s(t0 ) : s22j−1 (t0 ) + s22j (t0 ) = 1,

(28)
(29)

where σ is a function satisfying (27). Parameters ωj ∈
R>0 in (28) are supposed to be rationally-independent:
Pp

j=1

ωj kj 6= 0, ∀ kj ∈ Z.

Asymptotic properties of the observer

Definition 4 A function α : R≥t0 × Ωλ → Rp is λUniformly Persistently Exciting (λ-UPE with T , µ), denoted by α(t, λ) ∈ λUPE(T, µ), if there exist T, µ ∈ R>0 :

(30)

Equations (28)–(30) are straightforward generalizations
from the example system of which the phase curves are
shown in Fig. 1. If the term γσ(ky(t) − ŷ(t)kε ) in the
right-hand side of (28) is substituted with 1, these equations satisfy the requirements P1–P3. Indeed, we can immediately see that in this case (s2j−1 (t, s0 ), s2j (t, s0 )) =
(cos(ωj (t − t0 ) + aj ), sin(ωj (t − t0 ) + aj )), aj ∈ R. Thus
properties P1, P2 hold. Trajectories s1 (·, s0 ), s3 (·, s0 ),
. . . s2p−1 (·, s0 ) evolve on a corresponding p-dimensional
invariant torus. Since ωj are rationally-independent
these trajectories densely fill the torus (cf. [3], [33],
[20]) or, alternatively, they densely fill the hypercube
[−1, 1]p . This implies that Ωs , the ω-limit set of s0 , is
Ωs = {col (s1 , s2 , . q
. . , s2p ) ∈ Rp | col (s1 , s3 , . . . , s2p−1 ) ∈

R t+T
t

α(τ, λ)αT (τ, λ)dτ ≥ µIp , ∀ t ≥ t0 , λ ∈ Ωλ .
(31)

In contrast to the conventional definitions of persistency
of excitation (cf. Definition 2), uniform persistency of
excitation requires existence of µ, T ∈ R>0 in (31) that
are independent on λ for all λ ∈ Ωλ . This is a stronger
restriction; we will, however, require that it holds for
ϕ(t, λ, y(t)) (as a function of t, λ on R≥t0 × Ωλ ) in (7).
Since parametrization of (7) is allowed to be nonlinear,
it is natural to expect that reconstruction of model parameters might require a nonlinear version of standard
persistency of excitation. Here we employ the following
generalization of the standard notion (cf. [8]):

[−1, 1]p , s2j = ± 1 − s22j−1 , j = 1, . . . , p}. Noticing
that the image of Ωs under transformation β coincides
with Ωλ we conclude that P3 holds.

Definition 5 Let E be a set-valued map defined on D ⊂
Rd and associating a subset of D to every p ∈ D. A
function α : R≥t0 × D × D → Rk is said to be weakly
Nonlinearly Persistently Exciting in p wrt E (wNPE with
L, β, E ), denoted by α(t, p, p′ ) ∈ wNPE(L, β, E), if there
exist L ∈ R>0 , t1 ≥ t0 , and β ∈ K∞ :

Concerning the structure of Sw , no additional modeldependent constraints are imposed on (28) (or, in general, on (26)), apart from the general requirements P1–
P3. Model-specific nonlinearities are accounted for in
the “converging” part, Sa , of the observer producing the
estimates for θ and x. The information about the values of λ is transferred to the exploratory part, Sw , by
means of ky(t) − ŷ(t)kε . The latter variable modulates
the speed of exploration in Ωλ along a search trajectory.

∀ t ≥ t1 , p, p′ ∈ D ∃ t′ ∈ [t, t + L] :
kα(t′ , p, p′ )k ≥ β (dist(E(p), p′ )) .

7

(32)

The proof of Theorem 7 is presented in the next section.

If the set E(p) contains just one element, p, then the
inequality in (32) reduces to kα(t′ , p, p′ )k ≥ β(kp−p′ k).
Taking the above notions into account we formulate the
main technical assumption on the nonlinearities in (7):

Let us briefly comment on the assumptions made in the
theorem statement. Assumptions 3.1, 3.2 are standard;
condition A1 in Assumption 4.1 is the conventional requirement ensuring exponential convergence of x̂(t), θ̂(t)
to x(t) and θ provided that the value of λ and hence
the values of ϕ(t, λ, y(t)) are known (cf. Theorem 3); A2
ensures that the distance from (λ̂, θ̂) to the set E(λ, θ)
is inferable from the values of y(t) − ŷ(t) over [t0 , ∞)
(cf. Lemma 12). Note that nonlinear dependence of η
on λ, λ′ can impose certain technical and computational
difficulties whilst checking that this condition holds. Finally, observe that the state estimation in the proposed
scheme requires that E0 (λ, θ) = E(λ, θ). System (15) illustrates that violation of this assumption may prevent
the reconstruction of the state from observed output
data.

Assumption 4.1 The functions ϕ, g in the right-hand
side of (7) are such that
A1) the restriction of the function α1 : R × Rp → Rm ,
α1 (t, λ) = ϕ(t, λ, y(t)) on R≥t0 ×Ωλ is λ-UPE with
T, µ;
A2) the restriction of α2 : R × Rp+m × Rp+m → R,
α2 (t, (λ, θ), (λ′ , θ′ )) = η(t, λ, θ, λ′ , θ′ ), where η(·)
is defined in (19), on R≥t0 ×Rp+m ×Rp+m is weakly
nonlinearly persistently exciting in (λ, θ) wrt to the
map E(λ, θ) determined by (18).
Remark 6 Checking that condition A1 holds is
straightforward if e.g. ϕ(t, λ, y(t)) is periodic in t. Regarding condition A2, we note that, according to (19),
η(t, λ, θ, λ′ , θ′ ) can be expressed as η(t, λ, θ, λ′ , θ′ ) =
r(t, λ, θ) − r(t, λ′ , θ) + ϕ(t, λ′ , y(t))T (θ − θ′ ), where

The value of γ̄ and the functions r1 , r2 , r3 could in principle be given explicitly. However, due to dependence of
these functions on A, B, C, parameters Dϕ , Dg , Mϕ ,
Mg and T , L, µ, β specified in Assumptions 3.1, 3.2 and
Definitions 4, 5, explicit expressions for γ̄ and r1 , r2 , r3
are too lengthy and thus are removed from the theorem’s
statement. They are, nevertheless, provided in the proof
(see e.g. (59), (69)). A procedure for finding the values
of γ̄ and ε is discussed in Section 7.

r(t, λ, θ) = ϕ(t, λ, y(t))T θ + g1 (t, λ, y(t), u(t))
Rt
+C̃T t0 eΛ(t−τ ) Gg(τ, λ, y(τ ), u(τ ))dτ.

If ϕ, g are differentiable then r(t, λ, θ) − r(t, λ′ , θ) =
R1 ∂
R(t, λ, λ′ , θ) (λ − λ′ ), where R(t, λ, λ′ , θ) = 0 ∂s
r(t
, s(ξ, λ, λ′ ), θ)dξ, s(ξ, λ, λ′ ) = λξ + (1 − ξ)λ′ . Hence
η(t, λ, θ, λ′ , θ′ ) =
(ϕT (t, λ′ , y(t)), R(t, λ, λ′ , θ))(θ − θ ′ , λ − λ′ )

The value of ε, viz. the accuracy of the estimation, is
determined by the bound ∆ξ on the amplitude of perturbation ξ(t). This dependency is established through
the function r1 determining a lower bound for parameter ε in (28). If no perturbation ξ(t) is present in the
right-hand side of (7) then the value of ε can be chosen arbitrarily small. Note that the convergence itself is
asymptotic and not necessarily exponential. This is the
price for the presence of unknown parameters λ in (7).

(33)

It is therefore clear that if (ϕT (t, λ′ , y(t)), R(t, λ, λ′ , θ)),
t ≥ t0 , λ, λ′ ∈ Ωλ , θ ∈ Ωθ , is (λ, λ′ , θ)-uniformly persistently exciting, then the system is uniquely identifiable, it satisfies condition A2, and E0 (λ, θ) coincides
with E(λ, θ).

Remark 8 The estimate λ̂(t) is guaranteed to converge
to a single element of Ωλ (see (60)); estimates θ̂(t) may
oscillate due the influence of ξ(t). These oscillations
are bounded, and will eventually be confined to the
2r2 (ε)-neighborhood of E(λ, θ). Hence, for t1 sufficiently
large and all t ≥ t1 ≥ t0 , the 2r2 (ε)-neighborhood of
(λ̂(t), θ̂(t)) will always contain an element of E(λ, θ).
This element may not necessarily be from Ωλ × Ωθ .
If the elements of E(λ, θ) are separated by distances
exceeding 3r2 (ε) then the estimates are guaranteed to
converge to the r2 (ε)-vicinity of just one element. This
point in E(λ, θ) will depend on ξ, x0 , and on the initial
state of the observer.

We are now ready to state the main result:
Theorem 7 Consider system (7) together with the observer defined by (22), (28)–(30). Suppose that Assumptions 3.1, 3.2, and 4.1 hold. Then there exist a constant
γ̄ ∈ R>0 and functions r1 , r2 ∈ K such that if γ, ε are
the corresponding parameters of (28), and γ ∈ (0, γ̄),
ε > r1 (∆ξ ), then
lim supt→∞ dist

λ̂(t)
θ̂(t)

!

!

, E(λ, θ)

≤ r2 (ε).

(34)

Remark 9 The function β in Definition 5 can be allowed to depend on (λ, θ). In view of Remark 6, this relaxes the requirement that (ϕT (t, λ′ , y(t)), R(t, λ, λ′ , θ))
in (33) (as a function of t, (λ, λ′ , θ) for t ≥ t0 ) is
(λ, λ′ , θ)-UPE to the condition that (ϕT (t, λ′ , y(t)) ,

If, in addition, E(λ, θ) coincides with E0 (λ, θ) then there
is an r3 ∈ K:
lim sup kx̂(t) − x(t)k ≤ r3 (ε).

(35)

t→∞

8

R(t, λ, λ′ , θ)) is λ′ -UPE. Note that this will make r2 in
(34) dependent on (λ, θ). Finally, note that A2 need not
hold for all (λ, θ) ∈ Rp+m and can be restricted to the
union of Ωλ × Ωθ and the domain to which (λ̂(t), θ̂(t))
belong for t ≥ t0 .

In the third part of the proof we show that condition
A2 of Assumption 4.1 guarantees that (35) holds and
that, subject to the condition that E0 (λ, θ) coincides
with E(λ, θ), property (34) holds too.

5

Lemma 10 Consider system (38), and suppose that

Part 1 of the proof is contained in the following.

Proof of Theorem 7

C1) the matrices A, B, and C satisfy Assumption 3.1
C2) the restriction of the function α in the right-hand
side of (38) on R≥t0 × Ωλ is λ-UPE with constants
T, µ as in (31)
C3) the function α(t, ·) is Lipschitz in Ωλ uniformly in
t, t ≥ t0 : there is a D ∈ R>0 such that kα(t, λ) −
α(t, λ′ )k ≤ Dkλ − λ′ k ∀ λ, λ′ ∈ Ωλ , t ≥ t0
C4) the function α and its partial derivatives wrt t, λ
are bounded;
thatis there is a constant
oM such that
n
 ∂α(t,λ)   ∂α(t,λ) 
max kα(t, λ)k ,  ∂t  ,  ∂λ  ≤ M ∀ λ ∈
Ω λ , ∀ t ≥ t0 .

According to Assumption 3.2 and (8), functions ϕ, g and
ξ are continuous and are bounded in R≥t0 × Ωλ × Dy ,
R≥t0 × Ωλ × Dy × Du and R≥t0 respectively. Therefore solutions of the combined system, (7), (22), (28)–
(30) exist and are defined for all t ≥ t0 . Let us denote
e = col(e1 , e2 ), e1 := x̂ − x, e2 := θ̂ − θ, α(t, λ̂) =
ϕ(t, λ̂, y(t)). Then according to (7) and (22) the following holds along the solutions of (7), (22), (28)–(30)
ė1
ė2

!

A + ℓ CT

=

BαT (t, λ̂(t))

−γθ α(t, λ̂(t))CT
+

0
!
v(t, λ̂(t), λ, y(t), u(t))

!

e1
e2

!

Then there exist ρ, Dρ ∈ R>0 such that for all λ̂ : R≥t0 →
Ωλ , λ̂ ∈ C 1 :
˙
(39)
kλ̂(t)k ≤ Mλ
2
0 ≤ Mλ ≤ µr/(2DM T ), r ∈ [0, 1)
(40)
the following holds

(36)

0

where the function v:
v(t, λ̂, λ, y, u) = B(ϕT (t, λ̂, y) − ϕT (t, λ, y))θ
+ g(t, λ̂, y, u) − g(t, λ, y, u) − ξ(t).

kΦ(t2 , t1 )pk ≤ e−ρ(t2 −t1 ) Dρ kpk, p ∈ Rn+m ,

(37)

where t2 ≥ t1 ≥ t0 , and Φ(·, ·) is the fundamental (normalized) solution matrix of (38).

is continuous and bounded for all y ∈ Dy , u ∈ Du ,
λ, λ̂ ∈ Ωλ and t ≥ t0 .

The proof of Lemma 10 and other auxiliary results are
provided in Appendix.

The proof of the theorem is split into three parts. In the
first part we consider systems
ė1
ė2

!

=

A+ℓC

T

−γθ α(t, λ̂(t))C

T

Bα (t, λ̂(t))
T

0

!

e1
e2

!

(41)

Part 2. Consider now the interconnection of (7) with the
observer (22), (28)–(30). The dynamics of the combined
system in the coordinates e, λ̂ is described by (36), (28)–
(30). Recall that e(t), λ̂(t) are defined for all t ≥ t0 .
With respect to e(t), the following holds

,

(38)
where γθ ∈ R>0 and λ̂ : R≥t0 → Ωλ is a continuous,
differentiable and bounded function. Let Φ(t) be a fundamental solution matrix of (38), and denote Φ(t, t0 ) =
Φ(t)Φ(t0 )−1 . Since Φ(t0 , t0 ) is the identity matrix we
say that Φ(t, t0 ) is the normalized solution matrix of
(38). We show that if Assumptions 3.1, 3.2 and condition A1 of Assumption 4.1 hold then there are positive numbers Mλ , ρ, and Dρ such that the fundamental
(normalized) matrix of solutions, Φ(t, t0 ), of (38) with
˙
λ̂ : kλ̂(t)k ≤ Mλ satisfies the inequality kΦ(t, t0 )pk ≤
Dρ e−ρ(t−t0 ) kpk, p ∈ Rn+m , t ≥ t0 .

e(t) = Φ(t, t0 )e(t0 )
+

Rt

t0

Φ(t, τ )

v(τ, λ̂(τ ), λ, y(τ ), u(τ ))
0

!

dτ.

(42)

The variable λ̂ in the combined system, as a function of t,
is bounded, continuous, and differentiable with bounded
derivative. Moreover, for any A, B, C, θ ∈ Ωθ , λ ∈ Ωλ
˙
and for any choice of ℓ , γθ in (22) we have that |λ̂j (t)| ≤
λi,max −λi,min
γMσ maxi |ωi |
, where Mσ , ωi , and γ are de2
fined in (27), (30), and (28). Thus ∀ t ≥ t0 we have:

Using this result, in the second part of the proof we
demonstrate existence of γ̄ and an ε, dependent on ∆ξ ,
such that setting γ ∈ (0, γ̄] ensures that the estimate
λ̂(t) converges to a λ∗ from Ωλ : limt→∞ λ̂(t) = λ∗ .

√
˙
λ
−λ
λ̂(t) ∈ Ωλ , kλ̂(t)k ≤ γ pMσ maxi |ωi | i,max 2 i,min ,
(43)

9

[−1, 1]p (see e.g. [20], Proposition 1.4.1). Consider the
function β : R2p → Rp :

where p is the dimension of λ. According to Assumption 3.2, function α is bounded, differentiable, and Lipschitzin the second
argument.
In particular, kα(t, λ̂)k ≤


 α(t,λ̂) 
 α(t,λ̂) 
Bϕ ,  ∂t  ≤ Mϕ ,  ∂ λ̂  ≤ Dϕ ∀ t ≥ t0 , λ̂ ∈ Ωλ .

βj (q) = λj,min +

Hence there is an M = max{Bϕ , Mϕ , Dϕ } such that condition C4 of Lemma 10 holds. Moreover, according to A1
in Assumption 4.1, the restriction of α on R≥t0 × Ωλ is
λ-UPE with T, µ. This, together with (43), implies that
requirements C1–C4 of Lemma 10 are satisfied.

∀ λ ∈ Ωλ , ∆λ ∈ R>0 , t ≥ t0
∃ t′ > t : kλ − λ̄(t′ )k < ∆λ .

µr
2Dϕ max{Bϕ ,Mϕ ,Dϕ }T 2 ×

√
−1
λ
−λ
pMσ maxi |ωi | i,max 2 i,min
.

(44)

λ̂(t) = λ̄(t0 + γ

q̇2j = ωj · (q2j−1 + q2j −

σ(ky(τ ) − ŷ(τ )kε )dτ ).

(50)

The function λ̄(·) is Lipschitz: kλ̄(t1 ) − λ̄(t2 )k ≤
√
|ω |(λ
−λi,min )
|t1 − t2 |, t1 , t2 ∈ R≥t0 . Thus
p maxi i i,max
2
(50), (51) imply that
kλ − λ̂(t)k ≤ kλ − λ̄(t′ )k + kλ̄(t′ ) − λ̂(t)k ≤ ∆λ
√
−λi,min )
+ Dλ kh(t)k, Dλ = p maxi |ωi |(λi,max
(52)
.
2

− λk∞,[t0 ,t]

Taking (46) and (52) into account we can conclude that
the dynamics of the combined system (7), (22), (28)–
(30) obeys the following set of constraints:

(46)

Let j ∈ {1, . . . , p}, and consider the solution of
q̇2j−1 = ωj · (q2j−1 − q2j −

t0

kλ − λ̂(t)k ≤ kλ − λ̄(t′ )k + kλ̄(t′ ) − λ̂(t)k
(51)
= kλ − λ̄(t′ )k + kλ̄(t′ ) − λ̄(t′ − h(t))k.

and hence

2
2
q2j−1 (q2j−1
+ q2j
)),
2
2
q2j (q2j−1 + q2j )),

Rt

Rt
Denoting h(t) = t′ −t0 −γ t0 σ(ky(τ )− ŷ(τ )kε )dτ , where
the value of t′ is chosen such that (49) holds, we arrive
at the following estimate:

kv(t, λ̂(t), λ, y(t))k ≤ Dv kλ̂(τ ) − λk∞,[t0 ,t] + ∆ξ , (45)
Dv = Dϕ kBkkθk + Dg ,

D ∆
+ ρρ ξ .

(49)

t ≥ t0 the variable λ̂(t) defined in (28) can be expressed
in terms of λ̄(T (t)) as

kv(τ, λ̂(τ ), λ, y(τ ))kdτ . The functions ϕ, g in the definition of the function v, (37), are Lipschitz with respect
to λ. Furthermore, according to (8) the following holds:
kξ(t)k ≤ ∆ξ . Therefore, in accordance with (37), (8),
and Assumption 3.2

Dρ Dv
ρ kλ̂(τ )

(48)

Noticing that s(t, s0 ) = q(T (t), s0 ), where T (t) = t0 +
Rt
γ t0 σ(ky(τ ) − ŷ(τ )kε )dτ , we can conclude that for all

This and (43) imply that the requirement (39) of the
lemma is satisfied. Hence if γ ∈ (0, γ ∗ ] then according
to Lemma 10 the matrix Φ(t, t0 ) in (42) satisfies (41).
This ensures the existence of ρ, Dρ ∈ R>0 such that,
along the solutions of (36), (28)–(30), for all t ≥ t0 we
Rt
have: ke(t)k ≤ e−ρ(t−t0 ) Dρ ke(t0 )k + Dρ t0 e−ρ(t−τ ) ·

ke(t)k ≤ e−ρ(t−t0 ) Dρ ke(t0 )k +

+ 1),

and define λ̄(t) = β(q(t, s0 )). System (47), (48) satisfies conditions P1–P3, and hence we can conclude that
trajectory λ̄(·) satisfies the recurrence property (24):

Let γ ∈ (0, γ ∗ ], where γ ∗ is defined as:
γ∗ =

λj,max −λj,min
(q2j−1
2

ke(t)k ≤ e−ρ(t−t0 ) Dρ ke(t0 )k +

(47)

+

Dρ Dv ∆λ
ρ

h(t) = h(t0 ) − γ

satisfying the initial condition q2j−1 (t0 ) = s2j−1 (t0 ),
q2j (t0 ) = s2j (t0 ); the parameters ωj and values
of s2j−1 (t0 ), s2j (t0 ) are supposed to coincide with
those defined in (28), (29). The solution of (47) satisfying initial condition (29) is obviously unique,
and can be expressed as a function q : R → R2p :
q2j−1 (t) = cos(ωj t + ϑj ), q2j = sin(ωj t + ϑj ), ϑj ∈
R, j = {1, . . . , p}. Parameters ϑj are determined in accordance with: cos(ωj t0 + ϑj ) = s2j−1 (t0 ), sin(ωj t0 +
ϑj ) = s2j (t0 ). Given that ωj in (47) are rationallyindependent we conclude that the ω-limit set of
(q1 (t, s0 ), q3 (t, s0 ), . . . , q2p−1 (t, s0 )) is the hypercube

+
Rt
t0

Dρ ∆ξ
ρ ,

Dρ Dv Dλ
kh(τ )k∞,[t0 ,t]
ρ

(53)

σ(kCT e1 (τ )kε )dτ.

To proceed further we need an auxiliary result below.
Lemma 11 Consider a system of which the dynamics
for all t ≥ t0 satisfy the following inequalities
kx(t)k ≤ ̺(t − t0 )kx(t0 )k + ckh(τ )k∞,[t0 ,t] + ∆,
Rt
− t0 γ0 (kx(τ ) + d(τ )kε )dτ ≤ h(t) − h(t0 ) ≤ 0,

(54)

where x : R≥t0 → Rn , h : R → R are trajectories reflecting the evolution of the system’s state, d : R → Rn ,
10

where v3 (t) = −ξ(t) and

kd(τ )k∞,[t0 ,∞) ≤ ∆d is a continuous and bounded function on [t0 , ∞), ̺ is a strictly monotonically decreasing
function with, ̺(0) ≥ 1, lims→∞ ̺(s) = 0; c, ∆ ∈ R>0 ,
and γ0 : R → R≥0 :
|γ0 (s)| ≤ Dγ |s|, Dγ ∈ R>0 .

v1 (t, θ̂, λ̂, θ, λ) = B(ϕT (t, λ̂, y(t))θ̂ − ϕT (t, λ, y(t))θ)

v2 (t, λ̂, λ) = g(t, λ̂, y(t), u(t)) − g(t, λ, y(t), u(t)). (63)

(55)

Next steps make use of the following lemma.

Then x(·), h(·) in (54) are globally bounded in forward
time, for t ≥ t0 , provided that the following conditions
hold for some d ∈ (0, 1), κ ∈ (1, ∞):


κ
ε ≥ ∆ 1 + ̺(0) κ−d
+ ∆d ,
(56)


−1
h(t0 )
 .(57)
̺−1 κd
Dγ ≤ κ−1
κ̺(0)
κ
̺(0)kx(t )k+|h(t )|c 1+
0

0

Lemma 12 Consider
ẋ = Ax + u(t) + d(t),

where A and C are defined as in (1), and u, d :
R → Rn , u ∈ C 1 , d ∈ C. Let u, u̇, d be bounded:
max{ku(t)k, ku̇(t)k} ≤ B, kd(t)k ≤ ∆ξ for all t ≥ t0 .

(1−d)

The proof of Lemma 11 is provided inRthe Appendix.
t
Notice that h(t) in (53) satisfies −γDσ t0 ke(τ )kε dτ ≤
h(t) − h(t0 ) ≤ 0. Indeed, kCT e1 kε ≤ kekε by virtue of
definition of k · kε and C, and the function σ in (53) is
Lipschitz (see (27)). Thus (53) is of the form (54) where
c = Dρ Dv Dλ /ρ, ∆ = Dρ Dv ∆λ /ρ + Dρ ∆ξ /ρ,
̺(s) = Dρ e−ρs .

Then, if the solution of (64) is globally bounded for all
t ≥ t0 , there exist κ1 , κ2 ∈ K:
ky(τ )k∞,[t0 ,∞) ≤ ε ⇒ ∃ t′ (ε, x0 ) ≥ t0 :
kz1 (τ ) + u1 (τ )k∞,[t′ ,∞) ≤ κ1 (ε) + κ2 (∆ξ ),

(58)

where z1 = (1 0 . . . 0)z,

Notice also that because (49) holds, the value of t′ in
(52) can be chosen arbitrarily large. This implies that
the value of h(t0 ) in (53) may be chosen arbitrarily large
too. Having this in mind, and invoking Lemmas 10, 11
we can conclude that choosing ε, γ in (28) as


κ
ε ≥ r0 (∆), r0 (∆) = ∆ 1 + Dρ κ−d
,
0 < γ < γ̄ = min{γ ∗ , Dγ,∞ }

−1
ρ
κ−1
Dγ,∞ = D
ln Dρ κd
c(1+κDρ /(1−d)) ,
σκ

ż = Λz + Gu(t), Λ =
G=

limt→∞

t0

limt→∞ λ̂(t) = λ∗ , λ∗ ∈ Ωλ .

+ v2 (t, λ̂(t), λ) + v3 (t),

(65)

, z(t0 ) = 0,

(1 0 . . . 0)eΛ(t−t0 ) p + z1 (t) + u1 (t) = 0.

(66)

The proof of Lemma 12 is provided in the Appendix. According to (8) and Assumption 3.2, v1 (·, θ̂(·), λ̂(·), θ, λ),
v2 (·, λ̂(·), λ), v3 (·) and v̇1 , v̇2 are bounded. Thus assumptions of Lemma 12 are met for equations (62), (63),
and hence (61) implies that there is a t1 (ε) ≥ t0 and
κ1 , κ2 ∈ K such that ∀ t ≥ t1 (ε) we have:

(60)

 T
ϕ (t, λ̂(t), y(t))θ̂(t) − ϕT (t, λ, y(t))θ + v2,1 (t, λ̂(t), λ)

Rt
+C̃T t0 eΛ(t−τ ) Gv2 (τ, λ̂(τ ), λ)dτ  ≤ κ1 (ε) + κ2 (∆ξ ),

(61)

where G ∈ R(n−1)×n , C̃ ∈ Rn−1 are defined as
in (18)–(20), and v2,1 (·) is the first component of
Rt
v2 (·). Given that t0 eΛ(t−τ ) Gv2 (τ, λ̂(τ ), λ)dτ =
Rt
R t Λ(t−τ )
e
Gv2 (τ, λ∗ , λ)dτ + t0 eΛ(t−τ ) G(v2 (τ, λ̂(τ ), λ)
t0

Part 3. Let us rewrite the equation for ė1 in (36) as:
ė1 = (A + ℓ CT )e1 + v1 (t, θ̂(t), λ̂(t), θ, λ)

,

y(t) = 0 ∀ t ≥ t0 ⇒ ∃ p ∈ Rn−1 : ∀ t ≥ t0

Noticing that σ(kCT e(τ )kε ) is uniformly continuous and
using Barbalat’s lemma we conclude that
limt→∞ supτ ∈[t,∞) kCT e1 (τ )k ≤ ε.

−b In−1



0

!

Moreover, if d(t) ≡ 0, then

(59)

T

σ(kC e1 (τ )kε )dτ = h̄, h̄ ∈ R,



−b

In−2

and b = col (b1 , . . . , bn−1 ): real parts of the roots of
sn−1 + b1 sn−2 + · · · + bn−1 are negative.

where γ ∗ is defined as in (44), ensures that the function
h(·) in (53) is bounded. Given that h(·) by construction
is monotone and bounded, the Bolzano-Weierstrass theorem implies that h(t) converges to a limit, and hence
Rt

(64)

y = CT x, x(t0 ) = x0 , x0 ∈ Rn ,

(62)

11

−v2 (τ, λ∗ , λ))dτ , noticing that Λ is Hurwitz and that,
according to (60) v2 (t, λ̂(t), λ) − v2 (t, λ∗ , λ) → 0 as
t → ∞, we can conclude that there is a t2 (ε) ≥ t1 (ε)
such that η(t, θ̂(t), λ∗ , θ, λ) defined as in (19) satisfies:

6
6.1

Recall that the restriction of α2 (t, (λ′ , θ ′ ), (λ, θ)) =
η(t, θ′ , λ′ , θ, λ) on R≥t0 × Rp+m × Rp+m is wNPE with
L, β, E. Let t3 (ε) be such that kCT e1 (t)k < 2ε for all
t ≥ t3 (ε) (existence of such t3 (ε) follows from (61)). Consider the sequence {τi }∞
i=0 , τi = max{t3 (ε), t2 (ε)} + iL.
Since ϕ(·, λ̂(·), y(·)) is bounded, there is an Mθ ∈ R>0 :

ẋ = Ax + Ψ(t, λ, y)θ + g(t, λ, y, u(t)) + ξ(t),
!
0
I
n−1
y = CT x, A =
, C = col(1, 0, . . . , 0),
0 0

dist

θ̂(t)

!

!

, E(λ, θ)

(68)

Let B = col (1, b1 , . . . , bn−1 ) be a vector such that the
polynomial sn−1 + b1 sn−2 + · · · + bn−1 is Hurwitz. As
an observer candidate for (70) we propose a system in
which Sw is defined as in (28), and Sa is given as follows:
Ṁ = (A − BCT A)M + (In − BCT )Ψ(t, λ̂(t), y(t)),
˙
ζ̂ = Aζ̂ + ℓ (CT ζ̂ − y(t)) + BϕT (t, λ̂(t), y(t), [λ̂, y])θ̂

≤ 2εMθ +

+g(t, λ̂(t), y(t), u(t)),
(71)
˙
T
θ̂ = −γθ (C ζ̂ − y(t))ϕ(t, λ̂(t), y(t), [λ̂, y]), γθ ∈ R>0 ,

β −1 (κ1 (ε) + κ2 (∆ξ ) + ε(Mθ Bϕ + 1)) ∀ t ≥ t′ (ε).

x̂ = ζ̂ + Mθ̂, M ∈ Rn×m , M(t0 ) = 0,

Notice that r0 in (59) is a class K∞ function of ∆. PaD D ∆
rameter ∆, as defined in (58), is the sum: ∆ = ρ ρv λ +

where

Dρ ∆ξ
ρ . Given that the

value of ∆λ can be chosen arbitrarily, we pick ∆λ = ∆ξ . This renders r0 in (59) a class K∞
(and hence class K) function of ∆ξ . Denote this function
as r1 , then ε > r1 (∆ξ ) implies that
β −1 (κ1 (ε) + κ2 (∆ξ ) + ε(Mθ Bϕ + 1)) + 2εMθ <
β −1 (κ1 (ε) + κ2 (r1−1 (ε)) + ε(Mθ Bϕ + 1))
+2εMθ = r2 (ε).

(70)

where Ψ : R × Rp × R → Rn×m , Ψ ∈ C 1 , is Lipschitz in
λ, and Ψ(·, λ, y(·)), Ψ̇(·, λ, y(·)) are bounded on R≥t0 .
The function ξ and parameters are defined as in (7), and
the function g satisfies Assumption 3.2.

for all t ≥ τ0 . Hence ∀ t : t ∈ [τi , τi+1 ], i ∈ N, we have:
kη(t, θ̂(τi ), λ∗ , θ, λ)k ≤ κ1 (ε) + κ2 (∆ξ ) + ε(Mθ Bϕ + 1).
This, however, implies
that !
there is an N ∈ N such
!
∗
λ
, E(λ, θ) ≤ β −1 (κ1 (ε) + κ2 (∆ξ ) +
that dist
θ̂(τi )
ε(Mθ Bϕ + 1)) for all i ≥ N . Therefore, taking (43), (68)
into account, we can conclude that there is a t′ (ε):
λ̂(t)

Removing passivity requirement (Assumption 3.1)

Theorem 7 requires that A, B, C in (7) satisfy Assumption 3.1. Here we invoke the idea of filtered transformations [26], [27] to show how observer (22), (28) can be
modified so that this condition could be replaced with
the requirement that the pair A, C is observable. Consider a generalization of (7)


kη(t, θ̂(t), λ∗ , θ, λ)k = ϕT (t, λ∗ , y(t))θ̂(t)−
Rt
ϕT (t, λ, y(t))θ + C̃T t0 eΛ(t−τ ) Gv2 (τ, λ∗ , λ)dτ

+v2,1 (t, λ∗ , λ) ≤ κ1 (ε) + κ2 (∆ξ ) + ε ∀ t ≥ t2 (ε).(67)

kθ̂(τ ) − θ̂(τi )k∞,[ti ,ti+1 ] ≤ ε2γθ Bϕ L = εMθ

Discussion and generalization

ϕT (t, λ̂(t), y(t), [λ̂, y]) =CT AM(t, [λ̂, y])
+ CT Ψ(t, λ̂(t), y(t)).

(72)

The first row of M is zero for all t ≥ t0 , and ŷ = CT x̂ =
CT ζ̂. Since Ψ(·, λ̂(·), y(·)) is bounded on R≥t0 and Lipschitz in λ̂, M(·, [λ̂, y]), Ṁ(·, [λ̂, y]) are globally bounded
on R≥t0 , and M(t, [λ̂, y]) is Lipschitz in λ̂ for λ̂ = const.
Let ζ = x − Mθ, then using (70)–(72) we can write

(69)

Thus (34) holds.

ζ̇ =Aζ + Bϕ(t, λ̂(t), y(t), [λ̂, y])θ + (Ψ(t, λ, y(t), u(t))
Finally, if E(λ, θ) coincides with E0 (λ, θ), then Assumption 3.2 and (34) imply that kv1 (t, θ̂(t), λ̂(t), θ, λ) +
v2 (t, λ̂(t), λ)k < M1 r2 (ε) for some M1 ∈ R>0 , t ≥ t′ (ε).
Since A + ℓ CT in (62) is Hurwitz, (35) follows. 

− Ψ(t, λ̂(t), y(t), u(t)))θ + g(t, λ, y(t), u(t)) + ξ(t).
Dynamics of (70), (71) in the coordinates e1 = ζ̂ − x +
12

Rt
Ψ(t, λ, y(t))θ + g1 (t, λ, y(t), u(t)) +C̃T t0 eΛ(t−τ ) G
(Ψ(τ, λ, y(τ ))θ + g(τ, λ, y(τ ), u(τ )))dτ , on R≥t0 × Ωλ .

Mθ, e2 = θ̂ − θ is
ė1
ė2

!

A + ℓ CT

=

−γθ α(t)CT
+

!
!
BαT (t)
e1

Consider now systems (9). Since A, C is observable,
there is a coordinate transformation x 7→ T (A)x bringing system (9) into the form (70), albeit with the functions Ψ, g and vector θ defined differently. An example
illustrating the viability of this approach is provided in
Section 7. Notice also that observability of A, C implies
that the system ẋ = Ax+ Ψ̃(t, λ, x)θ + g̃(t, λ, x, u(t))+
ξ(t), y = CT x, in which the functions Ψ̃, g̃ are bounded
and Lipschitz in x can be brought into the form (70) by
using an auxiliary high-gain observer (cf. [16]).

0

e2
!
ṽ(t, λ̂(t), λ, y(t), u(t))

(73)

0

where α(t) = ϕ(t, λ̂(t), y(t), [λ̂, y]), ṽ(t, λ̂, λ, y, u) =
(Ψ(t, λ̂, y) − Ψ(t, λ, y))θ + g(t, λ̂, y, u) − g(t, λ, y, u) −
ξ(t). Since the pair A, C is observable there always is an
ℓ so that (2) holds. The structure of (73) is now identical to that of (36), and Assumptions 3.1, 3.2 hold for the
functions ϕ, g in (71). Finally, consider the function η 1 :

6.2

η 1 (t, λ, θ, λ′ , θ′ ) = ϕT (t, λ′ , y(t), [λ′ , y])(θ′ − θ)+
T

′

′

C (Ψ(t, λ , y(t)) − Ψ(t, λ, y(t)))θ + g1 (t, λ , y(t), u(t))−
g1 (t, λ, y(t), u(t)) + q(t, λ′ , λ, θ),
where q(t, λ′ , λ, θ) = C̃z(t, λ, λ′ , θ), ż = Λz +
G((Ψ(t, λ′ , y(t)) − Ψ(t, λ, y(t)))θ + g(t, λ′ , y(t), u(t)) −
g(t, λ, y(t), u(t))), z(t0 ) = 0, and C̃, Λ, G are defined
as in (20). The following is now immediate:

Presence of measurement noise

Suppose now that observations of system (7) output, y,
are corrupted by noise. That is, instead of y = CT x we
can access only the variable yd = CT x + d, yd ∈ Dy ,
where d : R → R, d ∈ C 1 , kd(τ )k∞,[t0 ,∞) ≤ ∆d , ∆d ∈
˙
R≥0 , and |d(t)|
is bounded. In this case the variable y in
the observer definition (22), (28) is replaced by yd , and
the dynamics of e1 = x̂ − x, e2 = θ̂ − θ becomes:
ė1

Theorem 13 Consider (70), (71), (28)–(30). Suppose that condition A1 of Assumption 4.1 holds for
the function α3 : R≥t0 × Ωλ → Rm , α3 (t, λ) =
ϕ(t, λ, y(t), [λ, y]), where ϕ is defined as in (71). Furthermore, let the restriction of α4 : R × Rp+m ×
Rp+m → R, α4 (t, (λ, θ), (λ′ , θ′ )) = η 1 (t, λ, θ, λ′ , θ′ ) on
R≥t0 × Rp+m × Rp+m be weakly nonlinearly persistently
exciting in (λ, θ) wrt to the map E1 :

ė2

!

=

A + ℓ CT
−γθ α(t, λ̂(t))CT
+

!
!
BαT (t, λ̂(t))
e1
0

!
v(t, λ̂(t), λ, yd (t), u(t))
0

+

e2
!
ξ 1 (t)

ξ 2 (t)

where αT (t, λ̂) = ϕT (t, λ̂, yd (t)), v(t, λ̂, λ, yd , u) =
B(ϕT (t, λ̂, yd ) − ϕT (t, λ, yd ))θ + g(t, λ̂, yd , u) −
g(t, λ, yd , u) − ξ(t), and ξ 1 (t) = B(ϕT (t, λ, yd (t)) −
ϕT (t, λ, y(t)))θ+(g(t, λ, yd (t), u(t))−g(t, λ, y(t), u(t)))−
ℓ d(t), ξ 2 (t) = −γθ d(t)α(t, λ̂(t)). It can now be seen that
if ϕ, g are Lipschitz in y then there is an Md > 0 such
that max{kξ1 (τ )k∞,[t0 ,∞) , kξ2 (τ )k∞,[t0 ,∞) } ≤ Md ∆d .
Thus invoking Lemmas 10, 11 and following the argument provided in proof of Theorem 7 one can establish
existence of γ > 0 and ε > 0 such that (60), (61) hold.
Convergence of the estimates will also follow subject
to corresponding persistency of excitation requirements
(cf. part 3 of the proof). An illustration of the influence
of measurement noise on performance of the observer is
provided in Section 7, Fig. 2.

E1 (λ, θ) = {(λ′ , θ′ ), λ′ ∈ Rp , θ′ ∈ Rm |B(θ′ − θ)T ·
ϕ(t, λ′ , y(t), [λ′ , y]) + (Ψ(t, λ′ , y(t)) − Ψ(t, λ, y(t)))θ
+ g(t, λ′ , y(t), u(t)) − g(t, λ, y(t), u(t)) = 0, ∀ t ≥ t0 }.
Then there exist a constant γ̄ ∈ R>0 and functions
r1 , r2 , r3 ∈ K such that if γ, ε are the corresponding parameters of (28), and γ ∈ (0, γ̄), ε > r1 (∆ξ ), then (35),
(34) hold (with E replaced by E1 ) for the interconnection
(70), (71), and (28).
The proof is largely identical to that of Theorem 7 (a
sketch is presented in the Appendix). According to Remarks 6, 9 one can replace the requirement that the restriction of α4 on R≥t0 × Rp+m × Rp+m is wNPE with
L, β, E1 with that of the λ′ -uniform persistency of excitation of the the restriction of α5 :

7

Examples

Consider the following system:

α5 (t, λ′ ) = (ϕT (t, λ′ , y(t), [λ′ , y]), R1 (t, λ, λ′ , θ)),
(74)
R1 ∂
′
′
where R1 (t, λ, λ , θ) =
r (t, s(ξ, λ, λ ), θ)dξ,
0 ∂s 1
s(ξ, λ, λ′ ) = λ′ ξ + (1 − ξ)λ, and r1 (t, λ, θ) = CT

ẋ = Ax + Bθ + B(sin(λ cos(t)) + eλ sin(t) ) + ξ(t), (75)
!
!
y = CT x, x(t0 ) = x0 ,
−2 1
1
A=
, B=
,
C = col(1, 0, . . . , 0),
−1 0
1
13

example

where θ ∈ [0, 1] = Ωθ , λ ∈ [0.1, 1] = Ωλ are unknown
parameters, and x0 is only partially known. The function ξ : R → R2 , ξ(t) = 0.001col (sin(t), cos(t)), stands
for the unmodeled dynamics and is supposed to be unavailable for direct observation.



P=

−1 1

!

, Q=

6 −3

−3 2

!

, ℓ =

0
0

!

and ρ = 0.5, Dρ = 4.242. Picking d = 0.2, κ = 2 results
in γ̄ = 0.00286. Since the values of d, κ are now defined,
we can set the value of ε. Taking (58), (59) into account
and noticing that ∆λ = 0 (this is because trajectories of
(77) in which the term tanh(kCT x̂ − y(t)kε ) is replaced
with 1 will pass through
every point

 of Ωλ = [0.1, 1]), we
Dρ ∆ξ
κ
arrive at ε ≥ ρ
1 + Dρ κ−d , where ∆ξ : kξ(t)k ≤
∆ξ . Given that ∆ξ = 0.001 we obtain: ε ≥ 0.018.

,

and Assumption 3.2 with Dϕ = 0, Dg = Bg = Mg =
√
2(1 + e), Bϕ = 1, and Mϕ = 0. According to (22)–
(28) the observer candidate is:
x̂˙ = Ax̂ + Bθ̂ + B(sin(λ̂ cos(t)) + eλ̂ sin(t) )
˙
θ̂ = −γθ (CT x̂ − y(t))

(76)

λ̂ = 0.1 + 0.45(s1 + 1), s21 (t0 ) + s22 (t0 ) = 1.

(77)






Φ(t, t0 ) = eA1 (t−t0 ) , A1 = 
 −1 0 1  ,
−1 0 0

Let the task be to infer the values of x, θ, λ from the
measurements of y over time. System (75) belongs to the
class of equations described by (7) with ϕ(t, λ, y) = 1
∀ t, λ, y, and g(t, λ, y, u) = B(sin(λ cos(t)) + eλ sin(t) ).
Moreover, it satisfies Assumption 3.1 with
2 −1

−2 1 1

Computer simulation results of the combined system
(75) – (77) with parameters θ = 0.2, λ = 0.7, γ = 0.0028,
γθ = 1, ε = 0.018 are summarized in Fig. 2. As can
be observed, the system has two weakly attracting sets
(marked as white circles). These sets correspond to the
true values of θ and λ. Even though trajectories of the
system are converging to the attracting sets asymptotically, small neighborhoods of these sets are not forwardinvariant. Hence the sets themselves are not globally
asymptotically stable, albeit they are clearly attracting.
Middle panel depicts typical trajectories of λ̂ and θ̂. To
show how the proposed observer behaves in presence
of measurement noise we simulated the model-observer
system in which signal y(t) = x1 (t) in the observer subsystem was replaced with yd (t) = x1 (t) + 0.05 sin(2t).
The value of ε was changed to 0.068 to account for this
perturbation. The observer retained functionality, albeit
with lower precision of estimation.

ṡ1 = γ tanh(kCT x̂ − y(t)kε )(s1 − s2 − s1 (s21 + s22 ))
ṡ2 = γ tanh(kCT x̂ − y(t)kε )(s1 + s2 − s2 (s21 + s22 ))

Parameters γ, γθ , and ε will be specified in a later stage.
Note that ϕ(t, λ, y) = 1. Hence condition A1 in Assumption 4.1 holds. Notice also that the function η, as
defined in (19), in this case becomes: η(t, λ, θ, λ′ , θ′ ) =
′
θ − θ′ + sin(λ cos(t)) + eλ sin(t) − sin(λ′ cos(t)) − eλ sin(t) .
One can now check that condition A2 is satisfied as well.
According to Theorem 7, system (76), (77) is an adaptive observer for (75) subject to the choice of γ, γθ , and
ε. A procedure for setting specific values of these parameters can be derived from the argument provided in the
proof of the theorem. Let us show how this procedure
works in this example.

In order to illustrate the behavior of the system in
the case of multiple equivalent parameterizations,
we simulated a modified version of the combined
system (75)–(77), in which the nonlinearly parameterized terms, i.e. B(sin(λ cos(t)) + eλ sin(t) ) in (75)
and B(sin(λ̂ cos(t)) + eλ̂ sin(t) ) in (76), are replaced
2
with B(sin((λ − 0.45)2 cos(t)) + e(λ−0.45) sin(t) ) and
2
B(sin((λ̂ − 0.45)2 cos(t)) + e(λ̂−0.45) sin(t) ) respectively.
Assumptions 3.1, 3.2 and A1 in Assumption 4.1 still
hold for the modified system (with the same values
of parameters). Yet, system (75) with the modified
2
g(t, λ) = B(sin((λ − 0.45)2 cos(t)) + e(λ−0.45) sin(t) ) is
no longer uniquely identifiable since g(t, 0.7) = g(t, 0.2)
for all t. Simulation results of the modified system are
presented in Fig, 2, right panel. Instead of two attracting sets as in the previous configuration, the modified
system has four weakly attracting sets corresponding
to two equivalent parameterizations θ = 0.2, λ = 0.7
(true) and θ = 0.2, λ = 0.2 (spurious). Parameter estimates converge to small vicinities of these alternative
parameterizations. Note that the estimates do not jump

According to the theorem, parameter γθ is an arbitrary
positive number; here, for simplicity, we set γθ = 1. Parameters γ, ε are to satisfy (58), (59). The choice of γ is
subjected to two constraints. The first constraint is γ ∈
(0, γ ∗ ], where γ ∗ is specified in (44). It ensures that the
restriction of ϕ(·, λ̂(·), y(·)) on R≥t0 is persistently exciting. In our case ϕ(t, λ̂(t), y(t)) is independent on λ̂(t),
and this property holds for any γ > 0. The second con
−1
ρ
κ−1
straint is: γ < D
ln Dρ κd
c(1+κDρ /(1−d)) , κ >
σκ
1, d ∈ (0, 1),
√ Dσ = 1, c = Dρ Dv Dλ /ρ, where Dλ =
0.45, Dv = 2(1 + e) (see (52), (45)), and ρ and Dρ are
such that the fundamental matrix of solutions of (38),
Φ(t, t0 ), satisfies: kΦ(t, t0 )pk ≤ Dρ e−ρ(t−t0 ) kpk. In this
14

Figure 2. Left panel: a qualitative picture of the system dynamics in the coordinates (s1 , s2 , θ̂). Shaded regions depict envelops of
15 trajectories of the system for various initial conditions. Actual trajectories of the system are very oscillatory, and individual
trajectories are hardly distinguishable. Qualitatively, their behavior is shown by the black arrowed lines. Middle panel: typical
simulated trajectories of θ̂, λ̂ as functions of t. Solid and dark grey curves correspond to the case when no measurement noise
is added; dashed and light grey curves show trajectories of the estimates in presence of additive measurement noise. Dotted
lines indicate true values of θ and λ. Right panel: envelops of the modified system solutions in the coordinates (s1 , s2 , θ̂).

between neighborhoods of θ = 0.2, λ = 0.7 and θ = 0.2,
λ = 0.2, which is consistent with Remark 8.
Finally, we illustrate the applicability of our approach
to models (9). Consider the third example from Table 1
with nominal parameter values as follows: τm = 0.1666,
τs = 5, Af = 1, σf = 2, σs = 0.8. Suppose that true
values of these parameters are unknown, but it is known
that they are within ±25% of their nominal values.
Since the pair A, C is observable, there is a parameterdependent coordinate
transformation x 7→ T x,
!
1
0
T =
rendering the original equations
−1
τs−1 −τm
!
y tanh(λy) 0
0
into (70) with Ψ(t, λ, y) =
,
0
0
y tanh(λy)


A
Af
s
g = 0 and θ = col − τ1s − τ1m , τmf , − 1+σ
τm τs , τm τs ,

Figure 3. Estimates θ̂, λ̂ as functions of t. True values of θ,
λ are shown as dashed lines.

ered as: τ̂s = θ̂2 /θ̂4 , τ̂m = −1/(θ̂1 + 1/τ̂s ), Âf = τ̂m θ̂2 ,
σ̂s = −θ̂3 τ̂s τ̂m − 1, σ̂f = Âf λ̂. Further examples may be
found in the supplementary material [38].
8

Conclusion

We derived an observer that can reconstruct asymptotically the unknown state and parameter values of a
class of systems with general nonlinear parametrization.
This class can be viewed as an extension of the adaptive observer canonical forms [5], [25]. In contrast to
earlier approaches addressing the problem of nonlinear
parametrization in the problem of adaptive observer design [12],[15], [14], [16], [22], [39], the class of parameterizations for which the reconstruction is guaranteed is
not limited to convex/concave or one-to-one functions.
We showed that reconstruction can be achieved, subject
to additional conditions of linear/nonlinear persistency
of excitation, if nonlinearly parameterized functions in
the model are bounded, differentiable and Lipschitz.

σf
Af

λ =
. Note that A, Ψ and θ differ from those in
the original parametrizaton. Let B = col (1, 1) and consider M(t, [λ, y]) = (mij (t, [λ, y])), i = 1, 2, j = 1, . . . , 4
in (71). It is clear that the polynomial s + 1 formed
by the coefficients of B is Hurwitz, m1,j (t, [λ, y]) = 0,
m2,j (t, [λ, y]) are defined as ṁ2,1 = −ṁ2,1 − y(t),
ṁ2,2 = −ṁ2,2 − tanh(λy(t)), ṁ2,3 = −ṁ2,3 + y(t),
ṁ2,4 = −ṁ2,2 + tanh(λy(t)), m2,j (t0 ) = 0, and that
ϕ(t, [λ, y]) = col(m2,1 (t, [y]) + y(t), m2,2 (t, [λ, y]) +
tanh(λy(t)), m2,3 (t, [y]), m2,4 (t, [λ, y])). Given that Af ,
σf vary within 25% of their nominal values we obtain that Ωλ = [1.2, 3.33]. For the given system, y is
bounded, ϕ(t, [λ, y]), as a function of t, λ on R≥t0 × Ωλ ,
is λ-UPE with T = 100, µ = 0.08. Moreover the restriction of α5 , defined in (74), on R≥t0 × Ωλ is λ′ -UPE
with T = 100, µ = 0.0054. Hence assumptions of Theorem 13 are met. We simulated the system and observer
(71), (28) with γθ = 4, γ = 0.004, and ε = 0 for various
initial conditions and values of θ, λ; θ̂, λ̂ approached
true values of θ, λ asymptotically as prescribed. An
example of typical behavior of the estimates is shown in
Fig. 3. Original parameters of the model can be recov-

The set to which the estimates converge is not guaranteed to be asymptotically stable. Yet the set is attracting in a weaker, Milnor sense, cf. [31]. Numerical simulations revealed that the convergence time in our approach depends heavily on the dimension of λ; it does
not, however, depend crucially on the dimension of θ.
This renders the method more efficient than exhaustive
search; the smaller the dimension of λ the more advantageous our method becomes. In this respect a related

15

question arises: is there a “best” parametrization for a
given physical model in the class of systems (7) or (9)?
The answer is likely to require quantitative assessment
of the performance of various observers for all admissible
parametrizations. We do not answer this question here,
but hope to be able to address it in future.

[18] E. Izhikevich. Dynamical Systems in Neuroscience: The
Geometry of Excitability and Bursting. MIT Press, 2007.

References

[20] A. Katok and B. Hasselblatt. Introduction to the modern
theory of dynamical systems. Cambridge Univ. Press, 1999.

[17] A. Ilchmann. Universal adaptive stabilization of nonlinear
systems. Dynamics and Control, (7):199–213, 1997.

[19] T. Johnson and W. Tucker.
Rigorous parameter
reconstruction for differential equations with noisy data.
Automatica, 44:2422–2426, 2008.

[21] Z. Lin and C. Knospe. A saturated high-gain contol for
a benchmark experiment. In Proceedings of the American
Control Conference, pages 2644–2648, 2000.

[1] H.D.I. Abarbanel, D. Creveling, R. Farisian, and M. Kostuk.
Dynamical state and parameter estimation. SIAM J. Applied
Dynamical Systems, 8(4):1341–1381, 2009.

[22] X. Liu, R. Ortega, H. Su, and J. Chu. On adaptive control
of nonlinearly parameterized nonlinear systems: Towards a
constructive procedure. Systems Control Letters, 60:36–43,
2011.

[2] A. Alessandri, M. Baglietto, and G. Battistelli. Movinghorizon state estimation for nonlinear discrete-time
systems: New stability results and approximation schemes.
Automatica, 44:1753–1765, 2008.

[23] A. Loria. Explicit convergence rates for MRAC-type systems.
Automatica, 40(8):1465–1468, 2004.

[3] V.I. Arnold. Mathematical Methods in Classical Mechanics.
Springer-Verlag, 1978.
[4] G. Bastin and D. Dochain. On-line Estimation and Adaptive
Control of Bioreactors. Elsevier, 1990.

[24] A. Loria and E. Panteley. Uniform exponential stability of
linear time-varying systems: revisited. Systems and Control
Letters, 47(1):13–24, 2003.

[5] G. Bastin and M. Gevers. Stable adaptive observers for
nonlinear time-varying systems. IEEE Trans. on Automatic
Control, 33(7):650–658, 1988.

[25] R. Marino. Adaptive observers for single output nonlinear
systems. IEEE Trans. on Automatic Control, 35(9):1054–
1058, 1990.

[6] G. Besancon. Remarks on nonlinear adaptive observer design.
Systems and Control Letters, 41:271–280, 2000.

[26] R. Marino and P. Tomei. Global adaptive observers for
nonlinear systems via filtered transformations. IEEE Trans.
on Automatic Control, 37(8):1239–1245, 1992.

[7] J.D. Boskovic. Stable adaptive control of a class of first-order
nonlinearly parameterized plants. IEEE Trans. on Automatic
Control, 40(2):347–350, 1995.

[27] R. Marino and P. Tomei. Global adaptive output-feedback
control of nonlinear systems, part I: Linear parameterization.
IEEE Trans. on Automatic Control, 38(1):17–32, 1993.

[8] C. Cao, A.M. Annaswamy, and A. Kojic.
Parameter
convergence in nonlinearly parametrized systems. IEEE
Trans. on Automatic Control, 48(3):397–411, 2003.

[28] R. Marino and P. Tomei.
Global adaptive outputfeedback control of nonlinear systems, part II: Nonlinear
parameterization. IEEE Trans. on Automatic Control,
38(1):33–48, 1993.

[9] M. Chapell. Structural identifiability of models characterizing
saturable binding: Comparison of pseudo-steady-state and
non-pseudo-steady state model formulations. Mathematical
Biosciences, 133:1–20, 1996.

[29] B. Martensson. The order of any stabilizing regulator is
sufficient a priori information for adaptive stabilization.
Systems and Control Letters, 6(2):87–91, 1985.

[10] L. Denis-Vidal and Joly-Blanchard.
Equivalence and
identifiability analysis of uncontrolled nonlinear dynamical
systems. Automatica, 40:287–292, 2004.

[30] B. Martensson and J.W. Polderman.
Correction and
simplification to “the order of any stabilizing regulator is
sufficient a priori information for adaptive stabilization”.
Systems and Control Letters, 20(6):465–470, 1993.

[11] J. Distefano and C. Cobelli. On parameter and structural
identifiabiliy: Nonunique observability/reconstructibility for
identifiable systems, other ambiguities, and new definitions.
IEEE Trans. on Automatic Control, AC-25(4):830–833, 1980.

[31] J. Milnor. On the concept of attractor. Commun. Math.
Phys., 99:177–195, 1985.
[32] S. Moreau and J.-C. Trigeassou. Modelling and identification
of a non-linear saturated magnetic circuit: Theoretical study
and experimental results. Mathematics and Computers in
Simulation, 71:446–459, 2006.

[12] M. Farza, M. M’Saad, T. Maatoung, and M. Kamoun.
Adaptive observers for nonlinearly parameterized class of
nonlinear systems. Automatica, 45:2292–2299, 2009.
[13] A. Gorban and I. Karlin. Invariant Manifolds for Physical
and Chemical Kinetics. Lecture Notes in Physics, Springer,
2005.

[33] V.V. Nemytskii and V.V. Stepanov. Qualitative theory of
differential equations. Princeton Univ. Press, 1960.

[14] H. Grip. State and parameter estimation for linear systems
with nonlinearly parameterized perturbations. In Proceedings
of the 48-th IEEE Conference on Decision and Control, pages
8218–8225, 2009.

[34] Jean-Baptiste Pomet. Remarks on sufficient information for
adaptive nonlinear regulation. In Proceedings of the 31-st
IEEE Conference on Decision and Control, pages 1737–1739,
1992.

[15] H.F. Grip, T.A. Johansen, L. Imsland, and G.O. Kaasa.
Parameter estimation and compensation in systems with
nonlinearly parameterized perturbations.
Automatica,
46(1):19–28, 2010.

[35] A. Poyton, M. Varziri, K. McAuley, P. McLellan, and
J. Ramsey. Parameter estimation in continuous-time dynamic
models using principal differential analysis. Computers and
Chemical Engineering, 30:698–708, 2006.

[16] H.F. Grip, A. Saberi, and T.A. Johansen. Estimation of
states and parameters for linear systems with nonlinearly
parameterized perturbations. Systems and Control Letters,
60(9):771–777, 2011.

[36] C.V. Rao, J.B. Rawlings, and D.Q. Mayne. Constrained state
estimation for nonlinear discrete-time systems: Stability and
moving horizon approximation. IEEE Trans. on Automatic
Control, 48:246–258, 2003.

16

Rt
Noticing that z1 = C̃T t0 eΛ(t−τ ) Gu(τ )dτ and denoting κ1 (ε) = 2 D
k (kakε + kbkυ1 (ε)) + υ1 (ε), κ2 (∆ξ ) =
(∆
+
kbkυ
2D
ξ
2 (∆ξ )) + υ2 (∆ξ ) we can conclude that
k
there is a t′ (ε, x0 ) ≥ t1 (ε) such that

[37] P. Rowat and A. Selverston. Modeling the gastric mill central
pattern generator with a relaxation-oscillator network.
Journal of Neurophysiology, 70(3):1030–1053, 1993.
[38] I. Tyukin, E. Steur, H. Nijmeijer, and C. van Leeuwen.
Supplementary material for: Adaptive observers and
parameter estimation for a class of systems nonlinear in the
parameters. http://arxiv.org/abs/1304.4020, 2013.

kz1 (τ ) + u1 (τ )k∞,[t,∞) ≤ κ1 (ε) + κ2 (∆ξ ) ∀ t ≥ t′ (ε).

[39] I.Yu. Tyukin, D. V. Prokhorov, and C. van Leeuwen.
Adaptation and parameter estimation in systems with
unstable target dynamics and nonlinear parametrization.
IEEE Trans. on Automatic Control, 52(9):1543–1559, 2007.

Noticing that y(t), d(t) ≡ 0 ⇒ e(t) ≡ 0 ensures that
(66) holds too.


[40] I.Yu. Tyukin, E. Steur, H. Nijmeijer, and C. van Leeuwen.
Non-uniform small-gain theorems for systems with unstable
invariant sets. SIAM Journal on Control and Optimization,
47(2):849–882, 2008.

A

R t+T
Proof of Lemma 10. Consider J(λ, t) = zT t

R
t+T
α(τ, λ)αT (τ, λ)dτ z =
kzT α(τ, λ)k2 , where
t
z ∈ Rn+m , z 6= 0, for t ∈ R≥t0 . According to C2 we
have: J(λ, t) ≥ µkzk2 ∀ λ ∈ Ωλ . Let λ̂ : R≥t0 → Ωλ
be a differentiable function, and consider J(λ̂(t), t) −
R t+T T
R t+T T
kz α(τ, λ̂(τ ))k2 dτ
=
kz α(τ, λ̂(t))k2
t
t
R
t+T
T
2
T
− kz α(τ, λ̂(τ ))k dτ =
kz α(τ, λ̂(t))k2 −
t
zT α(τ, λ̂(t))αT (τ, λ̂(τ ))z + zT α(τ, λ̂(t))αT (τ, λ̂(τ ))z−
R t+T T
kzT α(τ, λ̂(τ ))k2 dτ = t
z α(τ, λ̂(t))[αT (τ, λ̂(t)) −
R
t+T T
αT (τ, λ̂(τ ))]z + t
z [α(τ, λ̂(t)) − α(τ, λ̂(τ ))]
αT (τ, λ̂(τ ))zdτ . Applying the Cauchy-Schwarz inequality to the last equality, and invoking C4 and
R t+T T
C3 we obtain: J(λ̂(t), t) − t
kz α(τ, λ̂(τ ))k2 dτ ≤
 12
R t+T T
2
kz
[α(τ,
λ̂(t))
−
α(τ,
λ̂(τ
))]k
dτ
2M T kzk ≤
t
˙
2
2
2DM T kzk
maxτ ∈[t,t+T ] kλ̂(τ )k. Thus (39), (40)
R t+T T
ensure that t
kz α(τ, λ̂(τ ))k2 dτ ≥ J(λ̂(t), t) −
2
rµkzk . This, in accordance with C2, guarantees
R t+T T
that t
kz α(τ, λ̂(τ ))k2 dτ ≥ (1 − r)µkzk2 . Hence
α(t, λ̂(t)) is persistently exciting in the sense of Definition 2. The value of (1 − r)µ does not depend on the
choice of λ̂ as long as (40), (39) hold. Finally, notice
that C4 and (39) guarantee boundedness of α(·, λ̂(·))
and its derivative: max{kα(t, λ̂(t))k, kα̇(t, λ̂(t))k} ≤
µr
M + M Mλ = M + 2DT
2 . Taking C1 and Theorem 3
into account we conclude that the lemma follows.


Appendix

Lemma 14 Consider ẏ = ky + u(t) + d(t), k ∈ R, u, d :
R≥t0 → R, u ∈ C 1 , d ∈ C 0 , and let max{|u(t)|, |u̇(t)|} ≤
B, |d(t)| ≤ ∆ξ . Then kyk∞,[t0 ,∞) ≤ ε ⇒ ∃ t1 (ε) ≥ t0 :
√
√
kuk∞,[t1 (ε),∞) ≤ ε(1 + e|k| ε + B) + ∆ξ .
Proof of Lemma 14. Noticing that y(t) for t ≥ t0 +T ,
T > 0, can be expressed as: y(t) = y(t − T )ekT +
Rt
ek(t−τ ) (u(τ ) + d(τ ))dτ and using the Meant−T
value theorem we obtain: y(t) − y(t − T )ekT =
′
T ek(t−τ ) (u(τ ′ ) + d(τ ′ )), τ ′ ∈ [t − T, t]. Hence
′
ε(1+ekT ) ≥ T ek(t−τ ) (|u(t)|−T B −∆ξ ), and ∆ξ +T B +
ε(1+ekT )
T min{1,ekT }

kT

ε(1+e )
≥ ∆ξ +T B + T min{1,e
k(t−τ ′ ) } ≥ |u(t)| for all
t ≥ t0 + T . Given that T can be chosen
arbitrarily we
let
√
√
√
√
k ε
−k ε
T = ε, and thus |u(t)| ≤√ ε(1+e
) max{1, e
}+
√
√
√
B ε + ∆ξ ≤ ε(1 + e|k| ε + B) + ∆ξ ∀ t ≥ t0 + ε. 

Proof of Lemma 12. Let us rewrite (64) as
ẏ = a1 y + C̃x̃ + u1 (t) + d1 (t)
x̃˙ = Ãx̃ + ãy + bu1 (t) + Gu(t) + d̃(t),
where ã = col(a2 , . . . , an ), C̃ = col(1,
 0, . . . , 0),

d̃(t) = col(d2 (t), . . . , dn (t)), and G = −b In−1 ,
!
0 In−2
Ã =
. Let ky(t)k∞,[t0 ,∞) ≤ ε and de0 0

Proof of Lemma 11. According to conditions of the
lemma, (57), we conclude that h(t0 ) ≥ 0. Introduce a
strictly decreasing sequence: {σi }, i = 0, 1, . . . , σi =
(1/κ)i , κ ∈ (1, ∞). Further, let {ti }, i = 1, . . . , t1 <
t2 < · · · < tn < · · · be an ordered infinite sequence:

note e(t) = C̃T x̃(t) + u1 (t). According to Lemma
14, there is a t1 (ε) > t0 and υ1 , υ2 ∈ K such that
ke(t)k = kC̃T x̃(t) + u1 (t)k ≤ υ1 (ε) + υ2 (∆ξ ) ∀ t ≥ t1 (ε).

h(ti ) = σi h(t0 ).

(A.1)

If {ti } satisfying (A.1) does not exist then it is clear that
h(t0 ) ≥ h(t) ≥ 0 for all t ≥ t0 . Hence, in accordance
with (29), x(·) is bounded for all t ≥ t0 , and nothing
remains to be proven. Let us now show that if (56), (57),
and (A.1) hold then

Using the notation above we obtain: x̃˙ = (Ã −
bC̃T )x̃ + ãy(t) + G̃u(t) + be(t) + d̃(t). Matrix
Ã − bC̃T = Λ is Hurwitz, and hence there are
D, k ∈ R>0 such that keΛ(t−t0 ) k ≤ De−k(t−t0 ) .
Rt
≤
Therefore kC̃T x̃(t) − C̃T t0 eΛ(t−τ ) Gu(τ )dτ k
D
−k(t−t0 )
De
kx̃(t0 )k+ k (kakε+kbk(υ1(ε)+υ2 (∆ξ ))+∆ξ ).

h(t) → 0 ⇒ t → ∞.
17

(A.2)

from (A.4), are bounded from below by:

Consider Ti = ti − ti−1 . It is clear from (55) that
T i Dγ

max

τ ∈[ti−1 ,ti ]

kx(τ )+d(τ )kε ≥ h(t0 )(σi−1 −σi ). (A.3)

Ti ≥

In addition, maxτ ∈[ti−1 ,ti ] kx(τ )+ d(τ )kε = kx(τ ) +
d(τ )k∞,[ti−1 ,ti ] −ε if kx(τ ) + d(τ )k∞,[ti−1 ,ti ] > ε, and
maxτ ∈[ti−1 ,ti ] kx(τ ) + d(τ )kε = 0 overwise, we can see
from (A.3) that

Ti ≥









h(t0 )(σi−1 −σi )
1
Dγ
kx(τ )+d(τ )k∞,[ti−1 ,ti ] −ε ,

kx(τ ) + d(τ )k∞,[ti−1 ,ti ] > ε;

(A.7)

−1
Consider σi−1
(kx(τ )+ d(τ )k∞,[ti−1 ,ti ] − ε). Taking (A.6)
into account we derive that:
−1
σi−1
(kx(τ ) + d(τ )k∞,[ti−1 ,ti ] − ε) ≤ ̺(0)̺i−1 (τ ∗ )×

κi−1 kx(t0 )k + k i−1 Pi−1 + k i−1 ∆d − k i−1 ε =
P
j ∗ j
̺(0)̺i−1 (τ ∗ )κi−1 kx(t0 )k + ch(t0 )̺(0)κ[ i−2
j=0 ̺ (τ )κ ]
P
j ∗
+ch(t0 ) + κi−1 [∆(̺(0) i−2
j=0 ̺ (τ ) + 1) + ∆d − ε].

(A.4)

∞, kx(τ ) + d(τ )k∞,[ti−1 ,ti ] ≤ ε.

Noticing that τ ∗ is chosen in accordance with (A.5) one
can therefore obtain:

Consider the case when kx(τ ) + d(τ )k∞,[ti−1 ,ti ] − ε > 0
for all i = 1, 2, . . . . Let us pick
τ ∗ = ̺−1 (d/κ) , d ∈ (0, 1),

σi−1 −σi
h(t0 )
.
−1
σi−1 Dγ σi−1
(kx(τ )+d(τ )k∞,[ti−1,ti ] −ε)

−1
σi−1
(kx(τ ) + d(τ )k∞,[ti−1 ,ti ] − ε) ≤ ̺(0)kx(t0 )k+
P
j
ch(t0 ) + ch(t0 )̺(0)κ i−2
j=0 d +
Pi−2 j
κi−1 [∆(̺(0) j=0 κd + 1) + ∆d − ε]

(A.5)

and select the value of Dγ such that (57) holds. Given
that kx(τ ) + d(τ )k∞,[t0 ,t1 ] − ε ≤ ̺(0)kx(t0 )k + ch(t0 ) +
∆ + ∆d − ε, conditions (57), (56), and (A.5) imply Dγ ≤
h(t0 )
h(t0 )(σ0 −σ1 )
κ−1
1
1
≤ (kx(τ )+d(τ
κ ̺(0)kx(t0 )k+c|h(t0 )| τ ∗
)k∞,[t0,t1 ] −ε) τ ∗ .
This, as follows from (A.4), guarantees that T1 ≥ τ ∗ .

≤ ̺(0)kx(t0 )k + ch(t0 )(1 +

̺(0)κ
1−d )+

̺(0)
k i−1 [∆( 1−d/k
+ 1) + ∆d − ε].

Without loss of generality suppose that there is an i ≥ 2:
Tj ≥ τ ∗ for all 1 ≤ j ≤ i − 1. Let us show that Ti−1 ≥
τ ∗ ⇒ Ti ≥ τ ∗ . This will ensure that (A.2) is satisfied and
that the lemma hold. Consider kx(τ )k∞,[ti−1 ,ti ] ; (54) and
(A.1) imply that: kx(τ )k∞,[ti−1 ,ti ] ≤ ̺(0)kx(ti−1 )k +
cσi−1 h(t0 ) + ∆. Hence

̺(0)
+ 1) + ∆d − ε ≤ 0.
Condition (56) implies that ∆( 1−d/k
−1
Hence σi−1 (kx(τ )+ d(τ )k∞,[ti−1 ,ti ] − ε) ≤ ̺(0)kx(t0 )k +
ch(t0 )(1 + ̺(0)κ
1−d ). Substituting the latter estimate into
(A.7) and using (57) yields Ti ≥ τ ∗ . Thus h(·) is bounded
for t ≥ t0 , and hence so is x(·).


kx(τ )k∞,[ti−1 ,ti ] ≤ ̺(0)[̺(Ti−1 )kx(ti−2 )k + cσi−2 h(t0 )]
+ ̺(0)∆ + ch(t0 )σi−1 + ∆ ≤ ̺(0)̺(τ ∗ )kx(ti−2 )k + P1 ,

T
Proof of Theorem 13. Let Λ0 = A − BC
R t A,
T
T
G0 = In − BC . Consider ϕ̃(t, λ̂(t), T1 ) = C A t−T1

eΛ0 (t−τ ) G0 Ψ(τ, λ̂(t), y(τ ))dτ + CT Ψ(t, λ̂(t), y(t)). It
is clear that for any ε1 > 0 there are T1 , t1 sufficiently
large and γ1 sufficiently small:

where P1 = ̺(0)cσi−2 h(t0 ) + cσi−1 h(t0 ) + ̺(0)∆ + ∆.
Invoking (54) again results in
kx(τ )k∞,[ti−1 ,ti ] ≤ ̺(0)̺2 (τ ∗ )kx(ti−3 )k + P2 ,

kϕ̃(t, λ̂(t), T1 ) − ϕ(t, λ̂(t), y(t), [λ̂, y])k < ε1

where P2 = ch(t0 )̺(0)[̺(τ ∗ )σi−3 + σi−2 ] + cσi−1 h(t0 ) +
̺(0)[̺(τ ∗ )∆ + ∆] + ∆, and

(A.8)

for all γ ∈ (0, γ1 ) and t ≥ t1 ≥ t0 . Indeed, conR t−T
sider δ1 (T1 , t) = CT AeΛ0 T1 t0 1 eΛ0 (t−T1 −τ ) G0
Rt
Ψ(τ, λ̂(τ ), y(τ ))dτ , δ2 (T1 , t) = CT A t−T1 eΛ0 (t−τ )

kx(τ )k∞,[ti−1 ,ti ] ≤ ̺(0)̺3 (τ ∗ )kx(ti−4 )k + P3 ,

G0 (Ψ(τ, λ̂(τ ), y(τ )) − Ψ(τ, λ̂(t), y(τ )))dτ , pick ε1 >
0, and let T1 be so large that |δ1 (T1 , t)| < ε1 /2
for t ≥ t0 + T1 . Let γ1 ∈ R>0 be so small that
|δ2 (T1 , t)| < ε1 /2 for all t ≥ t0 + T1 (such a choice is
always possible due to that Ψ is Lipschitz in λ̂). Noticing that ϕ(t, λ̂(t), y(t), [λ̂, y]) = δ1 (T1 , t) + δ2 (T1 , t) +
ϕ̃(t, λ̂(t), T1 ) we can conclude that (A.8) holds.

where P3 = ch(t0 )̺(0)[̺2 (τ ∗ )σi−4 + ̺(τ ∗ )σi−3 +
σi−2 ] + cσi−1 h(t0 ) + ∆̺(0)[̺(τ ∗ )2 + ̺(τ ∗ ) + 1] +
P
∆ = ch(t0 )̺(0)[ 2j=0 ̺j (τ ∗ )σi−j−2 ] + ch(t0 )σi−1 +
P2
∆̺(0)[ j=0 ̺j (τ ∗ )] + ∆. After i − 1 steps we obtain
kx(τ )k∞,[ti−1 ,ti ] ≤ ̺(0)̺i−1 (τ ∗ )kx(t0 )k + Pi−1 , (A.6)

Given that the restriction of ϕ(t, λ, y(t), [λ, y]) (as a
function of t, λ) on R≥t0 × Ωλ is λ-UPE with T, µ, there
is a ε1 in (A.8) such that ϕ̃(t, λ, T1 ) ∈ λUPE(T, µ − ǫ),

Pi−2
with Pi−1 = ch(t0 )̺(0)[ j=0 ̺j (τ ∗ )σi−j−2 ]+ch(t0 )σi−1
Pi−2
+ ∆̺(0)[ j=0 ̺j (τ ∗ )] + ∆. The values of Ti , as follows
18

ǫ ∈ (0, µ/3). On the other hand (see the first part of the
proof of Lemma 10), there is a γ2 such that ϕ̃(t, λ̂(t), T1 )
is persistently exciting with parameters T, µ − 2ǫ for
all γ ∈ (0, γ2 ). Choosing γ ∈ (0, min{γ1 , γ2 }) and taking (A.8) into account we conclude that the restriction
of ϕ(·, λ̂(·), y(·), [λ̂, y]) on R≥t2 is persistently exciting
(t2 > t1 > t0 ) provided that ε1 is small enough and t2
is sufficiently large. Thus, invoking the argument presented in Part 2 of the proof of Theorem 7 we can conclude that (61) and (60) hold for the combined system.
Convergence of state and parameter estimates can now
be shown similarly to the 3d part of the proof of Theorem 7.


19

