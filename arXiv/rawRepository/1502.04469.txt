arXiv:1502.04469v4 [cs.LG] 12 Mar 2015

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Classification and its applications for drug-target
interaction identification
Jian-Ping Mei, Chee-Keong Kwoh,
Peng Yang and Xiao-Li Li
School of Computer Science, Nanyang Technological University
50 Nanyang Avenue, Singapore 639798.
Classification is one of the most popular and widely used supervised
learning tasks, which categorizes objects into predefined classes based on
known knowledge. Classification has been an important research topic
in machine learning and data mining. Different classification methods
have been proposed and applied to deal with various real-world problems.
Unlike unsupervised learning such as clustering, a classifier is typically
trained with labeled data before being used to make prediction, and
usually achieves higher accuracy than unsupervised one.
In this chapter, we first define classification and then review several
representative methods. After that, we study in details the application
of classification to a critical problem in drug discovery, i.e., drug-target
prediction, due to the challenges in predicting possible interactions between drugs and targets.

1. Classification
Classification is the process of finding a model or function that describes and
distinguishes data classes or concepts.1 It is one of the most important tasks
that supervised learning is applied to. Supervised learning is an important
machine learning method which learns a model or a function with the help
of supervision. Other than classification, supervised learning is also used
for regression analysis. The goal of classification analysis is simply to know
the class label while regression analysis is to learn a function.
The rapid development of technologies, such as microarrays, highthroughput sequencing, genotyping arrays, mass spectrometry, and automated high-resolution imaging acquisition techniques, has led to a dramatic increase in availability of biomedical data.2 Facing large amount of
1

March 13, 2015

2

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

data, computational method, which is cheaper and more efficient, arises
to be useful complement to support traditional experimental method in
many biomedical researches and applications. As an important data analysis tool, classification has been applied for handling many important
tasks in bioinformatics, including Sequence annotation,3–5 Protein function prediction,6 Protein structure prediction,7 Gene regulatory network
inference,8,9 Protein-protein interaction prediction,10 disease gene identification11–13 and drug-target interaction prediction.14–17 Many of these tasks
are to search the answer of a question with “yes” or “no”. For example,
to predict whether two proteins interact or not, a protein is enzyme or non
enzyme, a piece of sequence is coding or non-coding, and so on. This type of
prediction can be directly handled with binary classification, where “yes”
and “no” are treated as two class labels. It also can be solved through
regression methods. Instead of directly answer “yes” or “no”, binomial
regression methods produce the likelihood or the degree of being “yes” or
“no”, based on which the final result can be easily obtained by cutting with
a certain threshold, i.e., “yes” if the likelihood is larger than the threshold,
and “no” if the likelihood value is below the given threshold. Next we give
more detailed introduction on several representative classification methods,
which are most widely used in computational biology.
In the following subsections, we represent the training data consisting
of n labeled examples or data objects as D = {xi , yi }ni=1 , where each xi is
a p-dimensional vector, i.e., xi = (xi1 . . . xip )T and yi is its associated class
label.
1.1. k-Nearest Neighbor (K-NN)
k-Nearest Neighbor (K-NN) is instance-based classification. In K-NN, an
unlabeled object is assigned to the most common class among its k most
nearest neighbors in the training set. In order to decide the k nearest
neighbors of the given object, the distance or closeness between this object
and all the labeled objects need to be calculated. The number of neighbors
k is an important parameter in k-NN. Setting k to different values, k-NN
may produce different results.
Now we use a simple example to illustrate how labeled data is used in
k-NN to predict the class labels of those unlabeled objects. Fig. 1 shows
a simple two-dimensional dataset. This dataset consists of seven labeled
objects belonging to two classes and two unlabeled objects. First, we set
k = 1. In this case, each unlabeled object is assigned to the same class as its

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

3

1
2
1
?
1
?

Fig. 1.

1

2

2

A simple two-dimensional dataset.

nearest neighbor. Fig. 2 shows the classification result of the two unlabeled
objects with k = 1. Since the nearest neighbor of the first unlabeled object,
i.e., the one that is located at the left lower corner, is labeled as class 1,
this object is also labeled as 1. Similarly, since the nearest neighbor of the
other unlabeled object is labeled as class 2, the class label is also predicted
as class 2 for this object. When k > 1, the neighbors of an unlabeled object
possibly have different class labels, and in such cases, the unlabeled object
is typically assigned to the most common class among its neighbors. Fig.
3 shows the classification result of k-NN with k = 3. It is seen that the
object in the left lower corner is still labeled as class 1 as all its three nearest
neighbors are in this class. However, the other object is now labeled as class
1 as two of its nearest neighbors belong to class 1, although its most nearest
neighbor belongs to class 2. Here, once k is decided, all the neighbors are
considered to be equally important in deciding the class of the unlabeled
object. Another way is to assign different weights to the neighbors so that
the k neighbors have different levels of significance of their votes.
1.2. Support Vector Machine
The classic Support Vector Machine (SVM) is a linear binary classifier.
Given a p-dimensional dataset where the training samples belong to two
classes, the goal of a linear classifier is to find a p−1 dimensional hyperplane
which separates the samples in the two classes as illustrated in Fig. 4.
Among many of such kind of hyperplanes, the one maximizes the separation
or margin of the two classes is of most interest, and the corresponding
classifier is called the maximum margin classifier. In SVM, the margin is

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

4

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

1

1

2

2
1

1
?
1
?

2

2

1

1

1

2

Fig. 2.

2

1

2

Classification result of k-NN with k = 1.

1

1

2

2
1

1
?

1
?

1

2

1

Fig. 3.

1
2

1

1

2
2

Classification result of k-NN with k = 3.

the distance from the hyperplane to the nearest samples in each of the
classes. Samples located on the boundary of each class are called support
vectors.
1.2.1. Linear SVM
Now we formally define the linear SVM. For a set of n training samples,
where each object with label −1 or 1 is a p-dimensional vector, we may
represent it as D = {(xi , yi )|xi ∈ Rp , yi ∈ {−1, 1}}ni=1, where X represents
the data and Y represents the label information. Assume that the dataset
is linearly separable, then there exist w and b such that the inequalities
are valid for all xi ∈ D:
w · xi − b ≥ 1 if yi = 1

w · xi − b ≤ −1 if yi = −1

(1)
(2)

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

5

Fig. 4. Example of linearly separable dataset in a two dimensional space. An optimal
hyperplane is the one that maximizes the distance between two classes.

The above two equations can be written into one as below
yi (w · xi − b) ≥ 1

(3)

Among the training samples, vectors xi for which
yi (w · xi − b) = 1

(4)

are called support vectors, which define the boundary of the two classes.
2
The distance or margin between the two classes is kwk
. The goal is to
find the optimal hyperplane or to decide w and b to maximize this margin
subject to (3), which requires all the training samples to be correctly clas2
is equivalent to minimizing 12 kwk2 , we can
sified. Since maximizing kwk
solve the above maximization problem by solving the equivalent minimization problem as below
1
min kwk2
2

(5)

yi (w · xi − b) ≥ 1 for i = 1, 2, . . . , n.

(6)

subject to

This constrained optimization problem can be solved with the method
of Lagrange. By introducing Lagrange multipliers αi , the Lagrangian is
constructed as
n
X
1
αi (yi (w · xi − b) − 1)
kwk2 −
2
i=1

(7)

March 13, 2015

6

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

which can be solved by standard quadratic programming techniques. According to the Karush-Kuhn-Tucker conditions, the solution of w is in the
form as below:
w=

n
X

αi yi xi

(8)

i=1

The above formula shows that w is a linear combination of the training
samples. When yi (w · xi − b) = 1, αi > 0; for other cases, αi = 0. This
means that w is only defined by a small number of support vectors, i.e.,
the training samples located at the boundary of the classes, rather than all
the training samples.
In the above formulation, we assume that the dataset is linearly separable, or there exist a hyperplane that can divide the samples according to
their class labels without any classification error. In cases that such kind
of hyperplane does not exist, we may want to find a hyperplane that correctly divide the samples as many as possible. This is called the soft margin
method. Slack variables ξi ≥ 0 are introduced to formulate this idea. The
constraints are now become
yi (w · xi − b) ≥ 1 − ξi

(9)

Since a larger ξi corresponds to a larger error in the classification of xi ,
we want to penalize large ξi through minimizing the objective function as
below
n

X
1
ξi
min kwk2 + C
2
i=1

(10)

where C is the weight parameter of the penalty term. With Lagrange
multipliers αi ≥ 0 and βi ≥ 0, the problem to be solved is written as
n
n
n
X
X
X
1
βi ξi
αi (yi (w · xi − b) − 1 + ξi ) −
ξi −
min kwk2 + C
2
i=1
i=1
i=1

(11)

1.2.2. Kernel SVM
In many cases, the data is not linearly separable. As illustrated in Fig. 5,
mapping the original space into a high or infinity dimensional feature space,
i.e., x → φ(x), possibly makes the data easier to be separated. Kernelbased approach use a kernel function κ to calculate the inner product of

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

7

Fig. 5.

A non-linearly separable dataset becomes linearly separable after mapping φ

the vectors in the high dimensional space in terms of the vectors in the
original space:
κ(xi , xj ) = φ(xi ) · φ(xj ) = φ(xi )T φ(xj )

(12)

As kwk = wT w, substituting (8) into (7), the dual of SVM is the
following optimization problem:
max =
αi

n
X
i=1

n

n

αi −

1 XX
αi αj yi yj xTi xj
2 i=1 j=1

(13)

subject to
αi ≥ 0 for i = 1, 2, . . . , n.
n
X
αi yi = 0

(14)
(15)

i=1

By substitute xi with φ(xi ) in the above formula, we get the objective
function in the mapped space, and with the kernel function given in (12),
we have the following form without defining the mapping explicitly:
max =
αi

n
X
i=1

n

αi −

n

1 XX
αi αj yi yj κ(xi , xj )
2 i=1 j=1

(16)

Below are the three commonly used kernels:
• Polynomial kernel
κ(xi , xj ) = (xi · xj + 1)d
• Gaussian kernel
κ(xi , xj ) = e−

kxi −xj k2
β

(17)

(18)

March 13, 2015

8

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

• Hyperbolic tangent kernel
κ(xi , xj ) = tanh(hxi · xj + c)

(19)

where h is the scale factor and c is the offset.
Since any positive-definite matrix could be treated as a kernel matrix, kernel
SVM can be used to make prediction based on a similarity matrix, which
records pairwise similarities between objects. To make sure kernel SVM
performs stably, some preprocess is needed if the similarity matrix given is
not positive-definite.
1.3. Bayesian classification
Bayesian classifiers are statistical classifiers based on Bayes theorem. A
Bayesian classifier generates the probability or membership of an object
with respect to each of the classes. Assume X is an object that is to
be classified or labeled and Y is the hypothesis that X belongs to some
class, then P (Y = c/X) is the probability that X belongs to the cth class.
According to the Bayes theorem, this posterior probability of Y = c conditioned on X can be calculated with posterior probability P (X/Y = c), and
prior probabilities P (X) and P (Y ):
P (Y = c/X) =

P (X/Y = c)P (Y = c)
P (X)

(20)

In the above formula, P (X) is constant for any c. If P (Y = c) is unknown,
it is usually assumed that all classes have equal probability or it is estimated
by nnc , the ratio of the number of objects in class c. The left problem is
how to calculate P (X/Y = c). To simplify computation, the values of
attributes are assumed to be conditionally independent to each other, i.e.,
given the class label of an object, there are no dependence relationships
among the attributes. Assume xj is the value of the jth feature, and there
are p features in total, then based on this assumption,
P (X/Y = c) =

p
Y

P (xj /Y = c)

(21)

j=1

and the classifier is called the Naive Bayes Classifier.
If the kth attribute is categorical, then
njc
P (xj /Y = c) =
nc

(22)

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

9

where nc is the number of objects in class c, and njc is the number of
objects in class c that have the value of the kth attribute equal to xj .
If the kth attribute is continuous-values with a probability distribution
g, e.g., the Gaussian distribution with mean µc and standard deviation δc ,
then
2

(x −µc )
1
− j 2δ2
c
e
P (xj /Y = c) = g(xj ) = √
2πδc

(23)

Once the posterior probabilities P (X/Y = c) for all c = 1, 2, . . . , k are
calculated, X is assigned to the class with the largest posterior probability,
i.e., X is labeled as class f , where f = arg maxc P (X/Y = c).
1.4. Decision Trees
A decision tree is a tree structure where each internal node denotes a test
on an attribute, each branch denotes an outcome of the test, and each leaf
node represents a class. Once a decision tree has been constructed with
training data, a new sample is tested against the decision tree from the top
node to the leaf node which corresponds to the predicted class of the new
sample.
Given a set of training objects, a decision tree is built in a top-down
recursive divide and conquer manner. A critical problem need to be considered in construction of the tree is how to select the attributes for testing.
Entropy or equivalently information gain and Gini index are commonly
used for attribute selection. The entropy measures the purity of the partitions, the smaller the entropy or the larger the information gain, the purer
the partitions are. Thus, the attribute with the minimum entropy or highest information gain is chosen as the test attribute for the current node.
Assume the training data consists of n labeled objects are distributed in
k classes, each class contains nc objects, then the expected information
needed to classify a given sample is
E=−

k
X

pc log2 pc

(24)

c=1

where pc is the probability that an arbitrary object belongs to class c. It is
estimated by nnc . For a feature a, which has h distinct values, the entropy

March 13, 2015

10

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

id
1
2
3
4
5
6
7
8
9
10
11
12
13
14

Outlook
Sunny
Sunny
Overcast
Rainy
Rainy
Rainy
Overcast
Sunny
Sunny
Rainy
Sunny
Overcast
Overcast
Rainy

Table 1. Weather Data
Temperature
Humidity
Hot
High
Hot
High
Hot
High
Mild
High
Cool
Normal
Cool
Normal
Cool
Normal
Mild
High
Cool
Normal
Mild
Normal
Mild
Normal
Mild
High
Hot
Normal
Mild
High

Windy
False
True
False
False
False
True
True
False
False
False
True
True
False
True

Play
No
No
Yes
Yes
Yes
No
Yes
No
Yes
Yes
Yes
Yes
Yes
No

based on the partitioning into k subsets by a is calculated by
E(a) =

h
X

P (j)E(j)

(25)

j=1

n

where P (j) = nj , nj is the number of objects of which the value of feature
a is equal to j, and
E(j) = −

k
X

pcj log2 pcj

(26)

c=1

is the entropy of the jth value of the ath attribute, pcj =
G(a) = E − E(a)

ncj
nj

(27)

Now we use the Weather data in Table 1 as an example to show how
to calculate the Entropy of each attribute. This data consists of fourteen
samples described by four attributes: Outlook, Temperature, Humidity and
Windy. These fourteen samples belong to two classes: Play or Not-Play. It
5
9
, P (P lay = N o) = 14
, So the Entropy
is shown that P (P lay = Y es) = 14
of Play or the expected information needed to classify a sample is
E = −(

9
9
5
5
log2 + log2 ) = 0.940
14
14 14
14

(28)

Now we calculate the Entropy of the attribute Outlook. This attrbute has
three values Sunny, Overcast, and Rainy, which occurs 5, 4, and 5 times,
4
5
5
, P (Overcast) = 14
and P (Rainy) = 14
.
respectively, i.e., P (Sunny) = 14

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

11

Among the five samples of which Outlook is Sunny, two are Play, three are
Not-Play, thus the Entropy of Sunny is
2 3
3
2
E(Sunny) = −( log2 + log2 ) = 0.971
5
5 5
5

(29)

Similarly
4
4
E(Overcast) = −( log2 + 0log2 0) = 0
4
4
3 2
2
3
E(Rainy) = −( log2 + log2 ) = 0.971
5
5 5
5

(30)
(31)
(32)

So the Entropy of Outlook is
E(Outlook)

(33)

= P (Sunny)E(Sunny) + P (Overcast)E(Overcast) + P (Rainy)E(Rainy)
(34)
4
5
5
0.971 + 0 + 0.971 = 0.694
(35)
=
14
14
14
and the Information Gain of Outlook is
Gain(Outlook) = E − E(Outlook) = 0.940 − 0.694 = 0.246

(36)

With the same steps, we can calculate the Gain of the other three attributes: Gain(T emperature) = 0.029, Gain(Humidity) = 0.152, and
Gain(W indy) = 0.048. Since Outlook has the largest Gain, it is the best
attribute of the current stage that should be selected for testing.
Once the best attribute is decided and represented as an intermediate
node of the tree, branches below this node are added where each branch
corresponds to a possible value this attribute takes. For each value, take
the subset of samples having this value of the current attribute as the input
of the next iteration for further splitting. This process continues until all
samples under consideration have the same class label. A complete decision
tree of the Weather data is shown in Fig. 6.
The tree constructed to correctly classify all the training samples may
be over-fitting. Pruning handle the over-fitting problem by removing least
reliable branches. Other than a higher classification accuracy, pruning also
results in a simplified tree which makes the test process faster. Pruning
performed during the construction of the tree is called Prepruning. It stops
the construction early with less purity. Pruning can also be performed
by removing branches from a fully grown tree. This type is called the

March 13, 2015

1:18

12

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

Outlook
Sunny

Rainy
Overcast
Windy

Humidity

Yes

Normal

High

True

False
Yes

Yes

Fig. 6.

PL < 2.45

No

No

The decision tree of the Weather data.

PL >= 2.45

PL < 2.45

PW < 1.75

setosa

PL >= 2.45

PW >= 1.75

PW < 1.75

setosa

PL < 4.95

PW < 1.65

versicolor

PW >= 1.65

PL >= 4.95

virginica

virginica

PL < 4.95

versicolor

PW >= 1.75

PL >= 4.95

virginica

virginica

virginica

Fig. 7. The decision tree of the Iris data. (a) The unpruned three, (b) The tree with
pruning.

post-pruning. Fig. 7 shows the unpruned and pruned decision three of
the Fisher’s iris data. This dataset consists of 50 samples from each of
three species of Iris (setosa, virginica and versicolor). Four features were
measured from each sample: sepal length (SL), sepal width (SW), petal
length (PL), and petal width (PW).
ID3 is a popular decision tree algorithm proposed by Ross Quinlan,18
and C4.519 is an extension of ID3 with improved computing efficiency, and
other more functions, including dealing with continuous values, handling
attributes with missing values, and avoiding over fitting. Another algorithm called Classification and regression trees (CART) proposed by Leo
Breiman20 produces either classification or regression binary trees, depending on whether the dependent variable is categorical or numeric, respectively. The study in2 reviews tree-based classification approaches and their
applications in bioinformatics.

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

13

1.5. Regression models for classification
Other than these previously reviewed supervised learning methods which
are widely used for classification, regression models may also be used for
classification analysis. Regression methods the relationship between a dependent variable and one or more independent variables. Specifically, regression is to analyse how the value of the dependent variable changes when
any one of the independent variable varies while other independent variables
fixed. The dependent variable is the output variable or response variable,
and the independent variables are input variables or explanatory variables.
Next we discuss two regression models namely the Logistic Regression and
Regularized Least Squares, which are frequently used for classification purpose.
1.5.1. Logistic Regression
Logistic Regression is a type of binomial regression that predicts the probability of the outcome of a “yes or no” type trial using logistic function.
Formally, the Logistic Regression models the relation between dependent
variable yi and independent variables xi = (xi1 . . . xip )T by
yi =

1
T
e−(xi β+ǫi ) + 1

(37)

where β = (β1 . . . βp )T are regression coefficients, and ǫi is the error term.
Let
t=

p
X

βj xij + ǫi = xTi β + ǫi f or i = 1, 2, . . . n

(38)

j=1

then yi = f (t), where f (t) is the logistic function
f (t) =

1
e−t + 1

(39)

A property of the logistic function is like distribution function, its output
is between 0 and 1 for any input in the full range from negative infinity
to positive infinity, i.e., f (t) ∈ [0, 1] for t ∈ (−∞, ∞). The coefficients
are usually estimated with maximum likelihood estimation with iterative
algorithms such as Newton’s Method. Once the coefficients are learned, the
logistic regression can be used for binary classification where the predicted
value ŷi is the probability of being “yes”.

March 13, 2015

14

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

1.5.2. Regularized Least Squares
Unlike many other regression models, such as Logistic Regression, the Regularized Least Squares (RLS) method does not require the examples to be
represented as feature vectors explicitly as it learns the model and makes
prediction with a kernel matrix K, where each entry kij ∈ K = κ(xi , xj )
is defined by a certain kernel function, e.g., Gaussian kernel in (18). For
a dataset with labels y = (y1 y2 . . . yn )T , and kernel matrix K, the Regularized Least Squares (RLS) is to find coefficients c = (c1 c2 . . . cn )T to
minimize the following value
δ
1
ky − Kck22 + cT Kc
(40)
2
2
where the first term is the least squares term and the second term is the
regularization term with weight δ. The solution of c that minimizes the
above value has a simple closed form as below
c = (K + δI)−1 y

(41)

Once c is obtained, we can use it to predict the label ŷ of a new data object
x̂ by
ŷ = k̂T (K + δI)−1 y

(42)

k̂ is an n-dimensional vector where each dimension kˆi is the value of the kernel function between this object and a training example, i.e., kˆi = κ(x̂, xi ).
In real applications, the similarity matrix recording a certain type of
similarity between each pair of examples may be treated as a kernel matrix.
Since kernel matrix is positive definite, some preprocessing may be needed
to transform the given similarity matrix into a positive definite matrix.
1.6. Ensemble classifier
An ensemble classifier is not a specific type of classifier as those introduced
earlier. Instead, it is a classifier ensemble, which combines or aggregates
the predictions of several individually trained classifiers called base classifiers to produce a final result. A simple enselble classifier is illustrated
in 8. Through aggregating, the prediction of an ensemble classifier is usually more accurate than any of the individual classifiers. An important
problem is how to train each of the base classifiers. Since ensemble makes
sense only if the outputs of the base classifiers are different. To generate
disagreements in the prediction, base classifiers may be trained with different initial weights, different parameters, different subsets of features, and

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

15

different portions of training set. The two well known ensemble methods:
Bagging,21 and Boosting22,23 mainly focus on the last way to train the base
classifiers, and the other well known method Random Forest24 makes use
of the last two ways.
In the Bagging method, each classifier is trained on a random sample of
the training set. More specifically, a set of sample to be used for training
a base classifier is generated by randomly drawing with replacement from
the training samples. Although each individual classifier could result in
higher test-set error when trained with a subset of training samples, the
combination of them can produce lower test-set error than using the single
classifier trained with all the training samples.21 showed that Bagging is
effective on “unstable” learning algorithms, such as decision tree and neural
network, where small changes in the training set result in large changes in
predictions. Unlike Bagging, where the generation of training set for one
classifier is independent on other classifiers, in Boosting,23,25 the training
set used for each base classifier is chosen based on the performance of the
earlier classifiers. Examples that are incorrectly predicted by previous classifiers are selected more often than those were correctly predicted. Doing
this, Boosting attempts to make subsequent classifiers be better able to predict examples for which the current ensemble’s performance is poor. The
Random Forest24 combines the Bagging idea to select training samples and
random selection of features. The selection of a random subset of features
is an example of the random subspace method,26 which is especially useful
for handling high-dimensional data, e.g., gene expression data. Projecting
the original high dimensional space into different low subspaces so that the
problems caused by high-dimensionality are avoid. Although decision tree
is often used as base classifiers in these ensemble methods, other types of
classifiers may also be used to produce base predictions in an ensemble.
Once all the base classifiers are trained, they generate predictions for
new samples to be classified. Voting is a commonly used way to combine
these predictions to give the final class label for the input. Assuming that
the majority of the classifiers would make the correct prediction, voting
labels the sample as the class that predicted by most of the base classifiers.
Instead of equally weighting the classifiers, the aggregating weights for each
base classifier may also be adapted according to their performance.23

March 13, 2015

16

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

Output

Combine

Classifier 1

Classifier 2

Classifier N

Input

Fig. 8.

An ensemble of classifiers.

2. Drug-target interaction prediction
In this section, we take the drug-target interaction prediction as an example
to present detailed discussion on how classification is used to handle a specific task in biology. Some background knowledge of drug-target interaction
prediction is first given. After that, recently studies on using classification
for drug-target interaction prediction are discussed. Finally, experimental
studies on benchmark datasets are given to evaluate the performance of
several different classification approaches in drug-target interaction prediction.
2.1. Background
Identification of drug-target interaction is an important part of the drug discovery pipeline. The great advances in molecular medicine and the human
genome project provide more opportunities to discover unknown associations in the drug-target interaction network. These new interactions may
lead to the discovery of new drugs and also are useful for helping under-

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

17

stand the causes of side effects of existing drugs. Since experimental way to
determine drug-target interactions is costly and time-consuming, in silico
prediction comes out to be a potential complement that provides useful
information in an efficient way.
Traditional approaches for this task are generally categorized into drugbased approaches and target-based approaches. Drug-based approaches
screen candidate drugs, compounds or ligands to predict whether they interact with a given target based on the assumption that similar drugs share
the same target. The similarity of two drugs are measured in different ways
with respect to different aspects. Other than comparing drugs according
to their chemical structures,27 side-effect has also been used to measure
the similarity between drugs.28 Assuming that similar targets bind to the
same ligand, target-based approaches, on the other hand, compare proteins
to predict whether they bind to the given ligand, or whether they are the
targets of the given drug or compound. More specifically, for a given drug,
new targets are identified by comparing candidate proteins to the known
targets of this drug with respect to certain descriptors such as amino acid
sequence, binding sites, or ligands that bind to them. The authors of 29 review computational methods to find new targets for already approved drugs
for the treatment of new diseases based on the structural similarity of their
binding sites. Candidtae targets are compared by the chemical similarity
of ligands that bind to them.30 Different from these classic drug-based or
target-based approaches, chemogenomics approaches have been proposed to
consider the interactions between drugs and a protein family rather than a
single target.31–34
Recently, machine learning approaches have been applied to this task to
explore the whole interaction space. In the supervised bipartite graph learning approach,14 the chemical space and the geometric space are mapped into
a unified space so that those interacted drugs and targets are close to each
other while those non-interacted drugs and targets are far away from each
each other. After the mapping function to such a unified space is learned,
the query pair of drug and target are also mapped in the same way to
that unified space, and the probability of interaction between them is the
closeness that they are in the mapped space. It has been shown that the
combination of supervised learning independently based on drug and target performs very well.15 This approach is called the Bipartite Local Model
(BLM). For a query pair of drug and target, a model of the query drug is
learned with a certain classifier based on the information of its known targets. Then the probability of interaction between this drug and the query

March 13, 2015

18

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

target is predicted with this model. The same procedure is applied to
obtain the probability of interaction between them from the target side.
Finally, an overall probability of interaction for the query pair is calculated
by combing these two probabilities. It has been reported that the result
based the knowledge of both directions, i.e., from the drug side and from the
target side, is much better than those based on each single one. The same
idea is adopted by another two following work. Semi-supervised approach is
used instead of supervised approach to learn the local model.35 Laarhoven
found that only use the kernel based on the topology of the known interaction network is able to obtain a very good performance, although together
with other types of similarities can further improve the results.36 Other
than using one type of drug-drug similarity and one type of target-target
similarity,37 use multiple types of drug-drug similarities and target-target
similarities and combine them as features to describe each drug-target pair
to learn the logistic regression model. Next, we present the details of how
the drug-targe prediction task is handled by three types of classification
problems.
2.2. A binary classification problem
A relatively stratforward way to predict whether a given pair of drug-target
interacts is to model it as a binary classification problem as in Ref.37 The
key problem is how to extract a set of features based on different biological
sources to charactorize or represent each drug-target pair. This has been
done in three steps in Ref.37 First, five drug-drug similarities and three
gene-gene similarities are calculated based on different bilological and chemical sources. Then, the drug and gene similarity measures are combined as
features to describe each drug-target pair. Feature selection is performed to
select important features. Finally, the classifier is trained with the labeled
samped decribed with selected features. In this study, Logistic regression
is used for classification.
The whole process is shown in Fig. 9.37 The drug-drug similarity measures were computed using chemical strucute, Ligand, drug side effects,
drug response gene expression profiles, and the Anatomical, Therapeutic and Chemical (ATC) classification system code. The gene-gene similarity measures used are based on protein-protein interactions, sequence,
and Gene Ontology (GO). Once all these drug-drug similarities and targettarget similarities are obtained, each feature is constructed based on one
drug-drug similarity and one target-target similarity. Specifically, calcu-

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

19
Table 2.

Comparison of AUC and AUPR for the four datasets
All features
0.905
0.935
selected features
0.908
0.935
Ligand
Sequence similarity
0.851
0.867
Ligand
GO semantic similarity 0.845
0.867
Predicted Side Effect
GO semantic similarity 0.832
0.863
ATC similarity
GO semantic similarity 0.81
0.858
Ligand
PPI closeness
0.809
0.844
Chemical
GO semantic similarity 0.805
0.84
ATC similarity
PPI closeness
0.762
0.809
Chemical
Sequence similarity
0.749
0.763
Predicted Side Effect
PPI closeness
0.729
0.759
Co-expression
Sequence similarity
0.724
0.748

lated by combining the drug-drug similarities between the query drug and
other drugs and the gene-gene similarities between the query gene and other
target genes across all true drug-target associations. Therefore, fifteen features are constructed in such a way. After feature selection, ten features
are finally selected. Table 2 shows the results in terms of AUC (area under ROC curve) and AUPR (area under precision-recall curve) with all the
features, all the selected features and each single selected feature. Here
AUC and AUPR are two performance evaluation measures. It is shown
that using ten selected feature gives a comparable result with all the fifteen features, which is much better than using any of a single feature. It
is also shown that when used indivudually, the combination of Ligand and
sequence similarity gives the best feature. Once each drug-target pair is
represented as a vector of these feaures, the prediction problem of whether
a query pair interacts simply becomes a binary classification problem that
can be solved by many existing classification algorithms, e.g., the Logistic
regression as used in this paper. Other than develping a good data presentation through aggregation of multiple data sources, some other studies
focus more on design of new learning algorithms. Next we introduce two recently proposed learning algorithms, namely the Bipartite Graph Learning
and the Bipartite Local Model.
2.3. Bipartite graph learning (BGM)
We assume that the problem under consideration is to predict new interactions between nd drugs and nt targets. An nd × nt matrix A is used to
record these known interactions, i.e., aij ∈ A = 1 if the ith drug denoted as
di , is known to interact with the jth target denoted as tj . All other entries
of A are 0. Assume ni interactions in total involves md drugs and mt tar-

March 13, 2015

20

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

Fig. 9. Algorithm pipeline. (A) comprised of formation of drug-drug and gene-gene
similarity matrices, (B) integration of the similarities to classification features, (C) classification with feature selection.

gets and md < nd and mt < nt . This means there are some new drug and
target candidates and the corresponding rows and columns of A are all 0.
Other than the interaction network, Sd and St are the chemical similarity
matrix of drug and the sequence similarity matrix of target, respectively.
The bipartite graph learning method learns the correlation between the
chemical/genomic space and the interaction space, which is called the ‘pharmacological space’. As illustrated in Fig. 10,14 first, the compounds and
proteins are embedded into a unified space called ‘pharmacological space’.

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

21

The mapping function or model between the chemical/genomic space and
the pharmacological space is learned. With this model, any query pair of
compounds and proteins are mapped onto the same pharmacological space.
The compound-protein pairs under testing are predicted to be interacting
if the two are closer than a threshold in the pharmacological space. The
whole process consists of the following steps:14
• Step 1: construct a graph-based similarity matrix


Kcc Kcg
K=
KTcg Kgg
where the entries of each matrices are calculated as
d2c c
Kcc = exp(− i2 j )
h
d2gi gj
Kgg = exp(− 2 )
h
d2ci gj
Kcg = exp(− 2 )
h

(43)

(44)
(45)
(46)
(47)

where d is the shortest distance between two objects (compounds
or proteins) on the bipartite graph. The symmetric matrix K has
a scale of (nc + nd ) × (nc + nd ). After K is constructed, eigenvalue
decomposition is performed to K to get U:
K = ΓΛ1/2 Λ1/2 ΓT = UUT

(48)

where Λ is the diagonal matrix with the diagonal elements
the eigenvalues and the columns of matrix Γ are the corresponding eigenvectors. Write U with its row vectors: U =
(uc1 , . . . , ucnc , ug1 , . . . , ugng )T .
• Step 2: For i = {1, . . . , cn } and j = {1, . . . , gn }, learn wci and wgj
by assuming the following relation, which is a variant of the kernel
regression model:
uci =

nc
X

sc (x, xci )wci + ǫ

(49)

sg (x, xgj )wgj + ǫ

(50)

i=1
ng

ugj =

X
j=1

(51)

March 13, 2015

22

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

Fig. 10.

Bipartite graph model.

• Step 3: mapping the query compound cq and protein gq with
learned Wc , and Wg :
ucq =

nc
X

sc (cq , ci )wci

(52)

sg (gq , gj )wgj

(53)

i=1
ng

ugq =

X
j=1

(54)
• Step 4: The score of interaction between cq and gq denoted as
pcq ,gq is calculated as the inner product of the feature vectors in
the mapped space
pcq ,gq =< ucq , ugq >

(55)

2.4. Bipartite local model (BLM)
To predict pij , the probability that a drug di and a target tj interacts, the
basic bipartite local model is described as follows. A local model of di is first
learned based on the known targets of this drug and the similarities between
these targets. This model is then used to predict pdij→t the probability of
interaction between this drug to the tested protein. The model learning
and prediction process is performed independently from the query target
side to get ptij→d . Once both pdij and ptij are calculated, they are combined
with some function f to get the final result pij = f (pdij→t , ptij→d ). Fig. 1116

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

23

illustrates the idea of drug-target interaction prediction with learning from
the drug and target independently.
This framework was first proposed in,15 and then was further studied in
Ref.35 and Ref.36 Under the same BLM framework, different results may be
produced due to the differences in drug-drug similarity Sd and target-target
similarity St , the classifier, and the way how pdij→t and ptij→d is combined,
i.e., the function f . For example, in,15 Support Vector Machine (SVM) is
used as the classifier using the chemical structure similarity for drug and
sequence similarity for protein targets, respectively. The same types of
similarity data is used in,35 but with a semi-supervised approach for local model learning. In,36 network topology based similarity for drug and
target are calculated and combined with the chemical structure similarity
and sequence similarity, respectively, to give the final pairwise drug similarities and pairwise target similarities, and the Regularized Least Squares
(RLS) is used for model learning. So far, simple combination functions
are shown good enough to get the final prediction based on the two individually obtained ones, e.g., pij = max{pdij→t , ptij→d } is used in,15 and
pij = 0.5(pdij→t + ptij→d ) is used in.36

2.5. Enhanced BLM with training data inferring for new
drug/target candidates
Generally, supervised learning performs better than unsupervised learning.
However, a good performance of supervised learning is largely dependent
on the amount and quality of the labeled training data. When the drug
candidate is new, it has no existing targets that can be used as positive
labeled training data and the model for this drug thus cannot be learned.
Similarly, supervised local model learning does not work for new target
candidates. To extend the application domain of BLM to new drug and
target candidates, in Ref.,16 we present a training data inferring procedure
and integrate it into BLM. Based on the assumption that drugs which are
similar to each other interact with the same targets, training data for a new
drug candidate could be possibly inferred from its neighbors. The neighbors
of a new drug candidate generally refer to those drugs that share some
similar properties with the new drug candidate, e.g. similar in chemical
structure.
For a drug candidate di that has no known targets, we infer the weighted
interaction profile for di with the following formula

March 13, 2015

24

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

Given:
Sd -- drug-drug similarity,
St -- target-target similarity
A -- drug-target interactions

?

To predict:
interaction between ݀௜ and ‫ݐ‬௝

ௗ՜௧
:
‫݌‬௜௝
Interaction prediction from
the drug-candidate ݀௜

+

௧՜ௗ
‫݌‬௜௝
:
Interaction prediction from
the target-candidate ‫ݐ‬௝

௧՜ௗ
ௗ՜௧
)
, ‫݌‬௜௝
‫݌‬௜௝ = f (‫݌‬௜௝

The final possibility of
interaction between ݀௜ and ‫ݐ‬௝

Fig. 11. Drug-target interaction prediction with learning from the drug and target
independently.

l(i) = sdi A

(56)

nd
X

(57)

where each dimension
lj (i) =

sdih ahj

h=1

Here vector sdi is the ith column of Sd , which records the similarities between di and all the other drugs, sdih is the similarity between two drugs di
and dh , and vector l(i) is the inferred interaction profile for di , where each

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

25

dimension lj (i) corresponds to the weight of the interaction between di and
tj . The above formula shows that the interaction weight of di with respect
to the jth target is the sum of interactions between its neighbors and this
target weighted by the similarity between this drug and its neighbors. More
specifically, this simple formula defines that for a given new drug candidate
di , its weight of interaction with respect to a target is high if many of its
neighbors interact with this target, and the final weight to a target is influenced more by a neighbor with a larger similarity than those with smaller
similarities. To allow neighbors with large similarities only to contribute, a
threshold may be used to reduce the impact of those non-important neighbors to 0. Alternately, an exponential function with bandwidth β given as
below may be introduced:
d

l(i) = e(si /β) A

(58)

To ensure the value of each lj (i) is in the range of [0, 1], linear scale is
performed subsequently. The procedure of inferring training data for new
target candidates is not discussed in details here as it is similar to the
procedure of inferring training data for new drug candidates as presented
above.
Learning from neighbors allows drugs and targets to obtain training
data when themselves do not have any known interactions. This procedure actually introduces some degree of globalization into the original local
model to give more chances or an enlarged scope for the learning process.
However, too much globalization is not desired as it will decrease the local
characteristics and make the models for each drug or target less discriminative. Moreover, the low quality of neighbors may add in noise and cause
a negative impact when neighbors’ preferences are too much relied upon.
In the current study, we only activate the neighbor-based training data inferring for totally new candidates. For other cases, we still train the model
locally on its own preference, i.e., the known interactions.
3. Experimental study
Now we give some experimental results to compare the performance of the
BGM method, the BLM method and the BLMN method for the task of
drug-target interaction prediction. From the experimental results, we have
the following observations: first, BLM-based approaches outperform BGM;
second, with neighnor-based training data inferring, BLMN performs better
than the classic BLM; third, network topology based similarity is helpful

March 13, 2015

1:18

26

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li
Table 3. Some statistics of the four datasets. nd : the total number of drugs, nt : the total number of targets, E: the total number of interactions, D̄d : the average number of targets for each
drug, D̄t : the average number of targeting drugs for each target,
Dd = 1: the percentage of drugs that have only one target, and
Dt = 1: the percentage of targets that have one targeting drug.
Dataset
Enzyme
Ion Channel
GPCR
Nuclear Receptor
nd
445
210
223
54
nt
664
204
95
26
E
2926
1476
635
90
D¯d
6.58
7.03
2.85
1.67
D̄t
4.41
7.24
6.68
3.46
Dd = 1(%)
39.78
38.57
47.53
72.22
Dt = 1(%)
43.37
11.27
35.79
30.77

to improve the prediction.
3.1. Datasets
The four groups of datasets have been first analysed by14 and then later by
several other researchers.15,35,36,38 These four datasets correspond to drugtarget interactions of four important categories of protein targets, namely
enzyme, ion channel, G-protein-coupled receptor (GPCR) and nuclear receptor, respectively a . Table 3 gives some statistics of each of the datasets.
Each dataset is described by three types of information in the form of
three matrices. Together with the drug-target interaction information, the
drug-drug similarity, and target-target similarity are also available. Four
interaction networks were retrieved from the KEGG BRITE,39 BRENDA,40
SuperTarget41 and DrugBank42 these four databases. The drug-drug similarity is measured based on chemical structures from the DRUG and COMPOUND sections in the KEGG LIGAND database39 and is calculated with
SIMCOMP.43 The target-target similarity is measured based on the amio
acid sequences from the KEGG GENEsS database39 and is calculated with
a normalized version of Smith-Waterman score.
3.2. Approaches compared
We compare the following approaches:

a The

• BGM:14 Bipartite graph model;
• BY(2009):15 Bipartite local model;

datasets
were
http://web.kuicr.kyoto-u.ac.jp/supp/yoshi/drugtarget/

download

from

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

27

• Laarhoven et al (2011):36 Bipartite local model with networkbased; similarity
• BLM: Ignoring ‘new candidate’ in BLMN;
• BLMN: BLM with neighbor-based training data inferring .
Among the above methods, BGM requires eigendecomposition of a (nc +
nd )×(nc +nd ) matrix, which is computational consuming for large datasets.
The BY(2009), Laarhoven et al (2011) and BLM are three variants of the
classic BLM method, which is not applicable to new candidates. BLMN
is the modified BLM method which can be used to predict the interaction
between any compounds and proteins.
3.3. Evaluation
Leave-one-out cross validation (LOOCV) is performed. In each run of prediction, one drug-target pair is left out by setting the corresponding entry
of matrix A to 0. Then we try to recover its true value using the remaining
data. We measure the quality of the predicted interaction matrix P by
comparing it to the true interaction matrix A in terms of the area under
ROC curve or true positive rate (TPR) vs. false positive rate (FPR) curve
(AUC) and the area under the precision vs. recall curve (AUPR). TPR is
equivalent to recall. Assume that TP, FP, TN, FN represent true positive,
false positive, true negative, and false negative, respectively, then
TP
(59)
TP + FN
FP
FPR =
(60)
FP + TN
TP
(61)
precision =
TP + FP
Since in the current task, the known interactions are much less than those
unknown ones, the precision-recall curve should be a better measurement
than the ROC curve here as has been discussed in.44
T P R/recall =

3.4. Performance comparison
Table 4 gives the AUC and AUPR scores of five approaches on the four
datasets. The results of BGM, BY (2009), and Laarhoven et al (2011) are
the best ones reported in15 and.36 Both BLMN and BLM are run with three
different groups of inputs: Chem-Seq, Network-based, and Hybrid. ChemSeq denotes that chemical similarity is used for drug and sequence similarity

March 13, 2015

28

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

is used for target; Network-based denotes that the drug-drug similarity and
target-target similarity are derived from the existing interaction network;
Hybrid denotes that the drug-drug similarity and target-target similarity
are combinations of the two types of similarities.
It is shown from the table that with a low time complexity, four BLMbased approaches, including three BLM variants and BLMN, produce better
results than the BGM method. Among the three BLM variants, the results
of BLM and BY(2009) with Chem-Seq are similar as the only difference
between them is the former use RSL as the classifier while the later use
SVM. The results of BLM and Laarhoven et al (2011) with Network-based
are also close in most of the cases although the later used Kronecker product, which is a more complicated way to combine two types of similarities.
In all the cases, BLMN produced better results than the three classic BLM
algorithms. This clearly show that neighbor-based training data inferring
is very useful for improving the final result when the dataset contains new
drug/target candidates.
Despite the consistent improvements of BLMN compared to the other
three on all the four datasets, the amounts of improvements differ for different datasets. If we compare the improvements of the proposed approaches
over the four datasets, it is seen that the improvement with respect to BLM
on Nuclear Receptor is the most significant while the improvement on Enzyme and Ion Channel are not so significant. Such kind of differences in
performance of the proposed approach are consistent with our expectation
according to the differences in the structure of the datasets. Although all
the datasets do not contain new drug/target candidates, in our experiment,
the real interaction to be predicted is leave out. This means drugs and targets with degree equal to 1 turn out to have no positive training data and
thus they are simulated to be “new” in the experiments. As shown in Table
3, Nuclear Receptor has a much larger portion of “new” drugs and targets
than Ion Channel. Therefore, it has more chances for BLMN to improve
the results for Nuclear Receptor where the training data inferring is applied
more frequently.
It is also observed that although network-derived similarity alone provides good information, combining biological information can further improves the result especially when the network is sparse, e.g., the results
of both BLM and BLMN for Ion Channel with only Network-based is very
close to those with Hybrid while significant improvements are achieved for
both approaches on Nuclear Receptor when Chem-Seq is further combined
with Network-based similarity. This shows that combining multiple types

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

29
Table 4.
Dataset
Enzyme

Comparison of AUC and AUPR for the four
Data
Method
Chem-Seq
BGM
BY(2009)
BLM
BLMN
Network-based
Laarhoven et al (2011)
BLM
BLMN
Hybrid
Laarhoven et al (2011)
BLM
BLMN
Ion Channel
Chem-Seq
BGM
BY(2009)
BLM
BLMN
Network-based
Laarhoven et al (2011)
BLM
BLMN
Hybrid
Laarhoven et al (2011)
BLM
BLMN
GPCR
Chem-Seq
BGM
BY(2009)
BLM
BLMN
Network-based
Laarhoven et al (2011)
BLM
BLMN
Hybrid
Laarhoven et al (2011)
BLM
BLMN
Nuclear Receptor
Chem-Seq
BGM
BY(2009)
BLM
BLMN
Network-based
Laarhoven et al (2011)
BLM
BLMN
Hybrid
Laarhoven et al (2011)
BLM
BLMN

datasets
AUC
96.7
97.6
96.1
98.0
98.3
98.2
99.1
97.8
98.2
98.8
96.9
97.3
97.0
97.8
98.6
98.5
99.0
98.4
98.5
99.0
94.7
95.5
95.1
98.1
94.7
94.4
97.5
95.4
95.7
98.4
86.7
88.1
86.9
96.9
90.6
90.9
95.7
92.2
94.0
98.1

AUPR
83.1
83.3
85.8
87.3
88.5
88.0
93.1
91.5
91.3
92.9
77.8
78.1
81.9
84.6
92.7
92.5
95.6
94.3
92.7
95.0
66.4
66.7
68.1
78.8
71.3
70.6
84.6
79.0
76.2
86.5
61.0
61.2
58.4
80.7
61.0
62.9
80.7
68.4
72.4
86.6

of similarities usually gives better results when no single type of similarity
is good enough.

March 13, 2015

1:18

30

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

4. Summary
Classification is an important data analysis tool that have been studied extensively. Many computational biology tasks are binary classification problem that predicts the outcome of a trial is positive or negative. We have
introduced several popular supervised learning methods for classification
including popular classification methods, regression models used for classification, and ensemble classification. We give more detailed discussion of
how different classification methods can be used for drug-target interaction
prediction. Experimental studies are given to compare the performance of
different approaches with benchmark datasets.
Other than the specific learning method, the classification result is also
highly dependent on the amount and quality of the given training data and
the way the data represented, e.g., a set of features or similarity measures.
Given the same set of training data, a good data representation with a
simple classifier may already produces a good result. Nevertheless, with
the same data representation, an advanced classification algorithm is able
to make use of it more effectively and hence produce a better result. This
chapter focus on algorithm design.

References
1. J. Han, M. Kamber, and J. Pei, Data Mining: Concepts and Techniques, 3
edn. Morgan Kaufmann (2012).
2. X. Chen, M. Wang, and H. Zhang, The use of classification trees for bioinformatics, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 1(1), 5563 (2011).
3. E. C. Uberbacher and R. J. Mural, Locating protein-coding regions in human
dna sequences by a multiple sensor-neural network approach, Proc. Nati.
Acad. Sci. USA. 88, 11261–11265 (1991).
4. S. Salzberg, Locating protein coding regions in human dna using a decision
tree algorithm, J. Comput. Biol. 2, 473485 (1995).
5. R. Gupta, P. Wikramasinghe, A. Bhattacharyya, S. P. Francisco A Perez, and
R. V. Davuluri, Annotation of gene promoters by integrative data-mining of
chip-seq pol-ii enrichment data, BMC Bioinformatics. 11(Suppl 1), S65
(2010).
6. K. M. Borgwardt, C. S. Ong, S. Schonauer, S. V. N. Vishwanathan, A. J.
Smola, and H.-P. Kriegel1, Protein function prediction via graph kernels,
Bioinformatics. 21 (Suppl. 1), i47i56 (2005).
7. M. WA and B. HM, Statistical models for discerning protein structures containing the dnabinding helix-turn-helix motif, J Mol Biol. 330, 4355 (2003).

March 13, 2015

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

31

8. F. Mordelet and J.-P. Vert, Sirene: supervised inference of regulatory networks, Bioinformatics. 24(16), i76–i82 (2008).
9. L. Cerulo, C. Elkan, and M. Ceccarelli, Learning gene regulatory networks
from only positive and unlabeled data, BMC Bioinformatics. 11 (2010).
10. Q. Y, B.-J. Z, and K.-S. J., Evaluation of different biological data and computational classification methods for use in protein interaction prediction,
Proteins. 63, 490500 (2006).
11. P. Yang, X.-L. Li, J.-P. Mei, C.-K. Kwoh, and S.-K. Ng, Positive-unlabeled
learning for disease gene identification, Bioinformatics. 28(20), 2640–2647
(2012).
12. P. Yang, X. Li, M. Wu, C.-K. Kwoh, and S.-K. Ng, Inferring gene-phenotype
associations via global protein complex network propagation, PloS one. 6(7),
e21502 (2011).
13. P. Yang, X. Li, H.-N. Chua, C.-K. Kwoh, and S.-K. Ng, Ensemble positive
unlabeled learning for disease gene identification, PloS one. 9(5), e97079
(2014).
14. Y. t. Yamanishi, Prediction of drug-target interaction networks from the
integration of chemical and genomic spaces, Bioinformatics. 24, i232–i240
(2008).
15. K. Bleakley and Y. Yamanishi, Supervised prediction of drug-target interactions using bipartite local models, Bioinformatics. 25(18), 2397–2403 (2009).
16. J.-P. Mei, C.-K. Kwoh, P. Yang, X.-L. Li, and J. Zheng. Globalized bipartite
local learning model for drug-target interaction prediction (2012).
17. J.-P. Mei, C.-K. Kwoh, P. Yang, X.-L. Li, and J. Zheng, Drug–target interaction prediction by learning from local information and neighbors, Bioinformatics. 29(2), 238–245 (2013).
18. J. R. Quinlan, Induction of decision trees, Mach. Learn. 1, 81–106 (1986).
19. J. R. Quinlan, C4.5: Programs for Machine Learning. Morgan Kaufmann,
San Mateo, CA (1993).
20. L. Breiman, J. Friedman, R. Olshen, and C. Stone, Classification and Regression Trees. CRC Press, Boca Raton, FL (1984).
21. L. Breiman, Bagging predictors, Machine Learning. 24(2), 123–140 (1996).
22. R. Schapire, The strength of weak learnability, Machine Learning. 5(2),
197–227 (1990).
23. Y. Freund and R. E. Schapire, A decision-theoretic generalization of on-line
learning and an application to boosting, Journal of Computer and System
Science. 55, 119139 (1997).
24. L. Breiman, Random forests, Machine Learning. 45(1), 532 (2001).
25. B. L. Bias, variance, and arcing classifiers. Technical Report 460, UCBerkeley, Berkeley, CA (1996).
26. T. K. Ho, The random subspace method for constructing decision forests,
IEEE Transactions on Pattern Analysis and Machine Intelligence. 20(8),
832844 (1998).
27. Y. C. e. a. Martin, Do structurally similar molecules have similar biological
activity?, J. Med. Chem. 45, 4350–4358 (2002).
28. M. a. Campillos, Drug target identification using side-effect similarity, Sci-

March 13, 2015

32

1:18

World Scientific Review Volume - 9in x 6in

classification˙app

Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li

ence. 321(5886), 263–266 (2008).
29. V. J. Haupt and M. Schroeder, Old friends in new guise: repositioning
of known drugs with structural bioinformatics, Breifings in Bioinformatics
(2011).
30. M. J. t. Keiser, Predicting new molecular targets for known drugs, Nature.
462, 175–181 (2009).
31. P. R. e. a. Caron, Chemogeominc approaches to drug discovery, Curr. Opin.
Chem. Biol. 5, 464–470 (2001).
32. H. Kubinyi and G. Müller, Chemogenomics in Drug Discovery. Wiley-VCH,
Weinheim (2004).
33. D. Rognan, Chemogenomic approaches to rational drug design, British Journal of Pharmacology. 152, 38–52 (2007).
34. L. Jacob and J.-P. Vert, Protein-ligand interaction prediction: an improved
chemogenomics approach, Bioinformatics. 24(19), 2149–2156 (2008).
35. Z. t. Xia, Semi-supervised drug-protein interaction prediction from heterogeneous biological spaces, BMC Systems Biology. 4 (Suppl 2), S6 (2010).
36. T. V. Laarhoven, S. B. Nabuurs, and E. Marchiori, Gaussian interaction
profile kernels for predicting drug-target interaction, Bioinformatics (2011).
37. L. Perlman, A. GOTTLIEB, N. ATIAS, E. RUPPIN, and R. SHARAN,
Combining durg and gene similarity measures for drug-target elucidation,
Journal of computational biology. 18, 133–145 (2011).
38. X. t. Chen, Drug-target interaction prediction by random walk on the heterogeneous network, Molecular BioSystems. DOI: 10.1039/c0xx00000x
(2012).
39. M. t. Kanehisa, From genomics to chemical genomics: new developments in
kegg, Nucleic acids res. 34(Database), D354–357 (2006).
40. I. t. Schomburg, Brenda, the enzyme database: updates and major new
developments, Nucleic Asids Res. 32(supl-1), D431–433 (2004).
41. S. t. Gnther, Supertarget and matador: resources for exploring drug-target
relationships, Nucleic acids res. 36(Database issue), D919–D922 (2008).
42. D. S. t. Wishart, Drugbank: a knowledgebase for drugs, drug actions and
drug targets, Nucleic acids res. 36(Database issue), D901–906 (2008).
43. M. t. Hattori, Development of a chemical structure comparison method for
integrated analysis of chemical and genomic information in the metabolic
pathways, J. Am. Chem Soc. 125(39), 11853–11865 (2003).
44. J. Davis and M. Goadrich. The relationship between precision-recall and roc
curves. In Proc. 23rd International Conference on Machine Learning, pp.
233–240 (2006).

Outlook
Sunny

Rainy
Overcast
Windy

Humidity
Normal

Yes
High

True

False
Yes

Yes

No

No

