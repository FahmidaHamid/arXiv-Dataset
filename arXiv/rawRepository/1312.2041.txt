Probabilistic models of genetic variation in structured
populations applied to global human studies

arXiv:1312.2041v2 [q-bio.PE] 4 Mar 2015

Wei Hao1∗ , Minsun Song1∗+ , and John D. Storey1,2 †
1. Lewis-Sigler Institute for Integrative Genomics, Princeton University, Princeton, NJ 08544
2. Department of Molecular Biology, Princeton University, Princeton, NJ 08544
∗
+

These authors contributed equally to this work

Present address: Division of Cancer Epidemiology and Genetics, National Cancer Institute,
National Institutes of Health, Rockville, MD 20850
†

To whom correspondence should be addressed: jstorey@princeton.edu

Contents
A BSTRACT

2

1 I NTRODUCTION

2

2 M ETHODS

4

3 R ESULTS

10

4 D ISCUSSION

13

5 F IGURES AND TABLES

15

6 S UPPLEMENTARY M ATERIAL

21

7 S UPPLEMENTARY F IGURES AND TABLES

27
33

R EFERENCES

1

A BSTRACT
Modern population genetics studies typically involve genome-wide genotyping of individuals from a diverse network of ancestries. An important problem is how to formulate and estimate probabilistic models of observed genotypes that account for complex population structure. The most prominent work
on this problem has focused on estimating a model of admixture proportions of ancestral populations
for each individual. Here, we instead focus on modeling variation of the genotypes without requiring a
higher-level admixture interpretation. We formulate two general probabilistic models, and we propose
computationally efficient algorithms to estimate them. First, we show how principal component analysis
(PCA) can be utilized to estimate a general model that includes the well-known Pritchard-StephensDonnelly admixture model as a special case. Noting some drawbacks of this approach, we introduce
a new “logistic factor analysis” (LFA) framework that seeks to directly model the logit transformation of
probabilities underlying observed genotypes in terms of latent variables that capture population structure. We demonstrate these advances on data from the Human Genome Diversity Panel and 1000
Genomes Project, where we are able to identify SNPs that are highly differentiated with respect to
structure while making minimal modeling assumptions.

1

I NTRODUCTION

Understanding genome-wide genetic variation among individuals is one of the primary goals of modern human genetics. Genome-wide association studies aim to identify genetic variants throughout the
entire genome that are associated with a complex trait [1–3]. One of the major challenges in analyzing
these studies is the problem of spurious associations due to population structure [4], and methods to
deal with this are still in development [5–7]. A related effort is underway to provide a comprehensive,
genome-wide understanding of how genetic variation among humans is driven by evolutionary and demographic forces [8]. A rigorous characterization of this variation will lead to a better understanding of
the history of migration, expand our ability to identify signatures of natural selection, and provide important insights into the mechanisms of human disease [9,10]. For example, the Human Genome Diversity
Project (HGDP) is an international project that has genotyped a large collection of DNA samples from
individuals distributed around the world, aiming to assess worldwide genetic diversity at the genomic
level [10–12]. The 1000 Genomes Project (TGP) is comprehensively cataloging human genetic variation by producing complete genome sequences of well over 1000 individuals of diverse ancestries [13].
Systematically characterizing genome-wide patterns of genetic variation is difficult due to the numerous and complex forces driving variation. There is a fundamental need to provide probabilistic
models of observed genotypes in the presence of complex population structure. A series of influen-

2

tial publications have proposed methods to estimate a model of admixture, where the primary focus
is on the admixture proportions themselves [14–16], which in turn may produce estimates of the allele frequencies of every genetic marker for each individual. Here, we instead focus directly on these
individual-specific allele frequencies, which gives us potential advantages in terms of accuracy and
computational efficiency.
We propose two flexible genome-wide models of individual-specific allele frequencies as well as
methods to estimate them. First, we develop a model that includes as special cases the aforementioned
models; specifically, the Balding-Nichols (BN) model [17] and its extension to the Pritchard-StephensDonnelly (PSD) model [14]. However, we identify some limitations of our method to estimate this model.
We therefore propose an alternative model based on the log-likelihood of the data that allows for rapid
estimation of allele frequencies while maintaining a valid probabilistic model of genotypes.
The estimate of the first model is based on principal component analysis (PCA), which is a tool
often applied to genome-wide data of genetic variation in order to uncover structure. One of the earliest
applications of PCA to population genetic data was carried out by Menozzi et al. [18]. Exploratory
analysis of complex population structure with PCA has been thoroughly studied [18–22]. We show
that a particular application of PCA can also be used to estimate allele frequencies in highly structured
populations, although we have to deal with the fact that PCA is a real-valued operation and is not
guaranteed to produce allele frequency estimates that lie in the unit interval [0,1].
The estimate of the second model is based on a generalized factor analysis approaches that directly model latent structure in observed data, including categorical data [23] in which genotypes are
included. We utilize a factor model of population structure [24] in terms of nonparametric latent variables, and we propose a method called “logistic factor analysis” (LFA) that extends the PCA perspective
towards likelihood-based probabilistic models and statistical inference. LFA is shown to provide accurate and interpretable estimates of individual-specific allele frequencies for a wide range of population
structures. At the same time, this proposed approach provides visualizations and numerical summaries
of structure similar to that of PCA, building a convenient bridge from exploratory data analysis to probabilistic modeling.
We compare our proposed methods to existing algorithms (ADMIXTURE [16] and fastStructure [25])
and show that when the goal is to estimate all individual-specific allele frequencies, our proposed approaches are conclusively superior in both accuracy and computational speed. We apply the proposed
methods to the HGDP and TGP data sets, which allows us to estimate allele frequencies of every SNP
in an individual-specific manner. Using LFA, we are also able to rank SNPs for differentiation according
to population structure based on the likelihoods of the fitted models. In both data sets, the most differentiated SNP is proximal to SLC24A5, and the second most differentiated SNP is proximal to EDAR.
Variation in both of these genes has been hypothesized to be under positive selection in humans. In

3

the TGP data set, the second most different SNP is rs3827760, which confers a missense mutation in
EDAR and has been recently experimentally validated as having a functional role in determining a phenotype [26]. We also identify several SNPs that are highly differentiated in these global human studies
that have recently been associated with diseases such as cancer, obesity, and asthma.

2
2.1

M ETHODS
Models of Allele Frequencies

It is often the case that human and other outbred populations are “structured” in the sense that the genotype frequencies at a particular locus are not homogeneous throughout the population [5]. Geographic
characterizations of ancestry often explain differing genotype frequencies among subpopulations. For
example, an individual of European ancestry may receive a particular genotype according to a probability different than an individual of Asian ancestry. This phenomenon has been observed not only across
continents, but on very fine scales of geographic characterizations of ancestry. Recent studies have
shown that population structure in human populations is quite complex, occurring more on a continuous
rather than a discrete basis [10]. We can illustrate the spectrum of structural complexity with Figure 1,
which shows dendrograms of hierarchically clustered individuals from the HapMap (phase II), HGDP,
and TGP data sets. The HapMap samples strongly indicate explicit membership of each individual to
one of three discrete subpopulations (due to the intended sampling scheme). On the other hand, the
clusterings of the HGDP and TGP individuals show a very complex configuration, more representative
of random sampling of global human populations.
Let us introduce Z as an unobserved variable capturing an individual’s structure. Let xij be the
observed genotype for SNP i and individual j (i = 1, . . . , m, j = 1, . . . , n), and assume that xij is
coded to take the values 0, 1, 2. We will call the observed m × n genotype matrix X. For SNP i,
the allele frequency can viewed as a function of Z , i.e. πi (Z). For a sampled individual j from an
overall population, we have “individual-specific allele frequencies” [27] defined as πij ≡ πi (z j ) at SNP

i. Each value of πij informs us as to the expectation of that particular SNP/individual pair, supposing
we observed a new individual at that locus with the same structure; i.e. E[xij ]/2 = πij . If an observed
SNP genotype xij is treated as a random variable, then under Hardy-Weinberg Equilibrium πij serves
to model xij as a Binomial parameter: xij ∼ Binomial(2, πij ). The focus of this paper is on the
simultaneously estimation of all m × n πij values.
The flexible, accurate, and computationally efficient estimation of individual-specific allele frequencies is important for population genetic analyses. For example, Corona et al. (2013) [28] recently
showed that considering the worldwide distribution of allele frequencies of SNPs known to be asso-

4

ciated with human diseases may be a fundamental component to understanding the relationship between ancestry and disease. Testing for Hardy-Weinberg equilibrium reduces to testing whether the
2 , 2π (1 − π ), and (1 − π )2 for all individuals
genotype frequencies for SNP i follow probabilities πij
ij
ij
ij

j = 1, . . . , n. It can be shown that the well-known FST measure can be characterized for SNP i using
values of πij , j = 1, 2, . . . , n (Section 6.5). Finally, we have recently developed a test of association
that corrects for population structure and involves the estimation of log



πij
1−πij



[29]. Therefore, flexible

and well-behaved estimates of the individual-specific allele frequencies πij are needed for downstream
population genetic analyses.
It is straightforward to write other models of population structure in terms of Z . For the BaldingNichols model, each individual is assigned to a population, thus z j indicates individual j ’s population
assignment. For the Pritchard-Stephens-Donnelly (PSD) model, each individual is considered to be an
admixture of a finite set of ancestral populations. Following the notation of [14], we can write z j as a
vector with elements qkj , where k indexes the ancestral populations, and we constrain qkj to be between
0 and 1 subject to

P

k qkj

= 1. Assuming the PSD model allows us to write each πij =

P

k

pik qkj and

leads to a matrix form: F = PQ, where F is the m × n matrix of allele frequencies with (i, j) entry

πij , P is the m × d matrix of ancestral population allele frequencies pik , and Q is the d × n matrix of
admixture proportions. The elements of P and Q are explicitly restricted to the range [0, 1].
The PSD model is focused on the matrices P and Q, which have standalone interpretations, but we
aim instead to estimate all πij with a high level of accuracy and computational efficiency. Writing the
structure of the allele frequency matrix F as a linear basis, we have:

F = ΓS

Model 1:

(1)

where Γ is m × d and S is d × n with d ≤ n. The d × n matrix S encapsulates the genetic population
structure for these individuals since S is not SNP-specific. The m × d matrix Γ maps how the structure

S is manifested in the allele frequencies. Operationally, each SNP’s allele frequency are a linear combination of the rows of S, where the linear weights for SNP i are contained in row i of Γ. We define the
dimension d so that d = 1 corresponds to the case of no structure: when d = 1, S = (1, 1, . . . , 1) and

Γ is the column vector of marginal allele frequencies.
This model is not necessarily the most effective way to estimate πij when working in the context of a
probabilistic model or with the likelihood function given the data. Model 1 resembles linear regression,
where the allele frequencies are treated as a real-valued response variable that is linearly dependent on
the structure. A version of regression for the case of categorical response variables (e.g., genotypes)
with underlying probability parameters is logistic regression. We developed an approach we call “logistic
factor analysis”, which is essentially an extension of nonparametric factor analysis to {0, 1, 2} valued
genotype data. Much of the justification for LFA is similar to that of generalized linear models [30].
5

The log-likelihood is the preferred mathematical framework for representing the information the
data contain about unknown parameters [31]. Suppose that Hardy-Weinberg equilibrium holds such
that xij ∼ Binomial(2, πij ). We can write the log-likelihood of the data for SNP i and individual j as:



x
`(πij |xij ) = log (Pr(xij |πij )) ∝ log πijij (1 − πij )2−xij


πij
+ 2 log(1 − πij ).
= xij log
1 − πij
The log-likelihood of SNP i for all unrelated individuals is the sum:

Pn

j=1 `(πij |xij ).

The term log



πij
1−πij



is the logit function and is written as logit(πij ). logit(πij ) is called the “natural parameter” or “canonical
parameter” of the Binomial distribution and is the key component of logistic regression. An immediate
benefit of working with logit(πij ) is that it is real valued, which allows us to directly model logit(πij ) with
a linear basis.
Let L be the m × n matrix with (i, j) entry equal to logit(πij ). We formed the following parameterization of L:

L = AH

Model 2:

(2)

where A is m × d and H is d × n with d ≤ n. In this case we can write

logit(πij ) =

d
X

aik hkj ,

k=1

where all parameters are free to span the real numbers R.
We call the rows of H “logistic latent factors” or just “logistic factors” as they represent unobserved
variables that explain the inter-individual differences in allele frequencies. In other words, the logit of
the vector of individual-specific allele frequencies for SNP i can be written as a linear combination of
the rows of H:

[logit(πi1 ), . . . , logit(πin )] = logit(π i ) =

d
X

aik hk ,

k=1

where hk is the k th row of H. Likewise, we can write:

i
a
h
k=1 ik k
hP
i.
(πi1 , . . . , πin ) = π i =
d
1 + exp
a
h
ik
k
k=1
exp

hP
d

The relationship between our proposed LFA approach and existing approaches of estimating latent
variables in categorical data is detailed in Section 6.6. Specifically, it should be noted that even though
we propose calling the approach “logistic factor analysis”, we do not make any assumptions about the
distribution of the factors (which are often assumed to be Normal). A technically more detailed name of
the method is a “logistic nonparametric linear latent variable model for Binomial data.”
6

2.2

Estimation and Algorithms

The two models presented earlier make minimal assumptions as to the nature of the structure. For
example, in Model 1, both Γ nor S are real valued. This allows us to apply an efficient PCA-based algo-

e, Γ
e , and S
e . In essence, F
e is estimated
rithm directly to the genotype matrix X, obtaining estimates of F
by forming the projection of X/2 onto the top d principal components of X with an explicit intercept
for the d = 1 case. One drawback of this approach is that because PCA is designed for continuous

e to be in the range [0, 1]. However, we show below that F
e is still
data, we have to artificially constrain F
an extremely accurate estimate of the allele frequencies F for all formulations of F considered here,
including the PSD model.
Algorithm 1: Estimating F from PCA
1. Let µ
ei be the sample mean of row i of X. Set x∗ij = xij − µ
ei and let X∗ be the m × n matrix with

(i, j) entry x∗ij .
2. Perform singular value decomposition (SVD) on X∗ which decomposes X∗ = U∆VT . Note
that the rows of ∆VT are the n row-wise principal components of X∗ and U are the principal
component loadings.
∗

∗

∗
T
e
e
3. Let X
d−1 be the projection of X on the top d−1 eigen-vectors of this SVD, Xd−1 = U1:(d−1) ∆1:(d−1) V1:(d−1) .
∗

∗

e by adding µ
e
4. Construct F
ei to row i of X
d−1 (for i = 1, . . . , n) and multiplying the resulting matrix
∗

e =Γ
eS
e where
by 1/2. In mathematical terms, F

7



1
e1
2µ


e = 1U
Γ
 1:(d−1) ∆1:(d−1)

.. 
. 


2





=



e=
S

1
2 u11 δ1
1
2 u21 δ1

···

1
2 um1 δ1

···
!

···

..
.

VT1:(d−1)

1
em
2µ



1
2 u1,d−1 δd−1
1
2 u2,d−1 δd−1

1
e1
2µ

1
e2 

2µ

1
2 um,d−1 δd−1

1
em
2µ

,
.. 
. 


..
.

1 1 ... 1

v
v21
···
 11
 v
v22
···
 12
 .
..
.
=
.
 .

v1,d−1 v2,d−1 · · ·

1



···

1

vn1




vn2 

.. 
. 
,

vn,d−1 

1
∗

∗ to be the (i, j) entry of F
e .
and δi is the ith diagonal entry of ∆. Let π
eij
∗ are such that π
∗ < 0 or π
∗ > 1, we truncate these. The
5. Since it may be the case that some π
eij
eij
eij

e where the (i, j) entry π
final PCA based estimate of F is formed as F
eij is defined to be


C



∗
π
eij = π
eij




1−C

∗ ≤C
if π
eij
∗ <1−C
if C < π
eij
∗ ≥1−C
if π
eij

e = logit(F)
e .
for some C & 0. An estimate of L can be formed as L
Here we used C =

1
2n .

e is a projection of X into its top principal components, scaled by
In summary, F

1/2, and truncated so that all values lie in the interval (0, 1).
e found
For Model 2, we propose a method for estimating the latent variables H. Starting from the F
by Algorithm 1, we apply the logit transformation to the subset of rows where we did not have to
adjust the values that were < 0 or > 1, and then extract the right singular vectors of this transformed
subset. As long as the subset is large enough to span the same space as the row space of L, this
approach accurately estimates the basis of H. Next, we calculate the maximum likelihood estimation

b to yield A
b and then L
b =A
bH
b . This involves performing a logistic regression
of A parametrized by H
b . In order to estimate the individual-specific allele frequency matrix F, we
of each SNP’s data on H
8

b = logit−1 (L)
b . An important property to note is that all π
calculate F
bij ∈ [0, 1] due to the fact that we
are modeling the natural parameter.
Algorithm 2: Estimating Logistic Factors
∗

e from Step 4.
1. Apply Algorithm 1 to obtain the estimate F
∗

∗ is the (i, j) entry of F
e , we choose some C & 0 and form
2. Recalling that π
eij
∗
< 1 − C, ∀j = 1, ..., n}.
S = {i : C < π
eij

e ∗ where the logit function can be applied stably. Here we use C = 1 .
S identifies the rows of F
2n


e S to be the corresponding subset of rows of F
e ∗ , and calculate L
e S = logit F
e S . Let L
e0
3. Define F
S
eS .
be the row-wise mean centered and standard deviation scaled matrix L
e 0 resulting in L
e 0 = TΛWT . Set H
b to be the d × n matrix composed of the
4. Perform SVD on L
S
S
0

b stacked on the row n-vector (1, 1, · · · , 1):
top d − 1 right singular vectors of the SVD of L
S
!

WT1:(d−1)

b =
H

1 1
···
1 1


w11
w21
···
wn1


 w
w22
···
wn2 
 12

 .

.
.

..
..  .
=  ..



w1,d−1 w2,d−1 · · · wn,d−1 


1

···

1

1

Algorithm 3: Estimating F and L from LFA

b.
1. Apply Algorithm 2 to X to obtain H
2. For each SNP i, perform a logistic regression of the SNP genotypes xi = (xi1 , xi2 , . . . , xin ) on

b , specifically by maximizing the log-likelihood
the rows of H
b =
`(π i |xi , H)

n
X


xij log

j=1

under the constraint that logit(πij ) =

Pd

πij
1 − πij

k=1 aik hkj .

b

because b
hdj = 1 ∀j by construction.

9


+ 2 log(1 − πij )

It should be noted that an intercept is included

3. Set b
aij (j = 1, . . . , n) to be equal to the maximum likelihood estimates from the above model fit,

b=A
bH
b, F
b = logit−1 (L)
b , and π
b:
for each of i = 1, . . . , m. Let L
bij be the (i, j) entry of F
exp
π
bij =

nP
d

1 + exp

aik hkj
k=1 b

o

b

nP
d

aik b
hkj
k=1 b

o.

PCA-based estimation of Model 1 requires one application of singular value decomposition (SVD)
and LFA requires two applications of SVD. We leverage the fact that n  d to utilize Lanczos bidiagonalization which is an iterative method for computing the singular value decomposition of a matrix [32].
Lanczos bidiagonalization excels at computing a few of the largest singular values and corresponding
singular vectors of a sparse matrix. While the sparsity of genotype matrices is fairly low, we find that
in practice using this method to perform the above estimation algorithms is more effective than using
methods that require the calculation of all the singular values and vectors. This results in a dramatic
reduction of the computational time needed for the implementation of our methods.

3

R ESULTS

We applied our methods to a comprehensive set of simulation studies and to the HGDP and TGP data
sets.

3.1

Simulation Studies

To directly evaluate the performance of the estimation methods (Section 2.2), we devised a simulation
study where we generated synthetic genotype data with varying levels of complexity in population structure. Genotypes were simulated based on allele frequencies subject to structure from the BN model,
the PSD model, spatially structure populations, and real data sets. For the first three types of simulations, the allele frequencies were parameterized by Model 1, while for the real data simulations, the
allele frequencies were taken from model fits on the data themselves.
A key property to assess is how well the estimation methods capture the overall structure. One

e from the PCA based method (Algorithm 1) estimates
way to evaluate this is to determine how well S
b from LFA estimates the true H. Note that even though
the true underlying S, and likewise how well H
b by converting with L =
the genotype data was generated from the F of Model 1, we can evaluate H
e and calculated the average R2 ; similarly,
logit(F). To evaluate PCA, we regressed each row of F on S
b and calculated the average R2 value. The results are
for LFA we regressed each row of L on H
presented in Table 1. Both methods estimate the true latent structure well.

10

We specifically note that when the PSD model was utilized to simulate structure, we were able to
recover the structure S very well (Supplementary Figure 6) without needing to employ the computationally intensive and assumption-heavy Bayesian model fitting techniques from ref. [14]. Additionally, it

e largely captures the geometry of S where it may be the case that S can be recovered
seems that the S
e back into the simplex. By comparing the results on the
with a high degree of accuracy by transforming S
real data (Figures 2-3) with the simulated data (Supplementary Figure 6), one is able to visually assess
how closely the assumptions of the PSD model resemble real data sets. When structure was simulated
that differed substantially from the assumptions of the PSD model, our estimation methods were able
to capture that structure just as well (Supplementary Figure 7). This demonstrates the flexibility of the
proposed approaches.
We also compared PCA and LFA to two methods of fitting the PSD model, ADMIXTURE [16] and
fastStructure [25], by seeing how well the methods estimated the individual specific allele frequencies

πij (Table 3). For the real data scenarios, we generated synthetic genotypes based on estimates of
F from the four different methods, thus giving each method an opportunity to fit its own simulation.
The methods were compared by computing three different error metrics with respect to the oracle F:
Kullback-Leibler divergence, absolute error, and root mean squared error. PCA and LFA significantly
outperformed ADMIXTURE and fastStructure, which confirms the intuitive understanding of the differences between the models: the goal of Model 1 and 2 is to estimate the allele frequencies πij , while
the PSD model provides a probabilistic interpretation of the structure by modeling them as admixture
proportions.
The computational time required to perform the proposed methods was also significantly better than
ADMIXTURE and fastStructure. Both proposed methods completed calculations on average over 10
times faster than ADMIXTURE and fastStructure, with some scenarios as high as 150 times faster.
This is notable in that both ADMIXTURE and fastStructure are described as computationally efficient
implementations of methods to estimate the PSD model [16, 25].

3.2

Analysis of the HGDP and TGP Data

We analyzed the HGDP and TGP data using the proposed methods. The HGDP data consisted of n =

940 individuals and m = 431, 345 SNPs, and the TGP data consisted of n = 1500 and m = 339, 100
(see Supplementary Section 6.1 for details). We first applied PCA and LFA to these data sets and made
bi-plots of the top three PCs and top three LFs (Figures 2 and 3). It can be seen that PCA and LFA
provide similar visualizations of the structure present in these data. We next chose a dimension d for
the LFA model (Model 2) for each data set. This was done by identifying the value of d that provides
the best overall goodness of fit with Hardy-Weinberg equilibrium (Supplementary Section 6.2). We
identified d = 15 for HGDP and d = 7 for TGP based on this criterion.
11

One drawback of utilizing a PCA based approach (Algorithm 1) for estimating the individual-specific
allele frequencies F is that we are not guaranteed that all values of the estimates lie in [0, 1], so some
form of truncation is necessary. We found that 65.4% of the SNPs in the HGDP data set and 26.5%
in the TGP data set resulted in at least one estimated individual-specific allele frequency < 0 or > 1

e is necessary
before the truncation was applied. Therefore, the truncation in forming the estimate F
when employing Algorithm 1 to estimate F from Model 1. On the other hand, due to the formulation of
Model 2, all estimated allele frequencies fall in the valid range when applying LFA (Algorithms 2 and 3).
The LFA framework provides a natural computational method for ranking SNPs according to how
differentiated they are with respect to structure. Note that existing methods typically require one to
first assign each individual to one of K discrete subpopulations [33] which may make unnecessary
assumptions on modern data sets such as HGDP and TGP. In order to rank SNPs for differentiation,
we calculate the deviance statistic when performing a logistic regression of the SNPs genotypes on the
logistic factors. Specifically we calculated the deviance by comparing the models logit(π i ) = aid hd vs.

logit(π i ) =

Pd

k=1 aik hk ,

where the former model is intercept only (i.e., d = 1, no structure).

Our application of LFA to identify SNPs with allele frequencies differentiated according to structure
can be developed further. First, the recently proposed “jackstraw” approach [34] provides a manner in
which statistical significance can be assigned to these SNPs. Assigning statistical significance to the
population differentiation of SNPs has traditionally been a difficult problem [35]. Second, we found the
deviance measure tends to have more extreme values for SNPs with larger minor allele frequencies
(MAFs). Therefore, the ranking of SNPs may be made more informative if MAF is taken into account.
Third, although this ranking is identifying differentiation and not specifically selection, it may provide a
useful starting point in understanding methods that attempt to detect selection.
The most differentiated SNPs (Supplementary Tables 4 and 5) reveal some noteworthy results,
especially considering the flexible approach to forming the ranking. SNPs located within or very close
to SLC24A5 were the top ranked in both HGDP and TGP. This gene is well known to be involved in
determining skin pigmentation in humans [36] and is hypothesized to have been subject to positive
selection [37]. The next most highly ranked SNPs in both studies are located in EDAR, which plays
a major role in distinguishing phenotypes (e.g., hair follicles) among Asians. SNP rs3827760 is the
second most differentiated SNP in the TGP data, which has also been hypothesized to be under positive
selection in humans and whose causal role in the hair follicle phenotype has been verified in a mouse
model [26]. SNPs corresponding to these two genes for both studies are plotted in increasing order of

π
bij values, revealing subtle variation within each major ancestral group in addition to coarser differences
in allele frequency (Figure 4). Other noteworthy genes with highly differentiated proximal SNPs include:

• FOXP1, which is a candidate gene for involvement in tumor progression and plays an important
regulatory role with FOXP2 [38, 39];
12

• TBC1D1 in which genetic variation has been shown to confer risk for severe obesity in females
[40];

• KIF3C, a novel kinesin-like protein, which has been hypothesized to be involved in microtubulebased transport in neuronal cells [41];

• KCNMA1, a recently identified susceptibility locus for obesity [42];
• CTNNA3 in which genetic variation has been shown to be associated with diisocyanate-induced
occupational asthma [43];

• PTK6, breast tumor kinase (Brk), which is known to function in cell-type and context-dependent
processes governing normal differentiation [44].
We have provided information on the 5000 most differentiated SNPs for both TGP and HGDP in supplementary files.

3.3

Software

An R package called lfa is available at https://github.com/StoreyLab/lfa.

4

D ISCUSSION

We have investigated two latent variable models of population structure to simultaneously estimate all
individual-specific allele frequencies from genome-wide genotyping data. Model 1, a direct model of
allele frequencies, can be estimated by using a modified PCA and Model 2, a model of the logit transformation of allele frequencies, is estimated through a new approach we called “logistic factor analysis”
(LFA). For both models, the latent variables are estimated in a nonparametric fashion, meaning we do
not make any assumptions about the underlying structure captured by the latent variables. These models are general in that they allow for each individual’s genotype to be generated from an allele frequency
specific to that individual, which includes discretely structured populations, admixed populations, and
spatially structured populations. In LFA, we construct a model of the logit of these allele frequencies in
terms of underlying factors that capture the population structure. We have proposed a computationally
efficient method to estimate this model that requires only two applications of SVD. This approach builds
on the success of PCA in that we are able to capture population structure in terms of a low-dimensional
basis. It improves on PCA in that the latent variables we estimate can be straightforwardly incorporated into downstream statistical inference procedures that require well-behaved estimates of allele

13

frequencies. In particular, statistical inferences of Hardy-Weinberg equilibrium, FST , and marker-trait
associations are amenable to complex population structures within our framework.
We demonstrated our proposed approach on the HGDP and TGP data sets and several simulated
data sets motivated by the HapMap, HGDP, and TGP data sets as well as the PSD model and spatially
distributed structures. It was shown that our method estimates the underlying logistic factors with a
high degree of accuracy. We also showed that applying PCA to genotype data estimates a row basis
of population structure on the original allele frequency scale to a high degree of accuracy. However,
problems occur when trying to recover estimates of individual-specific allele frequencies because PCA
is a real-valued model that does not always result in allele frequency estimates lying between 0 and 1.
Although PCA has become very popular for genome-wide genotype data, it should be stressed that
PCA is fundamentally a method for characterizing variance and special care should be taken when
applying it to estimate latent variables. The authoritative treatment of PCA [45] eloquently makes this
point throughout the text and considers cases where factor analysis is more appropriate than PCA
through examples reminiscent of the population structure problem. Here, we have shown that modeling
and estimating population structure can be understood from the factor analysis perspective, leading to
estimates of individual-specific allele frequencies through their natural parameter on the logit scale. At
the same time, we have avoided some of the difficulties of traditional parameteric factor analysis by
maintaining the relevant nonparametric properties of PCA, specifically in making no assumptions about
the underlying probability distributions of the logistic factors that capture population structure.

14

5

F IGURES AND TABLES
HapMap

● YRI
● CEU
● JPT+CHB

Human Genome Diversity Project (HGDP)

●
●
●
●
●
●
●

AFRICA
AMERICA
CENTRAL_SOUTH_ASIA
EAST_ASIA
EUROPE
MIDDLE_EAST
OCEANIA

1000 Genomes Project (TGP)

●
●
●
●
●
●
●

AFRICA_IN_AFRICA
AFRICA_IN_AM
EAST_ASIA_IN_AM/ASIA
EUROPE_IN_AM/EU
MEXICO_IN_AM
SOUTH_AMERICA
SOUTH_ASIA_IN_AM/ASIA

Figure 1: A hierarchical clustering of individuals from the HapMap, HGDP, and TGP data sets. A dendrogram was drawn from a hierarchical clustering using Ward distance based on SNP genotypes (MAF

> 5%). Whereas the HapMap project shows a definitive discrete population structure (by sampling
design), the HGDP and TGP data show the complex structure of human populations.

15

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●

●
● ●
●
●
●
●
●
●
●
●
●
●
●
●

●

●●
● ● ●
●
●
●
●
●
●● ● ●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
● ●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●
●
●
●

●
●
●●
●
●●
● ● ●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
● ●
●

●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
● ●● ●
●●●
●
●
●
●
●
●
●
●●
●●● ●
●●●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
● ● ●
●●●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
● ● ●●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●● ●●
●
●
●
●
●
●●
●
●
●●
● ●
●
●●
● ●

●

●

●

●
●
● ● ●●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●

●

●
●
●
●
● ●
●
●
●
●
●

●
●
●
●
●●●
● ●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●●
●
●●
●●
●
●
●●●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
● ● ● ●●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
● ●
●
●●
● ●

PC3

●
●

PC3

PC2

●

●

●●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●

●
●
●

●
●
●
●
●

●●●
●
● ●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●

●●
●●
●●
●●●
●● ●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

PC1

●
●
●
●
●
●
●
●
●
●
●
●
●
●

PC1

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●

●
●
● ●●
●
●
●
●
●
●
●
●
●
●

● ●●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●

●●

●
●
●
●
●
●
●
●
● ●● ● ● ●●●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●

PC2

●
●
●●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●●
●● ●●

●
●
●

●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●●● ● ●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●● ● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
● ●
●
●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●● ●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●●
●●●
●●
●●
● ●
●
●

●

●

●

AFRICA
AMERICA
CENTRAL_SOUTH_ASIA
EAST_ASIA
EUROPE
MIDDLE_EAST
OCEANIA

● ● ● ●●
●
●
●
●● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●

●

LF3

●

LF3

LF2

●

●

●
●

●
●
●
●●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●●●
●
●
●●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●●● ●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●

LF1

●
●
●

●●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●●●
●●●●
●
●●
●
●
●
●
●●
●
●●
●●
●●●
●
●●
●●
●
●
●
●
●
●
●●
●
●
● ●●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●●● ●
●●
●
●●
●
●
●
●
●● ● ●●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
● ●
●
●
●
●
●
●

●

●
●●
●
● ●●
●● ●
●
●
●
●
●
●
●●
●●
●
●
●●●●
●
●
●
●
●
●
●●
●
●

●●●
●
● ●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●

LF1

●
●
●
●
●●
●
●●
●
●
●
●●
●
●
●

LF2

Figure 2: Principal components versus logistic factors for the HGDP data set. The top three principal
components from the HGDP data are plotted in a pairwise fashion in the top panel. The top three
logistic factors are plotted analogously in the bottom panel. It can be seen that both approaches yield
similar visualizations of structure.

16

●
●
●

●

●
●●
●

●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

● ●
●●●
●
●
●
● ●
●
●●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●●
●
●● ●
● ●●
●
●● ●
●● ●
●●●
●●
● ● ●
● ● ●●●
●
●
●
●
●
●
●
●●●●
●●
●●●
●● ●
● ●●●
●
●●●
●●●
● ●
●
●
●
●
●●
●
●
●●
●
● ● ● ●●
●●
●
●● ●
●●● ● ●●●
●●
●
●
●●●
●●●● ●
●●
●●
●● ●● ● ●●
●●
●
●
● ●
● ● ●● ●●
●●
● ●●●● ●
●●●
●
●
●
●
●
●
●
●
● ●●
●
●●●● ●
●
●●
●●
●
●●
● ●●
●
●
●●●●●
●●
●●●
●
●●
●
●
●
●
●
●
●
● ●● ●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

PC1

●

●

● ●

●
●

●
● ●

●
●
● ● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●●●
●
●
●
●
●
●
●
●
●●
●● ●

●

●●●
●
●

●

●

●
●●

●

●

●

●
●●
● ●●
●●
●
●●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●

LF1

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●
●● ●
●
●●●
● ●●
●●● ●
●●●
●●
●
●●
● ●
●
● ●●●
●
●
●●
●
●
●
●
●●●●●
●●
●●
●●
●
●●●●
● ●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●●
●●●● ● ●
●● ●
●● ●
●●● ●●
●
●
●●
●
●●●●●●
●
●●
●
●
●
●●
●
●●
●● ●
●
●●
● ●●
●
●
●
●● ●
●●●
●
●
●
●
●●
●
●
●●
●
●
●●
●●
●●●
●
●
●
●
●
●
●
●
● ●
●
●
●●
●●
●
●
●
●
●
●●●●
●
●●
●
●
●
●
●
●
●
●
●
●
● ● ●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

LF3

●
●●
●
●
●●●
●
●
●
●
● ●
●●
●●
●●
●
●
● ●
●●●
●●●

●
●

PC2

●

LF3

LF2

●

●●

●
●
●
●
●
●
●
●●
●
●●
●
● ●
●
●
●
●
● ●
●● ●
●
●
●
● ●●
●●
●
●
●
●
● ● ● ●●
●
●●
●
● ●●
●
● ●●
● ●
● ●
●● ●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
● ●
●
●
● ●●●
●
●
●●
●
●
●
●● ● ●
●
● ●
● ●●●
●●
● ●●
●●
●
● ●
●
●● ●
● ● ●
●
●●
●●●● ● ●
●
●
●
● ●●
●●
●
●
● ●
●●
●
●
● ●●
●
●●
●
●●●
● ● ●●
● ●
● ●
●
●●
●●●● ●
●●
● ●●
●●
●
●
●
●
●
●
●
●
● ●
●
●
● ●
●
●
● ●
● ●
●
●●
●●
●
● ● ●●
●● ●●
●●●●
●
●
●
●●
●●
●
●
●●● ●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●● ● ● ●
●
●
●
●
●
●●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
● ● ●●
●
●
● ●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

PC1

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●●●
●
●
●●
●●●
●●
●
●
● ●●
●●●
●
●
●
●●● ●●
●
●
●● ●
●
●
●
● ●●
●
●
●●
●●
●●
●●●●●●
●
●
●
●
●
●
●●● ● ●
●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●● ●●●●●
●
●
●●●
●
●
●
● ●
● ●
●
●
●
●
●
●
●
●●
●●
●
●●
●●
●
●
●
●
●
●
●● ●●
●
●●●
●
●
●
●
●● ●●
●
●
●● ●●●
●● ●
●
●
●
●
●
●●●●●
●●
●●
●
●
●
●●
●
●●
●
●●
●●● ● ●●●
●● ●
●● ●●
● ●
●● ●
●●●●
●
● ●
● ●● ●●
●●
●
●
●
●●●●●●●●
● ●●
●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●

PC3

●
●●
●
●
●
●
●
●●
●
●●
●
●●
●●
●
●
●●
●●●
● ●
●
●
●●
●
●
●●●

PC3

PC2

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●●●
●
●
●
●● ● ●
●●
●
●
●
●
●
●●
●
●●
● ●
●
●
●
●●
●●
●
●
●●
●●
●
●●
●● ●
●●
●
●
●
●●
●
●
●
●●●
●
● ●
● ●●
●
●●
● ● ●●●●
●●
●
●● ●● ●●●●
●
●●●
●
●
●
● ●
●
●
●
●
●●●
●
●●
●
●
● ●●●●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●● ●
●
● ●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●●
●●
●
●
●
●
● ● ●●●
●
●
● ●●
●
●●
●●
●
●●●●
●●
●
●
●
●
● ●● ●
●
●● ●●●
●
●●●
●
●
●● ●
●●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●

●

● ●●
●
●●
●
●
● ●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●●●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●●●
●
●
●●●
●● ●
● ●

LF1

●
●
●
●
●
●
●

AFRICA_IN_AFRICA
AFRICA_IN_AM
EAST_ASIA_IN_AM/ASIA
EUROPE_IN_AM/EU
MEXICO_IN_AM
SOUTH_AMERICA
SOUTH_ASIA_IN_AM/ASIA

●
●
●
●
●
●

●●
●
●
●
●●
●
●
●●
●
●
● ● ●●
● ●
●
●
●
● ●●
●
●●
●
●
●● ●
●●
●
●
●
●
●
● ● ●●
●
● ●
● ●●
●
●
● ● ●
●
● ●
●● ●
●●
●
●
●
●
●●
●
● ●
● ●
● ●●
● ●
●●●
●
●
● ●●
●
●●
●●
●●
●
●
●●
●
●●●
●● ● ●
●
● ●
●
●
●
●
●
● ●● ●● ●
● ●●
●
● ●
●●
●
●
●
●●
●
●
●
● ●●
● ●
●
●
●●
●● ● ● ●●
● ●
●
●
● ●● ● ● ●●
●●●●
● ●●
●
●
●
●●●
●
●
●
●●●●●
● ●●
●
●
● ●
●●●
● ●●
● ● ●●
●●
●●
●
●
●
●
●
●● ●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●●
●
●
●
●●
●
●●●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●● ●●
●
●
●
●●
●
●
●
●
●
●
●●
●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●

LF2

Figure 3: Principal components versus logistic factors for the TGP data set. The top three principal
components from the TGP data are plotted in a pairwise fashion in the top panel. The top three logistic
factors are plotted analogously in the bottom panel. It can be seen that both approaches yield similar
visualizations of structure.

17

18
Individuals Ordered by Allele Frequency

0.00
750

0.00
500

0.25

0.75

1.00

0.00

0.25

OCEANIA

MIDDLE_EAST

EUROPE

EAST_ASIA

CENTRAL_SOUTH_ASIA

AMERICA

AFRICA

0.50

250

rs260690 − EDAR

geo

0.25

0.50

0.75

1.00

0.50

0.75

1.00

0.00

0.25

0.50

0.75

HGDP
rs1834640 − SLC24A5

rs3827760 − EDAR

1000

Individuals Ordered by Allele Frequency

500

TGP
rs1426654 − SLC24A5

1500

geo

SOUTH_ASIA_IN_AM/ASIA

SOUTH_AMERICA

MEXICO_IN_AM

EUROPE_IN_EU

EAST_ASIA_IN_AM/ASIA

AFRICA_IN_AM

AFRICA_IN_AFRICA

ancestry. The horizontal bars on the sides of the plots denote the usual allele frequency estimates formed within each ancestral group.

to LFA are shown for the HGDP and TGP data sets. For each SNP, the π
bij values are ordered and they are colored according reported

Figure 4: SNPs with highly differentiated allele frequencies with respect to structure. Two of the most highly different SNPs according

Estimated Individual−Specific Allele Frequencies

1.00

Estimated Individual−Specific Allele Frequencies

Table 1: Accuracy in estimating linear bases for S. Column 1 shows the scenario from which the data
were simulated. Columns 2 and 3 display the estimation accuracy of the PCA based method (Column 2)
and LFA (Column 3). Column 2 shows the mean R2 value when regressing the true (πi1 , πi2 , . . . , πin )

b from PCA, averaging across all SNPs. Column 3 shows the mean R2 value when regressing the
on S
b from LFA, averaging across all SNPs. All estimated
true (logit(πi1 ), logit(πi2 ), . . . , logit(πin )) on H
standard errors fell between 10−6 and 10−8 so these are not shown. Note for each scenario, R2 values
are higher for the method from which the true F matrix was generated. All but the two scenarios marked
with an asterisk (*) are from Model 1, while the two marked scenarios are from Model 2, where we took

F = logit−1 L.

Mean R2
Scenario

e
F∼S

b
logit(F) ∼ H

TGP fit by PCA

0.9998

0.9722

TGP fit by LFA *

0.9912

0.9990

HGDP fit by PCA

0.9996

0.9614

HGDP fit by LFA *

0.9835

0.9983

BN

0.9999

0.9999

PSD α = 0.01

0.9998

0.9974

PSD α = 0.1

0.9998

0.9879

PSD α = 0.5

0.9996

0.9827

PSD α = 1

0.9993

0.9844

Spatial a = 0.1

0.9999

0.9964

Spatial a = 0.25

0.9999

0.9962

Spatial a = 0.5

0.9999

0.9964

Spatial a = 1

0.9998

0.9970

19

7.8E-5

5.2E-4
4.8E-4
4.4E-4
5.5E-4

1.2E-3
1.1E-3
1.4E-3
1.6E-3

6.6E-5
6.3E-5

4.1E-4
4.3E-4
5.4E-4
4.1E-4

1.0E-3
9.9E-4
1.6E-3
1.4E-3

a = 0.5

a = 1.0

PCA

LFA

ADX

FS

PCA

LFA

ADX

FS

9.5E-5

1.1E-4

6.9E-5

1.2E-4

7.4E-5

a = 0.25

6.1E-5

α = 1.0

8.5E-5

7.3E-5

6.3E-5

α = 0.5

9.2E-5

a = 0.1

6.7E-5

7.3E-5

6.8E-5

6.9E-5

7.0E-5

LFA

3.1E-2

2.3E-3

1.3E-2

1.3E-2

7.8E-4

5.0E-3

2.4E-3

2.8E-3

1.2E-2

1.0E-2

8.6E-3

8.2E-3

3.3E-2

5.4E-2

3.6E-2

1.6E-2

2.6E-3

ADX

Median KL

PCA

α = 0.1

α = 0.01

Scenario

2.9E-2

2.3E-3

1.2E-2

1.4E-2

9.2E-4

5.5E-3

2.7E-3

3.4E-3

1.2E-2

1.0E-2

8.6E-3

8.1E-3

3.3E-2

5.4E-2

3.6E-2

1.6E-2

2.6E-3

FS

2.6E-2

2.6E-2

2.2E-2

2.3E-2

1.3E-2

1.5E-2

1.3E-2

1.3E-2

5.7E-3

5.6E-3

5.6E-3

5.5E-3

5.6E-3

5.6E-3

5.6E-3

5.6E-3

5.8E-3

PCA

column is the accuracy of a method’s fits with the given metric.

2.7E-2

2.6E-2

2.4E-2

2.5E-2

1.5E-2

1.3E-2

1.4E-2

1.5E-2

6.4E-3

6.9E-3

7.4E-3

7.6E-3

6.3E-3

6.8E-3

6.9E-3

5.8E-3

5.8E-3

LFA

1.4E-1

5.6E-2

1.2E-1

1.2E-1

5.6E-2

1.1E-1

7.9E-2

8.1E-2

1.1E-1

6.7E-2

9.3E-2

7.4E-2

1.4E-1

1.4E-1

1.6E-1

9.7E-2

3.7E-2

ADX

Mean Abs. Err.

1.3E-1

5.6E-2

1.2E-1

1.2E-1

5.8E-2

1.1E-1

8.1E-2

8.3E-2

1.1E-1

6.7E-2

9.3E-2

7.4E-2

1.4E-1

1.4E-1

1.6E-1

9.7E-2

3.7E-2

FS

3.6E-2

3.6E-2

3.5E-2

3.4E-2

1.8E-2

2.0E-2

1.8E-2

1.8E-2

7.4E-3

7.2E-3

7.2E-3

7.0E-3

7.4E-3

7.3E-3

7.2E-3

7.2E-3

7.5E-3

PCA

3.8E-2

3.7E-2

3.7E-2

3.6E-2

2.1E-2

1.9E-2

2.0E-2

2.1E-2

8.5E-3

9.2E-3

9.8E-3

1.0E-2

8.4E-3

9.0E-3

9.3E-3

7.6E-3

7.5E-3

LFA

ADX

2.2E-1

1.0E-1

2.2E-1

2.2E-1

1.3E-1

2.0E-1

1.4E-1

1.5E-1

1.7E-1

1.0E-1

1.6E-1

1.2E-1

2.2E-1

1.8E-1

2.4E-1

1.7E-1

5.8E-2

RMSE

2.1E-1

1.0E-1

2.2E-1

2.2E-1

1.3E-1

2.0E-1

1.5E-1

1.5E-1

1.7E-1

1.0E-1

1.6E-1

1.2E-1

2.2E-1

1.8E-1

2.4E-1

1.7E-1

5.8E-2

FS

Table 2: Accuracy in estimating πij parameters by the PCA based method and LFA. Each row is a different simulation scenario. Each

BN

PSD

Spatial

TGP fit

HGDP fit

20

6
6.1

S UPPLEMENTARY M ATERIAL
Data sets

The HGDP data set was constructed by intersecting the data available from the HGDP web site,

http://www.hagsc.org/hgdp/files.html, with the set of individuals “H952” identified by Rosenberg
(2006) [46] with a high confidence as containing no first and second-degree relative pairs. This yielded
complete SNP genotype data on 431,345 SNPs for 940 individuals.
In order to obtain data from the TGP we first obtained the genotype data that had been measured
through the Omni Platform, 2011-11-17, ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/

working. We removed related individuals based on the TGP sample information. We then sorted individuals according to least percentage of SNPs with missing data, and we selected the top 1500
individuals. This yielded complete SNP genotype data on 339,100 SNPs for 1500 individuals.
We utilized the HapMap data set in the simulated data described below. We obtained the HapMap
data release 23a, NCBI build 36 from www.hapmap.org consisting of unrelated individuals: 60 from European ancestry group (CEU), 60 from Yoruba, Africa (YRI) , and 90 from Japan and China (JPT+CHB).
We identified all SNPs with observed minor allele frequency ≥ 5% and with no missing data. The
total number of SNPs used after filtering in each population were CEU: 1,416,940, YRI: 1,539,314,
JPT+CHB: 759,452. We then identified all SNPs common to all three populations resulting in a total of
363,955.

6.2

Choosing the model dimension

The model dimension d was determined for the HGDP and TGP data sets under the rationale that when

d is large enough, then the great majority of SNPs should appear to be in HWE. When d is too small,
then the structure which has not been accounted for will lead to spurious deviations from HWE. Values

d = 1, 2, . . . , 20 were considered for each data set, and we ended up identifying d = 15 for HGDP and
d = 7 for TGP. We note that these choices could also be interpreted as reasonable according to a scree
plot when PCA was applied to the genotype data.

b using the LFA method. We calculated a HWE goodness of fit
For a given d value, we formed F
statistic for each SNP i as follows:
2
X
k=0

hP

i2
 k
P
= k) − nj=1 k2 π
bij (1 − π
bij )2−k

,
Pn
2 k
bij (1 − π
bij )2−k
j=1 k π

n
j=1 1(xij

 k
P
= k) is the observed number of genotypes equal to k and nj=1 k2 π
bij (1− π
bij )2−k is
b to simulate five instances
the expected number of genotypes equal to k under HWE. We then utilized F

where

Pn

j=1 1(xij

21

of a genotype matrix X0 under HWE where we simulated x0ij ∼ Binomial(2, π
bij ). On each simulated

b 0 and calculate HWE goodness of fit statistics.
genotype matrix X0 , we again applied LFA to obtain F
These goodness of fit statistics were then pooled across all five simulated data sets and across all SNPs
to form the null distribution, which then allowed us to calculate a HWE p-value for each observed SNP.
(It should be noted that we also formed a separate null distribution according to minor allele frequency
bins of length 0.05, and we arrived at the same conclusion.) We then compared these p-values to the
Uniform(0,1) distribution and also against the p-values from the d + 1 case. This allowed us to identify
a value of d where the HWE p-values were both close to the Uniform(0,1) distribution and to the HWE
p-values from the d + 1 case.

6.3

Simulated data

For each simulation scenario, genotypes X were simulated such that xij ∼ Binomial(2, πij ), where πij
were elements of the allele frequency matrix F. The results from the simulated data are summarized in
Tables 1 and 2.
Balding-Nichols (BN). For each SNP in the HapMap data set, we estimated its marginal allele frequency
according to the observed frequency and estimated its FST value using the Weir & Cockerham estimate
[47]. We set the simulated data to have m = 100, 000 SNPs and n = 5000 individuals with d = 3.
Using Model 1, the S matrix was generated by sampling its columns sj i.i.d. from (1, 0, 0)T , (0, 1, 0)T ,
and (0, 0, 1)T with respective probabilities 60/210, 60/210, and 90/210 to reflect the original data’s
subpopulation proportions. For each row i of Γ, we simulated i.i.d. draws from the Balding-Nichols
i.i.d.

model: γ i1 , γ i2 , γ i3 ∼ BN(pi , Fi ), where the pair (pi , Fi ) was randomly selected from among the
marginal allele frequency and FST pairs calculated on the HapMap data set.
PSD. We analyzed each SNP in the HGDP data set to estimate its marginal allele frequency according
to the observed marginal frequency and FST using the Weir & Cockerham estimate [47]. To estimate

FST , each individual in the HGDP data set was assigned to one of K = 5 subpopulations according to
the analysis in Rosenberg et al. (2002) [10]. We set m = 100, 000 SNPs and n = 5000 individuals with
i.i.d.

d = 3. Again utilizing Model 1, each row i of Γ was simulated according to γi1 , γi2 , γi3 ∼ BN(pi , Fi ),
where the pair (pi , Fi ) was randomly selected from among the marginal allele frequency and FST pairs
i.i.d.

calculated on the HGDP data set. To generate S, we simulated (s1j , s2j , s3j ) ∼ Dirichlet(α) for

j = 1, . . . , 5000. We considered α = (0.01, 0.01, 0.01), α = (0.1, 0.1, 0.1), α = (0.5, 0.5, 0.5), and
α = (1, 1, 1). It should be noted that as α → 0, the draws from the Dirichlet distribution become
increasingly closer to assigning each individual to one of three discrete subpopulations with equal
probability. When α = (1, 1, 1), the admixture proportions are distributed uniformly over the simplex.
Spatial. This scenario is meant to create population structure that is driven by spatial position of the
22

individual. We set the simulated data to have m = 100, 000 SNPs and n = 5000 individuals with d = 3.
i.i.d.

Rows i = 1, 2 of S were simulated as sij ∼ Beta(a, a) for j = 1, . . . , 5000, and row 3 of S contained
the intercept term, s3j = 1. We considered four values of a: 0.1, 0.25, 0.5, and 1. The first two rows
of S place each individual in a two-dimensional space (Figure 7), where the ancestry of individual j is
located at (s1j , s2j ) in the unit square. When a = 1, the Beta(a, a) distribution is Uniform(0, 1), so this
scenario represents a uniform distribution of individuals in unit square. As a → 0, the Beta(a, a) places
each individual with equal probabilities in one of the four corners of the unit square. The matrix Γ was
i.i.d.

created by sampling γij ∼ 0.9 × Uniform(0, 1/2) for j = 1, 2 and γi3 = 0.05. It should be noted that
all πij ∈ [0.05, 0.95] by construction.
Real Data. For the HGDP and TGP scenarios, we estimated an allele frequency matrix F from the real
data via four different methods. For HGDP we had m = 431, 345 SNPs by n = 940 individuals with

d = 15, and for TGP we had m = 339, 100 and n = 1, 500 with d = 7. The four methods are:
e estimated via Algorithm 1.
• PCA: F was taken to be the matrix F
b , where L
b was estimated via Algorithm 3.
• LFA: F = logit−1 (L)
• ADX: F was taken to be the matrix formed by computing the marginal allele frequencies in the
Pritchard-Stephens-Donnelly model, i.e. F = PQ, and P and Q were estimated via the software
ADMIXTURE [16].

• FS: Same as above except P and Q are estimated via the software fastStructure [25].

6.4

Error Measures Used to Evaluate Estimates of F and L

Estimates of πij were evaluated with three different metrics. Let π
bij be the estimate for any given
method.
The Kullback-Leibler divergence for the binomial distribution allows us to measure the difference
between the distribution from the estimated allele frequencies to the distribution from the oracle allele
frequencies:


KL = πij ln

πij
π
bij




+ (1 − πij ) ln

1 − πij
1−π
bij

Mean absolute error compares the allele frequencies directly:
m

n

1 XX
MAE =
|πij − π
bij | .
m×n
i=1 j=1

Root mean squared error:
23


.

v
u
u
RMSE = t

m

n

1 XX
(logit(πij ) − logit(b
πij ))2 .
m×n
i=1 j=1

6.5 FST for individual-specific allele frequencies
By considering the derivation of FST for K discrete populations as described in Weir (1984, 1996)
[47, 48], it can be seen that a potential generalization of FST to arbitrary population structure is

FST = 1 −

EZ [Var(x|Z)]
,
Var(x)

where, as described in Section , Z is a latent variable capturing an individual’s population structure
position or membership. The allele frequency of a SNP conditional on Z can be viewed as being
a function of Z , which we have denoted by π(Z). If n individuals are sampled independently and
homogeneously from the population1 such that z 1 , . . . , z n are i.i.d. from the distribution on Z , then for
SNP i in HWE, it follows that Var(xij |zj ) = 2πij (1 − πij ) and
a.s.

FST = lim 1 −

1
n

Pn

Pn

j=1 πij /n

− πij )

π i (1 − π i )

n→∞

where π i =

j=1 πij (1

,

is the marginal allele frequency among the n individuals. Thus, good estimates

of the πij values may be useful for estimating FST in this general setting. One example would be to
form a plug-in estimate of FST by replacing πij with π
bij from the proposed LFA method.

6.6

Relationship of LFA to existing models and methods

The problem of modeling a genotype matrix X in order to uncover latent variables that explain cryptic
structure is a special case of a much more general problem that has been considered for several years
in the statistics literature [49, 50]. Under a latent variable model, it is assumed that the “manifest”
(observed) variables are the result of the “latent” (unobserved) variables. Different types of the latent
variable models can be grouped according to whether the manifest and latent variables are categorical
or continuous. For example, factor analysis is a latent variable method for the case where both manifest
variable and latent variable are continuous. A proposed naming convention [23] is summarized as
follows:
1

When the individuals are not sampled homogeneously throughout the population (e.g., in the HapMap data with 60, 60,

and 90 observations from three discretely defined subpopulations), then it may be the case that the above quantity should be
modified to reflect the stratified or non-homogeneous sampling.

24

Manifest variables
Latent variables

Continuous

Categorical

Continuous

Factor analysis

Latent trait analysis

Categorical

Latent profile analysis

Latent class analysis

The problem we consider is that the manifest variables (observed gentoypes) are categorical, and they
are driven by latent variables (population structure) that may either be categorical (discrete population
structure) or continuous (complex population structure). Therefore, the LFA method may be described
as a nonparametric latent variable estimation method that jointly captures latent trait analysis and latent
class analysis. Another naming convention that we could apply to LFA would be to call it a nonparametric latent variable model for Binomial data. The naming conventions of latent variable models are
inconsistent and often confusing [23].
Bartholomew (1980) [51] proposed a model related to equation (2) to identify latent variables that
influence the probabilities of a collection of Binomial random variables. See also Bartholomew et al.
2011 for a comprehensive treatment of this area, which they call “general linear latent variable models”
(GLLVM). In particular, when the manifest variables xij ∼ Bernoulli(πij ) and the latent variables hkj
are continuous variables, the GLLVM in this case is Model 2 , logit(πij ) =

Pd

k=1 aik hkj .

While we

begin with this model, there are some key differences. The number of manifest variables in the data
considered in Bartholomew (1980) and related work is notably smaller than genome-wide genotype
data, so the assumptions and estimation approach differ substantially. Model assumptions are typically
made about the probability distributions of the latent variables; we consider these model assumptions
too strong and also unnecessary for the genome-wide genotype data considered here, although they
may be quite reasonable for the problems considered in other contexts. Existing methods typically
estimate Model 2 by calculating the joint posterior distribution of the hkj based on an assumed prior
distribution of the latent variables.
Our LFA approach for estimating the row basis of L is nonparametric since it does not require a prior
assumption on the distribution of latent variables, H. The model fitting methods of ref. [23] are too computationally intensive for high-dimensional data, requiring many iterations and potential convergence
issues. Our proposed algorithm requires performing SVD twice, which leads to a dramatic reduction in
computational burden and difficulties. Engelhardt and Stephens (2010) [24] make an interesting connection between classical factor analysis models of F and other models of population structure, but the
factor analysis model runs into the difficulty that the latent factors are assumed to be Normal distributed,
and the constraint that alleles frequencies are in [0, 1] is not easily accommodated by this continuous,
real-valued model.
Several extensions of PCA to categorical data have been proposed [52–54]. We found that the
algorithms perform very slowly on genome-wide genotyping data, and the estimation can be quite poor
25

when d > 1. Also, PCA is essentially a method for characterizing variance in data [45], and the
latent variable approach is more directly aimed at uncovering latent population structure. Non-negative
matrix factorization (NMF) [55] is another matrix factorization for count data (e.g., Poisson random
variables). This identifies two non-negative matrices whose product approximates the original matrix.
However, similarly to PCA, we do not find that this approach easily translates into interpretable models
of population and it is computationally intensive. NMF has proven to be quite useful as a numerical tool
for decomposing images into parts humans recognize as distinct [56].

26

7

S UPPLEMENTARY F IGURES AND TABLES
L	  	  	  	  	  	  	  	  	  	  	  =	  	  	  	  	  	  A	  	  	  	  H	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  ≈	  	  	  	  	  ⌃	  
A	  	  	  	  	  ⌃	  
H	  
Logis&c	  Factor	  Analysis	  	  

( )

(LFA)	  

logit π ij

X	  
0	  2	  2	  1	  1	  
0	  1	  0	  2	  1	  
0	  1	  2	  .	  .	  .	  

€

~	   ~	  
F	  	  	  	  	  	  	  	  	  	  	  =	  	  	  	  	  	  	  Γ	  	  	  	  	  S	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  ≈	  	  	  	  Γ	  	  	  	  	  	  	  S	  	  	  	  
SNP	  Data	  
Principal	  Components	  Analysis	  

π ij

(PCA)	  

€

⇒	  values	  in	  (−∞,∞)	  
⇒	  values	  in	  (0,1)	  

Figure 5: A comparison of LFA model (2) and its estimate to model (1) and its PCA estimate. The
proposed LFA approach first models the logit of the individual-specific allele frequencies in terms of
the product of two matrices, the left matrix establishing how population structure is present in allele
frequencies, and the right matrix giving the structure. Whereas the LFA approach preserves the scale
of the model through the estimate (all real-valued numbers), the same is not true to PCA. This leads
to issues in the estimation of individual-specific allele frequencies when utilizing PCA. We have shown,
however, that PCA estimates very well a row basis for S from Model 1. This connects PCA to an explicit
model of population structure.

27

1.00

0.75

0.50

0.25

0.00

S

\tilde{S}

alpha=0.01

alpha=0.01

●
●
●
●
●
●
●
●
●●
●
●●
●
●
●●
● ●
●
●
●
●
●
●
●
●
● ● ●●●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
● ●●
●
●
●
●
●
●●●
●
●●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
● ●●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●●
●●
●
●
●
●●
●●
●
●● ●
●●●
●
● ●●●●
●●● ●
●●● ● ● ●
● ●●● ●● ●
●●● ●●
●
●●
●●●●●●
●●
●
●
●
●●
●●
●
●
●
●

0.02
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
● ●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●●
●●●
●
●●
●
●
●
●●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
● ●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.00

−0.02

alpha=0.1

0.75

0.50

0.25

S2

0.00

alpha=0.1

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●●●●
●
●
●
●
●●●
●●●
●
●
●
●
●
●
●
●
●●●●● ●●
●
●
●●
●
●
●
●
●
●●
● ●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●●●
●
● ●● ●
●●
●
●
●
●
●
●●
●
●●● ●● ● ● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
● ●●● ●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●
●●
●●●● ●●●●●● ● ●
●
●
●
●
●
●
●
●
●●
●
●
● ● ●●
●
●
● ●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●● ●
●
●
●●
●
●●
●
● ●● ●●
●
●
●
●
●● ●
●
●
●
●
●● ●
●
●
●
●
●
●●●●
●
● ●●●
●
●
●
●
●
●
●
●
●
●
●
● ●●●●
●
●
●
●●●
●●
●
●
●●
● ● ●● ●
●
●
●
●
●●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●
●●
●● ●●●
●
●
●
●●●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
● ●
●
●
●
●
●
●
● ●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ● ● ●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●● ● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ● ●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●●
●●
● ●
● ●
●
● ●
●
●●
●
● ●●●
●
●
●
●
●
●
● ●●●●
●
●
●
●●
●
●
●
●
●
●●●
●●
●●
●
●
●●
●
●●
●
●
●
●
●
●● ●
●
●
●●
●
●
●
●●
●
● ●
● ●● ● ●
●●
●
●
●●
●
●
● ●● ●
●
●
●
●
●
●
●
● ●● ● ●
●
●
●●● ● ● ●● ●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●
● ●● ●●●
● ●●●
●
●
●
● ●
●
●
●
●
●
●
● ●●
●●● ● ●
●
● ● ●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
● ● ●
● ●● ●
●
●
●
●
●
●
●
● ●
●
●● ● ●
● ●●
●●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●● ● ●
● ●●
●
●●
●●
●
●
●
●
●●
●●
●
●
●● ●●
●
●
●
●●● ●
●
●
●
●
●
●
●
●
● ●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●● ●
● ●● ● ●
●
●
●
●
●●
●
●
● ●●
●
●
●● ● ●
● ●● ●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●● ●● ● ●● ● ●
●
●●
●
●
●
● ● ● ●●● ● ●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●● ●●●
●●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●●●●
●●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

alpha=0.5
1.00

0.75

0.50

0.25

0.00

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●● ●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●● ●
●
●
●●
●
●
●
●
●
●●
●
●●
●●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
● ● ●●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●● ●●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
● ●●●
●
●
●
●● ●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●●●● ● ● ●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
● ●
●
● ●● ●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
● ●●●
● ●● ●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●● ●
●
●
● ●
●
●
●
●
●●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●●
●●
●
●
●
●
●
●
●
●
● ● ●● ●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
● ● ● ● ● ●● ●●●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●●
●
●
● ●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●● ●
●
●●●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●● ●
●
●
●
●
●
●
●
●● ● ●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●●●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●
●●●●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●

0.02

0.00

−0.02

S3
1.00
0.75

S2

1.00

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●●
●●
●
●
●
●
●●●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●●
●●
●
● ●●
●
●
●
●
●●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●●●
●
●
●●●●
●●●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●●
●
●● ●
● ●●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●●
●
●
●
●
●
●
● ● ●● ●●
●
●
●
●
●
●
●●●●●
●
●
●●
●
●
●
●● ●
●
● ●●●
●
●
●
●
●
●
●●
●
●
●●●●●
●●
●●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
● ●● ● ● ● ● ●
●● ●●●●
●
●
●
●●
●
●●● ●
●
●●●
● ●●●
●
●● ●● ●
●●
●
●
●●●
●
●
●
●●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●●●●●●●
●
●●●●
●●●
●
●
●●●●●
●
●
●●
●●
●
●
●
●
●●●●
●
●
●
●
● ●●●
●●
●
●
●
●
●●
●●
●●
●
●
●
●
●
●●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●
●●●
● ●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●● ●
● ●●●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●●
●
●
●●
● ●● ●
●● ●
●● ●
●●
●
●●●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●
●●
●●●
●● ●●● ● ● ● ●● ● ●
●●
●
●
●●
●
●●●
●
●
●
●
●
●●
●●●●
●● ●● ●
●●●●
●
●
●
●●●
●
●
●
●
● ●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●●
●
●
●●
●
●
●●●●
●●●
●●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●●●
●
●
●
●
●● ● ●
●
●
●●
●●
●●●● ●
●
●
●
●
●●
●
● ●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●●
●
●
●
●
● ●●
●
● ●
●●
●
●
●●
●
●●●
●
● ●●
●●
●●●●
● ●
●
●●● ●●
●●
● ●●
●
●
●
●
●
●●
●●
●●
●●● ●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●●
● ● ●
●
●● ● ● ●
●
●●
●●
●● ●
●
●●●
●● ●●●
●● ●●●
●
●
●
●
●
●●●
●●● ●●
●
●
●●
● ●●
●
●
●●●
●
●
●●●●●
●
●
●●
● ●●
● ●●●●
●
●●
●
●●
●
●●
●●
●●
●●
●
●
●
●
●● ●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
● ● ●●
●
●●●
● ●
● ●● ●● ●● ●
●
●●● ●
●
●●●
●
●●
● ●
●●
●●
● ●●
●●●
●
●
●
●
●●
●
●
●
●●
●●
●●
● ●● ●●
●
●● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●● ●
●
●
●
● ●● ● ●●●●
●
●
●● ● ●● ●
●●
●●●
●
●●●●●●
●
●
●●
● ●●
●
●●
●
●
●
●
● ●
●
● ●
●●
●●
● ● ● ●●
●
●
●● ●
●
●
●●
●●
●●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●● ● ●●●●●● ●
●
●●●●
●●● ●
●
● ●●●●
●
●●●●
●
●
●
●●●
●●
●
●●
●●●
●●
●
●●●
●
●
●
●
●
●
●● ●●●●
●●●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●●
●●●●
●●●●● ●
●●
●
● ●
●
●●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●● ●●●● ●●
●
● ●
●●
●●
●●● ●●
●
●●● ● ●● ●
●
●●
●●
● ●●
●●
●●●●●
●
●
●
●● ●●
●
●
●
●
●
●●
●
●●
●●
●●
●
●●●●●
●●
●● ●
●●●●●●
●●●
●
●
●●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●● ● ●● ●●●
●
●●
●
● ●● ●
●
●
●
●
●
●●
●
● ●●
●●
●
●
●● ●
● ●●
●
●
●●●● ●
●
●
●
●
●
●
● ●
●
●●●
●●●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●● ● ●
●
●
●
●
●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
● ●
●●
●
●●
●
●
●
● ●
●● ●●
●
●
●
●
●●●
●
● ● ●●
●
●
●
●● ●
● ●
●
●
●●
●●
●
●
●
●●
●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●●
●
●●
●●●
●●
●
●
●
●
●
●●
●●●
●
●
●
●●
●●
●
●●
●●
●
●● ● ●● ● ●
●
●●
●●
●
●●
●●●●
● ●●●●●
● ●●●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●●
●●
●
● ●●
●
●
● ●●● ●● ●●
● ●●● ●
●
●●
●●
●
●●●
●●
●●
●
●●● ●●
●●
●
●
●●
●●
●●
●
●●●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
● ●●●
●
●
●
●
●
●
●
●
●
● ●● ●
●
●●●
●●
●
●
●
●
●●
●●
●●
●
●
●●●
●●
●●●
●
●●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●
●
●●
●●
●●
●
● ●●
●
●●
●
●●
●
●
●●
●
●●● ●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
● ●●●
●
●
●
●
●
●
●●
●
●
●
●
●● ●●● ●
●
●
●●●
●
●
●
●●
●
●
●
●●
●
●● ●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●●
●●●●
●
●●
●
● ●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●● ●●
●
●●●
●
●
●●●
●
●●
● ●●
●●●●
●
●●● ●
●
●
●
●●
●●
●
●● ● ●●●
●
●
●●
●
●
●●● ●
●
●
●
●
●
●
●●●
●
●●
●●
●
●
●
●
●●
●●
●
●●
●
●
●●●
●●●
●●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●
●
●
●●●
●
●
●
●●
●●
●●●
●●
●
●
●●
●
●●●●
●
●
●
●●●
●
●
●●●
●
●
●●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
● ●●
● ●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●
●
●
●●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●●
●
●●
●
●● ●
●●●●●●
●●●●
●
●
●
●
●
●
●
●●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●●
●
●
●● ●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●
●●
●
●
●
●
●
●●●
●
●
● ●●
●
●
●
●
●
●
●
●●●
●●●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●●●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●●● ●
●
● ●● ● ●
●
●●●
●● ●●
●●
●
●
●● ● ●
●
●
●● ●●●●● ●●
● ●●●●●
●
●●●●● ●●● ●
●
●
●●●●●●
● ●
●
●

alpha=0.5

0.25
0.00
0.02

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●●
●
●
●●
●●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●●●
●●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●●
●●
●●
●
●
●
●
●●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●●
●
●
●●●
●
●
●
●●
● ●●●●● ●●
●●
●●
●● ●
●
●
●●
●
●●
●●●
●
● ●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●●●●●●●
●●
●●
●
●
●
●
●
●
●
●●●●
●
●
●
●●●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●●
●●●
●
● ●●
●
●
●
●
●
●
●●
● ●●
●
●●●●●●
●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●●
●
●
●
●●
●●●●●●
●
●●
●
●
●
●
●
● ●
●
●
●●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●●●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●●
●
●●
●●●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●●
●●
●
●●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●●
●
●
●●
●
●
●
●
●●●●
●
●
●
●
●●●●●●
●
●●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●●
●●
●
●●●
●
●
●●
●
●
●
●●
●●●
●
●●
●
●●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●●
●
● ●
●
●●●
●
●
●
●
●
●
●●●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●●
●
●●
●
●
●●
●● ●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●●
●
●●
●
●
● ●● ●
●
●
●
●
●
●
●●
●●●●
●
●●●
●●
●●
●●
●
●●
●
●
●
●
●
●●●●
●●
●●●●
●●
●
●
●●●
●
●●●
●●
●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●●
●
●●
●●
●
●●
●●
●
●●
●●
●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●●●●●
● ●
●●
●
●
●●
●
●
●
●
●
●
●
●
● ●●
●
●●
●● ●
● ●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●●
●
●
●●
●●● ●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●●●
●
●●
●
●
●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●●
●●
●●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
● ●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
● ●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
● ●●
●
●
●
●
●●●
●
● ●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●●●
●
●
●
●
●
●
●
●●●
●●
●
●●
●●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●
●●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●●●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●●●
●
●
●●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●●●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.00

−0.02

alpha=1
1.00

0.75

0.50

0.25

0.00

alpha=1

●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
● ●
●
●
●●●
●
●
●●●
●
●
●
●●
●●
●●
●
●
●
●●
●
●
●
●
●
●●
●
●●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●●
●
●●
●●
●●
●
●
●●
●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●●
●●●
●
●●
●
●●
●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●●●●
●●
●●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●●●● ●●●●
●
●● ●
●
●
●●
●●
●
●
●●
●
●●
●●●
●●
●●
●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
● ●●
●
●
●●●
●
●●●●
●●●●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●●
●●●●
●●
●
●●
●
●
●
●●●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●
●
●●●
●●
●
●
●●
●
●
●
●●●
●●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●●●
●● ●●●
●
●
●
●●
●●
●
●●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
● ●●●● ●●●●●●
●●●
●●
●
●●
●
●
●
●●●
●
●
●
●
●
●
● ●●
●●●
●
●●●●
●
●
●●
●●
●
●
● ●●
●
●
●
●●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●● ●●
●
●
●
●●●
●
●
●
●
●
●●
● ●●
●
●
●
●
●
●
● ●●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
● ●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●●
●
●
●●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●●
●●
●
●
●
●●
●
●
●● ●
●
●
●
●●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●●
●●●
●●
●● ● ●●
●
●
●
●
●
●● ●
●
●
●
●
●
●●●
●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●●
●
●
●
●●
●●
●●●
●●
●
●●
●●
●
●
●
●●
●●
●
●
●
●
● ●
●●●
●
●
●
●
●
●●
●
●
●●
●
●
●●●●
●●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●●
●●
●
● ●
●●
●
●●
●
●
●●●
●
●
●
●
●
●
●●●●
●
●●●●
●●
●
●
●●
●
●
●
●
●●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
● ●●●●
●●
●●
●●
●●●
●●●
●●
●
●
●
●
●●●
●●
●
●
●
●●
●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●●●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●●
●
●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●●●●
●
●●
●
●
●
●
●●
●●
●●●●
●●●
●
●●
●●
●
●●
●●
●●●
●●●●
● ●●
●
●
●●●
●
●●
●
●
●●●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●●
●●
●
●
●●
●
●●
●●●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●● ●●●
●
●●
●●
●
●●
●
●●●
●●●
●●
●
●
●
●
●●●
●
●●
●
●
●
●●
●
●
●●●
●
●
●●
●
●
●
●●
●
●
●
●●
●
●
●●●
●●
●●
●●
●
●
●
●
●●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●●
●●
●
●●●
●
●●
●●●●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●●
●
●
●●
●
●● ●
●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●●●
●
●●
●
●●
●●
●●
●●
●
●●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●●●
●●
●●
●
●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●●
●●●●
●●
●
●●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●● ●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●●● ●●●
●●
●●●●
●●
●
●
●●●●● ●●●
●
●
●
●
●
●
●●
●
●
●
●
●●● ●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●●
●
●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●●
●●●
●●
●●
●
●● ●
●
●
●●
●
●
●
●●
●
●●●
●
●●●
●
●●
●●
●
●
●●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●●●●
●
●●●
●
●●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●●●
●
●●
●●
●●●●
●
●●
●
●
●●
●
●
●●●●
●●
●
●●●●
●
●●
●
●
●
●●
●
●
●●
●
●
●●●
●
●
●●
●●●
●
●●
●
●
●
●●
●
●
●●●
●
●●●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●
●
●●
●●
●●
●● ●
●●
●●●
●
●●
●
●
●
●●
●●●
●●
●
●
● ●●●●
●
●
●
●●
●
●
●●
●
●
●
●
●
●● ●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●●
●●
●
●
●
●●●
●
●
●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●●
●●
●
●
●
●
●
●
●●●
●●
●
●●
●
●●
●
●
●●
●
●●
●● ●
●●
●
●
●
●
●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
● ●●●
●●
●
●
●
●●
●
●●●●
●●●●
●
●
●
●●●
●●●
●
● ●
●
●
●●
●●●●
●
●
●●
●
●●
●●●
●●●●
●● ●●
●
●●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
● ●●
●
●●
●
●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●●
●●
●●●
●●
●●
●●
●
●
●
●●
●
●●
●
●
●
●●
●
●●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●
●● ●●
●
●●●●
●
●●●● ● ●
● ●●
● ●
●●●
●
●●
●●
●
●
●●●●
●
●
●
●●
●
●●●
●
●
●
●●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●
●●
●
●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●●●
●●●
●●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●
●●
●
●●●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●●
●
●●
●●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●●
●
● ● ●●
●
●●
●
●●
●●
●
●●
●●●
●
●
● ●●●● ●
●●
●
●
●
●●
●●
●
●●
●●
●
●
●●●
●●
●
●
●●
●●
●
●●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●●●
●
●
●●
●
●●
●
●
●●●
●
●●
●
●
●●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●●●
●
● ●
●
●●●●
●
●
●
●
●
●●
● ●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●●
●●● ● ●
●
●
●
●●●
●
●●
●
●
●●
●●
●●
●
●●
●
●●●●●
●
●
●●●
●●●
●
●●
●
●●
●
●
●
●●
● ●●●●
●
●
●
●●●●
●
●
●
●●●
●
●●
●●
●●●
●
●
●●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●●
●●
●●●
●
●
●
●●
●
●
●●
●
●
●
●●
●●
●
●●
●
●
●●
●
● ●●
●●
●
●
●
●
●
●●●
●●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●●
●●●
●●
●●
●
●●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●●●
●●
●●●●● ● ●●●● ● ● ●●
●●
●
● ●●● ●●●● ●
●
●●
●●● ●●●●● ●● ●●●●●
●●
●●
●●
●●
●
●●●●

0.00

0.25

0.50

0.50

0.75

0.02

0.00

−0.02

●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●●●
●
●●
●●
●
●
●
●
●
●
●
●
●●
●●●
●
●●
●
●●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●●
●
●
●
●
●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●● ●
●●●●●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●
●●
●
●●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●● ●
●
●●
●
●
●●
●●
●●
● ●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●●
●
●
●
●●
●●
●●●
●●●
●
●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●●●●
●
●●
●
●●●●●
●
●●●
●●
●● ●
●●●
●
●
●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●●
●●
●●
●●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●●
●●●
●●
●●
●●●
●●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●●●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●● ●●●
●
●
●
●
●
●
●●
●●
●
●
●●●●●
●
●
●
●●●
●●●
●
●●
● ●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●●●
●
●
●●
●
●
●
●
●
●●●●●
●●
●
●●
●●
●
●●
●
●
●
●●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●● ●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
● ●●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●●●
●●
●
●
●●
●
●●●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●●●●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●●
●
●●
●
●
●
●
●●
●
●
●
●
●●
●
●●●
●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●●
●●
●●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●●
●●
●
●
●
●
●
●
●●
●●
●●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●●
●
●
●●
●
●
●●
●●
●
●
●
●
●●●
●●
●
●
●
●●
●
●
●●
●
●●
●●
●●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●●●●
●
●●●
●●
●●
●
●
●
●●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●●
●
● ●●
●●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●●●●●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●●
●
●●
●●
●●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●●
●●
●
●●
●●
●
●●
●●
●
●●
●●● ●
●
●
●●
●
●
●
●●
●●
●●
●●
●
●●
●
●●●
●●
●● ●
●
●●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●●
●
●●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●●●
●●
●●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●●●●
●●
●
●●●●●● ●●●
●●
●
●●●
●● ● ●
●
●
●
●
●
●
● ●
●
●●●
●
●●●
●●
●●●
●
●●
●
●●●
●
● ●●●●
●
●
●●
●
●
●●
●
●
●●●
●
●
●
●●
●
●
●●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●●
● ●●
●●
●
●●
●
●
●
●
●●
●
●
●
●
●●●
●
●
●
●●
●●
●●
●
●●
●
●
●
●
●
●
●●●
●●●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●● ●
●●●●
●●
●●●●●●
●●
●
●
●
●● ●
●●
●
●
●●●●
●
●●
●
●●
●
●
●
●
●●●
●
●●●●
●
●●
●●
●●
●●●●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●●
●
●●
●●
●
●●
●
●
●
●●
●●
●●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●●●●
●●
●
●
● ●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●
●
●●●●
●
●●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●●●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●●
●
●●
●●●
●
●●
●●
●
●●
●
● ●●
●
●●
●
●●
●●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●● ●
●
●●
●●
●
●●●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●●
●●
●
●●●
●
●●
●
●●●
●●●
●●
●●
●●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●●
●
●●●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●●
●●
●
●●
●●●
●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●●
●●
●
●
●
●
●
●
●●
●●
●
●
●
●
●●●
●
●
●
●●
●
●●●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●●
●
●
●●●
●
●
●
●
●●
●
●●●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●
●
●●●●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●●
●●
●
●●
●
●
●
●●
●
● ●●
●●●
●●●●
●●●●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●●
●
●●
●●
●
●
●
●●
●● ●●
●●●
●
●
●
●
●
●
●
●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●●●
●
●●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●●● ●
●●●●
●●
● ●●
●●●●
●
●●
●
●
●
●
●
●
●●●●
● ● ●●
●●
●
●●
●●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●● ●
●●
●
●
●●
●●
●
●●●●●
●
●
●
●● ●
●●
●
●
●●●
●●
●
●
●●
●
●●●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●●
●●
●●
●●
●
●
●
●
●
●
●●
●
●●
●
●●
●●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●●●
●●
●●
●●
●
●
●
●
●
●●
●
●
●●●
●
●●
●
●●
●●●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●●●●
●●●●
●
●
●
●●●●
●●
●● ●
●
●●●
●●●
●
●●●●
●●●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●●
●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●●
●●
●
●
●
●
●●
●●
●
●
●●●●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●●●
●
●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●● ●
●●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●●
●●
●
●
●
●
●
●
●
●●
●
●●
●●●
●
●
●
●
●
●●
●
●
●
●
●●
●●●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●●
●●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●● ●●●
●●
●
●
●● ● ●
●●●
●
●
●●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●

1.00

−0.02

S1

0.00

0.02

S1

e for four simulated S matrices under the PSD model. The left colFigure 6: A mapping from S to S
umn shows the simulated structure S for each of four scenarios (a–d) and the right column shows the
resulting estimated row basis of S produced from PCA. It can be seen that the scale on which S was
generated, all values in (0,1), is lost in the principal components, values in R.
28

1.00

0.75

0.50

0.25

0.00

S

\tilde{S}

a=0.1

a=0.1

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●
●
●●
●
●●
●
●
●
●●
●
●
●●
●
●
●●
●●●
●
●●
●
●
●
●
●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●● ●
●
●
●●
●
●
●
●
●
●
●
●● ●
●●
●
●
●●
●●
●
●●
●
●●
●
●● ● ●
●
●
●
●
●
●
●● ●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●● ●●● ● ●
●●
●
●
●
●
●
●
● ● ●● ●●
●●
●●
●
●●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●● ●
●
●●
●
●●
●●
● ●
●
●
●
●●
●●●●
●
●
●
●
●
● ●
●
●
●
● ●
● ●
●
●
●
● ●● ●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●● ●
●●
●
●
●●
●
●
●
●
●
●
●
●
●● ● ●
●
●
●
●
●
●●
●●
● ●
●
● ●
●
●
●●●
●●
●●
●● ●
●
●
●●
●
●● ●
●
●
● ●● ● ●
●
●
●
●
● ●●●
●
● ●
●
●
●
●
● ●
●
● ●
●
● ●
●
●● ●
● ●● ●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●●
●●
●
●
● ●
●
●
●
● ●
●
●
●●
●
●
●●
●
●
●
● ● ●●
●
●●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
● ●
●
●
● ● ●●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●● ● ●
●
●
●
●
● ●
●
●
●●
●
●●
●
●●
●●
●
●●●
● ●●
●
●
●●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●
●
●
● ●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ● ●● ●
●
●
●
● ●
●
●
●
●
●
●
●
●●● ●
● ●
●●
●
●
●
● ●
●
● ●●
●
●●
●
●
●
●
●● ● ●● ●
●
●
●
●
●
●
●●●
● ●●
●
●
●
●
●
●●
●
●● ●
●
●
●
●●
●●
● ●●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
● ●
●● ●
●
●
●
● ● ●● ● ●
● ●
●
●●
●
●
●
●●●
●
●
● ●
●
●
●
●
●●
●
●
●
●
●
● ●
●
●
●
●
●
●
● ●
●● ●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●● ● ●●
●●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
● ●
●
●
●
●
●
●●
●● ●●
●●
●
●
● ●
●●● ●
●
●
●
●
●●
●
●
●
●
●
●
●●●
●
●
●
● ● ●●
●
●
●
●
●
●● ●● ●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●●
●
●
●
● ●
●
● ●
●
●●
●●●●
● ●●
●
●
● ●
●
● ●
●
●● ●
●
●●
●
●
●
●
●
●
●
●
● ● ●
●
●
●
●●●
●
●
●
●●
●
●●●●●●
●
●●
●●●
●
● ●● ●
●
●
●●●● ●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●●
●
●
●●●●
●
●
●
●●
●●●●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●●
●
●
●●
●
●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●● ●
●
● ●●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●● ● ●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
● ● ●●●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●●
●
●
●
●
● ●●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
● ●●● ●●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●● ●●
●
●
●
●●
●●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
● ●● ●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●●
●
●
●
●
●
● ●
● ●● ●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●●●● ●
●
●●
●
●
●
●
●
●
●
●
●
● ● ●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●●
●
●
●
●
●
●
●
●
●
●
●
●
●● ● ● ● ● ●● ●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●●
●
● ● ● ● ●●●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●● ●
●
●
●
●
●
●●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
● ●●●●●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●● ● ●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ● ●● ● ●●● ●●●●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
● ●●
●
●●●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●● ●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
● ●●● ●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.02

0.00

−0.02

a=0.25

0.75

0.50

0.25

S2

0.00

a=0.25

●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●●
●
●●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●●●●●
●
●
●
●●
●
●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●●
●
●
● ●●
●●
●
●●
●
●●● ●●●●
●
●●●●
●●
●
●
●
●
●●
●
●
●
●
● ●
●
●
● ●●
●
●
●
● ●●
●
●
●●
●●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
● ●● ● ●● ●
●
●
●●
●
●● ●
●●
●●
●
●
●
● ● ●●●●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●
● ●●
●●●●●● ●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
● ● ●●● ●●●
●
●
●
●
● ●●●● ●
●
●
●
●
●
●
●
●
●●
● ●
● ●● ●
●
●
●
●
●
●
●
●●
● ●● ●
●
●
●
●
●
●●●● ●
●
● ●●●
●
●
●●● ●● ●
●
●● ●●● ●●●●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●●● ●●●● ●●● ●●
●
●
●
●
●●
●
●
●●
●●
● ●●● ●
●
● ●
● ●●
●●●
●
●
●
●
● ●
●
●
●
● ●
●
●
●
● ●●●● ●●●
●
●●
●
●
●● ● ● ●
●
●●
●
●
● ●
●
●
●
●●● ●● ●
●
●
●
●
●
●
● ●●●●
●
●● ●●● ● ●●●
●● ● ● ● ● ●
●●
●
●●
●
●
●●
●
●
● ●●
●
●
●
●●
● ●
●
●
●●●
●
●● ●●●●●
●●●● ● ●
● ●
●
●
●
●●●
●● ●
● ●● ●●
●
●
●
●● ●
●● ●
●
● ●●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●● ● ●●
●●
●
●●●
●
● ●● ●
●●●●
● ●●●
●●
●
●
●
● ●●
●●
●●●
●
●
●●●
●
●
●
●
●
●
● ● ●● ●
●●●
●●●
●
● ● ●
●
●
● ● ●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●
●● ●
●●
●● ● ●
●
●
●
●● ●
●
● ● ●● ●
●
● ●●●●
● ●
●
●
●
● ●●
●●●
●
●● ●
●● ● ● ● ● ●
●
●
●
●● ●
●●● ●● ● ● ●
● ●
●
●●●
●
●●●
●
●
●●
●●
●● ●
●●●●●●
●
● ● ●
●●
●
●
●
● ● ●
●
●
●
●●
●●
●
●●●●
●
●
●
●●
●
●
●
●
●
● ● ●● ● ●● ● ● ●
●●
●●
●
●
●●
● ●●
●
●
●
●
●
●
●
●
●●● ●
● ●
●●
●
●
● ●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ● ●
●
● ●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
● ● ●●●
●
●
● ●
●
●●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
● ●
●
●
●
●
●●
●●
●
●
●●●● ● ●● ● ●●●
●●
●
●
●
●
●
●
●
●
●
●●● ● ● ●●
● ●●
●
●
● ● ● ●
●
●
●●●
● ●●
●
●
●
●● ●● ● ●
●
●
●
●
●
●●
●●
● ● ●
● ●
● ● ●
●
● ● ● ●●
●●●
●●
●● ● ●● ●
●
●
●
●
●●
●
●
●●
● ●
●
●
●
●
●● ● ●● ●●
●
● ●●
●
● ●●
●●
● ● ●
●●
●
●●●● ●
●
●● ●
● ● ●●
●
●
●
● ● ● ●● ●
●
●
● ●
●
●●
●
●●
●
●
●● ●●
●
●
●●●
●
●
● ●
● ●● ●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
● ● ●
●
●
●
●●
●
●
●●● ●
●
●
● ● ● ●●
●● ●
● ●
●
● ●
●
●
● ●● ●
●●● ● ●
● ●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●● ● ●● ●
● ● ● ● ● ● ●●
●
●
●
●
●
●
●●
●
● ●
●
●
●
●
●
●●●●
●
●●
●
●
●
●●
●
●
●● ●●
●
●
●
●
●
●
●●
●
●
●
●●●
●
●
●●
●
●
●● ●● ●●
●
●
●
●
●●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●● ●
●
●
●
●● ●●
●
●
●
● ●● ●
●
●●
●●
●●
● ● ● ●
●
●
●
●
● ●●● ●● ● ●● ●
●
●
●●
●
●●
●
●
●
●
●
●
● ●●
● ● ●●
●
●
●●
● ●
●●
●
●
● ● ●●●
●
●
●
●
●●●●● ●●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●● ●
●
● ●● ●
● ●● ●
●
●
●
●
● ● ● ● ●
● ●
●●
●
●
●
●
●
●
●
● ● ●●
●
●
● ●●
●● ●
●
●●
●
● ●
●
● ● ●
●
●
●●
● ● ● ●
● ●
●
●
●
●
●
●
●
●● ●
●
● ●● ●
●
●
●
●
●●● ●
● ● ● ● ● ● ●●
●
●
●
●
●●
●
●
● ●● ●
●●
●● ●
●●
● ●● ●●
●
●
●
●●
●●
●
●
●● ● ● ●●
●●● ●●●
●● ●●
●
● ●●●
●
●
●●●
●
● ●
●
● ● ●●
●
●●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●● ●●● ●●● ●●●● ● ●
●
●●●
●●
●●●
●
●
●●●
● ● ● ●● ●
●●●
●
● ●
●
●
●
●
●
●
●●
●
●
●●
● ●● ●
●●
●
●● ● ● ● ●
●
●
●
●
●●
●
●●●
●
●
●●
●
●
● ● ●●
●
●
●
●
●●●●
●
● ●●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●●
●
●
●
●●
●●
● ●
●●●●●
●●
●●
●
●
●
●
● ●
●
●
●
●
●
●
●
● ● ●
●
●● ● ●● ●●●●
●
●
●●
● ●●● ● ●
● ●●
●●●
●
● ●●
●
●
●●
●
●
●
●
● ●
●
●●
●
● ● ●●
●
●
●
●
●
●●
●
●
●
●● ●
●
●
●
●
●
●
●●
●● ●●
● ●●
●●
●
●
●
●
●●● ● ●
●
●
●● ● ● ● ●●
●
●● ● ●● ●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●● ● ●● ● ● ● ●●● ●
●
●
●
●
●●●
●●
●● ●●
●●
●
●
●
●●●●●
● ●
●
● ●●● ● ●
● ●
●
●
●
●
●
●●
●
●
●
●
●●
●
●●● ● ●
●
●
●
●●
●
●
●●●●
●
●
●
● ●●●●●●●● ● ● ●●
●
●●
●●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●
●●
●
●
●
●
●
● ●●
●
●●
●
●●
●
●
●●● ●●
●
●
●●●
●
●
● ●● ●●
●
●
●
●
●●
●
●
●
● ●
●● ● ●●
●●● ●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●● ●
●●●
●
●
●
●●●
●
●●
●
●
●●
●
●
● ●
● ●●
●
● ●●●●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●● ●●●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
● ●●
●
●
●
●●
●
●
●●
●
●●
●●
●
●
●●●●●
●
●
●●
●●●●●● ●●●●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
● ●●●●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●● ● ●● ●●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
● ●●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●●●
●
●
●
●●
●
●
●
●●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

a=0.5
1.00

0.75

0.50

0.25

0.00
1.00

0.75

0.50

0.25

0.00

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
● ●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●● ●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●
●
●
●
●●
●
●
●
●
●
●
● ● ●●
●
●
●
●
●●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
● ●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●● ●
●
●●
●
●
●● ●● ●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●
●
●
● ●●
●
●
●
●
●
●
● ●●
● ●● ●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●●
●
●
● ●●●
●●●
●
●
●
●● ●
●
●
●●●
●
●
●
●●
●●
●●●
●
●
●
●
●●● ●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●●●
● ● ●●●
●
●
●
●
● ● ●
●
●
●
●
●
●
●●
●
●●● ●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●●
●●
● ●●●
●
●● ●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●●
●●● ●●●
●
●●
● ●●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●●
●
● ●● ● ● ●● ●●
●● ●
●
●
●
●●●
●
●●
●
●
●
●
●●
●
●
●
●● ●●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●
●
●
●
● ● ● ● ●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
● ●●● ●●●
●
●
●
●
●
●●
●●●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●●●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●● ●●●
●
● ●●
●
●
●
●
●●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●● ●●●● ● ● ● ●
●
●●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
● ● ● ●●●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●●●
●●
● ●
●
●
● ●●●●
●
● ●●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●
●●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
● ●● ●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●●●●●
●●●
●● ●
●
●●
●
●
●
●
●
●●
●
● ●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●●●
●
●
●
●
●
●
● ●●●●●
● ●
●
● ●●
●
●●●
●
●● ●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●● ●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●●●●●
●
●● ● ●● ● ●
●
●
●
●
●
●
●●
●
●●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●●●
●
● ●
●
● ●●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●●
●
●
●
●
●
●
●
●
● ●●●
●
● ● ●●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●● ●●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●●●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
● ●●●
●
●
●
●●
●● ●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●● ●●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●●●●● ●●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●●●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.02

0.00

−0.02

S2

1.00

●
●
●
●
●●
●●
●
●●
●●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●●
●
●●
●
●● ●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●
●●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●●●
●●
●
●
●●
●
●
●●●
●
●● ●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●●
●●●
●
●
●
●●
●
●
●●
●
●
●
●●●
●
●●
●
●●●
●
●●●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●●
●
●
●
●
●●●●
●●
●●●●●●● ● ●●●●●
●
●
●●
●
●●
●
●
●
●
●
●●●
●●●
●
●
●●●
●
●●
●
●
●●●
●●
●
●
●
●
●
●
●
●
● ●●
●●
●●
●
●●●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●● ●
●●
●
●
●
●●
●
●
●
● ●
●
●
●●
●
●● ●
● ●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●● ● ●●●
●
●●
●
● ●●
● ●
●
●
●
●
●●
●●●
●
●●
●
●
● ●● ●●
●
●● ●
●
●●●●
●●
●●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●●
●
● ●● ●●● ●
●●
●
●
●●
●
●
●●
●
●
●
●●
●●●
●●
● ●●
●
●
●
●●
●●
●●●●●●
●●
● ●
●
●
●
●
●●●
●●
●●
●●● ●
●
●
●
●
●
●●● ● ●● ● ●●
●
●
●
●
●
●●
●
●
●
●
● ●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●● ●
●
●
●●●●
●
●
●
●
●
●● ● ●
●
●
●
● ●●
● ●● ●
●
●
●●
●
●
●
●
●
●
●
●
●●● ● ● ●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●●
●
●
●●●
●
●●● ● ●
●
●
●●
●
●
●●
● ●● ● ●●●●●
●●
●
●●
●
●
●
●
●●●●●
●
●
● ●●
●●● ●●●● ●
●● ●
●● ● ●●●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ● ●
●●
●
● ●
●●
●
●
●
●●
●
●
●
●
●
●●
●●
●
●●
●●●● ●●●● ●
●●
●
●●●●● ●
●●● ●●
●
●
●
●
●
●
● ●● ●
●
● ● ●●
●
●●
●
●
●●
● ●●
●
●
●
●
●
●●●
● ●●●●
●
●● ●●
●●●
● ●●
● ●●
●
●●
●
●●● ●
● ● ●● ●
●●●
●
●● ●
●
●
●●
● ●
●● ●●●
●
●
●●
●●●●
●
● ●
●
●
●●
●●●
●●
●●
●
●
● ●●●● ● ●●●
●●
●
●
●●●●●
●●●
●
●●● ●
●● ●●●
●
●
●●●●
●● ●●
●
●
●
● ● ●
●
●● ●● ●● ● ●
●
● ●
● ●
●
● ●●
●
●
●●●●●●
●●
●●
●
●
●
●●
●●●●●●
● ●●●●
●●
●
●
●●
●
●●●
●●
●● ●
●
● ●●●●● ●●
●●
●●●
●●● ●●
●
●
●
●
●
●
●
● ●●
●● ● ● ●
●●
●●
●
●
●●
●●●● ● ●● ●
●
●●
●
●
●●●
●●
●●
●● ●
● ●
●
● ●●
●
● ●●
●
●●●
●
●
●
●
●
●
●● ●●●● ●●●● ●●● ●●
●
●●
●
●
●
●●● ● ● ●
●
●
●
● ●● ● ●
●
●●
● ●●●
●
●
● ●●●
●
●
●●
●
●●
●
●●
●
●
●
●●
● ●
●
● ●●● ●
●
● ●● ●●●
●●
●
●
●
●●●
●●● ●
●
●
●●
●●●●● ●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
● ● ● ● ●
● ●
●
●●
●● ●●
● ● ●●● ● ● ● ●●●●●
●
●●
●●●●
●
●
●
●
●
●
●
●
●●
● ● ●
●
●● ●●
●
● ●●● ●●●
●●
●
●
●● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ● ● ● ●●
●●
●●
●● ● ● ●●● ●●●
●
●● ●
● ●● ●
●
●
● ●
●● ●
●
●
●
●● ● ●
●
●●
●●
●
●●● ●
●
● ●
●
●
●●● ●
●● ● ● ● ●●● ●
●●● ●●●
●●
●
●●●●
●
● ●
● ●
●
●
●
● ● ●●●
●● ●●● ●●●●●
●
●●
●
●●●●
●
● ●●
●
●
●
● ●
●● ●●
●
●
●
●
●
●
●
●●
●●●
● ●●●●●●
●
●● ●
●●●● ●●
●
● ●
●● ● ● ●
●●●●
●
●
●●● ● ● ●
●●
●
●●
●
● ● ●●
●●
● ●●
●● ● ●●
●
●
●● ●
●
●
●●●
●● ● ● ●●
●
● ● ●● ●
●
● ●● ●
●
●● ● ● ● ● ●
● ● ● ●
●
●
●
●● ●●
●
●
●● ●
●● ●
● ● ●● ●●●●●●
●●
● ●●
●
●
● ●
●
●
●● ●
●
●
●● ●
●
●
●
●●●
●●
●● ●
● ●
●
●●
●
●
●●
●●●●● ●●●●●●
●
●
● ●●
●
● ●
●
● ●●
● ●●●
● ●● ●●
●
●
●
●
●
●
●
●● ●●●●●
●
●
●●● ●●●● ●
●
●●●
●● ●
●●●● ● ●
●
●● ●●●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●
●
●
●
●●
●
●
●
●
●●●
●
●
●●
●
●●●
●
●
●
●● ●
●●●
●●
●●●●
●
●●
●●● ●
● ● ●
●
●● ●● ●
● ●●● ●
●●●
●
●
●●●
● ●●
●●●●●
●●
● ●● ● ●
●
●● ●
●●
●
●●
●
● ●● ●
● ●
● ●●
●●
●
●
●
●●
● ●● ●
●
● ●
● ●● ●●
●●●
●●●
● ●●●● ● ●● ●
●
●
● ●●●
●
●
●● ● ●
●●
●
●●
●
●
●
●
●● ●● ●● ●
●
●● ●
●
●
●●●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
● ●
●
● ● ●● ●
●●● ● ● ●● ●
●
●●
●●●● ● ●
●
● ●● ●
● ●● ●
●
●
●●● ●●● ● ● ●● ●
● ●●●
●
●● ●
●
●
●●
●
●
●
●
●
● ●●
●
●
●
●
●
●● ●● ●
●● ●
● ●●
● ●●
●
●
●●
●●
●
●● ●●
●
●
●●
● ●●●
●
● ●● ●
●●
●
●
●
●
●
●
●
●●
● ● ●●
●
●●
●●
●●●●
●●
●●
●
●
●
●●●
● ● ● ●●
● ● ●●
●
●
●
●
●●
●
●●●●●
●
●● ●●
● ●
● ●
●● ●●●●●● ●
●● ●●
●
●
●●● ●●
●●
●
●●
●
●●● ●
●●● ●
●
●● ● ●
●●●
● ●
●
●
●● ●● ●●● ●● ● ●
● ●● ●
● ●●●●●
●
●●
●
●●●●●●●
●
●●
●
●
●●●●
●
●●●●
● ●●
● ●●● ●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●
● ● ●
●●
● ●● ●
●
● ● ●●● ●●
●●●● ● ● ●
● ● ● ●●
●●
●
●
●
●
● ●●
●●●●●●
●
●
●●
● ●● ●●
● ●●●●● ●
●
● ● ●●
●
●
●● ●
●●
●
● ●●●●
●●
●●
●
●
● ●● ●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●●
●● ●
●
●
● ● ● ● ● ●● ● ● ●
●●
●
●
●●● ● ●●
●●
●
● ● ●●
● ●● ●●
●●
●
●
●
●
●●●
●
●
●
●
● ● ●●
●● ●●
● ●●
●
●
●
●● ●
●
●
●● ●● ●● ●
● ●
●●
● ●●
●
●
●●
●●
● ●●
●●●●
●
●● ●
●●
●
●
● ●
●
●●
● ● ● ● ●● ●
●● ●
●
●
●
●● ●
●
●
●
● ●
●
●● ● ●
●●
●
●
●
●●
●
●●
● ● ● ●
●
●
●●
●●
●●●
●
●
●
●●
●
●●● ● ●
●
● ●●●●●
●
●●
● ● ●●●
●●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●●
● ●
●
●
●
●
● ●●●
● ●●●
●●
●●● ●● ●
●●● ● ●●
●● ●●
●
●
●
●
●●●
●
●●
●
●● ●
●
●●
●
●
●
●●
●
●
●●●
●●
● ●● ● ●
●
●●● ●●●
●
●
●
●●
●●
●
● ●●
●● ●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●●●
●
● ●●● ●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
● ●●●
●
● ● ● ●●●●
● ●●●
●
●
●●
● ● ●●● ● ● ● ● ●●●●
●●
●
●
●● ●●
●
●
●●
●●
●
●
●
●● ● ●
●
● ●●●●●
●
●
●
●
●
●
●
●
●
●●
●
●● ●●
●
●●●●●
●
●
●●● ● ● ●
●
●
●
●● ●● ●● ● ●
● ● ●●● ●
●
●
●
●● ●●
●●
●
●
●
●●
●●
●●
●
●
●
●
●
●
●
●
●●● ●●●● ●
●
●
●
●
●
● ●●
● ●●●
●●
●● ● ●
●
●●●●●● ● ●
●
●
●
●
●
●
●
●
●● ● ● ● ●
●●●● ● ●●●●●
●●
●●● ●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●● ●
●
● ● ●● ●
● ●●
●
●● ●● ● ●● ●
●
● ●●
●
●●●
●
●
●
●
● ●●●
●
●●
●●
●●●
●
●●●●
●
●●
●
●●●
●●● ● ●● ●
●
● ●●●
●●● ●● ●
●●
●
● ● ●● ●
●●●
●
●
●●●
●
●
●
●
●
●●●●
●
● ●● ●●●●●●
●
●
●
●
●
●●●●
●
●●
●●
●
●
●
●●
●
●
●
● ●
● ●●● ●●
● ●●
●
●
●
●
●
●
●●●
●
● ●
●
●
●● ●
●●
●● ●
●
●
●
●
●
●
●
●
●●
●●●
●●●
●
●●● ●●
● ●
●
●
● ● ●●●
●
●
●● ●●●●
●
● ●●●●
●●●
●●● ●
●●
●●
●
●
●
●●
●
●●
●
●
●●
●
●●●
●
●
●●● ●
●
●●● ●● ● ●●●●●
●
●●
●●
●
●● ●●●●● ●
●●
●●
●
●
●
●
●
●
●
●●
●
●● ●
●
●
●
●●
●●
●
●●●●
●
●
●●
●
●
●
●●
●
●
●●
●
●●●
●
●●
●
●
●● ●● ● ●
●●
●
●
●
●
●
●●●
●
●
●●
● ●●●●
●
●
●
●●●●
●●
●●
● ●●● ●
●
●
● ●●●
●●
●
●
●●●
●
●
●
●
●●
●
●●
●
●●●
●
●
●
●●
●
●
●●●
●
●
● ●●
●
●●●●●
●
●
●
●
●●
● ●●
●
●
●
● ●
●
●
●●
●●
●
●
●●
●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●● ●
●
●●
●●●
●
●●
●
●●●
●
●● ●●●●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●
●
●
●
●
●
●
●●●
●
●
●
●●●●
●
●●●●
●
●●
●●
●●
●
●
●
●
●●
●●●
●
●●
●
●
●
●
●
●●
●
●
●●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●●● ●
●
●●●
●●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●●
●●
●●
●
●●●
●
●
●
●
●●●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ● ●●●● ●
● ● ● ●●●
●
● ●
●●●
●
●●
●
● ●●
●● ● ●
● ●●●
●●
●●●●● ●●
●●● ● ●
●●
●
●

a=0.5

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●
●
●
●
●
●●
●●
●●
●●
●
●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●●
●
●
●
●
●●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
● ●●●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●●
●
●
●
●●●●● ●
●
●
●
●
●●
●●
●
●
●
●●●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●●
●
●
●
●
●
●
● ●
●
●
●●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●●●●●●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●●
●●
●
●
●
● ●●
●
●
●
●● ●●
●●
●
●●
●
●
●
●●
●● ●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●●
●●
● ●● ●
●
●●● ● ●●
●●
●●
●●
●●●
●●
●●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●●●
●
●
●●●
●
●●●
●●
●●
●
●
●
●
●
●● ●
●
●●●
●
●
●
●
●
●●
●●
●●●
●
●●
●
●●●
●●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●●
●
●
●●●
●
●
●
●
●●●
●●●
● ●●
●●
●
● ● ●●
●
●
●
●●●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●●
●
●
●
●
●●
●
●
●●
●
●
●
●●● ●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●●
●
●
●
●
●●●●●●
●● ●●●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●● ●●● ●
●●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●● ●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
● ●●
●
●
●●●●
●●
●
●●
●
●
●
●
●
●
●
●
●●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ● ● ● ● ●● ● ● ● ●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
● ●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●●
●●●●
●●
●●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●●
●
● ●●
●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●● ● ●●
●●
●●
●●
●●
●
●
●
●
●●
●
●
●
●●
●● ●● ●●
●
●●●
●
●
●
●
●
●
●●●●
●
●
●●
●● ●
●
●●●
●
●
●
●
●
●●●
●
●
●
●●●●●
●
●
●●●
●●
●
●
● ●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●
● ●● ●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●● ● ●
●●
●●●
●
●
●
●●
●●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●●● ●●
●●
●●● ●● ●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●●
●
●●●
●●
●
●
●
●● ●●
●●
●
●
●●
●
●●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●●●● ●
●
●●
●●
●●● ●
●
●
●
●●●
●
●
●●
● ● ● ● ●●
●
●
●
●●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●●●● ●●
●
●
●
●
●
●●
●
●
●
●
●●●●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
● ●
●
●
●●
●
●●
●
●
●●
●●●●
●●●
●●
●
●●
●
●
●
●●
●
●
●
●●●● ●●●● ●
●
●
●
●●
●
●
●●
● ●●
●
●
●●●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●●● ●●
●
●●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●●●●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●●
●●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●●●●
●
●● ●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●●●●●●
●
●●
●●
●
●●
●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●●
●●
●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●●●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.02

0.00

−0.02

a=1

a=1

●●
●●
●●●●●
●●
●
●●
●
●●
●
●● ●
●●
● ●
●●● ●
●
●
●
●● ●
●
●
●●●● ● ● ●●
●●
●
●
●●
●
●●● ●●
●
● ●
●
●● ●
●●●
●●● ●
●●●
●
●
●
●
●
●●
●
●●
●
●●
● ●
●
●●
●
●
●
●
● ●●●
●●
●●●
●
●
●●
●
●● ●
●
●
●●
●
●● ●
●●
● ●●
●
●
●
●●
●
● ●●●●
● ●●
●
●
●
●
●
●
● ●
●
●●●●
●
●
●●
●
●
●
●
●
●●●
● ●●
● ●
● ●●
●
● ●●
●
●
●
●●●
●● ● ●
●
●●●
●
●
●●●
●●●●●●
●●●
●●●
●● ●● ●
●●●
●● ●●
●
●●●
●●
● ●
●
●● ●
●
●
●
●
● ●
●
●●●
●●
● ●●●●●●●
● ●●● ●
●
●●●
●●●
●
●●
●●
●
● ●
●●●
●●
●
●● ● ●● ●●
● ●●
●●
●
●●
●●
●● ● ●
●●
●● ●
●
●
●●
●●●●
●●
●●●●●
●
● ●● ●
●●
●
● ● ●●
●
●
●
●●●●
●
●●●
● ●
●●
● ●
●● ●
●●
● ●●
●
● ●● ●●
●●
● ●●
●●●
●●
●
●●●
●
●
●●
●●
● ●
●●
●
●
●● ●●
●
●●●●
●●
●
● ●
●●
● ●
●●● ●
●●
●
●
●
● ●
●
●●●
●●
●
●
●●
●
● ●●●
● ●●
●
●
●
● ●●
●●●
●
●
●● ●
●
●
●● ● ●●
●●
●
● ●●●
●●
●●●
●●●
●● ●
●
●
●●
●
●
●
●
●●
●●
●
●●
●● ●
●
●●● ●●
●● ●
●
●●
●
●●
●
●● ●
●●● ●
● ●
●
●
●● ●●
●●●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●●●● ●●●
●● ●
● ●●●●● ●●
● ●●●●
●
●●
●
●●
●●●
●● ●●●● ● ●●●●●●●
●
●
●
●
● ●●
●
●
●
●● ●●●●
●
●●
● ●●
●
●●●
●
●
●●●●●
●
● ●●
●
●
●
●
●
●
●
●
●●
●
●●
●●● ●● ●● ●
●
●●
●● ●●●
●●●●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●
●●
●
● ●
●● ●●
●
● ●● ●
● ●●● ●
●●
●●
●●
●●●●
● ●●
●
●
●
●●
●● ●
●
●
●●
● ●
●●
●●●●
●
●
●
●
●●
●●
●
●
●●
●
● ●●●●●
●
●
●●
●
●
●
● ●●
●
●●●
●●
● ●●
●
●
●
●●
●
●●
●
●● ●
●
●●●●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
● ●●
● ● ●
● ●●
●
●
●
●●●●●●● ● ●
●●● ● ●
● ●
●
● ●
● ●●●●●
●●
●●
● ●
●
● ●
●
●
●
●●
● ●●
●●●●●
●
●
●●
●
●
●
●●
● ● ●●
●
●●
●●
●
●
● ●●●
●
●●
●● ●
●● ● ● ●
●
●
●●
●●●
●
●● ● ●
●●●●
● ●● ●●●●
● ●
●● ●
●●
●
●●●●
● ●●
●●●●
●●
●
●
●
●
●●
● ● ●●
●
●●●●
●
●●
●
●●
●
●
●●●● ●● ● ● ●●
●● ● ●
●●●●
●●●
●●
●●● ● ●
●
●
●●
●●●
●
●
●●
●
●
●●●
● ●●
●● ●
●
●● ●
●●
● ●●●
●
●
●
● ●
●●●●●●●●●●
●● ●
●
●●●●
● ●
●
● ● ● ●
●●●●
●●
●
● ●
●
●●
● ●●●
● ●
●● ●
●●
●
●●●●
●●
●
●●●●●
●●
●●
●● ● ●
●
●●●●●
●●
●● ●
● ●
●
●●●
●
●●●
●●
●
●
● ●
● ●●
● ● ● ●●●
●
●●●●●●●
●
●
● ●●
●
●
●
●
●
●
●●●
●●
●
●●●
●●
●● ●●
●●
●
●
●●
●●
●●●
●●
●●
●● ● ●
● ●
●●
●●●● ●●
●●●
●
●● ●●
●
●
●
●
●
●
●●●
●●
●●●
● ●●●●●
●
●●
●●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●
●●●
●
●●●●
●
● ●
●
●
●
●●●
●●●●
●●●
●●
●
●●
●●●
●●
● ●
●
●●
●
●●
●
●●
●
●●
●●
●
●●
●
●●●●
●
●
●
●
●
●●● ●
●●
●
●●
●●
●
●
●●●●
●
●●
●
●●
●
●
●
● ●●
●●
●
●
●●●
●
●●●
●
● ●
●
●●
●
●●
●●
●● ●
●
● ●
●
●●
●●●● ●●
●
●●
●
●●● ●
●
●
●
●●●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●
●● ● ●
●●●●● ●
●●
● ●
●
●● ●
●
●
●
●● ●● ●● ●●
●
●
●●
● ●
●●
●● ●
●
●
●●●●●
●
●
●
● ●
●
●● ●●
● ● ●●●●
●
●
●
●
●●●
●●
● ●
● ●
●
●●●
●
●●●●●
●● ●
●●
●●●●
●
●●
●
●
●●
● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●● ● ● ●
●● ● ●
●
● ●●●
●●
●● ● ●
●
●●
●
●
●
●
●
●
●●
●●●● ●●
●●●●● ●●●
●
●
●
●
●
●
●●
●● ●●● ● ●
●
●●●●●●
●● ●●
●●
●●
●
● ● ● ●●●
●●
●
●
● ●
● ●
●
●
●●
●●
● ●
●
●
●
● ● ●●●
●
●●●●●●
●
● ●●●●
●● ●
●
●●
●
●●
● ● ●●
● ●● ●●● ●
●
●●
● ●●●●●
●●
● ●●●
●
●●
●●
●●
●● ●●●
●
●●●●●
●
●
●
●●
●
●
●●
●
●●
●● ●●
●
●●●● ●●
● ●●●●
●
●●
●
●●
●
●●
●●●●
●●
● ●
●
●
●●●●●●●
●●
●●
●●
●
●
●●
●●
●
●
●
●●●
●
●●●
●
●●
●
●
●
●●
●
●
●
●
●● ●
●●
●●●
●
●
●
●
●
●
●
●
●●●●● ●●
●
●
●
●●●
●●
●
●
●●
●●●
●
●
●●
●●●
●●
●●
●
●
●
●●●
●
●
●●
●●
●●●
●●
●
●
●●
● ●● ●● ● ●
●●●●
●●
●
●●●
●●
●●
●
●●●●
●
●●
●
●
●
●●
● ●
●●
●
● ●
●
●●●●
●●●
●●
●
●●●
●
●●●●●●
●●●●
●●
●
●
●
●●
●● ● ●
●
●
●
●
●
●
●
●
●
●
●
●●●● ●●
● ●
●
●
●●
●●
●●●
●
●●●● ●●
●●
●
●●
●●
●●
●
●●●●●
●● ●
●●●●
●●
●●●
●● ●
● ● ●●
● ●
●
●
●●
●
●●
●● ●
●
●●
● ●
●
●●●
●
●●●
●●
●
●●
●●●● ●
●●
● ●
●●
● ●●
●
●
●
●●●
●
●
●
●●
●●●●
● ●
●●●
●
●●●●●
●
●
●● ●●
●
●●●
●
●●●●
●
●
●
●
●
●
●●
●●● ●
●
●● ●
●
●●
●●
●●●
● ●●
●
●
●
●
●●●
●
●
●● ●
●
●
●
●
●●
●
●●●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●
● ●● ●●
● ●
● ● ●
● ●●
●●
●●
●●
●●
●●
●
●● ●
●●● ●
●
●● ●
●●
●●
●
● ●
●
●
●
● ●●
●
●●
●
●● ●
●
● ●●●
●
●●●
●
●●
●●
●●
● ●●
●● ●●
●
●●
●●●●●
● ●● ●●
●
●
● ●
●
●
●
●●●● ●●●
●
●●
●
● ● ●●
●
●
●●●●●●
●●●●
●●
●●
●●
●
●
●
●
●●●●●
●
●
●●
●
●
●
● ●●
●
●● ●●
●
●●
●
●● ●
●
●●
● ●●
●
●●
●
●
●●
●●
●
●●
●
●
●● ● ●●●
●
●●
●●
●●●
●●●
●●●●●
●
●
●●
●
● ●
●● ●
●●●●
●
●
●
●● ●
●
●
●● ●
●●●
●
●
●●
●
●
●●
●
●●
●
●
● ● ●●
●● ●
●●●
● ●●
●●
●●●●
●●
●
●
●
●●
●●
● ●
● ●●●
●●●●
● ●●
●●●
●●
● ●
●●
●
●●● ●
●
●● ●
●
●●
●
●
●●
●●
●●●
●●
●
●
●●●
●●● ●
●●●
●● ●●
●
●
●●
●●
● ● ●●
●
●●
●
●
●●
●● ●
●
●●●●● ●
●●●
●●● ●
●
●●
●
●●
●
●
●●●
●●
●●
●●●
●
●
●
●●
●
●
●● ●●
●●
●
●
●●
●●
●
●
●●
●
● ●●
● ●●● ●●
●●●
●
●
● ●●●
●
●●
● ●●
●●
●●●●
●●
●●● ●● ●●
●
●
●
●
●
●●
● ●●
●●
●●
●
●
●●●●
●●●
●
●●
●●●●
●●●
●
●●●●
●
●●●
●●●
● ●●
●
●●●●●
●●
● ●
●●● ●●●
●
●
● ●
●●
●●●
● ●
●●
●●
● ●
●●
●
●●
●
●
●
●
●
●
●
● ●●●●
●●
● ●●●●●●●● ● ●
●●
●●
●●●●●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●● ●
●
●●●
●
●●●●
●●
●
●●
●
●● ● ●
●●
●
●●
●
●
●
●
●●
●●● ●
●
●
●
●●
●
●●
●● ● ●
●
●
●
●
●
●●
●●●●
●
●
●●●●●
●
●●●
●●
●
●●
●
●●●
●●
●
●●
●●
● ●
●
● ● ●●
●
●
●●
●
●●
●
●●
●
●●●●
●
●
●
●
●●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●●●●
●●
●●
●
●●● ●● ● ●
●
●● ● ●
●●●
●●
●●●
●●●●
● ●
●
● ●
●
●●●
●●
●
●
●●●
●●
●
●
●
●
● ●●●
●
●
●● ●
● ●●●●
●
●●●●●
●
●●●
●● ● ●
●●●●
●●●
●●
●●●●
●
●●
●
●●
●
●●
●
●
●●
●
●
●●
●●●
●
●
●
●
●
●●●
● ●● ●
●● ●●●●
●●
●
●
●
●
●● ●
●
●●●
●●
●
●
●
●●
●
●
●●● ●
●
●
●
●
●●
●
●●●●●●
●●
●
● ●●●
●
●●
●●
●
● ●● ●
●● ●●● ●
●
●
●●
● ●
●
●
●
● ●●
●
●
●
●●
●
●●●
●●
●●
●
●
●
●●●
●● ●●
●●●
● ●●
●●● ●
●
●
●
●●
● ●
●
●
●●
●
●
●●
●●
●
● ●●●
●
●●
●●
●
●
●
●●●
●●● ● ●●●●●●● ●
●
●
●● ●
●
●●
●
●
●
●●● ●
●
●
●●
●●●●●
●
●
●
●
●●
●●
●●●
●
● ●●
●●
●
●●
●
● ●●●
●
● ● ●
● ●●●●
●
●
●●●
●●●
●
●●
●
●●
●●●
●
●
●
●
● ●●●●● ●
●
●
●●●
●●●
●●● ●● ●●
●●
●●●
●●
●
●●●
●● ●
●●
●
●
●
●● ●●●
●
● ●●●
●
● ●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●●
●
●
●
●
●
●
●
●
●
●
●
●● ●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
● ●
● ● ●● ●● ●
● ●●●
● ● ● ●●
● ●●●● ●●
● ●●
●●●●●●
●
● ●●
●●●
●
●
● ●
●●●● ●●●●
●
●●●●●● ●
●● ●●●
● ●●● ● ●
●
●
●● ●
●
●
●
●
●●● ●
●
●● ●
●●
●
●●
●●
●●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●●
●
●
●
●
●● ●
●
●
● ●●●
●●
●● ●
●
● ●
●●
●
●● ● ●
●
●●● ●
●●●
●●
●
●●● ●
●●
●
● ●●
●● ● ●
●
● ●●
●
●●●
●
●
●
●●
●
●● ● ●●
●●●
●●●● ●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●●●●●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
● ●●
●
● ●●● ●●
● ● ●●●
●
●●
● ●● ●●●●● ●
●●●●●
●
●●●●
●
●●●
●●
●●●●● ●
●● ●
●●●●●
●●●●
●
●
●
●●
●
●●
●●●
●●●●
●●●●
●
●
●
●
●● ●
●●
●
●●●●●
●●
● ● ●
●●
●●
●●
● ●
●●
●●
●
●●●●
●●
●●●●●
●●
●●● ●
●
●
● ●●●
●
●●
●
●● ●●
●
●●
●●
●
●●●
●●●●●●
●●●
●
●
●●
●●●●
●
●
●
●● ●●
●●
●
●●●
●
●●
●●●
●●
●●
●● ●
●
●
●●
●●
●
●●●●
●●
●
●●
●
●● ●
●●
●
●
●
●●●
● ●●●
●
●●
● ● ●●●●
●●
● ●●
●
●● ●●
●
●
●●●
●
●
●●
●● ●
●●
●
●
●●
●
●
●●●
●●●
●
●
●●●
●
●●
●
●●
●
●
●
●●
●
●
●
●●●
●●● ●
●
●●●●● ●
●
●
●●
●●● ●
●●●
●
●
● ●●
●
●
●
● ●
●
●
●●●●
●
●●
●●
●
●● ●
●●
●
●●●
●● ●
●
●● ●
● ● ●● ●●
●
●
● ● ●● ●●
● ●●● ●●●
●
●●
●●●
●
●●●●
●●
●
●●
●●
●●
●●●
●●●
●
●
●
●
●
●●●
●●●
●
●● ● ●
●●●
●
●●●
●●● ●
●● ●
●
●● ● ● ●
● ●
● ●●
●
●
●
●
●●●●
●●●●●
● ●●
●●
●
●●
●
●
●●●●● ●●
● ●●
●●
●
●●
●●
●
● ●
●
●
● ●●
●●
●
●●
●
●
● ●
●●
●
●● ● ●●●
●●
●

●
●
●●
●
●
●●●
●●
●●●●
●
●●
●●
●●
●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●●
●
●● ●
●
●
●●
●
●
●
●
●
●●
●
●
●
● ●
●●
●
●
●●
●●
●●●
●
●●
●●
●●
●
●
●
●
●
●
●
●
●●●
●
●●
●●
●
●
●
●
●●●●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●●●
●
●
●●
●
●
●
●
●●
●
●
●
●●
●
●●
●
●
●●●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●●●
●●
●●
●
●
●
●
●
●●
●
●●
● ●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●●
●●
●●
●
●
●●
●
●
●●
●
●●●
●
●
●
●
●
●●
●
●
●
●●●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●●
●●
●● ●
●
●●●
●●●●
●
●
●●
●
●●
● ●●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●●
● ●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●
●
●
●●
●●●
●
● ●●●
●
●●
●
●
●●
●
●
●
●
●●●
●●●
●
●●●
●●●
●
●
●● ●
●
●●
●
●
●●●
●●
●●
●●
●
●
●●
●
●●
●
●
●
●
●●
●
●●
●
●
●
●●
● ●●
●●
●●
●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
● ● ● ●●
●
●●
●
●●
●
●
●
●●● ●
●●● ●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●
●●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●●●
●
●
●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●●
●●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
● ● ●●
●
●●
●●
●●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●●●●
●●
●●
●
●
●●
●
●●
●●
●●
●
●
●● ●
●
●
●●
●●
●●
●●
●
●●
●
●
●
●●●●
●●●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●●●
●●●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●●
●
●
●●
●
●
●
●
●
●●●●●●
●
●
●●
●●●●
●●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●●
●
●●●
●
●
●●
●●
●●●
●
●●
●
●
●
●●
●
●●
●
●
●
●●
●●
●
●
●
● ●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●● ●●
●
●
●●
●●
●
●●
●
●●
●
●
●
●●
●
●
●
● ●
●●●●
● ●●●
●●
●
●
●
●
●
●
●
●
●
●●●●●●
●
●
●
●
●
●●
●
●
●●
●●
●
●●
●
●
●
●
●●
●
●
●●
●●
●
●●●
●●
●
●
●●
●
●
●●
●●●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●●
●●●●
●
●
●●
●●
●●
●
●●●●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●●
●●
●●
●
●●●●
●
●
●●
●
●
●
●●
● ● ●●
●●
●●●
●●
●
●
● ●
●
●● ●
●
●●●
●●
●
● ●●
●●
●
●●
●●
●●
●●●
●
●
●
●
●
●●●
●
●●●
●
●
●●
●●●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●●●●
●●
●
●
●
●
●
●
●●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●●
●●
●
●●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●● ●
●
●
●
● ●●
●●●●
●●●
●
●
●
●●
●
●●
●●
●
●●●●●
●
●●●●
●●
●●
●
●●●
●
●
●
●
●●
●
●
●●
●●
●
●
●●
●
●
●●
●●
●
●
●
●
●●●
●●
●●●
●
●
●
●
●
●
●
●
●●
●●●
●
●●
●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●●●
●●●●
●●●●
●
●●
●●●
●
●●
●
●●
●
●
●
●●
●
●
●●
●
●●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●●
●
●●●
●●●●
●
●●
●●
●●●
●
●●
●●
●●
●● ●●
●
●
●
●
●●
●
●●
●
●●
●
●●
●●
●
●
●
●
●●
●
● ●
●●
●
●●
●
●● ●●●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●●
●
●●
●
●
●
●●
●●
●
●
●
●
●●
●
●●
●
●
●●
●
●
●
●
●●
●
●●
●●
●●
●
●
●
●
● ●●
●●
●●
●●
●●
●
●●
●●
●
●●
●●
●
●
●●
●●
●
●
●●
●● ●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●● ●
●●
●
●●
●
●
● ●●
●●
●
●
●●
●●
●●
●
●
●
●
●●
●
●●
●●●●
●●
●
●●
●●
●●●●●
●
●
●
●●
●
●
●●●
●
●●
●
●●
●●
●
●●●
●
●●
●●●
●●
●
●
●
●
●●●
●
●
●
●
●
●●●
●
●●●●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●●
●●
●
●
●●
●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●●
●
●
●●●
●
●
●
●
●
●●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●● ● ●
●
●●●● ●●●●
●
●●
●
●●
●
●
●●●●
●
●
●●
●●
●●●●
●
● ●
● ●●
●
●●●
●●
●●
●
●●
●●
●
●●●
●
●●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●●
●●●
●
●
●●●
●
●●
●
●
●
●
●●●●
●
●●●●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●●
●●●
●
●●
●
●
●
●
●●
●
●
●●●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●●●●●●●●
●
●●●●
●●
●
●●●
●●
●●●
●
●
●●●
●
●●●
●●
●
●
●
●●●
● ●●●
●●
●
●●
●
●
●
●
●
●
●●●● ●
●●●●●
●
●
●●
●
●
●
●●
●
●●
●
●
●●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●●●
●●●
●
●
●
●
●●
●
●●
●
●●●
●
●
●
●
●●●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●● ●●
●●
●●
●
●
●●●
●●
●
●●
●
●
●●●
●●
●
●● ●
●
●
●
●●● ● ●
●
●
●
●
●●
●
●
●
●●●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●●
●
●
●
●●●
●●
●
●●●
●
●
●●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●●
●●
●
●
●
●●
●●
●●●
●●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●●
●
●●
●●●●●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●
●●●●
●●
●
●●
●●●
●●
●●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●●●●
●●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●●
●●
●
●
●●
●
●
●●●●
●
●
●●
●
●
●
●●
●●
●
●
●
●●
●
●●●
●●
●●
●
●
●●
●
●●
●
●
●●
●
●
●●
●
●●
●
●
●
●●
●●
●
●●
●
●
●●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●●●
●
●●●
●
●● ●
●
●
●
●
●
●● ●
●
●
●
●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●●
●●
●
●●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●●●
●
●
●●
●
●●●
●
●
●●
●●
●●●
●
●
●
●
●●
●
●●
●
●● ●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
● ●
●
●
●●
●
●●
●
●
●
●
●●
●●
●●
●●
●●
●●
●
●
●
●
●●
●●●●
●
●
●●
●
●
●●
●
●●
●
●
●●
●
●●●
●
●
●●
● ●●
●
●●
●
●
●
●●
●●
●
●
●
●
●●
●●
●●●●
●
●
●●●
●
●
●●
●
●● ●●
●
●
●
●●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●●●● ●●
● ●●
●●
● ●●●●
●●
●●
●●
●●●
●●
●
●
●●●
●
●
●
●●●
●
●
●
●
●●●
●
●●
●
●●
●●●
●
●
● ●●
●
●●
●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●●
●
●●●
●
●
●
●
●
●●
●
●●
●
●●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●●
●
●●
●
●●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●●●
●
●
●
●
●●
●
●●●
● ●
●
●
●
●●
●
●
●
●
●●●
●
●●
●
● ●●●●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●●●
●●●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●●
●●
●●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●●●
●
●●●
●
●●
●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●●
●●
●
●
●
●
●
●
●●●
●
●●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●●
●●
●
●
●
●●
●●

0.00

0.25

0.50

0.75

0.02

0.00

−0.02

1.00

−0.02

S1

0.00

0.02

S1

e for four simulated S matrices under the Spatial model. The left
Figure 7: A mapping from S to S
column shows the simulated structure S for each of four scenarios (a–d) and the right column shows
the resulting estimated row basis of S produced from PCA. It can be seen that the scale on which S
was generated, all values in (0,1), is lost in the principal components, values in R.
29

7.8E-5

5.2E-4
4.8E-4
4.4E-4
5.5E-4

1.2E-3
1.1E-3
1.4E-3
1.6E-3

6.6E-5
6.3E-5

4.1E-4
4.3E-4
5.4E-4
4.1E-4

1.0E-3
9.9E-4
1.6E-3
1.4E-3

a = 0.5

a = 1.0

PCA

LFA

ADX

FS

PCA

LFA

ADX

FS

9.5E-5

1.1E-4

6.9E-5

1.2E-4

7.4E-5

a = 0.25

6.1E-5

α = 1.0

8.5E-5

7.3E-5

6.3E-5

α = 0.5

9.2E-5

a = 0.1

6.7E-5

7.3E-5

6.8E-5

6.9E-5

7.0E-5

LFA

3.1E-2

2.3E-3

1.3E-2

1.3E-2

7.8E-4

5.0E-3

2.4E-3

2.8E-3

1.2E-2

1.0E-2

8.6E-3

8.2E-3

3.3E-2

5.4E-2

3.6E-2

1.6E-2

2.6E-3

ADX

Median KL

PCA

α = 0.1

α = 0.01

Scenario

2.9E-2

2.3E-3

1.2E-2

1.4E-2

9.2E-4

5.5E-3

2.7E-3

3.4E-3

1.2E-2

1.0E-2

8.6E-3

8.1E-3

3.3E-2

5.4E-2

3.6E-2

1.6E-2

2.6E-3

FS

2.6E-2

2.6E-2

2.2E-2

2.3E-2

1.3E-2

1.5E-2

1.3E-2

1.3E-2

5.7E-3

5.6E-3

5.6E-3

5.5E-3

5.6E-3

5.6E-3

5.6E-3

5.6E-3

5.8E-3

PCA

column is the accuracy of a method’s fits with the given metric.

2.7E-2

2.6E-2

2.4E-2

2.5E-2

1.5E-2

1.3E-2

1.4E-2

1.5E-2

6.4E-3

6.9E-3

7.4E-3

7.6E-3

6.3E-3

6.8E-3

6.9E-3

5.8E-3

5.8E-3

LFA

1.4E-1

5.6E-2

1.2E-1

1.2E-1

5.6E-2

1.1E-1

7.9E-2

8.1E-2

1.1E-1

6.7E-2

9.3E-2

7.4E-2

1.4E-1

1.4E-1

1.6E-1

9.7E-2

3.7E-2

ADX

Mean Abs. Err.

1.3E-1

5.6E-2

1.2E-1

1.2E-1

5.8E-2

1.1E-1

8.1E-2

8.3E-2

1.1E-1

6.7E-2

9.3E-2

7.4E-2

1.4E-1

1.4E-1

1.6E-1

9.7E-2

3.7E-2

FS

3.6E-2

3.6E-2

3.5E-2

3.4E-2

1.8E-2

2.0E-2

1.8E-2

1.8E-2

7.4E-3

7.2E-3

7.2E-3

7.0E-3

7.4E-3

7.3E-3

7.2E-3

7.2E-3

7.5E-3

PCA

3.8E-2

3.7E-2

3.7E-2

3.6E-2

2.1E-2

1.9E-2

2.0E-2

2.1E-2

8.5E-3

9.2E-3

9.8E-3

1.0E-2

8.4E-3

9.0E-3

9.3E-3

7.6E-3

7.5E-3

LFA

ADX

2.2E-1

1.0E-1

2.2E-1

2.2E-1

1.3E-1

2.0E-1

1.4E-1

1.5E-1

1.7E-1

1.0E-1

1.6E-1

1.2E-1

2.2E-1

1.8E-1

2.4E-1

1.7E-1

5.8E-2

RMSE

2.1E-1

1.0E-1

2.2E-1

2.2E-1

1.3E-1

2.0E-1

1.5E-1

1.5E-1

1.7E-1

1.0E-1

1.6E-1

1.2E-1

2.2E-1

1.8E-1

2.4E-1

1.7E-1

5.8E-2

FS

Table 3: Accuracy in estimating πij parameters by the PCA based method and LFA. Each row is a different simulation scenario. Each

BN

PSD

Spatial

TGP fit

HGDP fit

30

Table 4: The top 50 SNPs most associated with structure in the HGDP data, identified by performing a logistic regression of
SNP genotypes on the logistic factors. Shown are the SNP ID and location, deviance measure of differentiation, gene closest
to the SNP, distance to gene (rounded to nearest 10bp), and the variant type (if none shown, then intergenic).
rsid

chr

position

deviance

genesymbol

locusID

distance

1

rs1834640

15

48392165

1605.28

SLC24A5

283652

21000

2

rs2250072

15

48384907

1313.82

SLC24A5

283652

28260

3

rs12440301

15

48389924

1263.83

SLC24A5

283652

23240

4

rs260690

2

109579738

1262.72

EDAR

10913

0

intron-variant

5

rs9837708

3

71487582

1189.48

FOXP1

27086

0

intron-variant

6

rs260714

2

109562495

1184.50

EDAR

10913

0

intron-variant

7

rs4918664

10

94921065

1178.40

XRCC6P1

387703

45340

8

rs10882168

10

94929434

1160.99

XRCC6P1

387703

36970

9

rs300153

2

17986417

1143.48

MSGN1

343930

11360

10

rs9809818

3

71480566

1135.58

FOXP1

27086

11

rs6583859

10

94893473

1119.25

NIP7P1

389997

26290

12

rs11187300

10

94920291

1114.22

XRCC6P1

387703

46120

13

rs260698

2

109566759

1111.64

EDAR

10913

0

intron-variant

14

rs1834619

2

17901485

1111.40

SMC6

79677

0

intron-variant

15

rs11637235

15

48633153

1104.45

DUT

1854

0

intron-variant

16

rs4497887

2

125859777

1097.13

RNA5SP102

17

rs7091054

10

95018444

1085.45

RPL17P34

18

rs7090105

10

75131545

1075.50

ANXA7

19

rs973787

4

38263893

1074.57

TBC1D1

23216

123090

20

rs4279220

4

38254182

1070.43

TBC1D1

23216

113380

21

rs7556886

2

17908130

1062.58

SMC6

79677

22

rs12473565

2

175163335

1056.31

LOC644158

23

rs6500380

16

48375777

1051.10

LONP2

83752

24

rs2384319

2

26206255

1033.88

KIF3C

3797

25

rs12220128

10

94975011

1023.79

XRCC6P1

26

rs17034770

2

109616376

1019.03

EDAR

27

rs3792006

2

26498222

998.96

HADHB

28

rs4918924

10

94976956

994.79

XRCC6P1

29

rs1984996

10

95008745

990.92

RPL17P34

30

rs3751631

15

52534344

987.33

MYO5C

55930

0

31

rs4578856

2

17853388

987.29

SMC6

79677

0

intron-variant

32

rs13397666

2

109544052

986.80

EDAR

10913

0

intron-variant

33

rs12619554

2

17352372

986.20

ZFYVE9P2

34

rs3736508

11

45975130

981.05

PHF21A

35

rs12472075

2

177691130

973.02

RPL29P8

36

rs9522149

13

111827167

965.50

ARHGEF7

8874

0

intron-variant

37

rs2917454

10

78892415

964.40

KCNMA1

3778

0

intron-variant

38

rs10882183

10

94974083

961.04

XRCC6P1

39

rs10079352

5

117494640

960.33

LOC100505811

40

rs10935320

3

139056584

958.33

MRPS22

41

rs9571407

13

34886039

957.04

LINC00457

42

rs6542787

2

109556365

955.56

EDAR

43

rs953035

1

36079508

954.67

PSMB2

44

rs4657449

1

165465281

951.72

LOC400794

45

rs9960403

18

13437993

949.43

LDLRAD4

46

rs203150

18

38037221

944.32

RPL17P45

100271414

47

rs2823882

21

17934419

942.05

LINC00478

388815

48

rs10886189

10

119753963

937.81

RAB11FIP2

22841

49

rs2441727

10

68224886

937.08

CTNNA3

50

rs310644

20

62159504

931.90

PTK6

100873373
643863
310

644158

0

25280
3640

0
0
810

10913

10540
0

387703

8030

643863

34980

51317
100131991

387703
100505811
56945
100874179

intron-variant

1390

6090

100420972

intron-variant

169180

387703
3032

variant type

intron-variant
upstream-variant-2KB

intron-variant

reference,synonymous-codon

113180
0

missense,reference

16650

5160
123620
6270
123540

10913

0

intron-variant

5690

0

intron-variant

400794

0

intron-variant

753

0

intron-variant

29119
5753

312750
0

intron-variant

10460
0
260

intron-variant
downstream-variant-500B

Table 5: The top 50 SNPs most associated with structure in the TGP data, identified by performing a logistic regression of
SNP genotypes on the logistic factors. Shown are the SNP ID and location, deviance measure of differentiation, gene closest
to the SNP, distance to gene (rounded to nearest 10bp), and the variant type (if none shown, then intergenic).
rsid

chr

position

deviance

genesymbol

locusID

distance

1

rs1426654

15

48426484

3129.76

SLC24A5

283652

0

missense,reference

2

rs3827760

2

109513601

2395.27

EDAR

10913

0

missense,reference

3

rs922452

2

109543883

2338.38

EDAR

10913

0

intron-variant

4

rs372985703

17

19172196

1975.16

EPN2

22905

0

intron-variant

5

rs4924987

17

19247075

1949.03

B9D1

27077

0

intron-variant,missense,reference

6

rs260687

2

109578855

1925.18

EDAR

10913

0

intron-variant

7

rs7209202

17

58532239

1890.67

APPBP2

10513

0

8

rs7211872

17

58550725

1890.67

APPBP2

10513

0

9

rs67929453

3

139109825

1890.57

LOC100507291

100507291

0

intron-variant,upstream-variant-2KB

10

rs260643

2

109539653

1850.71

EDAR

10913

0

intron-variant

11

rs260707

2

109574150

1838.37

EDAR

10913

0

intron-variant

12

rs1545071

18

67695505

1821.35

RTTN

25914

0

intron-variant

13

rs12729599

1

1323078

1812.91

CCNL2

81669

0

intron-variant

14

rs12347078

9

344508

1811.16

DOCK8

81704

0

intron-variant

15

rs12142199

1

1249187

1779.28

CPSF3L

54973

0

reference,synonymous-codon

16

rs12953952

18

67737927

1750.15

RTTN

25914

0

intron-variant

17

rs9467091

6

10651772

1746.75

GCNT6

644378

4270

18

rs7165971

15

55921013

1736.83

PRTG

283659

0

intron-variant

19

rs6132532

20

2315543

1730.64

TGM3

7053

0

intron-variant

20

rs959071

17

19142226

1729.18

EPN2

22905

0

intron-variant

21

rs10962599

9

16795286

1726.24

BNC2

54796

0

intron-variant

22

rs967377

20

53222217

1724.93

DOK5

55816

0

intron-variant

23

rs4891381

18

67595449

1723.79

CD226

10666

0

intron-variant

24

rs377561427

15

63988357

1713.98

HERC1

8925

0

frameshift-variant,reference

25

rs73889254

22

46762214

1711.40

CELSR1

9620

0

intron-variant

26

rs4918664

10

94921065

1700.64

XRCC6P1

27

rs2759281

1

204866365

1691.03

NFASC

28

rs12065033

1

173579034

1682.54

ANKRD45

29

rs9796793

16

30495652

1681.28

ITGAL

30

rs1240708

1

1335790

1675.48

LOC148413

31

rs2615876

10

117665860

1670.53

ATRNL1

32

rs2823882

21

17934419

1669.32

LINC00478

388815

33

rs8097206

18

38024931

1663.29

RPL17P45

100271414

34

rs8071181

17

58508582

1662.44

C17orf64

124773

0

35

rs1075389

15

64174177

1661.21

MIR422A

494334

10950

36

rs6875659

5

175158653

1657.54

HRH2

3274

22410

37

rs7171940

15

64170986

1654.01

MIR422A

494334

7760

38

rs2148359

9

7385508

1652.16

RPL4P5

158345

91440

39

rs7531501

1

234338303

1648.15

SLC35F3

148641

0

intron-variant

40

rs57742857

15

93567352

1645.21

CHD2

1106

0

intron-variant

41

rs931564

17

58631702

1636.86

LOC388406

42

rs4738296

8

73857539

1632.70

LOC100288310

100288310

0

43

rs4402785

2

104766351

1631.33

LOC100287010

100287010

228950

44

rs12988506

2

33162854

1630.14

LOC100271832

45

rs9410664

9

91196828

1625.48

NXNL2

46

rs2041564

2

72453847

1623.91

EXOC6B

47

rs6024103

20

54034601

1623.41

LOC101927796

48

rs6583859

10

94893473

1619.79

NIP7P1

389997

49

rs12913832

15

28365618

1611.23

HERC2

8924

0

intron-variant

50

rs632876

2

216572452

1610.26

LINC00607

646324

0

intron-variant

387703

variant type

45340

23114

0

intron-variant

339416

0

utr-variant-3-prime

3683

0

intron-variant

148413

0

intron-variant,upstream-variant-2KB

26033

0

intron-variant

0

intron-variant

388406

100271832
158046
23233
101927796

300460
reference,synonymous-codon

10200

0

intron-variant
intron-variant

6120
0

intron-variant

2270
26290

References
[1] McCarthy, M. I., Abecasis, G. R., Cardon, L. R., Goldstein, D. B., Little, J., Ioannidis, J. P. A., and
Hirschhorn, J. N. Genome-wide association studies for complex traits: consensus, uncertainty and
challenges. Nature reviews. Genetics 9(5), 356–369 (2008).
[2] Frazer, K. A., Murray, S. S., Schork, N. J., and Topol, E. J. Human genetic variation and its
contribution to complex traits. Nat Rev Genet 10(4), 241–251, Apr (2009).
[3] Wellcome Trust Case Control Consortium. Genome-wide association study of 14,000 cases of
seven common diseases and 3,000 shared controls. Nature 447(7145), 661–678, Jun (2007).
[4] Pritchard, J. K. and Rosenberg, N. A. Use of unlinked genetic markers to detect population stratification in association studies. Am J Hum Genet 65(1), 220–228, Jul (1999).
[5] Astle, W. and Balding, D. J. Population structure and cryptic relatedness in genetic association
studies. Statistical Science 24, 451–471 (2009).
[6] Price, A. L., Zaitlen, N. A., Reich, D., and Patterson, N. New approaches to population stratification
in genome-wide association studies. Nat Rev Genet 11(7), 459–463, Jun (2010).
[7] Kang, H. M., Sul, J. H., Service, S. K., Zaitlen, N. A., Kong, S.-Y., Freimer, N. B., Sabatti, C., and
Eskin, E. Variance component model to account for sample structure in genome-wide association
studies. Nature Genetics 42(4), 348–354 (2010).
[8] Jorde, L. B., Watkins, W. S., and Bamshad, M. J. Population genomics: a bridge from evolutionary
history to genetic medicine. Hum Mol Genet 10(20), 2199–2207, Oct (2001).
[9] Nielsen, R., Hellmann, I., Hubisz, M., Bustamante, C., and Clark, A. G. Recent and ongoing
selection in the human genome. Nat Rev Genet 8(11), 857–868, Nov (2007).
[10] Rosenberg, N. A., Pritchard, J. K., Weber, J. L., Cann, H. M., Kidd, K. K., Zhivotovsky, L. A., and
Feldman, M. W. Genetic structure of human populations. Science 298, 2381–2385 (2002).
[11] Cann, H. M., de Toma, C., Cazes, L., Legrand, M.-F., Morel, V., Piouffre, L., Bodmer, J., Bodmer,
W. F., Bonne-Tamir, B., Cambon-Thomsen, A., Chen, Z., Chu, J., Carcassi, C., Contu, L., Du,
R., Excoffier, L., Ferrara, G. B., Friedlaender, J. S., Groot, H., Gurwitz, D., Jenkins, T., Herrera,
R. J., Huang, X., Kidd, J., Kidd, K. K., Langaney, A., Lin, A. A., Mehdi, S. Q., Parham, P., Piazza,
A., Pistillo, M. P., Qian, Y., Shu, Q., Xu, J., Zhu, S., Weber, J. L., Greely, H. T., Feldman, M. W.,
Thomas, G., Dausset, J., and Cavalli-Sforza, L. L. A human genome diversity cell line panel.
Science 296(5566), 261–262, Apr (2002).

[12] Rosenberg, N. A., Mahajan, S., Ramachandran, S., Zhao, C., Pritchard, J. K., and Feldman, M. W.
Clines, clusters, and the effect of study design on the inference of human population structure.
PLoS Genet 1(6), e70, Dec (2005).
[13] 1000 Genomes Project Consortium. A map of human genome variation from population-scale
sequencing. Nature 467(7319), 1061–1073, Oct (2010).
[14] Pritchard, J. K., Stephens, M., and Donnelly, P. Inference of population structure using multilocus
genotype data. Genetics 155(2), 945–959, Jun (2000).
[15] Tang, H., Peng, J., Wang, P., and Risch, N. J. Estimation of individual admixture: analytical and
study design considerations. Genetic epidemiology 28(4), 289–301, May (2005).
[16] Alexander, D. H., Novembre, J., and Lange, K. Fast model-based estimation of ancestry in unrelated individuals. Genome Research 19(9), 1655–1664 (2009).
[17] Balding, D. J. and Nichols, R. A. A method for quantifying differentiation between populations at
multi-allelic loci and its implications for investigating identity and paternity. Genetica 96(1-2), 3–12
(1995).
[18] Menozzi, P., Piazza, A., and Cavalli-Sforza, L. Synthetic maps of human gene frequencies in
europeans. Science 201(4358), 786–792, Sep (1978).
[19] Sokal, Oden, and Thomson. A problem with synthetic maps. Human Biology 71, 1–13 (1999).
[20] Rendine, Piazza, and Cavailli-Sforza. A problem with synthetic maps: Reply to sokal et al. Human
Biology 71, 15–25 (1999).
[21] Novembre, J. and Stephens, M. Interpreting principal component analyses of spatial population
genetic variation. Nat Genet 40(5), 646–649, May (2008).
[22] Manni. Interview with luigi luca cavalli-sforza: Past research and directions for future investigations
in human population genetics. Human Biology 82, 245–266 (2010).
[23] Bartholomew, D. J., Knott, M., and Moustaki, I. Latent Variable Models and Factor Analysis: A
Unified Approach. Wiley Series in Probability and Statistics, (2011).
[24] Engelhardt, B. E. and Stephens, M. Analysis of population structure: a unifying framework and
novel methods based on sparse factor analysis. PLoS Genet 6(9) (2010).
[25] Raj, A., Stephens, M., and Pritchard, J. K. fastSTRUCTURE: Variational inference of population
structure in large snp datasets. Genetics 197, 573–589 (2014).

[26] Kamberov, Y., Wang, S., Tan, J., Gerbault, P., Wark, A., Tan, L., Yang, Y., Li, S., Tang, K., and
Chen, H. Modeling recent human evolution in mice by expression of a selected edar variant. Cell
152(4), 691–702 (2013).
[27] Thornton, T., Tang, H., Hoffmann, T., Ochs-Balcom, H., Caan, B., and Risch, N. Estimating kinship
in admixed populations. The American Journal of Human Genetics 91, 122–138 (2012).
[28] Corona, E., Chen, R., Sikora, M., Morgan, A., Patel, C., Ramesh, A., Bustamante, C., and Butte,
A. Analysis of the genetic basis of disease in the context of worldwide human relationships and
migration. PLoS Genetics 9(5), e1003447 (2013).
[29] Song, M., Hao, W., and Storey, J. D. Testing for genetic associations under arbitrarily structured
populations. bioRxiv http://dx.doi.org/10.1101/012682 (2014).
[30] McCullagh, P. and Nelder, J. A. Generalized Linear Models. Chapman and Hall, (1989).
[31] Lehmann, E. L. and Casella, G. Theory of Point Estimation. Springer Verlag, 2nd edition, (1998).
[32] Baglama, J. and Reichel, L. Restarted block lanczos bidiagonalization methods. Numerical Algorithms 43, 251–272 (2006).
[33] Coop, G., Pickrell, J. K., Novembre, J., Kudaravalli, S., Li, J., Absher, D., Myers, R. M., CavalliSforza, L. L., Feldman, M. W., and Pritchard, J. K. The role of geography in human adaptation.
PLoS Genet 5(6), e1000500, Jun (2009).
[34] Chung, N. C. and Storey, J. D. Statistical significance of variables driving systematic variation.
arXiv [stat.ME], 1308.6013 (2013). http://arxiv.org/abs/1308.6013.
[35] Akey, J. M., Zhang, G., Zhang, K., Jin, L., and Shriver, M. D. Interrogating a high-density snp map
for signatures of natural selection. Genome Res 12(12), 1805–1814, Dec (2002).
[36] Lamason, R. L., Mohideen, M.-A. P. K., Mest, J. R., Wong, A. C., Norton, H. L., Aros, M. C., Jurynec, M. J., Mao, X., Humphreville, V. R., Humbert, J. E., Sinha, S., Moore, J. L., Jagadeeswaran,
P., Zhao, W., Ning, G., Makalowska, I., McKeigue, P. M., O’donnell, D., Kittles, R., Parra, E. J.,
Mangini, N. J., Grunwald, D. J., Shriver, M. D., Canfield, V. A., and Cheng, K. C. Slc24a5, a putative cation exchanger, affects pigmentation in zebrafish and humans. Science 310(5755), 1782–6,
Dec (2005).
[37] Sabeti, P. C., Varilly, P., Fry, B., Lohmueller, J., Hostetter, E., Cotsapas, C., Xie, X., Byrne, E. H.,
McCarroll, S. A., Gaudet, R., Schaffner, S. F., Lander, E. S., and International HapMap Consortium.

Genome-wide detection and characterization of positive selection in human populations. Nature
449(7164), 913–918 (2007).
[38] Banham, A. H., Beasley, N., Campo, E., Fernandez, P. L., Fidler, C., Gatter, K., Jones, M., Mason,
D. Y., Prime, J. E., Trougouboff, P., Wood, K., and Cordell, J. L. The foxp1 winged helix transcription
factor is a novel candidate tumor suppressor gene on chromosome 3p. Cancer research 61(24),
8820–8829, Dec (2001).
[39] Shigekawa, T., Ijichi, N., Ikeda, K., Horie-Inoue, K., Shimizu, C., Saji, S., Aogi, K., Tsuda, H.,
Osaki, A., Saeki, T., and Inoue, S. Foxp1, an estrogen-inducible transcription factor, modulates cell
proliferation in breast cancer cells and 5-year recurrence-free survival of patients with tamoxifentreated breast cancer. Hormones & cancer 2(5), 286–297, Oct (2011).
[40] Stone, S., Abkevich, V., Russell, D. L., Riley, R., Timms, K., Tran, T., Trem, D., Frank, D., Jammulapati, S., Neff, C. D., Iliev, D., Gress, R., He, G., Frech, G. C., Adams, T. D., Skolnick, M. H.,
Lanchbury, J. S., Gutin, A., Hunt, S. C., and Shattuck, D. Tbc1d1 is a candidate for a severe obesity gene and evidence for a gene/gene interaction in obesity predisposition. Human molecular
genetics 15(18), 2709–2720, Sep (2006).
[41] Sardella, M., Navone, F., Rocchi, M., Rubartelli, A., Viggiano, L., Vignali, G., Consalez, G. G., Sitia,
R., and Cabibbo, A. Kif3c, a novel member of the kinesin superfamily: sequence, expression, and
mapping to human chromosome 2 at 2p23. Genomics 47(3), 405–408, Feb (1998).
[42] Jiao, H., Arner, P., Hoffstedt, J., Brodin, D., Dubern, B., Czernichow, S., van’t Hooft, F., Axelsson,
T., Pedersen, O., Hansen, T., Sørensen, T. I. A., Hebebrand, J., Kere, J., Dahlman-Wright, K.,
Hamsten, A., Clement, K., and Dahlman, I. Genome wide association study identifies kcnma1
contributing to human obesity. BMC medical genomics 4, 51, Jun (2011).
[43] Bernstein, D. I., Kashon, M., Lummus, Z. L., Johnson, V. J., Fluharty, K., Gautrin, D., Malo, J.-L.,
Cartier, A., Boulet, L.-P., Sastre, J., Quirce, S., Germolec, D., Tarlo, S. M., Cruz, M.-J., Munoz, X.,
Luster, M. I., and Yucesoy, B. Ctnna3 (α-catenin) gene variants are associated with diisocyanate
asthma: a replication study in a caucasian worker population. Toxicological sciences : an official
journal of the Society of Toxicology 131(1), 242–246, Jan (2013).
[44] Ostrander, J. H., Daniel, A. R., and Lange, C. A. Brk/ptk6 signaling in normal and cancer cell
models. Current opinion in pharmacology 10(6), 662–669, Dec (2010).
[45] Jolliffe, I. T. Principal component analysis. New York: Springer, 2nd edition, (2010).

[46] Rosenberg, N. A. Standardized subsets of the hgdp-ceph human genome diversity cell line panel,
accounting for atypical and duplicated samples and pairs of close relatives. Annals of Human
Genetics 70, 841–847 (2006).
[47] Weir, B. and Cockerham, C. Estimating F-statistics for the analysis of population structure. Evolution 38, 1358–1370 (1984).
[48] Weir, B. S. Genetic Data Analysis II: Methods for Discrete Population Genetic Data. Sunderland,
MA: Sinauer Associates, (1996).
[49] Bartholomew, D. J. The foundations of factor analysis. Biometrika 71, 221–232 (1984).
[50] Moustaki and Knott. Generalized latent trait models. Psychometrika 65, 391–411 (2000).
[51] Bartholomew, D. J. Factor analysis for categorical data. J Roy Stat Soc B 42, 293–321 (1980).
[52] Collins, M., Dasgupta, S., and Schapire, R. A generalization of principle component analysis to
the exponential family. In Proceedings of Advances in Neural Information Processiong Systems,
(2002).
[53] Schein, A. I., Saul, L. K., and Ungar, L. H. A generalized linear model for principal component
analysis of binary data. In Proceedings of the 9 th International Workshop on Artificial Intelligence
and Statistics, (2003).
[54] Guo, Y. and Schuurmans, D. Efficient global optimization for exponential family pca and low-rank
matrix factorization. In In Allerton Conf. on Commun., Control, and Computing, (2008).
[55] Paatero, P. and Tapper, U. Positive matrix factorization: A non-negative factor model with optimal
utilization of error estimates of data values. Environmetrics 5, 111–126 (1994).
[56] Lee, D. D. and Seung, S. Learning the parts of objects by non-negative matrix factorization. Nature
401, 788–791 (1999).

