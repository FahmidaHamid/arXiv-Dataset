arXiv:1502.03135v1 [q-bio.NC] 10 Feb 2015

On the role of time in perceptual decision making

Ádám Koblinger
Department of Cognitive Science
Central European University
Budapest, H-1023, Hungary
kob314@gmail.com

Máté Lengyel
Department of Engineering
University of Cambridge
Cambridge, CB2 1PZ, UK
m.lengyel@eng.cam.ac.uk

József Fiser
Department of Cognitive Science
Central European University
Budapest, H-1023, Hungary
fiserj@ceu.hu

Marjena Popović
Neuroscience Program
Brandeis University
Waltham, MA 02454, USA
marjenap@brandeis.edu

Abstract
According to the dominant view, time in perceptual decision making is used for
integrating new sensory evidence. Based on a probabilistic framework, we investigated the alternative hypothesis that time is used for gradually refining an internal
estimate of uncertainty, that is to obtain an increasingly accurate approximation
of the posterior distribution through collecting samples from it. In the context of
a simple orientation estimation task, we analytically derived predictions of how
humans should behave under the two hypotheses, and identified the across-trial
correlation between error and subjective uncertainty as a proper assay to distinguish between them. Next, we developed a novel experimental paradigm that
could be used to reliably measure these quantities, and tested the predictions derived from the two hypotheses. We found that in our task, humans show clear
evidence that they use time mostly for probabilistic sampling and not for evidence
integration. These results provide the first empirical support for iteratively improving probabilistic representations in perceptual decision making, and open the
way to reinterpret the role of time in the cortical processing of complex sensory
information.

1

Introduction

Making decisions is one of the most fundamental aspects of cognition and accordingly, its behavioral, neural and computational bases have been investigated extensively [1–3]. The prevailing
framework of perceptual decision making maintains that time in decision making is used for collecting evidence about the stimulus for the decision [4]. This proposal is corroborated by the fact
that with the progression of time, the error of and the uncertainty about the decision in behavioral
studies of decision making steadily decrease until they reach an asymptote presumably set by the
internal noise of the system [5]. These behavioral studies handle decision making typically as a deterministic process and investigate the simplest two-alternative-forced-choice version of the problem
yielding insights that are difficult to generalize to multiple options and estimation tasks (implying a
continuum of options) abundant in real life [6, 7].
In recent years, a prominent alternative framework of perception and cognition emerged based on the
concept of probabilistic representations and computation [8]. According to this view, humans and
animals code not only a single value of the sensory input, but multiple values with their subjective
uncertainty about those values, and thus representation and computation in the cortex is based on
1

probability distributions [9]. This proposal was supported by a number of studies finding that human
and animal behavior in different tasks could be best described as if the participants were aware and
used probabilistic representations to achieve their goals (approximately) optimally [10–13], and
gained further support from recent neurophysiological findings [14, 15]. However, the probabilistic
framework aggravates the concern of generalization: the exact probabilistic inference necessary to
reach a decision in a multiple-choice or estimation task is computationally clearly intractable, and
the potential of appropriate approximations has not been explored to date.
We selected one particular approximation strategy – Monte Carlo sampling – for probabilistic computation [16], and explored the consequences of this framework on the role of time in decision making. Approximating probabilistic representations and computations by sampling means that time is
needed to collect samples sequentially from the posterior distribution even when no new evidence
enters the system and thus the posterior itself remains static. This provides a new hypothesis with a
novel role of time in sensory systems that complements evidence integration: time is used for iteratively refining an approximation of the true posterior. Thus, the goal of the present paper is twofold.
First, to find evidence at the earliest possible stage of visual information processing for probabilistic
computations with iterative approximation. Second, to establish whether evidence integration or
probabilistic sampling dominates decision making in naturalistic estimation tasks.
Our paper presents the following three new contributions. First, we established a mathematical
model of decision making under evidence integration (EI) and probabilistic sampling (PS), and
by analytical derivation, we identified the measure of the trial-by-trial correlation between error
and uncertainty as an appropriate measure to distinguish whether participants are predominantly
occupied with EI or PS (Section 2). Second, we developed a novel behavioral test method, which can
be used for measuring immediate error and subjective uncertainty simultaneously on a trial-by-trial
basis within ∼700 msec after stimulus presentation (Section 3). Finally, we measured the correlation
between error and subjective uncertainty in this modified orientation estimation task as a function
of stimulus presentation time, and found that the pattern of this correlation confirms the predictions
derived from the PS hypothesis (Section 4). Thus, we conclude that probabilistic computations take
place from the earliest levels of cortical visual information processing, and that in simple perceptual
decision making tasks with static stimuli the majority of time is used for improving an internal
representation of the obtained information and not for incorporating new external evidence. We
discuss our results in the broader context of perceptual decision making in Section 5.

2

Theoretical predictions for evidence integration and probabilistic
sampling

In the Supplementary Information (SI) we derive analytical formulæ for how estimation error, uncertainty, and their correlation should vary over time1 in a perceptual (eg. orientation) estimation
task under two orthogonal scenarios:
Evidence integration (EI) The posterior distribution (over possible orientations) is represented exactly at any moment, and time is used to collect more evidence about the external stimulus
thus leading to a gradual refinement of the corresponding posterior, “homing in” on the true
stimulus value (orientation).
Probabilistic sampling (PS) The posterior itself does not change in time2 , but it is represented
approximately by drawing successive samples from it as in Monte Carlo techniques [17],
and so time is used to collect more samples from this static posterior.
The assumptions underlying our derivations were the following:
– The representation of uncertainty is self-consistent, in that uncertainty is predictive of the
errors made in estimation (which we confirmed empirically, see below).
– Across trials, the posterior changes its location and scale (mean and variance, which are
both finite), but not its shape (higher moments). Importantly, while we show results here
1

“Time” here corresponds to “presentation time” in the experiments described in the following sections.
This can be justified by evidence integration converging to a finite-width posterior due to fundamental
limits on perception, with convergence being fast because the stimulus is static.
2

2

evidence integration

probabilistic sampling
κx = 0

κx = 0

1.5

ϵ s a m p l e / ϵ exa ct

0.8
0.6
0.4
0.2

κx = 0

5
10
15
numbe r of dat a point s , n

20

0.6
0.4

0
0

0
0

1.5

100

200

300

1

5
10
15
numbe r of dat a point s , n

20

1

1

ρ∞
0.4
0.5

0.5
1

0.6

0
0.5

−0.5
0
0

0.2
0
5

10

15

20

−1

10

numbe r ofofdat
a point
s, n
number
data
points

0.8

asymptotic
correlation
ρ∞

0.6

1

0.8
ρ s a m p l ρe n /ρ
/ ∞ρ exa ct

1

ρ n /ρ ∞

normalized
error-uncertainty
correlation

1.5

0.8

0
0

400

0.2

ρ exa ct

0
0

1
1

0.80.5

ϵ n /ϵ 1

ϵ n /ϵ 1

normalized average
error & uncertainty

1

0.4

−1
0 0
10
ω
γ2

20

0.4

0.2
0

5
10
15
numbe r of dat a point s , n

0.6

−1

10

0

ω

10

1

0.2

10

γ
CV of task
difficulty
2

0
1
100

200
300
10 numbe r of s ample s , N
number of samples

400

−1

10

0

10
ωγ 2

1

10

Figure 1. Theoretical predictions. Normalized average error and uncertainty (top) and the correlation between error and uncertainty (bottom) for evidence integration (EI, left) and probabilistic
sampling (PS, right). Averages and correlations are computed across trials. Average error and uncertainty are normalised to their values obtained after the first data point for EI, and to the asymptotic
value in the limit of infinite samples for PS. (An above-zero asymptote for EI can be obtained by
introducing behavioural noise, see SI, or an asymptotically non-vanishing posterior variance.) Erroruncertainty correlation is normalised to its asymptotic value for both EI and PS, where the asymptotic value is obtained by using an exact representation of uncertainty, and is shown in the inset as
a function of the coefficient of variation (CV) of the task difficulty (ie. the CV of the across-trial
distribution of posterior variance).

for a Gaussian posterior, the derivations themselves do not assume Gaussianity and work
for any other shape with a known (and finite) kurtosis.
– Successive data points (for EI) or samples (for PS) are i.i.d. This means that the “number
of data points / samples” referred to in the following should be understood as “the effective
number of independent data points / samples” which is proportional to the number of data
points / samples and inversely proportional to some suitable measure of their statistical
dependency (e.g. the total autocorrelation of samples when computing the Monte Carlo
integral of a simple linear function under the posterior).
– For analytical tractability, there are two departures from the setting we tested experimentally: the variable over which the posterior needs to be represented is linear rather than
circular, and estimation error is measured as squared rather than absolute error. We expect
neither of these to impact our results qualitatively.
The main results of our derivations are the following (Fig. 1):
– In both scenarios, the average error and uncertainty decrease roughly as 1 / time. This is
interesting because a decrease in error with time has classically been taken as a hallmark
of EI. We show that it can equivalently result from PS, without EI.
– The time course of the correlation between error and uncertainty distinguishes between EI
and PS. In EI, it remains constant (or decreases if behavioural noise is considered), while in
PS it may potentially transiently decrease, but eventually always increases even well after
average error and uncertainty have already asymptoted.

3

Measuring trial-by-trial error and uncertainty in an orientation
estimation task

In order to asses whether human decision making is dominated by EI or PS, we developed a new
experimental paradigm (Figure 2). The basis of our paradigm was a standard orientation estimation
3

stimulus orientation
reported
orientation

error: εx

uncertainty: σx2

Figure 2. Experimental design. A. In each trial, the subject made an orientation estimation judgement and provided information about the orientation and their subjective uncertainty by drawing a
single stroke on a tablet. See text for details. B. Estimation error (εx ) between the true and reported
orientations, and level of uncertainty (σx2 ) were the dependent variables in the experiment.

task. In each trial, subjects first saw a blank screen with a fixation dot for 1100 msec. Next, a display
appeared with a variable number (1–6, randomly chosen) of 1-degree-long line segments equidistant (with a randomly chosen rotation) around a circle (extending 7◦ of visual angle in diameter).
Contrast levels were sampled randomly (without replacement within a display) from {10, 20, 30...
100%}, and orientations uniformly from 0◦ -180◦ . The display appeared for one of nine possible durations (“presentation time”): 50, 75, 100, 133, 167, 200, 300, 400, or 600 msec. After the display
disappeared, a mask of random noise appeared with a small red circle identifying the position of one
of the segments in the preceding display. The subject’s task was to report as quickly as they could
their estimate of the orientation of the segment in the cued position simultaneously together with
the subjective assessment of uncertainty in their estimate by drawing a line on a tablet with a stylus.
The orientation of the line indicated the estimated orientation of the line segment, while the length
of the line corresponded to subjective uncertainty. (A longer line indicated less certainty to avoid
the possible confound in measuring error-uncertainty correlations as shorter segments provide inherently less precision for orientation. ) After the subject responded, the mask and cue disappeared
and a small segment appeared at the tested location with the orientation chosen by the subject and
with a gray wedge around the line segment with a width (subtended angle) corresponding to the
reported uncertainty. (The true orientation of the segment was not displayed.) This feedback display
appeared for 500 msec, after which a new trial began.
To enhance the quality of subjective uncertainty estimation, a scoring function was used to assess
subjects’ performance on each trial. Subjects were instructed that their goal was to maximize their
score which was calculated by combining the accuracy and certainty of their response. As a scoring
function, we used the log probability of the true stimulus orientation under a circular Gaussian (von
Mises) distribution defined by the subject’s response (segment orientation – mean, wedge width –
concentration). This scoring function can be maximised if the subject’s uncertainty report reflects
their true subjective uncertainty which in turn is predictive of the errors they are making [18]. To
prevent subjects from developing simple feedback-based strategies while keeping them alert, subjects received only grouped feedback after every 10 trials in the form of an average score. Subjects
completed 3-4 sessions of 900 trials across multiple days. To familiarize themselves with the procedure and to facilitate the precision of mapping from uncertainty to line length, prior to each test
session subjects had a practice session with 50 trials during which they received feedback after every
trial including the true orientation of the cued segment. Data from these trials was not included in
the analyses.

4

Results

We collected data from a total of N = 5 subjects, four of whom were naive while the last one was
informed about the goal of the experiment. We found no difference in performance between the
naive and informed subjects confirming that the paradigm measured direct reactions of the subjects
without much cognitive influence.
4

o

120

90o

o

o

o

180

1000

120
o

0

counts

500

1000

o

60

o

30

500

90o

o

60

150

B

reported orientations

o

150
o

0

o

180

1000

30

500

0

counts

500

1000

o

0

C

180

120

60

0

normalized y coordinate

stimulus orientations

response angle (deg)

A

0.5

0

−0.5
0

60

120

stimulus angle (deg)

180

0

0.5

1

normalized x coordinate

Figure 3. Control measures. The experiment gives veridical trial-by-trial information about subjects’ error and subjective uncertainty. A. The distributions of the test line segments’ true and reported orientation. B. Trial-by trial correspondence between the line segments’ true and reported
orientation across all subjects and trials. C. Trajectories of strokes for all subjects normalized (rotated and scaled) such that they go from (0,0) to (1,0).
4.1

Basic measures and controls

First, we checked whether in our paradigm we measured the relevant aspects of human performance.
In order to measure the typical pattern of trial-by-trial error and uncertainty, the stimuli must cover
the entire space of orientation, the subject’s perception needs to follow the true stimuli, and response movements need to be ballistic. Figure 3 confirms that these requirements were fulfilled.
The stimulus distribution was nearly uniform in the space of orientations, and the subjects’ response
was similar without any evidence for a bias in the cardinal directions (Fig 2A). Subjects’ judgement
faithfully followed the true orientation of the target line segment (Pearson’s r=0.97, p<10−3 ) (Fig
2B), and their stroke was a straight line with average deviation from the straight line between the
starting and endpoints below 3.3±0.2% of the length of the stroke (Fig 2C). In addition, we calculated the time profile of the strokes and found that subjects’ mean duration of drawing was 450±110
msec (mean±s.e.) with standard deviation of 160±30 msec. This suggests that subjects drew the
line segments with a fast, single stroke without much fine-tuning, explicit cognitive deliberation, or
modulation by different aspects of the task.
4.2

The representation of error and uncertainty

Next, we tested whether subjects’ uncertainty reports were predictive of their estimation errors.
Figure 4 shows a typical subject’s result with trials binned by reported uncertainty and the resulting
error histograms fitted with a circular Gaussian. The error distribution remained centered at zero,
but showed a clearly increasing spread as the subject’s subjective report of uncertainty about the
correctness of the trial increased.
Figure 5 shows the same fitted circular Gaussians to each subject together with the underlying scatter
plots of (absolute) error vs. reported uncertainty. Despite individual variations, each subject showed
the same general relation of increasing uncertainty corresponding to steadily increasing error in
their performance. This suggests that subjects had a reliable representation of the quality of their
perceptual information and faithfully reported this through their stroke. Thus, our experimental
paradigm and response method successfully captured subjects’ trial-by-trial error and uncertainty.
4.3

The effect of task difficulty on error, uncertainty and their correlation

We investigated how task difficulty affects subjects’ error level, uncertainty and the correlation between the two. As task difficulty increased either by increasing the number of line segments in
the display or by decreasing the contrast of the target segment, both the error rate and uncertainty
of the judgment increased significantly (Figure 6, 1st column, top two panels; the absolute value
of Spearman’s ρ was between 0.60 and 0.95, with p<0.002 in all cases). Expressed in terms of
reaction times (RTs), we found the same trend: as RTs increased, presumably due to finding the
trial more difficult either because of the nature of the sensory input (more segments, less contrast)
or for some other reason (e.g. lapse of attention, or focusing on an irrelevant part of the display),
both subjects’ errors and level of uncertainty increased significantly (Figure 6, 1st column, bottom
panel; Spearman’s ρ>0.87, with p<0.001 in both cases). Thus all three manipulations had a significant modulatory effect on both error and uncertainty of subjects’ response. The effect of contrast
5

Bin: 0o− 15o
N = 898

frequency

1

Bin: 15o− 30o
N = 379

1

Bin: 30o− 45o
N = 122

1

Bin: 45o− 60o
N = 37

1

Bin: 60o− 75o
N = 24

1

0.8

0.8

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.2

0
-90

0
-90

90

0

signed error (deg)

90

0

0
-90

signed error (deg)

0

90

signed error (deg)

0
-90

0.2

0
-90

90

0

signed error (deg)

Bin: 75o− 90o
N = 42

1

90

0

0
-90

signed error (deg)

0

90

signed error (deg)

Figure 4. Relation between error and subjective uncertainty in a single subject. Signed orientation estimation errors fitted with a circular Gaussian in trials binned according to the reported
uncertainty (labels on top).

frequency

A

Subject 1

Subject 2

Subject 3

Subject 4

1

1

1

1

1

0.8

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0
−90

0

90

0
−90

signed error (deg)

0

90

0
−90

signed error (deg)

0

90

bins (deg):

0 − 15
15 − 30
30 − 45
45 − 60
60 − 75
75 − 90

0.2

0
−90

signed error (deg)

Subject 5 uncertainty

0

90

0
−90

signed error (deg)

0

90

signed error (deg)

error (deg)

B
90

90

90

90

90

60

60

60

60

60

75

75

45

75

45

30

45

30

15

45

30

15

0

75

45

30

15

0

75

30

15

0

15

0

0

0 15 30 45 60 75 90

0 15 30 45 60 75 90

0 15 30 45 60 75 90

0 15 30 45 60 75 90

0 15 30 45 60 75 90

uncertainty (deg)

uncertainty (deg)

uncertainty (deg)

uncertainty (deg)

uncertainty (deg)

Figure 5. Relation between error and subjective uncertainty across all subjects. Subjects’
orientation estimation error changed according to their subjective uncertainty. A. Circular Gaussian
fit of orientation estimation error histograms corresponding to different levels of reported uncertainty
(as in Fig. 4). B. Scatter plots of (absolute) error vs. uncertainty. Error bars show the mean±standard
deviation for the data points in each uncertainty bin, gray lines represent linear regression fitted to
the scatterplot. Note that it is the mean of absolute error plotted in panel B that is related to the
standard deviation of signed error in panel A.

is particularly remarkable, as each line segment in a display had a different contrast level. Thus, the
fact that the contrast level of the target line segment had an effect indicates that subjects possessed
a multivariate representation of uncertainty, whereby they represented their uncertainty for multiple
objects in the scene individually, rather than having just a single summary measure of uncertainty
about the whole scene.
In contrast, task difficulty did not have an effect the predictive relationship between error and uncertainty. We quantified the effect of task difficulty on the error-uncertainty regression (gray lines
in Fig. 5B) by first selecting trials based on either number of line segments, contrast level, or RT
duration for each subject, and then taking the slope and intercept of the regression line fitted to data
collected in these trials (Figure 6, right two columns). Neither the slope nor the intercept were sensitive significantly to changes in any of the three parameters of contrast, line number, or RT (p>0.24
for Spearman’s correlation in all but one case, for which p>0.098, i.e. still non-significant). This
means that while task difficulty reliably influenced subjects errors and uncertainty levels, it left the
calibration between the two essentially unaltered, suggesting that subjects used a single, universal
internal scale of uncertainty – in line with theories of probabilistic representations.
6

14
12
10
8
6

(deg)

1

2

3

(deg)

error
uncertainty

4

line number

5

0.3
0.2
1

2

3

4

line number

5

8

0.3
0.2
10−20

30−40

50−60

70−80

10

6

5

6

10−20

30−40

50−60

70−80

90−100

contrast level (%)
8

(deg)

0.5
0.4
0.3
0.2

6
4
2

0.1
0

4

4
0

90−100

(deg)

10

(deg/deg)

20

(deg)

14

3

line number

6

contrast level (%)

30

reaction time (msec)

2

2

0.1

contrast level (%)

500 600 700 800 900 1000 1100

1

8

0.4

0

10−20 30−40 50−60 70−80 90−100

4
0

6

(deg)

18

(deg/deg)

(deg)

9

(deg)

10

20

6
2

0.1

0.5

22

16

8

0.4

0

6

intercept of error − uncertainty regression

0.5

(deg)

30
25
20
15
10

slope of error − uncertainty regression
(deg/deg)

average uncertainty & error

500

600

700

800

900

1000

reaction time (msec)

1100

0

500

600

700

800

900

1000

reaction time (msec)

1100

Figure 6. Average error and uncertainty depend on task difficulty, but not the calibration of
uncertainty against error. Top row: dependencies on the number of line segments. Middle row:
dependencies on the contrast level of the target segment. Bottom row: dependencies on reaction
time. Left column: average error rates and uncertainty levels. Middle column: the slope of the
error-uncertainty regression. Right column: the y-intercept of the error-uncertainty regression.
4.4

Across-trial correlation between error and uncertainty as a function of stimulus
presentation time

To test the main prediction of the present paper, we analyzed error, uncertainty and the correlation
between the two as a function of presentation time. Both subjects’ errors and their uncertainty decreased significantly until around 133 msec (Figure 7A, shaded area) and then remained constant for
the rest of the duration of the trial (Figure 7A, area without shading). Despite this coordinated decrement in error and uncertainty, the correlation between the two did not increase but instead decreased
significantly, as the derivations overviewed in Section 2 predicted it both for EI and PS (Figure 7B,
shaded area). However, immediately after error and uncertainty asymptotes in Figure 7A, a rapid
increase in the correlation began until the curve saturated after 400 msec of presentation time (Figure 7B). A one-way ANOVA including data from beyond the cutoff point showed that the increase
in correlation was significant (p< 0.037) and specifically the correlation at time points at the trough
of the dip (167-200 msec) were significantly lower than those at the plateau (300-400-600 msec)
of the curve (p<0.0014). These results are in close match with the prediction of the PS model and
cannot be explained by the EI model.

5

Discussion

In this paper, we
1. presented a new hypothesis about the role of time in perceptual decision making based on
the idea of sequential approximate probabilistic computation in the cortex;
2. derived models and analytical predictions as to how to detect behaviorally whether this
proposed role or evidence integration dominates during a decision making task;
3. developed a novel paradigm for testing the models’ predictions; and
4. after conducting the experiment, we confirmed that, indeed, humans show the hallmarks of
probabilistic sampling in perceptual decision making tasks.
We envision two aspects of our contribution to be of significance for the field of computational
neuroscience. First, our paradigm involved orientation estimation, arguably one of the simplest
perceptual decision making tasks, and our results showed that the uncertainty-error correlation is
7

mean of error & uncertainty (deg)

A

error − uncertainty correlation

B

25

*

uncertainty
error

ns.
20
15

*

ns.

10
5

0.6
0.5

50 75 100

(

*

133

167

200

300
400
presentation time (msec)

600

*

)

0.4
0.3
0.2
0.1

50 75 100

133

167

200

300

400

600

presentation time (msec)

Figure 7. Presentation time-dependence of the trial-by-trial correlation between error and
uncertainty. The rising part of the correlation curve confirms probabilistic sampling during the task.
A. Average error (red) and uncertainty (blue) curves as a function of presentation time. The shaded
area represents the cutoff point (obtained independently for error and uncertainty, but coinciding
at the same presentation time) above which there was no significant change in values between two
neighboring points of the curves. Cutoff points were determined by recursively calculating the
significance of the difference between correlations computed from data at a particular presentation
time vs. data at all longer presentation times. Cutoff points were defined as the first time point (going
backwards from the last time point) at which this comparison yielded a significant difference. B.
Average correlation between error and uncertainty as a function of presentation time. The shaded
area is inherited from panel A. Error bars in both panels represent s.e.m.

not affected by dramatic changes in task difficulty, such as 80% contrast variation or 3-fold increase
in the number of potential targets. This indicates that uncertainty is not an “add-on” cognitive
metric that is assessed post-perceptually by a separate process, but rather an essential aspect of
sensory representation that is inherently encoded together with the feature value of the input from
the very beginning of the processing. This result provides a much needed extension of earlier studies
proposing approximate probabilistic coding in the cortex by generalizing those claims down to the
level of early sensory perception.
Second, our new paradigm allows us to investigate the effect of sampling-based probabilistic coding
with higher sensitivity than it was possible before. Earlier works explored the issue of sampling with
methods that used probability matching as the assay for sampling-based probabilistic coding, and
hence they could only hope to detect sampling when it is using one or very few samples [19–21].
This technical difficulty led to some confusions as to whether one can see evidence of sampling in
particular tasks. With our method, the effect of sampling can still be tracked in the regime of a few
hundred samples. Indeed, a rough estimate based on our results suggests that in the present task,
samples might arrive as fast as one every couple of milliseconds. This number is feasible only if the
cortex uses an overcomplete representation. Nevertheless, even this speed presents no insurmountable problem to our experimental assay, opening the possibility of conducting experimental tests of
probabilistic representations in a wide variety of contexts.
It is important to clarify that even though PS has been introduced in this paper as an alternative
to EI, the two are not mutually exclusive and not even clearly separable in time. Sampling starts
from the first moment evidence is obtained, and evidence integration does not “stop” at some point,
only its benefit vanishes. Thus, an obvious follow-up in our research program is to analyze the
relationship between evidence integration and probabilistic sampling in various tasks to shed light
8

on the common and separate aspects of the two processes. Furthermore, although we analyzed PS
as a particular approximate inference method the cortex may employ, there could be other such
candidates, and it will be interesting to see whether other methods that also iteratively refine the
approximate posterior, e.g. (loopy) belief propagation or expectation propagation, result in similar
temporal trends in the error-uncertainty correlation.
Acknowledgments
We thank useful discussions with R. Haefner, M. Sahani, P. Latham, P. Dayan, D. Wolpert, G. Orbán,
P. Berkes, and J. Solomon. This work was supported by an NSF (MP, JF, grant no. IOS-1120938), a
Marie-Curie grant by the EC (JF, grant no. CIG 618918) Wellcome Trust (ML).
References
1. Edwards, W. Psychol Bull 51, 380–417 (1954).
2. Smith, P.L. & Ratcliff, R. Trends Neurosci. 27, 161–168 (2004).
3. Ma, W.J., Beck, J.M., Latham, P.E. & Pouget, A. Nat. Neurosci. 9, 1432–1438 (2006).
4. Gold, J.I. & Shadlen, M.N. Annu. Rev. Neurosci. 30, 535–574 (2007).
5. Ratcliff, R. & McKoon, G. Neural Comput. 20, 873–922 (2008).
6. Churchland, A.K. & Ditterich, J. Curr. Opin. Neurobiol. 22, 920–926 (2012).
7. Tsetsos, K., Usher, M. & Chater, N. Psychol. Rev. 117, 1275–1293 (2010).
8. Knill, D.C. & Richards, W. Perception as Bayesian Inferences (Cambridge University Press,
1996).
9. Pouget, A., Dayan, P. & Zemel, R.S. Annu. Rev. Neurosci. 26, 381–410 (2003).
10. Ernst, M.O. & Banks, M.S. Nature 415, 429–433 (2002).
11. Körding, K.P. & Wolpert, D.M. Nature 427, 244–247 (2004).
12. Courville, A.C., Daw, N.D. & Touretzky, D.S. Trends Cogn. Sci. 10, 294–300 (2006).
13. Pouget, A., Beck, J.M., Ma, W.J. & Latham, P.E. Nat. Neurosci. 16, 1170–1178 (2013).
14. Berkes, P., Orbán, G., Lengyel, M. & Fiser, J. Science 331, 83–87 (2011).
15. Yang, T. & Shadlen, M.N. Nature 447, 1075–1080 (2007).
16. Fiser, J., Berkes, P., Orbán, G. & Lengyel, M. Trends in Cognitive Sciences 14, 119–130 (2010).
17. MacKay, D.J.C. Information theory, inference, and learning algorithms (Cambridge University
Press, 2003).
18. Jaynes, E.T. Probability Theory: the Logic of Science (Cambridge University Press, 2003).
19. Drugowitsch, J., Wyart, V. & Koechlin, E. Inference rather than selection noise explains behavioral variability in perceptual decision making. In Cosyne, (2013).
20. Acuna, D., Berniker, M., Fernandes, H. & Körding, K. An investigation of how prior beliefs
influence decision-making under uncertainty in a 2afc task. In Cosyne, (2013).
21. Vul, E., Goodman, N., Griffiths, T.L. & Tenenbaum, J.B. Cogn Sci 38, 599–637 (2014).

9

Supplementary Information
1

Introduction

We consider an estimation task, in which subjects have to estimate an environmental variable, x ∈ R
(which we assume to be scalar for simplicity). In each trial, the subject receives partial information
about the true value of x, x∗ , through making observation y and is asked to report two things:
1. their best estimate of x∗ – assuming that they optimise a squared error penalty between this
estimate and x∗ ;
2. their subjective uncertainty about this estimate, which we take to be their expected squared
error between their estimate and x∗ .
We will be interested in how subjective uncertainty, the actual squared error of the estimate, and the
correlation between these two quantities change as a function of time (within in a trial) – where time
is going to parametrise some key aspects of the model.

2

Exact representation of uncertainty

A complete representation of uncertainty means that the subject represents Px (x|y), a posterior
distribution over x given the partial information available to them in y. First, in this section, we
consider the case when the amount of information received about x does not change over the trial,
ie. Px (x|y) remains fixed, and the representation of uncertainty is exact, i.e. the relevant parameters
of Px (x|y) are directly and exactly accessible by the subject. This will then serve as the starting point
for derivations regarding evidence integration (EI, sections 3 and 6) and probabilistic sampling (PS,
sections 4-5).
We introduce the following quantities to characterise the statistics of x under Px (x|y):
µx = Ex [x]
h
i
2
σx2 = Ex (x − µx )
h
i
3
ξx = Ex (x − µx ) /σx3
h
i
4
κx = Ex (x − µx ) /σx4 − 3

mean

(1)

variance

(2)

skewness

(3)

excess kurtosis

(4)

(As an example, ξx = κx = 0 when Px (x|y) is a normal distribution.)
Note that, in general, Px (x|y) is going to be different on each trial using the same x∗ because the
particular information given by the observation y is different. Thus, the moments of Px (x|y) will
also be different on each trial, that is, importantly, they are random variables themselves. For simplicity, we will assume in the following that while µx and σx2 change across trials, ξx and κx remain
constant – i.e. the posterior only changes in its location and scale but not in its shape otherwise.
2.1

Task difficulty, γ, and properties of the posterior mean, µx , and variance σx2

An exact (and consistent) representation of uncertainty means that the true value of x, x∗ , behaves
as if it was actually sampled from Px (x|y). To simplify notation, however, and without loss of
generality, we will assume that on each trial x∗ = 0 and shift the reference frame for x accordingly.
In order to minimise squared error, the best estimate of x∗ the subject can report is their posterior
mean, µx . Remember (see above), that µx is itself a random variable. In fact, given our shift of
reference frame, the distribution of µx around x∗ = 0 is the mirror image of the distribution of x
around µx , ie. P(µx = x0 ) = Px (µx − x0 ).
10

We define the variance of µx as the (squared) difficulty of a trial, γ. This implies that the mean,
variance, and fourth (central) moment of µx are
Eµx |γ [µx ] = x∗ = 0
(5)
 2
2
Eµx |γ µx = γ
(6)
 4
4
Eµx |γ µx = (κx + 3) γ
(7)
where we have made use of the fact that the distribution of µx is the ‘mirror image’ of the posterior
(see above) and that it is centred on x∗ = 0 and thus its variance is equal its mean squared error.
Note that, in general, γ itself might also change from trial-to-trial, as some trials might be more or
less difficult, or we may be paying more or less attention to the stimuli, so we will treat it also as a
random variable with the following statistics:
Eγ [γ] = µγ
(8)
Vγ [γ] = σγ2
 
Eγ γ 2 = µγ 2 = µ2γ + σγ2
 
Vγ γ 2 = σγ22

(9)
(10)
(11)
2

It will also be useful later to define the squared coefficient of variation of γ :

ωγ22

=

σγ22 /µ2γ 2

≥ 0.

A consistent representation of subjective uncertainty should be predictive of the errors the subject
makes. Ideally thus, in the above estimation task, the reported uncertainty should be σx2 , the variance
of the posterior, which will be equal to the variance of µx , which in turn by definition is γ 2 .
σx2 |γ = γ 2
(12)
which in turn entails that the following is true for any k:
 
(13)
Eσx |γ σxk = γ k
It is also useful to note that the same equality (ie. that the distribution of σx2 given γ is a delta
distribution) also implies that σx2 and µx are independent given γ.
2.2

Some useful identities



We use the formula EX [f (X)] = EY EX|Y [f (X)] to derive the following.


Eµx [µx ] = Eγ Eµx |γ [µx ]

= Eγ [0]

=0
(14)

 
 

Eµx µ2x = Eγ Eµx |γ µ2x

 
= Eγ γ 2

= µγ 2
(15)

 

 
Eµx µ4x = Eγ Eµx |γ µ4x



= Eγ (κx + 3) γ 4

= (κx + 3) µ2γ 2 + σγ22
(16)

 

 
Eσx σx2 = Eγ Eσx |γ σx2

 
= Eγ γ 2

= µγ 2
(17)

 

 
Eσx σx4 = Eγ Eσx |γ σx4

 
= Eγ γ 4

= µ2γ 2 + σγ22
(18)







 
 
Eµx ,σx µ2x σx2 = Eγ Eµx ,σx |γ µ2x σx2 = Eγ Eµx |γ µ2x Eσx |γ σx2



= Eγ γ 2 γ 2

= µ2γ 2 + σγ22
(19)







 
Eµx ,σx µx σx3 = Eγ Eµx ,σx |γ µx σx3 = Eγ Eµx |γ [µx ] Eσx |γ σx3



= Eγ 0 γ 3

=0
(20)

2.3

Performance

The average squared error for an exact representation of uncertainty is simply the squared error of
the mean that is
 
ε = Eµx µ2x = µγ 2
(21)
11



2.4

Correlation between error and uncertainty

Now, we can ask the question how much the error in the estimate is going to correlate with the
subjective representation of uncertainty. For this we first compute the covariance of these two quantities:




 
 
Cµx ,σx µ2x , σx2 = Eµx ,σx µ2x σx2 − Eµx µ2x Eσx σx2

= µ2γ 2 + σγ22 − µγ 2 µγ 2

= σγ22 (22)
(23)

Likewise, we can compute the respective variances:
 
 
 2

Vµx µ2x = Eµx µ4x − Eµx µ2x
= (κx + 3) µ2γ 2 + σγ22 − µ2γ 2
 
 2
 
Vσx σx2 = Eσx σx4 − Eσx σx2

= µ2γ 2 + σγ22 − µ2γ 2

= (κx + 3) σγ22 + (κx + 2) µ2γ 2
(24)
= σγ22

(25)

So the correlation between squared error and subjective uncertainty is
σγ22
ρ = r

(κx + 3) σγ22 + (κx + 2) µ2γ 2 σγ22
ρ= q

1
κx + 3 + (κx + 2) ωγ−2
2

(26)

(27)

There are three things to note about this result:
1. The correlation is not going to be 1 in the general case, even though we are talking about
an exact representation of uncertainty. How can that be? This is because only the expected
value of the squared error, not the squared error itself is predicted by uncertainty, otherwise
we could be cheating (i.e. if we could predict the error itself, than we might as well subtract
it from our estimate thus making our error zero).iii
2. The correlation is going to be zero when ωγ22 → 0, so we need trials with varying difficulty
to get meaningful results. (This is was expected intuitively, as ωγ22 → 0 means that all
trials have the same difficulty, and associated posterior variance, and therefore the same
uncertainty estimate – and one obtains a trivial zero correlation between two quantities one
of which does not ever change.)
3. Nothing changes with time here – simply because we ascribed no role for time in the model.

3

Evidence integration

We now consider the case of a probabilistic representation that is still exact, as before, but it is
representing a posterior distribution that is changing in time because it is conditioned on increasing
amounts of data, Px (x|y1 . . . yn ).
In general, the variance of such a posterior scales inversely with n, the number of data points. Using
the equivalence of difficulty, γ 2 and posterior variance σx2 described in the previous section, this
means that the “effective difficulty” of trials in which n data points have been observed can be
expressed as
γn2 = γ 2 /n
(28)
iii
Well, we wouldn’t quite be cheating in a special case: when the error is known only up to its sign. In that
case, the squared error is known exactly but we still can’t subtract away our error because we don’t know its
sign. Indeed, in that case the distribution of µx is a mixture of two delta distributions (with equal weights)
sitting at equal distances from zero, and thus κx = −2, in which case the formula in Eq. 27 correctly predicts
ρ = 1.

12

where we use γ 2 here to describe the difficulty of trials with a single data point. In turn, this means
that the relevant moments characterising the distribution of γn2 scale as
µγn2 = µγ 2 /n

(29)

σγ2n2

(30)

=

σγ22 /n2

Note that this scaling implies that the coefficient of variation, which ultimately matters for the analysis of error-variance correlations remains constant, and so
ωγ2n2 = ωγ22

(31)

Beside the variance, the (excess) kurtosis of Px (x|y1 . . . yn ) is also expected to change and converge
towards zero, as the posterior tends to converge to a normal distribution with increasing amounts of
data (modulo degenaracies) – this is the argument on which the usual Laplace approximation is
based. We will make the assumptioniv that the kurtosis of the distribution also scales as 1/n
κn = κx /n

(32)

where we (re-)use κx to denote the kurtosis of the posterior after observing a single data point.
Putting the above scalings together and substituting them to the results of the previous section, we
can now derive the evolution of estimation errors and error-uncertainty correlations in the context of
evidence integration as the following:
µγ 2
ε=
(33)
n
1
ρ= q
(34)
κx /n + 3 + (κx /n + 2) ωγ−2
2

4

Approximate representations of uncertainty

We now return to the case of a static posterior distribution. In an approximate representation of
uncertainty, the posterior mean and variance are not directly accessible, only their approximate
estimates, µ̂x and σ̂x2 , respectively, which can be noisy and / or biased.
In this case, the performance and correlation between the reported uncertainty and actual squared
error can be computed as
 
 

(35)
ε = Eµ̂x µ̂2x
= Eµx ,σx Eµ̂x |µx ,σx µ̂2x
 2 2
Cµ̂ ,σ̂ µ̂ , σ̂
ρ= p x x x x
(36)
Vµ̂x [µ̂2x ] Vσ̂x [σ̂x2 ]
where
 
 
 2
Vµ̂x µ̂2x = Eµ̂x µ̂4x − Eµ̂x µ̂2x
 
 
 2
Vσ̂x σ̂x2 = Eσ̂x σ̂x4 − Eσ̂x σ̂x2




 
 
Cµ̂x ,σ̂x µ̂2x , σ̂x2 = Eµ̂x ,σ̂x µ̂2x σ̂x2 − Eµ̂x µ̂2x Eσ̂x σ̂x2


 

 2
= Eµx ,σx Eµ̂x |µx ,σx µ̂4x − Eµx ,σx Eµ̂x |µx ,σx µ̂2x
(37)

 4 

 2
= Eµx ,σx Eσ̂x |µx ,σx σ̂x − Eµx ,σx Eσ̂x |µx ,σx σ̂x2
(38)

 2 2 
= Eµx ,σx Eµ̂x ,σ̂x |µx ,σx µ̂x σ̂x −

 

 
− Eµx ,σx Eµ̂x |µx ,σx µ̂2x Eµx ,σx Eσ̂x |µx ,σx σ̂x2
(39)

Therefore, in order to be able to compute these quantities of interest, we need to know the following
five statistics of the approximate representation:
 
 
Eµ̂x |µx ,σx µ̂2x
Eµ̂x |µx ,σx µ̂4x
(40)
 2
 4
Eσ̂x |µx ,σx σ̂x
Eσ̂x |µx ,σx σ̂x
(41)
 2 2
Eµ̂x ,σ̂x |µx ,σx µ̂x σ̂x
(42)
iv

See Appendix.

13

A subtle note about approximate representations: remember, that the posterior variance was only
relevant for the subject in the context of the estimation task inasmuch as it predicted the average
squared error between the posterior mean and the true stimulus value – which was the case for an
exact representation of uncertainty. However, for an approximate representation of uncertainty, the
total estimation error is compounded by the fact that the posterior mean itself is also only estimated.
Thus, ideally, the subject should estimate their own estimation error of the posterior mean, based on
their estimate of the posterior mean and variance, and add it to the estimate of the posterior variance,
and report this total error estimate, σ̃x2 , as their uncertainty. We will return to this point later in the
context of sampling-based representations in section 5.4.

5

Sampling-based representations

We now consider the case of sampling-based representations, that is when we do not have direct
access to the relevant parameters of the posterior, Px (x|y), such as µx and σx2 , but only to a finite
number, N , of samples xi , i = 1 . . . N , that are iid. according to Px (x|y). We will start by considering some statistical properties of these samples that will be useful for the derivations later. We
will then derive unbiased estimators that estimate the mean and variance of the posterior from the
samples. Next, as a side, we will analyse the basic statistical properties of these estimators, namely
their variances and covariance (and correlations). Then, we will derive the correct measure of uncertainty in the context of a sampling based estimator, that takes into account its own estimation errors,
and derive the statistical properties of the mean and uncertainty estimate that are directly relevant
for computing the performance and error-variance correlation. Finally, by combining results from
previous sections we will derive how performance and error-variance changes with the number of
samples.
5.1

Statistics of the samples

The fact that xi , i = 1 . . . N are identically distributed according to Px (x|y) of course entails that
the mean and variance (and other moments) of any individual sample will match the corresponding
moments of the posterior such that
Ex [xi ] = µx

(43)

Vx [xi ] = σx2
h
i
h
i
3
3
Ex (xi − µx ) = Ex (x − µx ) = ξx σx3
h
i
h
i
4
4
Ex (xi − µx ) = Ex (x − µx ) = (κx + 3) σx4
5.2

(44)
(45)
(46)

Unbiased estimators of the mean and variance of the posterior

We will use standard unbiased estimators (and their extensions) for estimating the posterior mean
and variance from the iid. samples. (As a consequence, the squared error of these estimators will be
their variance, so we will use these terms interchangeably in this context.) From here on, to simplify
the algebra, we will assume (without loss of generality) that µx = 0 – unless otherwise noted. (Note
that this does not imply µ̂x = 0.)
The unbiased estimator for the mean of the posterior is simply the sample mean:
µ̂x =

N
1 X
xi
N i=1

(47)

and the unbiased estimator for the variance is the sample variance:
σ̂x2

N
X
1
1
2
=
(xi − µ̂x ) =
N − 1 i=1
N −1

14

N
X
i=1

!
x2i

−N

µ̂2x

(48)

5.3
5.3.1

Basic statistics of the sample mean and variance
Variance of the sample mean
 
2
Vx [µ̂x ] = Ex µ̂2x − (Ex [µ̂x ])
1 XX
= 2
Ex [xi xj ] − µ2x
N i j


N
N
X
X
1 
= 2 N Vx [xi ] +
Cx [xi , xj ]
N
i=1

(49)
(50)

(51)

j=1,j6=i

=
5.3.2

1 2
σ
N x

(52)

Variance of the sample variance

In the following, we will make use of the following identities – assuming µx = 0:
N X
N
X



Ex x2i x2j = N (κx + 3) σx4 + N (N − 1) σx4

(53)



Ex x2i xj xk = N (κx + 3) σx4 + N (N − 1) σx4

(54)

Ex [xi xj xk xl ] = N (κx + 3) σx4 + 3N (N − 1) σx4

(55)

i=1 j=1
N X
N
N X
X
i=1 j=1 k=1
N
N X
N X
N X
X
i=1 j=1 k=1 l=1

And so the variance of the sample variance is (omitting several lines of tedious but trivial algebra)
 
 
 2
Vx σ̂x2 = Ex σ̂x4 − Ex σ̂x2


κx
2
=
+
σx4
N
N −1
5.3.3

(56)
(57)

Covariance between the sample mean and the sample variance

In the following, we will make use of the following identities:
N X
N
X



Ex x2i xj = N ξx σx3

(58)

Ex [xi xj xk ] = N ξx σx3

(59)

i=1 j=1
N X
N X
N
X
i=1 j=1 k=1

So the covariance between the sample mean and sample variance is (again omitting several lines of
tedious but trivial algebra)




 
Cx µ̂x , σ̂x2 = Ex µ̂x σ̂x2 − Ex [µ̂x ] Ex σ̂x2
(60)
ξx 3
σ
(61)
=
N x
Note that this covariance is 0 if the posterior is normal, or in general when it is symmetric (because
ξx = 0).
15

Although we don’t need this in the following, but it may be useful to note that the correlation between
the sample mean and variance is thus


Corrx µ̂x , σ̂x2 = r

ξx
N
1
N

σx2

ξx
=q
κx +
5.4



κx
N

σx3
+

(62)
2
N −1



σx4
(63)

2N
N −1

An improved estimator of uncertainty

Remember that the objective of a representation of uncertainty is that it should be predictive of the
squared error of one’s estimate. In a sampling based representation the best estimator (in a squared
loss sense) is the sample mean, µx . The error of this estimator is


 
 
1 2
1
Ex,µx µ̂2x = Eµx µ2x +
σx = 1 +
σx2
(64)
N
N
Of course, we don’t have direct access to σx2 , but we have an unbiased estimator for it, σ̂x2 , using
which we can derive the improved estimator of uncertainty as


1
2
σ̂x2
(65)
σ̃x = 1 +
N
5.5
5.5.1

Statistical properties relevant for performance and correlation analysis
Statistics of the sample mean

In this section, we will make use of the following identities – without assuming µx = 0:
Ex [xi ] = µx
 
Ex x2i = σx2 + µ2x
 
Ex x3i = ξx σx3 + 3µx σx2 + µ3x
 
Ex x4i = (κx + 3) σx4 + 4ξx µx σx3 + 6µ2x σx2 + µ4x

(66)
(67)
(68)
(69)

Now we can derive the relevant statistics of the sample mean (once again omitting several lines of
tedious but trivial algebra):
N
N
 2
1 XX
Ex [xi xj ]
Ex µ̂x = 2
N i=1 j=1

= µ2x +

(70)

1 2
σ
N x

(71)
(72)

and
N
N
N
N
 4
1 XXXX
Ex µ̂x = 4
Ex [xi xj xk xl ]
N i=1 j=1

(73)

k=1 l=1

= µ4x +
5.5.2

6 2 2
4
κx + 3N 4
µ σ +
ξx µx σx3 +
σx
N x x N2
N3

(74)

Statistics of the improved uncertainty estimator
 
Ex σ̃x2 =
 
Ex σ̃x4 =


1+


1
N

1
1+
N



 
Ex σ̂x2

2

 
Ex σ̂x4


=

1+


=

16

1
N

1
1+
N



σx2

2 

(75)
κx
2
1+
+
N
N −1



σx4

(76)

5.5.3

Joint statistics of the sample mean and the improved uncertainty estimator

(We once again omit several lines of tedious but trivial algebra.)


 2 2


1
Ex µ̂x σ̃x = 1 +
Ex µ̂2x σ̂x2
N




1  κx
1
+ 1 σx4 + 2 ξx µx σx3 + N µ2x σx2
= 1+
N N
N
5.6

(77)
(78)

Performance and error-uncertainty correlation with a sampling-based representation

Note that the quantities
 
Ex µ̂2x (Eqs. 70-71)
 
Ex σ̃x2 (Eq. 75)

 
Ex µ̂4x (Eqs. 73-74)
 
Ex σ̃x4 (Eq. 76)


Ex µ̂2x σ̃x2 (Eqs. 77-78)

derived in the previous section are exactly the quantities (using slightly different notation as required
by the different contexts)
 
 
Eµ̂x |µx ,σx µ̂4x
Eµ̂x |µx ,σx µ̂2x
 
 
Eσ̃x |µx ,σx σ̃x4
Eσ̃x |µx ,σx σ̃x2


Eµ̂x ,σ̃x |µx ,σx µ̂2x σ̃x2
that are required to compute the error and error-variance correlation of sampling as an approximate
representation in section 4.
So, by making the appropriate substitutions (also from section 2.2), we can derive the mean squared
error of a sampling based representation as
 
ε = Eµ̂x µ̂2x
(79)


1
µγ 2
(80)
= 1+
N
Similarly, the variance of the error and the uncertainty are:
 2
 

 

Vµ̂x µ̂2x = Eµx ,σx Eµ̂x |µx ,σx µ̂4x − Eµx ,σx Eµ̂x |µx ,σx µ̂2x




1
1
κx
= 1+
σγ22 + σγ22 + 2 µ2γ 2 + σγ22
N
N
N

(81)
(82)

And so, finally, the error-variance correlation is
1+
ρ = r

κx + 3 + (κx + 2) ωγ−2
2 +

1
N




6 + 4 ωγ−2
+
2



κx
+N
ωγ−2
2 + 1
2



 

κx
3 + 2 ωγ−2
+N
ωγ−2
1 + κNx +
2
2 + 1
3

1
N

1
N2

2
N −1



(83)
The following are worth noting about these results:
1. The squared error, ε, decays as 1/N to the squared error of an exact representation of
uncertainty, µγ 2 .
2. The behaviour of the correlation ρ is more complicated and depends on two quantities:
ωγ 2 , quantifying how variable is the difficulty of trials across the experiment, and κx , the
(excess) kurtosis of the posterior.
As a brief summary of the time-dependence of ρ, we can note the following:
17

ωγ−2
2 + 1



1. The correlation, ρ is not always monotonically increasing as a function of the number of
samples, N . In particular, it can be a (transiently) decreasing function of N when the
following two conditions are satisfied:
(a) N is small,
(b) the kurtosis of the posterior, κx is greater than zero.
2. Importantly, the transient decrease in ρ is shorter for higher values of ωγ 2 (darker blues),
that is, if trials are sufficiently variable in terms of their difficulty, ωγ 2 > 1. In this case,
the transient is over by the time performance, measured by ε, converges.
3. After this transient, ρ is indeed a monotonically increasing function of N and converges
from below to the correlation exhibited by an exact representation of uncertainty.
4. The time constant of the convergence of ρ can be substantially longer than the time constant
of the convergence of ε, in particular for higher values of κx and lower values of ωγ 2 . This
means that the in general, ρ will continue to increase when ε is not discernibly changing
anymore.
Note that each of these features seem to be borne out in the data:
1.
2.
3.
4.

6

There is a transient decrease in ρ.
This transient is over by the time ε stabilises.
After this transient, ρ increases.
This increase continues beyond the time when ε stabilises.

Behavioural noise

One more important factor may influence the results derived so far: behavioural noise may corrupt
the reported posterior mean, µ̌, and variance, σ̌ 2 . This is particularly relevant for evidence integration, where error is predicted to asymptote at zero (see above) which is clearly unrealistic. As we
will see below, behavioural noise fixes this problem by yielding an above-zero asymptote for error.
We will assume that the behavioural noise in reporting these quantities is unbiased and independent
(from each other as well as from γ) and has variance 2µ̌ and 2σ̌2 , respectively, and derive these effects
for an exact representation (useful for the static case as well as for the case of evidence integration),
and for a sampling-based representation.
In this case, the relevant quantities are
V[µ̌] = V[µ̂x ] + 2µ̌
 
 
V µ̌2 = V µ̂2x + (κµ̌ + 2) 4µ̌ + 4 2µ̌ V[µ̂x ]
 
 
V σ̌ 2 = V σ̂x2 + 2σ̌2




C µ̌2 , σ̌ 2 = C µ̂2x , σ̂x2

(84)
(85)
(86)
(87)
(88)

6.1

Exact representation

The argument above gives the following formulæ for the squared error and error-variance correlation
in the static case:
ε = µγ 2 + 2µ̌
ρ = r
= r

(89)
σγ22

(κx +

3) σγ22

+ (κx +

2) µ2γ 2

(90)

+ (κµ̌ + 2)

4µ̌

+

4 2µ̌

µγ 2



4 2µ̌

ωγ−2
2

σγ22

+

2σ̌2



1
κx + 3 + (κx +

2) ωγ−2
2

+ (κµ̌ + 2)

4µ̌

2
ωγ−2
2 /µγ 2

18

+



1+

2σ̌2

2
ωγ−2
2 /µγ 2

 (91)

For the case of evidence integration, we obtain:
ε = µγ 2 /n + 2µ̌
ρ = r

(92)
1

4 2 −2
2
2 −2
κx /n + 3 + (κx /n + 2) ωγ−2
2 + (κµ̌ + 2) µ̌ n ωγ 2 /µγ 2 + 4 µ̌ ωγ 2



2
1 + 2σ̌2 n2 ωγ−2
2 /µγ 2

(93)

19



A

Convergence of the posterior kurtosis

In this section our aim is to derive, in the context of the evidence integration model, how the kurtosis
of the posterior, κn , converges as the number of data points, n, increases, in particular what is the
leading order (in n−1 ) of this convergence (at least asymptotically, in the n → ∞ limit).
Based on [1], the rth moment of the rescaled posterior distribution (of x̄ = b (x − a)v ) can be
written as

Ex [x̄r ] =

K
X



λrj n−j/2 + O n−(K+1)/2

(94)

j=r

where, in general, the constants λrj depend on the particular data points on which the posterior is
conditioned but all λrj for which j is odd are zero, and for even r, the leading term of the sum can
be given directly as

which for
yields

λrr = 2r/2 Γ((r + 1) /2) Γ−1 (1/2)
r=2
λ22 = 1

r=4
λ44 = 3

and
and

(95)
(96)
(97)

Applying Eqs. 94 and 97 to the case of the first four moments of the rescaled posterior, and expanding the first couple of terms for each, we obtain

Ex [x̄] =

7
X



λ1j n−j/2 + O n−(7+1)/2

(98)

j=1

= λ12 n−1 + λ14 n−2 + λ16 n−3 + O n−4
7


  X
Ex x̄2 =
λ2j n−j/2 + O n−(7+1)/2



(99)
(100)

j=2

= n−1 + λ24 n−2 + λ26 n−3 + O n−4
7


  X
Ex x̄3 =
λ3j n−j/2 + O n−(7+1)/2



(101)
(102)

j=3


= λ34 n−2 + λ36 n−3 + O n−4
7


  X
Ex x̄4 =
λ4j n−j/2 + O n−(7+1)/2

(103)
(104)

j=4

= 3 n−2 + λ46 n−3 + O n−4



(105)

v
The exact values of a and b will be unimportant later, but more specifically, a is the maximum likelihood
estimate of x, and b is related to the inverse score differential at the maximum likelihood estimate.

20

The second and fourth central moments of the rescaled posterior can then be written as
h
i
 
2
2
Ex (x̄ − Ex [x̄]) = Ex x̄2 − (Ex [x̄])
(106)



2
= n−1 + λ24 n−2 + λ26 n−3 − λ12 n−1 + λ14 n−2 + λ16 n−3 + O n−4
(107)
 2 −2


−1
−2
−3
−3
−4
= n + λ24 n + λ26 n − λ12 n + 2 λ12 λ14 n
+O n
(108)
 −2

−1
2
−3
−4
= n + λ24 − λ12 n + (λ26 − 2 λ12 λ14 ) n + O n
(109)
|
{z
}
|
{z
}
λ̄23
λ̄22
h
i




 
4
2
4
Ex (x̄ − Ex [x̄]) = Ex x̄4 − 4 Ex [x̄] Ex x̄3 + 6 (Ex [x̄]) Ex x̄2 − 3 (Ex [x̄])
(110)
= 3 n−2 + λ46 n−3 −



− 4 λ12 n−1 + λ14 n−2 + λ16 n−3 λ34 n−2 + λ36 n−3 +

2 

+ 6 λ12 n−1 + λ14 n−2 + λ16 n−3 n−1 + λ24 n−2 + λ26 n−3 −


4
− 3 λ12 n−1 + λ14 n−2 + λ16 n−3 + O n−4

= 3 n−2 + λ46 n−3 − 4 λ12 λ34 n−3 + 6 λ212 n−3 + O n−4
(111)

 −3
−4
−2
2
(112)
= 3 n + λ46 − 4 λ12 λ34 + 6 λ12 n + O n
{z
}
|
λ̄33
From these, the second and fourth central moments of the original posterior can simply be written
as
h
i
h
i


1
1
2
2
(113)
Ex (x − µx ) = 2 Ex (x̄ − Ex [x̄]) = 2 n−1 + λ̄22 n−2 + λ̄23 n−3 + O n−4
b
b
h
i
h
i


1
1
4
4
Ex (x − µx ) = 4 Ex (x̄ − Ex [x̄]) = 4 3 n−2 + λ̄33 n−3 + O n−4
(114)
b
b
The kurtosis of the posterior can then be written as
h
i
4
Ex (x − µx )
κn =  h
i2 − 3
2
Ex (x − µx )

3 n−2 + λ̄33 n−3 + O n−4
−3
=
2
n−1 + λ̄22 n−2 + λ̄23 n−3 + O(n−4 )

3 n−2 + λ̄33 n−3 + O n−4
= −2
−3
n + 2 λ̄22 n−3 + O(n−4 )
Expanding the first term, which is a rational function, in n−1 , one obtains


3 n−2 + λ̄33 n−3 + O n−4
= a0 + a1 n−1 + O n−2
−2
−3
−4
n + 2 λ̄22 n + O(n )
 



3 n−2 + λ̄33 n−3 + O n−4 = a0 + a1 n−1 n−2 + 2 λ̄22 n−3 + O n−4

= a0 n−2 + 2 a0 λ̄22 n−3 + a1 n−3 + O n−4


= a0 n−2 + 2 a0 λ̄22 + a1 n−3 + O n−4
from which one can derive, by matching terms of the same order on the two sides, that
a0 = 3
a1 = λ̄33 − 2 a0 λ̄22 = λ̄33 − 6 λ̄22
which means that κn can be written as


κn = 3 + λ̄33 − 6 λ̄22 n−1 − 3 + O n−2


= λ̄33 − 6 λ̄22 n−1 + O n−2
21

(115)

(116)
(117)

(118)
(119)
(120)
(121)
(122)
(123)
(124)
(125)

Thus, we have just derived that the kurtosis indeed goes to zero (because in the end, the zeroth order
term in the expansion of κn is zero), and asymptotically converges as 1/n (because the term in the
expansion that is first order in n−1 is non-zero). (On the way, in Eq. 113, we have also proven the
convergence of the posterior variance, and thus γn2 , also scales asymptotically as 1/n.)
References
1. Johnson, R.A. The Annals of Mathematical Statistics 41, 851–864 (1970).

22

