Attractor Metadynamics in Adapting Neural
Networks
Claudius Gros, Mathias Linkerhand, Valentin Walther

arXiv:1404.5417v1 [q-bio.NC] 22 Apr 2014

Institute for Theoretical Physics, Goethe University Frankfurt, Germany
{gros07,linkerhand,walther}@itp.uni-frankfurt.de
http://itp.uni-frankfurt.de/~gros

Abstract. Slow adaption processes, like synaptic and intrinsic plasticity, abound in the brain and shape the landscape for the neural dynamics
occurring on substantially faster timescales. At any given time the network is characterized by a set of internal parameters, which are adapting
continuously, albeit slowly. This set of parameters defines the number and
the location of the respective adiabatic attractors. The slow evolution of
network parameters hence induces an evolving attractor landscape, a
process which we term attractor metadynamics. We study the nature
of the metadynamics of the attractor landscape for several continuoustime autonomous model networks. We find both first- and second-order
changes in the location of adiabatic attractors and argue that the study
of the continuously evolving attractor landscape constitutes a powerful
tool for understanding the overall development of the neural dynamics.
Keywords: adiabatic attractors, attractor metadynamics, neural networks, adaption, homeostasis

1

Fast Neural Dynamics vs. Slow Adaption Processes

Complex dynamical systems are often characterized by a variety of timescales
and the brain is no exception here [1,2]. It has been observed that the neural
dynamics is contingent, for time scales ranging from hundreds of milliseconds
to minutes, on the underlying anatomical network structure in distinct ways [3].
This relation between anatomy and the timescale characterizing neural activity
is present even for autonomous systems, viz in the absence of external stimuli. It
has been proposed, complementarily, that certain temporal aspects of the brain
activity may reflect the multitude of timescales present in the environment [4],
and could be induced through adaptive processes [5].
The neurons in the brain are faced with the problem, in a related perspective,
of maintaining long-term functional stability on both the single neuron level, as
well as on the level of network activities, in view of the fact that the constituents
of the molecular and biochemical machinery, such as ion channel proteins and
synaptic receptors, have lifetimes ranging only from minutes to weeks [6]. This
situation results in the need to regulate homeostatically both the inter-neural

2

Attractor Metadynamics

synaptic strength [7], and the intra-neural parameters, the latter process termed
intrinsic plasticity [8,9].
Homeostatic mechanisms in the brain can be regarded as part of the generic
control problem of the overall brain dynamics [10], with the adaption of neural
parameters being necessary to achieve certain targets [11]. Here we study the
consequences of ongoing slow adaption for the time evolution of the landscape
of adiabatic attractors, viz of the attractors of the dynamical system obtained
by temporarily freezing the adaption process. We find that the locus of the
instantaneous attracting state guides the overall time evolution and that the
study of the attractor metadynamics, which find to be possibly both continuous
and discontinuous, constitutes a powerful tool for the study of evolving neural
networks.

2

Adapting Continuous-Time Recurrent Neural Networks

We consider continuous-time neural networks [12,13], defined by
X
1
.
ẋi = −Γ xi +
wij yj ,
yi =
ai (bi −xi )
1
+
e
j

(1)

One may consider either the firing rates yi = yi (t) as the primary dynamical
variables or, equivalently, the corresponding membrane potentials xi = xi (t),
with the sigmoidal g(z) = 1/(1 + exp(z)) constituting the standard non-linear
input-output relation for a single neuron. One denotes ai the gain (slope) of the
sigmoidal and bi the respective threshold. Here Γ > 0 sets the relaxation rate for
the membrane potentials xi and the wij are the inter-neural synaptic weights.
One speaks of intrinsic adaption when the internal parameters of an individual neuron adapt slowly over time [14,15]. In our case the bias ai and threshold
bi . This kind of internal adaption is necessary for keeping the output yi (t) ∈ [0, 1]
within the desired dynamical range, viz within the working regime of the dynamical system. Anatomical constraints such as the limited availability of energy are
imposed on the long-term firing statistics of each neuron. On a functional level,
the firing patterns are expected to encode maximal information. This distribution of maximal information is at the same time the least biased or ‘noncomittal’
with respect to the constraints [16] and it is obtained by maximizing Shannon’s
information entropy. Given a certain mean µ, here the mean target firing rate
[2], the desired output distribution is an exponential,
Z
pλ (y) ∝ eλy ,
µ = dy y p(y) ,
(2)
for the neural model (1), with λ being the respective Langrange multiplier. The
distance of a time series of data, like the neural firing rate yi (t) for a given
neuron i, relative to this target distribution function pλ (y) is captured by the
Kullback-Leibler divergence Ki [2]


Z
Z T
 dτ
pi (y)
,
pi (y) = lim
δ y − yi (t − τ )
, (3)
Ki = dy pi (y) log
T →∞ 0
pλ (y)
T

Attractor Metadynamics

3

20
19
18
17
16
15
14

p, Op (t)

13
12
11
10
9
8
7
6
5
4
3
2
1
600

800

1000

1200

1400

1600

1800

2000

t

Fig. 1. A network with N = 1000 neurons and Np = 20 encoded attractor states, see
Eq. (5). The neurons adapt, trying to optimized the relative information content (3)
of their respective activities. Shown are vertically displaced, as a function of time t,
time lines of the overlaps Op (t) ∈ [0, 1], see Eq. (6). Notice, that the polyhomeostatic
adaption (4) leads to transient-state dynamics, one attractor relict ξ p after the other
is transiently visited by the state of network activities, as evident by the bumps in the
respective time lines.

where pi (y) is the time-averaged distribution of yi (t). One can now optimize the
adaption by minimizing (3) with respect to the intrinsic parameters ai and bi
and one obtains [17,18]

ȧi = a 1/ai + (xi − bi )θ
(4)
ḃi = b (−ai )θ,
θ = 1 − 2yi + λ (1 − yi ) yi
with a and b being adaption rates for the gain ai and the threshold bi respectively. In effect, the system is given an entire distribution function pλ (y)
as an adaption target. The adaption rules (4) hence generalize the principle of
homeostasis, which deals with regulating a single scalar quantity, and have been
denoted polyhomeostatic optimization [19].

3

Transient State Dynamics

A convenient way to construct networks with a predefined set ξ p = (ξ1p , ξ2p , ..) of
attracting states, with p = 1, .., Np , is by selecting the synaptic weights as [20]
X p


wij ∝
ξi − ξ¯i ξjp − ξ¯j ,
(5)
p

4

Attractor Metadynamics
1

y

a, b
x

Firing Rate y

0.8
0.6
0.4
0.2

current fire rate
stable fixed point(s)
unstable fixed points

0
−0.2
−0.4

−0.2

0

0.2

0.4

0.6

0.8

1

1.2

Membrane Potential x

Fig. 2. Left: The autapse, a neural net with a single, self-coupled neuron. The output is
directly fed to the input with w11 = 1. Middle: Depending on the values of the intrinsic
parameters a and b there may be one or two stable fixpoints ẋ = 0 for the autapse, and
one unstable fixpoint. The number and the position of the adiabatic fixpoints change
when the gain a = a(t) and the threshold b = b(t) slowly adapt through (4). Shown
are y(x) (red solid line) and x (dashed black line), compare Eq. (1). Right: A three-site
network with inhibitory (red) and excitatory (green) synaptic weights.

where ξ¯j is a local activity, averaged over all encoded patterns ξ p . With the
Hopfield encoding (5) one can hence construct attractor networks having point
attractors
close to the patterns ξ p , with a given, predefined average activity level
P
p
p
ξ¯ = i ξi /N , where N is the number of neurons in the network.
As a first application we study a network of N = 1000 neurons with the
synaptic weights selected using the Hopfield encoding (5) and Np = 20 random
p
binary patterns ξ p = (ξ1p , .. , ξN
) drawn from an uniform distribution. We define
with
P p
sX
i ξi yi
Op (t) = p
zi2
(6)
,
||z|| ≡
||ξ || ||y||
i
the overlap between the current state y(t) = (y1 (t), .. , yN (t)) and a given stored
attractor state ξ p , in terms of the respective normalized scalar product.
In Fig. 1 we show a typical simulation result for the overlaps Op (t) ∈ [0, 1],
with the individual time lines being shown vertically displaced and color-coded.
The parameters used for the simulation are Γ = 1, a = 0.1, b = 0.01 and
ξ¯p = 0.2, µ = 0.2. Alternative values for the adaption rates a,b lead qualitatively
to similar behaviors, whenever the adaption process is substantially slower than
the neural dynamics (1). One observes two distinct features.
– For a = b = 0 the dynamics would eventually settle into a steady state
close to one of the stored patterns ξ p . The dynamical activity is, on the
other hand, continuous and autonomously ongoing when intrinsic adaption is
present, as evident from Fig. 1. This is due to the fact that the system tries to
achieve exponentially distributed firing-rate distributions. Without adaption
the individual pi (y) would be simple δ-functions in any fixpoint state and
this would lead to a very high and therefore sub-optimal Kullback-Leibler
divergence (3).

Attractor Metadynamics

Firing rate

4

5

3

0

2

1
0

Parameters a,b

Fig. 3. Left: Phase diagram of the autapse, as illustrated in Fig. 2. The activity
y ∈ [0, 1] of the fixpoint is color-coded, for fixed gains a and thresholds b of the sigmoidal, see Eq. (1), when only a single stable fixpoint is present. The greenish area
within the two white lines denotes the phase space containing two stable fixpoints.
The thick white line is the limiting cycle for polyhomeostatically adapting intrinsic
parameters (a(t), b(t)), compare Eq. (4). Right: The firing rate of the adiabatic fixpoint (stable/unstable: thick/dashed lines) as function of the intrinsic parameters. The
arrows and numbers indicate the section of the hysteresis loop in the landscape of adiabatic fixpoints corresponding to the equally labeled sections of the limiting cycle of
(a(t), b(t)) shown in the left panel.

– The overlaps Op (t) are, most of the time, relatively small with temporally
well defined characteristic bumps corresponding to dynamical states y(t)
approaching closely one of the initially stored patterns ξ p . This type of dynamics has been termed transient-state [21] and latching [22] dynamics and
may be used for semantic learning in autonomously active neural networks
[23,24].
The inclusion of intrinsic adaption hence destroys all previously present attracting states. When the adaption process is slow and hence weak, with the
actual values for the adaption rates a and b not being relevant, the system will
however still notice the remains of the original point attractors and slow down
when close by. The resulting type of network has been termed attractor relict
network [23].

4

Discontinuous Attractor Metadynamics

A complete listing of all attracting states and the study of their respective time
evolution is cumbersome for a large network like the one of Fig. 1. For an indepth study we have selected two small model systems, we start with a single,
self-coupled neuron, the autapse, as illustrated in Fig. 2 (left).
The fixpoint condition is x = y(x), for Γ = 1 = w11 , and it is depicted in
Fig. 2 (middle). Depending on the location of the turning point b of the sigmoidal
and on its steepness a, there may be either one or two stable fixpoints, the

Attractor Metadynamics
1

1

0.9

0.9

0.8

0.8

0.7

0.7

Firing Rate y3

Firing Rate y3

6

0.6
0.5
0.4
0.3

0.6
0.5
0.4
0.3

0.2

0.2

0.1

0.1

0
0

0.2

0.4

0.6

Firing Rate y1

0.8

1

0
0

0.2

0.4

0.6

Firing Rate y1

0.8

1

Fig. 4. For the three site network illustrated in Fig. 2 (right), the time evolution of the
firing rates (y1 (t), y3 (t)). The green line is the trajectory of the final limiting cycle and
the red line of the single adiabatic attractor present in the system. The black arrows
illustrate the instantaneous flow, attracting the current dynamical state (green filled
circles) to the current position of the adiabatic attractor (red filled circles). The right
panel follows in time shortly after the left panel.

respective phase diagram is presented in Fig. 3 (left). Additionally an unstable
fixpoint may be present (central region).
The actual values (a(t), b(t)) of the intrinsic parameters polyhomeostatically
adapt via (4), an example of an actual state is included in Fig. 2 (middle) and the
final limiting cycle in Fig. 3 (left). The internal parameters settle, after an initial
transient, in a region of phase space crossing two first-order phase transitions at
which the number of attractors changes from 1 ↔ 2 ↔ 1, resulting in a hysteresis
loop for the adiabatic attractor landscape, compare Fig. 3 (right).
In the limit of long times the internal parameters, as given by the white elongated eight-shaped loop in Fig. 3 (left), stay for finite time intervals in the regions
of the phase diagram characterized by a single fixpoint (top/bottom : blue/red).
The limiting cycle of the adaption trajectory hence overshoots the hysteresis
loop characterized by the vertical transitions illustrated in Fig. 3 (right).
The dynamics is relatively slow on the hysteresis branches 1 → 2 and 3 →
4 and becomes very fast when the local adiabatic fixpoints vanishes. At this
point the system is forced to rapidly evolve towards the opposite branch of the
hysteresis loop, an example of self-organized slow-fast dynamics.

5

Continuous Attractor Metadynamics

As a second model system we consider the three-site network depicted in Fig. 2
(right), with w12 = w21 = 1 = w32 = w23 and w31 = w13 = −1. At first sight
one may expect an attractor metadynamics equivalent to the one of the autapse,
since the three-site net also has two possible attracting states (y1∗ , y2∗ , y3∗ ), with
either y2∗ and y1∗ large and y3∗ small, or with y2∗ and y3∗ large and y1∗ small.

Attractor Metadynamics

7

There is indeed a region in phase space for which these two fixpoints coexist
[25], but the system adapts the six internal parameters ai (t) and bi (t) such that a
single adiabatic fixpoint remains, which morphs continuously under the influence
of the polyhomeostatic adaption (4).
In Fig. 4 we present the resulting limiting cycle of the full dynamics projected
onto the (y1 , y3 ) plane (the activity of y2 is intermediate and only weakly changing). One observes that the adiabatic fixpoint moves on a continuous trajectory,
an adiabatic limiting cycle. This behavior contrasts with the time evolution of
the attractor landscape observed for the autapse, as presented in Fig. 3 (right),
which is characterized by a discontinuous hysteresis loop.
The adiabatic fixpoint approaches (y1∗ ≈ 1, y3∗ ≈ 0) and (y1∗ ≈ 0, y3∗ ≈ 1)
repeatedly, as evident in Fig. 4. The corresponding phase space trajectory then
slows down, as one can observe when plotting the actual time evolution, an
example of transient-state dynamics.

6

Carrot and Donkey Dynamics

In the metaphor of the donkey trying to reach the carrot it carries itself, the
animal will never reach its target. The case of self-generated attractor metadynamics studied here is analogous. The current dynamical state is attracted by
the nearest adiabatic attractor, but the systems itself morphs the attractor continuously when the trajectory tries to close in. The locus of the attractor evolves,
either continuously or discontinuously, and the trajectory is then attracted by
the adiabatic fixpoint at its new locus.
This feature allows to characterize decision processes in the brain and in
model task problems dynamically [26,27], and choice options can be extracted
in terms of corresponding adiabatic fixpoints. Here we studied autonomous systems, starting from attractor networks, with the aim to obtain a first overview
regarding the possible types of self-generated attractor metadynamics.

Acknowledgments
The authors would like to thank Peter Hirschfeld for illuminating suggestions.

References
1. Izhikevich, E.M.: Dynamical systems in neuroscience. The MIT press (2007)
2. Gros, C.: Complex and adaptive dynamical systems: A primer. Springer Verlag
(2013)
3. Honey, C.J., Kötter, R., Breakspear, M., Sporns, O.: Network structure of cerebral
cortex shapes functional connectivity on multiple time scales. Proceedings of the
National Academy of Sciences 104(24) (2007) 10240–10245
4. Kiebel, S.J., Daunizeau, J., Friston, K.J.: A hierarchy of time-scales and the brain.
PLoS Computational Biology 4(11) (2008) e1000209

8

Attractor Metadynamics

5. Ulanovsky, N., Las, L., Farkas, D., Nelken, I.: Multiple time scales of adaptation in
auditory cortex neurons. The Journal of Neuroscience 24(46) (2004) 10440–10453
6. Marder, E., Goaillard, J.M.: Variability, compensation and homeostasis in neuron
and network function. Nature Reviews Neuroscience 7(7) (2006) 563–574
7. Turrigiano, G.G., Nelson, S.B.: Homeostatic plasticity in the developing nervous
system. Nature Reviews Neuroscience 5(2) (2004) 97–107
8. Daoudal, G., Debanne, D.: Long-term plasticity of intrinsic excitability: learning
rules and mechanisms. Learning & Memory 10(6) (2003) 456–465
9. Echegoyen, J., Neu, A., Graber, K.D., Soltesz, I.: Homeostatic plasticity studied
using in vivo hippocampal activity-blockade: synaptic scaling, intrinsic plasticity
and age-dependence. PloS one 2(8) (2007) e700
10. O’Leary, T., Wyllie, D.J.: Neuronal homeostasis: time for a change? The Journal
of physiology 589(20) (2011) 4811–4826
11. Ge, S., Hang, C.C., Lee, T.H., Zhang, T.: Stable adaptive neural network control.
Springer Publishing Company, Incorporated (2010)
12. Beer, R.D.: On the dynamics of small continuous-time recurrent neural networks.
Adaptive Behavior 3(4) (1995) 469–509
13. Beer, R.D., Gallagher, J.C.: Evolving dynamical neural networks for adaptive
behavior. Adaptive behavior 1(1) (1992) 91–122
14. Triesch, J.: A gradient rule for the plasticity of a neuron’s intrinsic excitability.
Artificial Neural Networks: Biological Inspirations–ICANN 2005 (2005) 65–70
15. Marković, D., Gros, C.: Intrinsic adaptation in autonomous recurrent neural networks. Neural Computation 24(2) (2012) 523–540
16. Jaynes, E.T.: Information theory and statistical mechanics. Physical review 106(4)
(1957) 620
17. Linkerhand, M., Gros, C.: Self-organized stochastic tipping in slow-fast dynamical
systems. Mathematics and Mechanics of Complex Systems 1(2) (2013) 129–147
18. Steil, J.J.: Online reservoir adaptation by intrinsic plasticity for backpropagation–
decorrelation and echo state learning. Neural Networks 20(3) (2007) 353–364
19. Markovic, D., Gros, C.: Self-Organized Chaos through Polyhomeostatic Optimization. Physical Review Letters 105(6) (August 2010)
20. Hopfield, J.J.: Neural networks and physical systems with emergent collective
computational abilities. Proceedings of the national academy of sciences 79(8)
(1982) 2554–2558
21. Gros, C.: Neural networks with transient state dynamics. New Journal of Physics
9(4) (2007) 109
22. Russo, E., Namboodiri, V.M., Treves, A., Kropff, E.: Free association transitions in
models of cortical latching dynamics. New Journal of Physics 10(1) (2008) 015008
23. Gros, C.: Cognitive computation with autonomously active neural networks: an
emerging field. Cognitive Computation 1(1) (2009) 77–90
24. Gros, C., Kaczor, G.: Semantic learning in autonomously active recurrent neural
networks. Logic Journal of IGPL 18(5) (2010) 686–704
25. Linkerhand, M., Gros, C.: Generating functionals for autonomous latching dynamics in attractor relict networks. Scientific Reports 3 (2013)
26. Beer, R.D.: Dynamical approaches to cognitive science. Trends in cognitive sciences
4(3) (2000) 91–99
27. Deco, G., Rolls, E.T., Romo, R.: Synaptic dynamics and decision making. Proceedings of the National Academy of Sciences 107(16) (2010) 7545–7549

