Fluctuations and information filtering in coupled populations of spiking neurons with
adaptation∗
Moritz Deger,1 Tilo Schwalger,1 Richard Naud,2 and Wulfram Gerstner1
1

arXiv:1311.4206v2 [q-bio.NC] 3 Mar 2015

School of Computer and Communication Sciences and School of Life Sciences, Brain Mind Institute,
École polytechnique fédérale de Lausanne, Station 15, 1015 Lausanne EPFL, Switzerland†
2
Department of Physics, University of Ottawa, 150 Louis Pasteur, K1N-6N5 Ottawa, Ontario, Canada
(Dated: March 4, 2015)
Finite-sized populations of spiking elements are fundamental to brain function, but also used
in many areas of physics. Here we present a theory of the dynamics of finite-sized populations
of spiking units, based on a quasi-renewal description of neurons with adaptation. We derive an
integral equation with colored noise that governs the stochastic dynamics of the population activity
in response to time-dependent stimulation and calculate the spectral density in the asynchronous
state. We show that systems of coupled populations with adaptation can generate a frequency band
in which sensory information is preferentially encoded. The theory is applicable to fully as well
as randomly connected networks, and to leaky integrate-and-fire as well as to generalized spiking
neurons with adaptation on multiple time scales.

I.

INTRODUCTION

Multi-scale modeling of complex systems has led to
important advances in fields as diverse as complex fluid
dynamics, chemical biology, soft matter physics, meteorology, computer science, and neuroscience [1–6]. In
these approaches, mathematical methods such as meanfield theories and coarse-graining provide the basis to link
properties of microscopic elements to macroscopic variables. In many cases, macroscopic variables fluctuate due
to a finite number of microscopic elements. For instance,
in the brain, neurons can be grouped into populations of
50 to 1000 neurons [7] with similar properties [8, 9]. Fluctuations of the global activity of such populations are not
captured in classical mean-field theories [10, 11], which
assume infinite system size. Here we put forward a theory for the fluctuating macroscopic activity in networks
of pulse-coupled elements occurring in neuronal networks
[12], queuing theory [13] and synchronizing fireflies [14].
Finite-size effects in networks of spiking elements have
been approached by different methods, including extensions of the Fokker-Planck equation for neuronal membrane potentials [10, 15], stochastic field theory [16], moment expansions in networks of generalized linear models
(GLM) [17], modified Hawkes processes [18–20], simplified Markov neuron models [21–27] and the use of a linear
response formalism for spike trains perturbed by finitesize fluctuations [28–30]. These studies lack, however,
slow cellular feedback mechanisms mediating adaptation.
Adaptation characterized by a reduced response of a

∗

†

This article was published as: M. Deger, T. Schwalger, R. Naud,
W. Gerstner, Fluctuations and information filtering in coupled
populations of spiking neurons with adaptation, Phys. Rev. E
90, 062704 (2014), DOI: 10.1103/PhysRevE.90.062704 .
Typesetting errors were corrected in the paragraph of Eq. (5)
and in Eqs. (8) and (13).
Corresponding authors: M. D., moritz.deger@epfl.ch ;
T. S., tilo.schwalger@epfl.ch .

neuron to slow compared to fast inputs is a wide-spread
phenomenon in the brain and has important implications
for signal processing [31–34] and the spontaneous activity
of single neurons [35–37]. On the population level, adaptation has been recently analyzed using a quasi-renewal
(QR) theory [38]. The QR framework uses tools of renewal point process theory [39] to treat neurons with arbitrary refractoriness. In particular, the dynamics of the
population activity is determined by an integral equation [11, 40, 41]. These studies, however, have assumed
an infinitely large population.

Here we present a theory for the interaction of finitesized populations of adapting neurons. The theory is
valid for the broad class of neuron models that can be
approximated by a QR point process. This includes
integrate-and-fire (IF) as well as GLM neurons, for which
parameters can be reliably extracted from experimental
data [9, 12, 42].

Based on this theory we analyze information filtering
(noise shaping) in neuronal populations. We show that
in a single population no noise shaping occurs, but that
in coupled populations, band-pass-like noise shaping is
possible due to adaptation and connectivity.

This article is structured as follows: First, we present
the general dynamics of the population activity and its
fluctuations. We then describe how randomly connected,
adapting neurons can be treated in this framework. For
fluctuations about a stationary state, we linearize the
dynamics and compute the spectral density of the population activity. We then determine its coherence with
external input signals and quantify information transmission.

2
single neuron model

+

population model

external
input

+

refractoriness & adaptation ﬁlter

neuronal population

membrane
ﬁlter

nonlinearity

spiking
mechanism

network (other neurons)

interval density

ˆ

+
mean activity &
ﬂuctuations

t

a(t) =

PH (t, t̂)A(t̂) dt̂,

(1)

−∞
other populations

Figure 1. (Color) Schematic of the spike response model (a
GLM for spike generation) with exponential escape noise and
the derived quasi-renewal population model.

II.
A.

still large enough to include many spikes of the population. For large N , the number of neurons that fire in the
time bin at t and had their last spike in bin t̂ is a Gaussian
random number with mean and variance n0 (t̂)PH (t, t̂)∆t,
´ t̂+∆t
where n0 (t̂) = N t̂
A(s) ds is the past spike count at
t̂ < t. Summing over t̂ and treating ∆t as macroscopically infinitesimal, we find the conditional mean activity
(see Appendix B)

RESULTS

Dynamics of globally coupled renewal models.

Our main quantity of interest is the population activPN
P
ity A(t) = N −1 i=1 si (t), where si (t) = k δ(t − tki )
is the spike train of neuron i with spike times tki and
N denotes the number of neurons. In experiments or
simulations, the measured activity Ā(t) would be determined by temporal
filtering of the population activity,
´∞
i.e. Ā(t) = −∞ A(t − s)f (s)ds with a normalized filter
function f (s) with finite support. Below we will use a
rectangular filter f (s) = θ(s)θ(∆t − s)/∆t, where θ(s) is
the Heaviside step function.
To determine the fluctuation statistics of A(t), we generalize the integral equation of an infinite population [11]
to large but finite N . Let us first consider a homogeneous population of all-to-all connected renewal neurons.
In this case, the spikes of each neuron occur with an instantaneous rate or hazard function ρH (t, t̂), which only
depends on its last spike time t̂ ≤ t and the synaptic
input determined by the history H(t) = {A(t0 )}t0 <t of
the population activity. Note that for uncoupled stationary networks, the hazard reduces to ρH (t, t̂) = ρ(t − t̂)
as it should be for a renewal model. The probability density of the next spike time t given t̂ is given by
PH (t, t̂) = ρH (t, t̂)SH (t, t̂), with the survivor function de´t
fined as SH (t, t̂) = exp(− t̂ ρH (t0 , t̂)dt0 ).
Our approach is to use the Gaussian approximation
for large N , i.e. we calculate the first- and second-order
statistics of A(t) as a functional of its past activity (see
Appendix B). However, the dynamics of A depends on its
own history H(t) and on the occupation density of refractory states t− t̂ across the population [43]. Thus, we have
to average over the possible refractory states consistent
with a given history H. To perform the Gaussian approximation, the dynamics of the full system is coarse-grained
by discretizing time with a small time step ∆t which is

which is equal to the population integral [11] for the infinite system. For finite N , A(t) will be of the form
A(t) = a(t) + δA(t),

(2)

where the deviation p
δA(t) has zero mean and a diverging
standard deviation a(t)/(N ∆t) because the variance
of the spike count in [t, t + ∆t] is given by N ∆t a(t).
Importantly, δA(t) cannot be described by a white noise
process but future values δA(t + τ ), τ > 0, are correlated
with δA(t), because they share a common history H(t).
In fact, a neuron that fired its last spike at t̂ < t cannot
have its next spike at both times t and t+τ , which induces
a negative correlation for the deviations at t and t +
τ . We find (see Appendix B) for τ ≥ 0 the conditional
correlation function
hδA(t + τ )δA(t)iH(t) = N −1 a(t)δ(τ )−
ˆ t
N −1
PH (t + τ, t̂)PH (t, t̂)A(t̂) dt̂,

(3)

−∞

where h·iH(t) denotes the average conditioned on the history of A before t. Thus, the correlation function is in
general explicitly time-dependent.

B.

Adaptation and random connectivity.

In the presence of adaptation, the instantaneous rate
of a neuron depends on all its previous spikes so that it
can no longer be described by renewal theory. Here we
describe how adapting neurons in networks may still be
approximated by a quasi-renewal process. Specifically,
we consider a homogeneous population of neurons modeled by the spike-response model with escape-noise [11],
also known as GLM [9, 12, 42], with hazard function
ρi (t) = c exp [(hi (t) − ϑi (t))/δu] ,
(Fig.
small
This
hi (t)

(4)

1). That is, neuron i produces a spike in a
time interval [t, ∆t) with probability ρi (t)∆t.
probability
h
P depends on ithe input potential
N
= κ∗
(t), which is driven
j=1 wij sj (t) + I

by presynaptic spike trains sj (t) (with synaptic weight
wij ) and external input I(t). The membrane filter kernel

e−ϑi (t) ≈ e−η(t−t̂) he−

´ t̂
−∞

si (t0 )η(t−t0 ) dt0

itki <t̂ .

Here, the average is taken over all previous spike times
tki < t̂. As shown
average
can be
h´ in [38], this

i approxit̂
−η(t−t0 )
0
0
mated by exp −∞ e
− 1 hsi (t )i dt . Replacing further the firing rate hsi (t0 )i by the population activity A(t0 ), the threshold ϑi (t) becomes
ϑ(t, t̂) = η(t − t̂) + (γt−t̂ ∗ A)(t),

(5)

for all neurons with last spike at t̂. The kernel γτ (s) =
θ(s − τ )(1 − e−η(s) ) represents the effect of adaptation
in the quasi-renewal (QR) approximation. Furthermore,
for homogeneous random networks and large N , the local field hi caused by synaptic input to neuronP
i is determined by A and an effective weight w̄ = N −2 i,j wij
[10], hence
h(t) = [κ ∗ (Js A + I)](t),

(6)

where Js = N w̄. The above steps enable us to treat
neuronal adaptation and network coupling in a quasirenewal framework with hazard function


ρH (t, t̂) = c exp h(t) − ϑ(t, t̂) .
(7)

I(t) [1/s]

25
20
15
10
5
0
180
0
180
1.0

A [1/s]

is given by κ(t) = θ(t − τs ) exp(−(t − τs )/τm ), where
τs and τm are the synaptic delay and the membrane
time constant, respectively. The
´ ∞ operation ∗ denotes
the convolution (f ∗ g)(t) = −∞ f (t − s)g(s)ds and
θ(t) is the Heaviside step function.
The variable
ϑi defined as ϑi (t) = (si ∗ η)(t) can be interpreted
as a dynamic firing threshold that is triggered by
the neuron’s own output spike train si (t) [9]. Here,
η(t) is a feedback kernel that consists of two parts,
η = ηa + ηr : a short-range refractory kernel ηr (t) =
θ(t) [Jr θ(t − τabs ) exp(−(t − τabs )/τm ) + θ(τabs − t)D]
mainly affected by the last spike, and a long-range adaptation kernel ηa (t) = Ja θ(t) exp(−t/τa ) that accumulates
the spike history on a longer time scale τa . An absolute
refractory period is included in ηr (t) by setting it to
D = 1012 for 0 < t < τabs . Our choice of the kernels
corresponds to a leaky IF model with dynamic threshold
[44] and a reset by a constant amount −Jr after each
spike.
The parameter c in Eq. (4) sets a baseline firing rate
and δu sets the strength of intrinsic noise (“softness”
of threshold). Fits of this model to pyramidal neuron
recordings yielded δu ≈ 4mV [45]. Our standard parameter set given below corresponds to an amplitude of single
post-synaptic potentials of 0.25mV (excitatory, exc.) and
−1.1mV (inhibitory, inh.). In the following, we measure
voltage in units of δu, so that δu = 1 in dimensionless
units. For the synaptic weights wij , we use a homogeneous random network as specified in Appendix A, below.
The dependence of the term exp(−ϑi ) in Eq. (4) which
describes the feedback of the neuron’s own spiking history
si can be approximated by the explicit contribution of
the last spike of the neuron at t̂ and the average effect of
previous spikes up to t̂ [38]:

A(t) [1/s]

3
4
2
0
0 1 2 3 4 5
a [1/ s]
p

1.2

1.4

p

1.6
1.8
time [s]

2.0

2.2

Figure 2. (Color) Quasi-renewal approximation of the population dynamics. Coarse-grained population activity Ā(t)
(Ā(t) = n0 (t)/(N ∆t), where n0 (t) is the spike count in
[t, t + ∆t), with ∆t = 2ms, black) resulting from 500 randomly connected adapting neurons (4) receiving a common
input current (I(t), blue). The theoretical expectation a(t)
(green, Eq. 1) based on QR approximation (7), and expected
1
1
fluctuations (red, one std., a(t) ± (N ∆t)− 2 a(t) 2 ). At time
0
t, a(t) depends on the actual history Ā(t ) (black) for t0 < t.
Standard parameters (see Appendix A), except for I(t) as
shown and c = 5s−1 . Inset shows mean (green) and √
std.
(red) of deviations δA(t) = Ā(t) − a(t) as a function of a,
averaged over 25 repetitions of the displayed I(t) (dots) vs.
theory (lines).

Note that ρH (t, t̂) is identical for all neurons which have
fired their last spike at t̂.
In Fig. 2 the population activity of a spiking neural
network simulation is compared to the theoretical prediction (2). To evaluate (1) numerically, we iteratively
compute SH (t + ∆t, t̂) = SH (t, t̂)(1 − ρH (t, t̂)∆t), with
SH (t, t) = 1, and use PH = ρH SH . The QR population
integral describes the response of the population activity
and its fluctuations for stationary, as well as for slowly
or rapidly varying inputs.

C.

Linearized population dynamics.

The amount of information transmitted and processed
in sensory areas of the brain is limited by the fluctuations
of the population activities [46–48]. Likewise, in decision
networks, finite-size induced fluctuations determine the
reliability of decisions [49, 50]. The spontaneous activity of cortical networks is typically asynchronous, and is
believed to underlie cortical information processing [51].
In order to analytically determine the power spectrum
of the spontaneous population activity, we linearize the
dynamics around the large N limit. To this end, we assume that in the limit N → ∞ and for constant external
input I(t) = I0 the network dynamics has an equilibrium
point with activity A0 corresponding to an asynchronous
firing state. With this equilibrium activity we can associate a renewal neuron model that is obtained from the
original model, Eq. (4), by replacing hi (t) and ϑi (t, t̂) by
h0 = κ ∗ (JA0 + I0 ) and ϑ0 (t − t̂) = η(t − t̂) + (γt−t̂ ∗ A0 ),
respectively. In the following, we will use the subscript

4
“0” to refer to quantities of the associated renewal model.
For finite system size, N < ∞, the activity will deviate
from A0 . Through (5) and (6) the fluctuations ∆A(t) =
A(t) − A0 also lead to fluctuations ∆h(t) = h(t) − h0 (t)
and ∆ϑ(t, t̂) = ϑ(t, t̂) − ϑ0 (t − t̂) , which in turn influence
A(t). Our goal is to determine the spectral properties of
∆A(t).
To simplify the derivations, we approximate the QR
kernel γτ by its average over the inter-spike-interval density P0 (τ ) 1 ,
ˆ ∞
P0 (τ )γτ (s)dτ = (1 − e−η(s) )(1 − S0 (s)) (. 8)
γ(s) =
0

Expanding (1)-(3) to first order in ∆A, ∆h and
∆ϑ, yields the linearized stochastic dynamics (see Appendix C)
p
A(t) = A0 + (Q ∗ ∆A)(t) + A0 /N ξ(t).
(9a)
d
Here, Q(t) = P0 (t) + A0 dt
(L ∗ [κJs − γ])(t) determines
the linear response of the expected activity a to a perturbation ∆A. For our model (4),
´ ∞the kernel L in this
expression is given by L(t) = θ(t) 0 ρ0 (s)S0 (s+t)ds but
L can be derived for most common neuron models [11],
or, alternatively, may be estimated from neural recordings. The noise term ξ(t) is stationary Gaussian noise
with correlation function
ˆ ∞
hξ(t)ξ(t + τ )i = δ(τ ) −
P0 (s + τ )P0 (s)ds
(9b)
0

for all τ ; cf. (3). Eq. (9) shows that the population
activity in the stationary state is a Gaussian process with
memory, where finite-size fluctuations are described by
the colored noise ξ(t).

D.

Fluctuations in coupled populations.

~ =
Let us now turn to K populations consisting of N
(N1 , . . . , NK ) neurons. Parameters of neurons and coupling are homogeneous within each population but may
differ between one group and the next. To incorporate
network coupling, the network input (6) becomes hk (t) =
~ k )(t) for k = 1, . . . , K, where J is the coupling
(κk ∗ (JA)
~
matrix and A(t)
is a vector of population activities. For
each population the dynamics are given by (9) but Q(t)
is now a K × K matrix of ´coupling kernels. Using the
∞
Fourier transform f˜(ω) = −∞ f (t)e−iωt dt, this matrix
can be written as Q̃(ω) = P̃0 + iωA0 L̃(K̃J − G̃), where
the matrices P0 , A0 , L, K, G are defined as the diagonal
~ 0 , L,
~ ~κ, ~γ , respectively. The
matrices of the vectors P~0 , A
power spectrum, defined as the Fourier transform of the

1

An alternative would be to average γτ (s) over the backward recurrence time A0 S0 (τ ).

~
~ T (t + τ )i, can
correlation function CA (τ ) = h∆A(t)∆
A
be obtained from the transformed Eq. (9) as C̃A (ω) =
(1 − Q̃)−1 N−1 A0 (1 − P̃0 P̃†0 )(1 − Q̃† )−1 . Here, † denotes the adjoint matrix (conjugate transpose). It is
instructive to rewrite this expression in terms of the
power spectrum of the associated renewal model [52]
C̃0 (ω) = A0 (1 − P̃0 )−1 (1 − P̃0 P̃†0 )(1 − P̃†0 )−1 as follows:
C̃A (ω) = B̃N−1 C̃0 B̃† ,

h
i−1
B̃ = 1 − R̃0 (J − K̃−1 G̃)
(10)

Here, R̃0 = iω(1 − P̃0 )−1 Ã0 L̃K̃ is the diagonal matrix
containing the linear response functions of the associated
~
renewal models with respect to current perturbations I(t)
[11]. Eq. (10) shows that finite-size fluctuations are characterized by the renewal spectrum C̃0 (ω), shaped by recurrent input (via J and K̃) and adaptation (via G̃),
and reduced by the factor N−1 . There are several known
limit cases: first, for vanishing adaptation, G̃ = 0, we recover the linear response result of [28–30] for networks of
white-noise driven IF neurons. Our formula shows that
adaptation appears as an additional diagonal term in the
effective coupling matrix J − K̃−1 G̃, and hence can be
interpreted as an inhibitory self-coupling. Second, if both
adaptation and recurrent connections vanish, G̃ = 0 and
J = 0, we arrive at C̃A (ω) = N−1 C̃0 (ω) because the superposition of independent spike trains does not change
the shape of the power spectrum. Third, our result also
includes the frequently employed Hawkes process [18–20],
which is recovered for a constant single neuron spectrum
C̃0 = A0 and vanishing adaptation, G̃ = 0. For a comparison of our result to simulations, see Fig. 3, which
will be discussed below. In section II F we make use of
Eq. (10) to quantify information filtering in neural populations (Fig. 4). A comparison to the special cases of
Eq. (10) described above is shown in Fig. 5.
In Fig. 3(a)-(b) the spectral density is shown compared
to simulations, where the frequency equals ω/(2π). The
spectra are well described by the novel theory, which captures refractoriness, recurrent feedback and the reduction
of power at low frequencies due to adaptation. The latter arises from negative correlations between ISIs typical
for adapting neurons [37]. Interestingly, this purely nonrenewal effect is well accounted for by our quasi-renewal
theory. Since adaptation effects are most prominent at
low frequency, we examined the dependence of the power
on the model parameters for ω → 0 (Fig. 3(c)-(d)). Our
theory describes the simulations well across the studied
parameter range.

E.

Influence of correlated external signals.

Neuronal networks in the brain are subject to external influences, either due to sensory input or ongoing
activity in other brain areas. How do neuronal populations respond to small, time-dependent input cur-

5

(b)

single population networks

1.0

0.5

10-1

101
100
frequency [Hz]

(c)

Js =5
Js =0
Js =-5
102

(d1)

spectral density [0.01/s]

spectral density [0.01/s]

(a)

4

excitation-inhibition network

exc.
cross
inh.

3
2
1
0

10-1

101
100
frequency [Hz]

(d2)

(d3)

4

4

4

1

2

2

2

0

0

CA (0) [0.01/s]

10

0 250 500 750
N

0

0.5 1.0 1.5
u

102

0 2 4 6 8 10
Js

5 4 3 2 1 0
Ja

Figure 3. (Color) Spectral density of the population activity in random networks. Simulation (light colored lines) vs. theory
(10) (solid lines). (a): single populations (K = 1), (b-d): coupled exc.-inh. network (K = 2), (in (b) green lines show the
amplitude of the cross-spectrum). (c-d): The limit ω → 0 in dependence on parameters for the K = 2 case shown in (b);
symbols mark simulation results for different connection probabilities (5, 4, ◦: p = 0.2, 0.5, 1) while keeping Js = N w̄ fixed,
solid lines show theory (10). Symbols mostly fall on top of each other indicating independence of results with respect to p.
In (c) colors are as in (b), in (d1-d3) only exc. population is shown, colors denote number of neurons (blue, green, red, cyan:
N = 50, 250, 500, 1000). Standard parameters (marked by arrows in (c-d), cf. Appendix A) were used except as indicated.

~
rents I(t)
= (I1 , . . . , IL ) with spectral density C̃I (ω)?
To answer this question, we proceed as before and linearize Eqs. (1)-(3) with respect to the small fluctuations
~ + MI)
~ k )(t) of the local field. Here,
∆hk (t) = (κk ∗ (J∆A
~ such
we restrict our analysis to independent inputs I,
that C̃I is diagonal with entries C̃I,i , but also included a
K ×L mixing matrix M, which allows us to model shared
input. The resulting spectral density is given by the sum
C̃A (ω) =B̃[N−1 C̃0 + R̃0 MC̃I M† R̃†0 ]B̃† .

(11)

Thus, additional fluctuations due to the stimulus I~ are
shaped by both the single neuron filter R̃0 M and the network and adaptation filter B̃ (10), combining the effects
of recurrent connectivity and adaptation.

F.

Information transmission.

|hI˜j∗ (ω)Ãi (ω)i|2
C̃I,j (ω)(C̃A (ω))ii

|(B̃R̃0 M)ij |2 C̃I,j
.
PL
2
2
k=1 |B̃ik | C̃0,k +
l=1 |(B̃R̃0 M)il | C̃I,l
(13)

Γij (ω) = PK

Our theory allows us to quantify the transmission of
~
information from external input signals I(t)
through a
system of coupled neural populations. The coherence
between the signal j and the activity of population i
Γij (ω) =

can be regarded as a frequency resolved measure of information transmission. Information theory [53–55] states
that the
´ ∞ mutual information rate is bounded from below
by − 0 log2 [1 − Γij (ω)] dω
2π .
Since adaptation attenuates the response to slowly
changing signals, one might expect that it also attenuates low frequency information content. For a single
population, however, this is not the case, but instead the
coherence is low-pass, i.e. it monotonically decreases for
increasing frequency [29]. Here we show that in coupled
populations of adapting neurons, coherences can be nonmonotonic allowing the neural circuit to preferentially
encode information in certain frequency bands. Put differently, a multi-population setup can realize an information filter.
Using Eq. (12), we find the general form of the coherence matrix:

(12)

In this expression, the numerator represents the contribution of the signal Ij (t) to the power spectrum of population i. This effective signal power is divided by the
total power spectrum of population i, which consists of
direct (k = i) and indirect (k 6= i) sources of variability. Both sources contain internally generated noise due

spectral coherence [1]
10-3 10-2 10-1 100

6

10-1

101
100
frequency [Hz]

102

Figure 4. (Color) Signal processing and coherence shaping in
a feed-forward chain of recurrently connected neural populations, simulation vs. theory. Left: Schematic of the network.
Right: spectral coherences Γi1 (13) of I1 and Aexc.
, theory
i
(13) (lines) vs. simulation result (light colored lines). Colors indicate coherences of signals I1 and population activity
Ai . The mutual information rate is closely related to the
spectral coherence, see text. Standard parameters, except
for increased coupling Js = 10. The input current I1 (t) is a
Gaussian white noise with spectral density C̃I,1 (ω) = 9s−1 .

to finite size Nk as well as signal power. However, the
diagonal elements (k = i) of the shaping matrices can
be much stronger than the off-diagonal elements (k 6= i)
depending on the coupling matrix J. Therefore, we expect that the direct source of variability dominates in the
denominator.
If there is only one population and signal (K = 1,
L = 1), the term |B̃11 (ω)|2 occurs in both numerator
and denominator and cancels. Thus, coupling and adaptation do not shape the coherence in a single population. Furthermore, the signal term |R̃0,1 (ω)|2 M11 C̃I,1 (ω)
is matched in both numerator and denominator, which
leads to a flat coherence at frequencies where the signal dominates the finite N noise. At high frequencies, the neural response amplitude |R̃0 |2 decays due to
the leaky membrane, but the spontaneous spectrum C̃0
has a constant high-frequency limit equal to A0 . One
therefore typically observes a low-pass like information
transfer characteristics of single neurons or populations
[29, 56, 57].
For several populations (K > 1), however, we can distinguish two cases: If the signal is read out at a receiving
population, Mij 6= 0, the signal power in the numerator
is matched by the dominating direct signal power in the
denominator, and hence the shaping of the signal power
cancels. In contrast, if read out at a different population, Mij = 0, the signal j contributes only indirectly to
the power spectrum of population i via synaptic connections. Thus, we expect that the shape of signal power and
power spectrum (i.e. numerator and denominator, respectively) is generally different if the transmission path
involves multiple populations.
As an example of this mechanism, we show a feedforward chain of excitatory and inhibitory populations
(Fig. 4, K = 6, L = 1). In the first layer, the effective signal power is reduced at low frequencies because of
adaptation and inhibitory feedback. However, the power
spectrum of Aexc,1 (t) is dominated by the same signal

power and hence exhibits a very similar reduction of lowfrequency power. Consequently, the coherence (being
the ratio of these two spectra) is rather flat at low frequencies and shows a decay at higher frequencies (Fig. 4,
red lines). This low-pass characteristics changes at later
stages in the chain (green & blue): The signal term is increasingly more shaped by adaptation and coupling properties, whereas the noise spectrum changes less. As a
result, the coherence shows a maximum at a finite frequency (Fig. 4, blue lines). This band-pass structure becomes more pronounced from layer to layer, representing
a form of information filtering. Coherence functions with
band-pass characteristics have been observed in neurons
postsynaptic to electro-receptor afferents in electric fish
[58, 59].

III.

CONCLUSIONS

We have shown that fluctuations in finite-sized networks of spiking neurons are captured by a colored noise
term added to the population integral equation of the infinite system. Our approach yields spectral densities of
the population activity in randomly or fully connected
multi-population networks which are in excellent agreement with simulation results. Our quasi-renewal theory
includes refractory effects and adaptation on multiple
time scales. In contrast to earlier treatments of neuronal
refractory effects in population dynamics [17, 19, 20] or
linear response formulas for adaptive neurons [60], the
QR population integral, derived directly from the neuron model definition, captures the time-dependent, nonlinear dynamics and adaptation of neural population activity.
We applied our theory to information filtering by coupled populations of spiking neurons with adaptation.
We showed that, although impossible in single populations due to a cancellation of signal and noise terms of
the coherence, coupled populations can filter information through adaptation mechanisms and neuronal interactions. This mechanism might be exploited in the layered structure of cortical circuits, or in sensory systems
of insects where signals traverse a sequence of nuclei.
In this paper we treated populations of point neurons
with static synapses and applied linear response theory.
How to generalize our theory to incorporate effects of
nonlinear dendritic integration, spike-synchrony detection and short-term synaptic plasticity, which all contribute to information filtering [61–63], is an important
question that merits further investigation. Nonetheless,
due to the versatility of GLM models, our theory already
provides a useful tool for interpreting neural data at the
population level. For example, our theory suggests that
in in-vitro experiments with optogenetically evoked input currents and simultaneous measurements of neural
activity [64], system parameters may be identified based
on the relation of the spectra, Eq. (11). Moreover, largescale neural systems can now be analyzed as coupled

7
populations of model neurons with single-cell parameters extracted from experiments, and simulated using a
muti-scale approach.

n0 (t)
nk (t)
mk (t)

ACKNOWLEDGEMENTS

δmk (t)

Research was supported by the European Research
Council (no. 268 689, T. Schwalger and W. Gerstner) and by the Swiss National Science Foundation (no.
200020_147200, M. Deger). We thank Laureline Logiaco
for helpful discussions.
M. D. and T. S. contributed equally to this work.

Appendix A: Network simulations

We compare our theoretical results to simulations of
networks of excitatory and inhibitory neurons defined by
~ = N
(4). In the case K = 1 (Fig. 3(a)), we use N
~ =
and J = Js , in the case K = 2 (Fig. 3(b)-(d)), N
(4/5N, 1/5N ), and J = ((Js , −1.1 Js ), (Js , −1.1 Js )). In
the case K = 6 (Fig. 4), each exc. (inh.) population
consists of 4/5N (1/5N ) neurons and J with entries as in
Fig. 4A, where exc. (inh.) couplings are Js (−1.1 Js ).
Generally, neurons of population i receive synapses from
a random subset of pNj neurons of population j, each
with synaptic weight wij = Jij /(pNj ) and delay τs . Unless the connection probability p is 1, self-connections
are excluded. Networks were simulated for 2 · 104 s using NEST [65] (neuron model pp_psc_delta, temporal
resolution 2ms). Standard parameters, unless indicated
otherwise: N = 500, c = 10s−1 , τm = 0.01s, τa = 0.3s,
∆t = τs = τabs = 2ms, Jr = 3, Ja = 1, Js = 5, p = 0.2,
I0 = 0, C̃I = 0. In the exc.-inh. network, for the inh.
neurons which typically show little adaptation [9] we deactivated adaptation by setting Ja = 0 and c = 5s−1 .
While it is possible to theoretically approximate the stationary interval distribution P0 by searching for a selfconsistent rate A0 as described in [38], here we use P0
from simulated inter-spike-intervals of each´population.
∞
From the measured P0 (t) we derive A0 = 1/ 0 tP0 (t)dt,
´∞
S0 (t) = t P0 (t0 )dt0 and ρ0 (t) = P0 (t)/S0 (t).
Appendix B: Detailed derivation of Eq. (3)

The aim is to find a dynamical equation for the population activity
A(t) =

N
1 X
n0 (t)
si (t) = lim
,
∆t→0 N ∆t
N i=1

ρ(t, t̂)
S(t, t̂)
P (t, t̂)

# of neurons which spike in [t, t + ∆t]
# of neurons which spiked in
[t − k∆t, t − (k − 1)∆t] ; nk (t) = n0 (t − k∆t)
# of neurons with last spike in
[t − k∆t, t − (k − 1)∆t]
# of neurons with last spike in
[t − k∆t, t − (k − 1)∆t] and next spike in [t, t + ∆t]
hazard function: rate at t given last spike at t̂
survivor function: probability of no spike in [t̂, t]
inter-spike-interval density: probability density
of next spike at t given last spike at t̂; P = ρ · S

Table I. Definitions of symbols used in Appendix B.

furthermore the total number of neurons that spiked in
the time bin [t − k∆t, t − (k − 1)∆t], k = 1, 2, . . . , but
had no further spike until time t. Let us denote this
number by mk (t) (Fig. 6 and Table I). The number of
neurons that spike in [t, t + ∆t] and had their last spike
in the bin [t − k∆t, t − (k − 1)∆t] shall be denoted by
δmk (t). These neurons decrease the number mk (t) of
neurons from group k that had survived until time t + ∆t
in the next time step, i.e.
δmk (t) = mk (t) − mk+1 (t + ∆t),

k = 1, 2, . . . . (B2)

The total number of spikes at time t, n0 (t), is the sum
over all possible last spike times, hence
n0 (t) =

∞
X

δmk (t).

(B3)

k=1

We will now express the activity n0 (t) in terms of
the past activity Ht = {nk (t)}k=1,2,... , using the Gaussian approximation. This requires to compute the mean
and correlation function of n0 (t) given the past values
nk (t), k = 1, 2, . . . . In the following, the averaging
bracket h·i has to be understood as the conditional average h·iHt = h·i{nk (t)}k=1,2,... , i.e. we will omit the conditioning subscript for simplicity. Although nk (t), the total
number of spikes in bin t−k∆t, is fixed, the number mk (t)
of neurons that had their last spike in bin t − k∆t is variable. It is this variability that we will average over (This
corresponds to a statistical ensemble of populations that
all have an identical history of population activity nk (t),
k = 1, 2, . . . .).
Suppose we know the value mk (t) of the group of neurons with their last spike in [t − k∆t, t − (k − 1)∆t]. Then
the expected number of spikes from that group in the
next interval is

(B1)

where n0 (t) is the total number of spikes in the interval
[t, t + ∆t]. More generally, we define nk (t), k ∈ Z, as the
total number of spikes in [t − k∆t, t − (k − 1)∆t], i.e. the
activity k time bins in the past. It is useful to consider

hδmk (t)imk (t) = ρ(t, t − k∆t) · ∆t · mk (t) ,

(B4)

where ρ(t, t̂) is the hazard function of the neurons (instantaneous rate at time t given last spike at t̂), and hxiy
denotes the expectation of x conditioned on y (in addition to the overall condition of a fixed history Ht =

8

(b)

single population networks

1.0

0.5
10-1

101
100
frequency [Hz]

Js =5
Js =0
Js =-5
102

spectral density [0.01/s]

spectral density [0.01/s]

(a)

excitation-inhibition network

4
3

exc.
cross
inh.

2
1
0

10-1

101
100
frequency [Hz]

102

Figure 5. (Color) Spectral density of the population activity in random networks (as Fig. 3(a)-(b)) with comparison to earlier
theories (special cases). Simulation (light colored lines) vs. theory (10) (solid lines). For comparison: uncoupled renewal
processes (B̃ = 1), dotted; coupled renewal processes (B̃−1 = 1 − R̃0 J), dash-dotted; Hawkes process (B̃−1 = 1 − R̃0 J,
C̃0 = A0 ), dashed lines. (a): single populations (K = 1), (b): coupled exc.-inh. network (K = 2). Blue dash-dotted and
solid lines coincide because inhibitory neurons here have no adaptation (Ja = 0). Standard parameters were used except as
indicated.
(a)

The average number of neurons that fired their last
spike in [t − k∆t, t − (k − 1)∆t] and survived up to t
can be expressed using the survival probability S(t, t̂) =
´t
exp(− t̂ ρ(t0 , t̂)dt0 ) as follows:
hmk (t)i = S(t, t − k∆t)nk (t)
= S(t, t − k∆t)n0 (t − k∆t) .

(b)

(B7)

We can now take the limit ∆t → 0 in (B6) and find
hn0 (t)i
→N
∆t

Figure 6. Illustration of negative correlations between δmk (t)
and δmk+1 (t + ∆t). (a): Expected number of spikes from
group k in bins [t, t + ∆t] and [t + ∆t, t + 2∆t]. (b): A large
fluctuation of δmk (t) leads to reduction of the number of available spikes mk+1 (t + ∆t). As a consequence, δmk+1 (t + 1)
tends to be small.

{nk (t)}k=1,2,... ). But since we do not know the exact
value of mk (t) we need to average
hδmk (t)i = hhδmk (t)imk (t) i
= ρ(t, t − k∆t) · ∆t · hmk (t)i

(B5)

where we have used (B4). We now use this result to
calculate the expected number of spikes in the interval
[t, t + ∆t]. Averaging over (B3) yields
hn0 (t)i =

∞
X
k=1

ρ(t, t − k∆t) · ∆t · hmk (t)i .

(B6)

ˆ

t

P (t, t̂)A(t̂) dt0 ,

∆t → 0,

(B8)

−∞

where P (t, t̂) = ρ(t, t̂)S(t, t̂) is the inter-spike-interval
density. Eq. (B8) is equivalent to Eq. (1).
To obtain the correlation function we can write for
q∈Z
hn0 (t)n0 (t + q∆t)i =

∞
X

hδmk (t)δml (t + q∆t)i. (B9)

k,l=1

Here, the spike numbers δmk (t) and δml (t + q∆t) that
refer to different groups k and l − q are uncorrelated.
Correlations only arise for δmk (t) and δmk+q (t + q∆t),
i.e. spikes that refer to the same group in the past. Thus,
hn0 (t)n0 (t + q∆t)i =

∞
X

hδmk (t)ihδml (t + q∆t)i

k,l=1

+

∞
X

hδmk (t)δmk+q (t + q∆t)i

k=1
∞
X

−

hδmk (t)ihδmk+q (t + q∆t)i, (B10)

k=1

9
so that the covariance is
h∆n0 (t)∆n0 (t + q∆t)i =

∞
X

hδmk (t)δmk+q (t + q∆t)i

k=1

−

∞
X

hδmk (t)ihδmk+q (t + q∆t)i, (B11)

k=1

where ∆n0 (t) = n0 (t) − hn0 (t)i. Therefore we need to
compute
hδmk (t)δmk+q (t + q∆t)i .

For q > 0, we employ (B2) twice and obtain
hδmk (t)δmk+q (t + q∆t)i = hmk (t) mk+q (t + q∆t)i
− hmk+1 (t + ∆t) mk+q (t + q∆t)i
− hmk (t) mk+q+1 (t + (q + 1)∆t)i
+ hmk+1 (t + ∆t) mk+q+1 (t + (q + 1)∆t)i. (B14)
In order to evaluate each of these four correlators, we
note that the probability that a neuron from group k
“survives” until time t + q∆t given that it survived until
time t is S(t + q∆t, t − k∆t)/S(t, t − k∆t) according to
Bayes law. Thus, out of the mk (t) neurons that survived
until time t, on average
S(t + q∆t, t − k∆t)
·mk (t),
S(t, t − k∆t)
(B15)

also survive until t + q∆t. Therefore, the correlator for
0 ≤ l < q can be written as
hmk+l (t + l∆q)mk+q (t + q∆t)i =
D
E
= mk+l (t + l∆q) hmk+q (t + q∆t)imk+l (t+l∆t) ,
=

h[mk (t)]2 i = h[mk (t) − hmk (t)i]2 i + hmk (t)i2
| {z }
O(∆t2 )

= nk (t)S(t, t − k∆t) [1 − S(t, t − k∆t)] + O(∆t2 ).
(B17a)

(B12)

To this end, let us consider the cases q = 0 and q > 0
separately.
For q = 0 and large N , the number of neurons that
spike in [t, t + ∆t] and had their last spike at t − k∆t is
a Poisson variable with mean and variance ∆t P (t, t −
k∆t) nk (t). Thus (B12) becomes



[δmk (t)]2 = ∆t P (t, t − k∆t) nk (t) + O(∆t3 ). (B13)

hmk+q (t + q∆t)i mk (t) =

in bin t − k∆t that survived until time t. Thus mk (t) can
be regarded as a binomially distributed random number
with n = nk (t) trials and survival probability p = S(t, t−
k∆t). This random number has mean np and variance
np(1 − p). Hence, the second moment reads


S(t + q∆t, t − k∆t) 
 2
· mk+l (t + l∆t) .
S(t + l∆t, t − k∆t)

Applying this result to (B14), we obtain

Likewise,
h[mk+1 (t + ∆t)]2 i = nk (t)S(t + ∆t, t − k∆t)
× [1 − S(t + ∆t, t − k∆t)] + O(∆t2 ) (B17b)
because nk+1 (t+∆t) = nk (t). Inserting (B17) into (B16)
we find
hδmk (t)δmk+q (t + q∆t)i = nk (t) ∆t2
S(t + (q + 1)∆t, t − k∆t) − S(t + q∆t, t − k∆t)
×
∆t
S(t, t − k∆t) − S(t + ∆t, t − k∆t)
×
∆t
= −P (t + q∆t, t − k∆t)P (t, t − k∆t) nk (t) ∆t2 , .
(B18)
Here, we have identified the derivative d/dtS(t, t̂) =
−P (t, t̂). Note that this expression is of order O(∆t3 ),
whereas hδmk (t)ihδmk+q (t+q∆t)i is of order O(∆t4 ). So
we can neglect the second term on the right-hand side of
Eq. (B11).
Putting all together, we find
1
h∆n0 (t)∆n0 (t0 = t + q∆t)i
∆t2
∞
1 X
=
hδmk (t)δmk+q (t + q∆t)i
∆t2
k=1
"∞
#
X
1
=
δq,0
P (t, t − k∆t)nk (t) + O(∆t)
∆t
k=1

−

∞
X

P (t + q∆t, t − k∆t)P (t, t − k∆t)nk (t)

k=1

ˆ

How can we calculate the second moment of mk (t)?
Recall that mk (t) is the part of the nk (t) neurons firing

P (t, t00 )A(t00 ) dt00

−−−−→ N δ(t − t )
∆t→0

hδmk (t)δmk+q (t + q∆t)i =
[S(t + (q + 1)∆t, t − k∆t) − S(t + q∆t, t − k∆t)]


h[mk+1 (t + ∆t)]2 i
h[mk (t)]2 i
×
−
(B16)
S(t + ∆t, t − k∆t) S(t, t − k∆t)

t

0

ˆ

−∞
t

P (t, t00 )P (t0 , t00 )A(t00 ) dt00 . (B19)

−N
−∞

Thus, using (B8), we arrive at the final result
ˆ

t

P (t, t0 )A(t0 ) dt0 + δA(t)

A(t) =
−∞

(B20)

10
where δA(t) is Gaussian with conditional correlation
function
ˆ ∞
hδA(t)δA(t + τ )i = N −1 δ(τ )
P (t, t0 )A(t0 ) dt0
−∞
ˆ ∞
0
−1
P (t + τ, t )P (t, t0 )A(t0 ) dt0 (B21)
−N
−∞

for τ ≥ 0. In the latter expression we extended the limits
of integration to infinity, which is possible if we assume
P (t, t0 ) = 0 for t < t0 . Equation (B21) tells us that
the noise correlation function consists of two parts: a
white (δ-correlated) part and a negative correlation due
to neural refractoriness. Intuitively, since δmk (t) and
δmk+1 (t + ∆t) share the same number of available neurons mk (t), a positive fluctuation of the number of spikes
in the bin t of the neurons of group k reduces the number
of neurons with last spike in bin k more than on average.
Thus, the number of neurons of group k that can still
fire in time bin t + ∆t is smaller than on average, which
explains the negative correlations (Fig. 6).

where we used the normalization of P0 (t) and the boundary condition ∆S(t, t) = 0.
The perturbation is given by

ˆ ∞
δS(t, t̂) 
· ∆g(s, t̂) ds.
∆S(t, t̂) =

−∞ δ∆g(s, t̂) ∆g=0
The functional derivative at ∆g = 0 reads
 ´

0
t

δ exp − t̂ ρ0 (t0 − t̂)e∆g(t ,t̂) dt0 
δS(t, t̂) 

=

δ∆g(s, t̂) ∆g=0
δ∆g(s, t̂)


∆g=0

= −θ(t − s)θ(s − t̂)S0 (t, t̂)ρ0 (s − t̂).
We insert this expression to compute the integral in
(C2)
ˆ

ˆ

t

−

ˆ

t

∞

θ(s − t̂)θ(t − s)

∆S(t, t̂)dt̂ =
−∞

−∞

−∞

× S0 (t − t̂)ρ0 (s − t̂)∆g(s, t̂) ds dt̂
| {z }
ˆ

ˆ

∞

=

x(t̂)

∞

θ(t−s)

θ(x)S0 (t−s+x)ρ0 (x)∆g(s, s−x)dxds
ˆ ∞
[8]
=
∆g(s) θ(t − s)
θ(x)S0 (t − s + x)ρ0 (x) dx ds .
−∞
0
{z
}
|
−∞
ˆ ∞

Appendix C: Detailed derivation of Eq. (9)

0

L(t−s)

We aim to linearize the population integral a(t) =
P (t, t̂)A(t̂) dt̂, Eq. (1), around an equilibrium point
−∞
A0 , with small fluctuations ∆A(t) = A(t) − A0 with a
mean of zero. To this end, let us first note that the hazard function Eq. (7) then can be written as
´t

ρ(t, t̂) = ce

h0 −ϑ0 (t−t̂) ∆h(t)−∆ϑ(t,t̂)

e

= ρ0 (t − t̂)e

∆g(t,t̂)

.

(C1)
Furthermore, recall that P(t, t̂) = −d/dtS(t, t̂), where
´t
S(t, t̂) = exp − t̂ ρ(s, t̂)ds . Expanding to first order
in ∆A yields S(t, t̂) = S0 (t − t̂) + ∆S(t, t̂) and P (t, t̂) =
´t
P0 (t − t̂) − d/dt∆S(t, t̂), where S0 (t) = exp(− 0 ρ0 (t0 )dt0 )
and P0 (t) = −d/dtS0 (t) define the zeroth-order terms.
Thus, the linearized population integral reads
a(t) = A0 + (P0 ∗ ∆A)(t) − A0

d
dt

ˆ

In the last step, we have used the approximation Eq. (8),
∆g(t, t̂) = ∆g(t) = [(κJs − γ) ∗ ∆A](t), which has no
dependence on the time of the last spike. Hence (C2)
becomes
a(t) = A0 + (P0 ∗ ∆A)(t) + A0

d
(L ∗ ∆g)(t).
dt

(C3)

This linearized equation for the mean activity is valid for
any small ∆A and ∆g in the past, also if due to finite
size fluctuations. Eqs. (C1)-(C3) generalize to additional
time-dependent inputs ∆I(t) by extending the definition
of ∆g to ∆g(t) = [(κJs − γ) ∗ ∆A + κ ∗ ∆I](t).
Furthermore, the noise correlation function Eq. (B21)
becomes to leading order
ˆ

t

hξ(t + τ )ξ(t)i = δ(τ ) −

P0 (t + τ − t̂)P0 (t − t̂) dt̂,
−∞

(C4)

t

∆S(t, t̂) dt̂,
(C2)

p
with A0 /N ξ(t) = δA(t). Eqs. (C3) and (C4) are equivalent to Eq. (9).

[1] M. L. Klein and W. Shinoda, Science 321, 798 (2008).
[2] G. S. Ayton, W. G. Noid, and G. A. Voth, Curr Opin
Struct Biol 17, 192 (2007).
[3] C. Peter and K. Kremer, Soft Matter 5, 4357 (2009).

[4] R. A. Pielke, Mesoscale Meteorological Modeling (Academic Press, 2002).
[5] M. Benaïm and J.-Y. Le Boudec, Perform Evaluation 65,
823 (2008).

−∞

11
[6] W. Gerstner, H. Sprekeler, and G. Deco, Science 338,
60 (2012).
[7] S. Lefort, C. Tomm, J.-C. F. Sarria, and C. C. H. Petersen, Neuron 61, 301 (2009).
[8] D. H. Hubel and T. N. Wiesel, J Physiol 160, 106 (1962).
[9] S. Mensi, R. Naud, C. Pozzorini, M. Avermann, C. C. H.
Petersen, and W. Gerstner, J Neurophysiol 107, 1756
(2012).
[10] N. Brunel and V. Hakim, Neural Comput 11, 1621
(1999).
[11] W. Gerstner, Neural Comput 12, 43 (2000).
[12] J. W. Pillow, J. Shlens, L. Paninski, A. Sher, A. M. Litke,
E. J. Chichilnisky, and E. P. Simoncelli, Nature 454, 995
(2008).
[13] V. Frost and B. Melamed, IEEE C M 32, 70 (1994).
[14] R. E. Mirollo and S. H. Strogatz, SIAM J Appl Math 50,
1645 (1990).
[15] M. Mattia and P. Del Giudice, Phys Rev E 66, 051917
(2002).
[16] M. A. Buice and C. C. Chow, PLoS Comput Biol 9,
e1002872 (2013).
[17] T. Toyoizumi, K. R. Rad, and L. Paninski, Neural Comput 21, 1203 (2009).
[18] A. G. Hawkes, Journal of the Royal Statistical Society.
Series B (Methodological) 33, 438 (1971).
[19] V. Pernice, B. Staude, S. Cardanobile, and S. Rotter,
Phys Rev E 85, 031916 (2012).
[20] M. Helias, T. Tetzlaff, and M. Diesmann, New J Phys
15, 023002 (2013).
[21] H. Soula and C. C. Chow, Neural Comput 19, 3262
(2007).
[22] M. A. Buice and J. D. Cowan, Phys Rev E 75, 051919
(2007).
[23] P. C. Bressloff, SIAM J Appl Math 70, 1488 (2009).
[24] M. Benayoun, J. D. Cowan, W. van Drongelen, and
E. Wallace, PLoS Comput Biol 6, e1000846 (2010).
[25] J. D. Touboul and G. B. Ermentrout, J Comput Neurosci
31, 453 (2011).
[26] G. Dumont, G. Northoff, and A. Longtin, Phys Rev E
90, 012702 (2014).
[27] F. Lagzi and S. Rotter, Front Comput Neurosci 8, 142
(2014).
[28] B. Lindner, B. Doiron, and A. Longtin, Phys Rev E 72,
061919 (2005).
[29] O. Ávila Åkerberg and M. J. Chacron, Phys Rev E 79,
011914 (2009).
[30] J. Trousdale, Y. Hu, E. Shea-Brown, and K. Josić, PLoS
Comput Biol 8, e1002408 (2012).
[31] J. Benda and A. V. M. Herz, Neural Comput 15, 2523
(2003).
[32] B. N. Lundstrom, M. H. Higgs, W. J. Spain, and A. L.
Fairhall, Nat Neurosci 11, 1335 (2008).
[33] C. Pozzorini, R. Naud, S. Mensi, and W. Gerstner, Nat
Neurosci 16, 942 (2013).
[34] F. Farkhooi, A. Froese, E. Muller, R. Menzel, and M. P.
Nawrot, PLoS Comput Biol 9, e1003251 (2013).
[35] S. A. Prescott and T. J. Sejnowski, J Neurosci 28, 13649
(2008).

[36] T. Schwalger, The interspike-interval statistics of
non-renewal neuron models, Ph.D. thesis, HumboldtUniversität zu Berlin (2013).
[37] T. Schwalger and B. Lindner, Front Comput Neurosci 7,
164 (2013).
[38] R. Naud and W. Gerstner, PLoS Comput Biol 8,
e1002711 (2012).
[39] D. Cox and V. Isham, Point processes (Chapman and
Hall, 1980).
[40] H. R. Wilson and J. D. Cowan, Biophys J 12, 1 (1972).
[41] M. Deger, M. Helias, S. Cardanobile, F. M. Atay, and
S. Rotter, Phys Rev E 82, 021129 (2010).
[42] W. Truccolo, L. R. Hochberg, and J. P. Donoghue, Nat
Neurosci 13, 105 (2010).
[43] C. Meyer and C. van Vreeswijk, Neural Comput 14, 369
(2002).
[44] Y. H. Liu and X. J. Wang, J Comput Neurosci 10, 25
(2001).
[45] R. Jolivet, A. Rauch, H.-R. Lüscher, and W. Gerstner,
J Comput Neurosci 21, 35 (2006).
[46] E. Zohary, M. N. Shadlen, and W. T. Newsome, Nature
370, 140 (1994).
[47] D. J. Mar, C. C. Chow, W. Gerstner, R. W. Adams, and
J. J. Collins, Proc Natl Acad Sci U S A 96, 10450 (1999).
[48] H. Sompolinsky, H. Yoon, K. Kang, and M. Shamir,
Phys Rev E 64, 051904 (2001).
[49] X.-J. Wang, Neuron 36, 955 (2002).
[50] G. Deco and E. T. Rolls, Eur J Neurosci 24, 901 (2006).
[51] A. Renart, J. de la Rocha, P. Bartho, L. Hollender,
N. Parga, A. Reyes, and K. D. Harris, Science 327, 587
(2010).
[52] R. L. Stratonovich, Topics in the theory of random noise
I, Mathematics and its applications; vol. 3 (New York:
Gordon and Breach, 1963).
[53] C. E. Shannon, Bell Syst. Tech. J. 27, 379 (1948).
[54] F. Gabbiani, Network Comp Neural 7, 61 (1996).
[55] A. Borst and F. E. Theunissen, Nat Neurosci 2, 947
(1999).
[56] M. J. Chacron, B. Lindner, and A. Longtin, Phys Rev
Lett 92, 080601 (2004).
[57] R. D. Vilela and B. Lindner, Phys Rev E 80, 031909
(2009).
[58] M. J. Chacron, L. Maler, and J. Bastian, Nat Neurosci
8, 673 (2005).
[59] R. Krahe, J. Bastian, and M. J. Chacron, J Neurophysiol
100, 852 (2008).
[60] M. J. E. Richardson, Phys Rev E 80, 021928 (2009).
[61] R. Rosenbaum, J. Rubin, and B. Doiron, PLoS Comput
Biol 8, e1002557 (2012).
[62] N. Sharafi, J. Benda, and B. Lindner, J Comput Neurosci
34, 285 (2013).
[63] F. Droste, T. Schwalger, and B. Lindner, Front Comput
Neurosci 7, 86 (2013).
[64] A. El Hady, G. Afshar, K. Bröking, O. M. Schlüter,
T. Geisel, W. Stühmer, and F. Wolf, Front Neural Circuits 7, 167 (2013).
[65] M.-O. Gewaltig and M. Diesmann, Scholarpedia 2, 1430
(2007).

