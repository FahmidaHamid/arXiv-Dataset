NOISE-INDUCED BEHAVIORS IN NEURAL MEAN FIELD
DYNAMICS

arXiv:1104.5425v2 [math.DS] 26 Mar 2015

JONATHAN TOUBOUL∗‡ § , GEOFFROY HERMANN¶, AND OLIVIER FAUGERAS¶
Abstract. The collective behavior of cortical neurons is strongly affected by the presence of noise
at the level of individual cells. In order to study these phenomena in large-scale assemblies of neurons,
we consider networks of firing-rate neurons with linear intrinsic dynamics and nonlinear coupling,
belonging to a few types of cell populations and receiving noisy currents. Asymptotic equations as
the number of neurons tends to infinity (mean field equations) are rigorously derived based on a
probabilistic approach. These equations are implicit on the probability distribution of the solutions
which generally makes their direct analysis difficult. However, in our case, the solutions are Gaussian,
and their moments satisfy a closed system of nonlinear ordinary differential equations (ODEs), which
are much easier to study than the original stochastic network equations, and the statistics of the
empirical process uniformly converge towards the solutions of these ODEs. Based on this description,
we analytically and numerically study the influence of noise on the collective behaviors, and compare
these asymptotic regimes to simulations of the network. We observe that the mean field equations
provide an accurate description of the solutions of the network equations for network sizes as small as
a few hundreds of neurons. In particular, we observe that the level of noise in the system qualitatively
modifies its collective behavior, producing for instance synchronized oscillations of the whole network,
desynchronization of oscillating regimes, and stabilization or destabilization of stationary solutions.
These results shed a new light on the role of noise in shaping collective dynamics of neurons, and
gives us clues for understanding similar phenomena observed in biological networks.
Key words. mean field equations, neural mass models, bifurcations, noise, dynamical systems
AMS subject classifications. 34C15, 34C23, 60F99, 60B10, 34C25

Introduction. The brain is composed of a very large number of neurons interacting in a complex nonlinear fashion and subject to noise. Because of these interactions,
stimuli tend to produce coherent global responses, often with high reliability. At the
scale of single neurons the presence of noise and nonlinearities often results in highly
intricate behaviors. However, at larger scales, neurons form large ensembles that share
the same input and are strongly connected, and at these scales, reliable responses to
specific stimuli may arise. Such population assemblies (cortical columns or cortical
areas) feature a very large number of neurons. Understanding the global behavior of
these large-scale neural assemblies has been a focus of many investigations in the past
decades. One of the main interests of large-scale modeling is to characterize brain
function at the scale most non-invasive imaging techniques operate. Its relevance is
also connected to the fact that anatomical data recorded in the cortex reveal its columnar organization at scales ranging from about 50µm to 1mm. These so-called cortical
columns contain on the order of hundreds to hundreds of thousands of neurons, and
are generally dedicated to specific functions. For example, in the visual cortex V1,
they respond to preferred orientations in visual stimuli. These anatomical and functional organizations point towards the fact that a significant amount of information
processing does not occur at the scale of individual neurons but rather corresponds to
a mesoscopic signal arising from the collective dynamics of many interacting neurons.
∗ The Mathematical Neuroscience Lab, Collège de France, Center of Interdisciplinary Research in
Biology, 11 place Marcelin Berthelot, 75005 Paris, CNRS UMR 7241m INSERM U1050, Université
Pierre et Marie Curie ED 158. MEMOLIFE Laboratory of excellence and Paris Science Lettre. 11,
place Marcelin Berthelot, 75005 Paris, France.
‡ BANG Laboratory, INRIA Rocquencourt, France
§ jonathan.touboul@inria.fr
¶ NeuroMathComp Laboratory, INRIA/ENS, 23 avenue d’Italie, 75013 Paris, France

1

2

J. TOUBOUL, G. HERMANN & O. FAUGERAS

In the computational neuroscience community, this problem has been mainly
studied using approaches based on the statistical physics literature. Most models
describing the emergent behavior arising from the interaction of neurons in large-scale
networks have relied on continuum limits since the seminal works of Wilson and Cowan
and Amari [3, 4, 57, 58]. Such models represent the activity of the network through
a global variable, the population-averaged firing rate, which is generally assumed to
be deterministic. Many analytical properties and numerical results have been derived
from these equations and related to cortical phenomena, for instance in the case of
the problem of spatio-temporal pattern formation in spatially extended models (see
e.g. [18, 25, 27, 10]). This approach implicitly makes the assumption that the effect
of noise vanishes in large populations.
However, increasingly many researchers now believe that the different intrinsic
or extrinsic noise sources participate in the processing of information. Rather than
having a pure disturbing effect there is the interesting possibility that noise conveys
information and that this can be an important principle of brain function [46]. In
order to study the effect of the stochastic nature of the firing in large networks, many
authors strived to introduce randomness in a tractable form. A number of computational studies that successfully addressed the case of sparsely connected networks
of integrate-and-fire neurons are based on the analysis of large assemblies that fire in
an asynchronous regime [1, 5, 13]. Because of the assumption of sparse connectivity,
correlations of the synaptic inputs can be neglected for large networks. The resulting
asynchronous irregular states resemble the discharge activity recorded in the cerebral
cortex of awake animals [20].
Other models have been introduced to account for the presence of noise in neuronal networks, such as the population density method and related approaches [16],
allowing efficient simulation of large neuronal populations. In order to analyze the
collective dynamics, most population density-based approaches involve expansions in
terms of the moments of the resulting random variables, and the moment hierarchy
needs to be truncated in order to get a closed set of equations, which can raise a
number of technical issues (see e.g.[37]).
Yet other models of the activity of large networks are based on the definition of
a Markov chain governing the firing dynamics of the neurons in the network, where
the transition probability satisfies a differential equation called the master equation.
Seminal works of the application of such modeling for neuroscience date back to the
early 90s and have been recently developed by several authors [44, 24]. Most of these
approaches are proved correct in some parameter regions using statistical physics tools
such as path integrals [14] and Van-Kampen expansions [8]. They motivated a number of interesting studies of quasicycles [9] and power-law distribution of avalanche
phenomena [7]. In many cases the authors consider one-step Markov chains, implying that at each update of the chain, only one neuron in the whole network either
fires or stops firing, which raises biological plausibility issues. Moreover, analytical
approaches mainly address the dynamics of a finite number of moments of the firing
activity, which can also raise such issues as the well-posedness [37] and the adequacy
of these systems of equations with the original Markovian model [51].
In the present study, we apply a probabilistic method to derive the limit behavior
resulting from the interaction of an infinite number of firing-rate neurons nonlinearly
interconnected. This approach differs from other works in the literature in that it
relies on a description of the microscopic dynamics (neurons in the network), and
does not make the assumption of a sparse connectivity. Our model takes into account

Noise effects in mean field dynamics

3

the fact that cortical columns feature different populations. The approach consists in
deriving the limit equations as the total number of neurons tends to infinity, based
on results obtained in the field of large-scale systems of interacting particles. This
problem has been chiefly studied for solving statistical physics questions, and has been
a very active field of research in mathematics during the last decades [40, 23, 49, 48].
In general, the equations obtained by such rigorous approaches are extremely hard
to analyze. They can be either seen as implicit equations in the set of stochastic
processes, or as non-local partial differential equations on the probability distribution
through the related Fokker-Planck equations. But in both cases, understanding the
dynamics of these equations is very challenging, even for basic properties such as the
existence and uniqueness of stationary solutions and a priori estimates [33]. It appears
even more difficult to understand qualitatively the effects of noise on the solutions and
to interpret them in terms of the underlying biological processes.
In the present article we aim at answering some of these questions. In the case
we address, the problem is rigorously reducible to the analysis of a set of ordinary
differential equations. This is because the solution of the mean field equations is a
Gaussian process. It is therefore completely determined by its first two moments which
we prove to be the solutions of ordinary differential equations. This allows us to go
much deeper into the analysis of the dynamical effects of the parameters, in particular
those related to the noise, and to understand their influence on the solutions. The
analysis of this Gaussian process also provides a rich amount of information about
the non-Gaussian solution of the network when its size is large enough.
The paper is organized as follows. In the first section we deal with the modeling,
the derivation of the mean field equations and of the related system of ordinary
differential equations. We then turn in section 2 to the analysis of the solutions of
these equations and the influence of noise. We show in details how noise strongly
determines the activity of the cortical assembly. We then return to the problem
of understanding the behavior of finite-size (albeit large) networks in section 3 and
compare their behavior with those of the solutions of the mean field equations (infinitesize network). The analysis of the network behaviors in the different regimes of the
mean field equations provides an interpretation of the individual behaviors responsible
for collective reliable responses. This has a number of consequences that are developed
in section 4.
1. Model and mean field equations. In all the article, we work in a complete
probability space (Ω, F, P) assumed to satisfy the usual conditions.
We are interested in the large scale behavior arising from the nonlinear coupling
of a large number N of stochastic diffusion processes representing the membrane
potential of neurons in the framework of rate models (see e.g. [19, 30]). Hence the
variable characterizing the neuron state is its firing rate, that exponentially relaxes to
zero when it receives no input, and that integrates both external input and the current
generated by its neighbors. The network is composed of P neural populations that
differ by their intrinsic dynamics, the input they receive and the way they interact
with the other neurons. Each population α ∈ {1, . . . , P } is composed of Nα neurons,
and we assume that the ratio Nα /N converges to a constant δα in ]0, 1[ when the total
number of neurons N becomes arbitrarily large. We define the population function
p that maps the index i ∈ {1, . . . N } of any neuron to the index α of the population
neuron i belongs to: p(i) = α.
For any neuron i in population α, the membrane potential Vti has a linear intrinsic
dynamics with a time constant τα . The membrane potential of each neuron returns

4

J. TOUBOUL, G. HERMANN & O. FAUGERAS

to zero exponentially if it receives no input. The neuron i in population α receives
an external current, which is the sum of a deterministic part Iα (t) and a stochastic additive noise driven by N independent adapted Brownian motions (B i )i=1...N
modulated by the diffusion coefficients λα (t). This additive noise term accounts for
different biological phenomena [2], such as intrinsically noisy external input, channel
noise [56] produced by the random opening and closing of ion channels, thermal noise
and thermodynamic noise related to the discrete nature of charge carriers.
Neurons also interact through their firing rates, classically modeled as a sigmoidal
transform of their membrane potential. These sigmoidal functions only depend on the
population α the neuron i belongs to: Sα (V i ). The functions Sα : R 7→ R are assumed
to be smooth (Lipchitz continuous), increasing functions that tend to 0 at −∞ and to
1 at ∞. The firing rate of the presynaptic neuron j, multiplied by the synaptic weight
Jij , is an input current to the postsynaptic neuron i. We classically assume that the
synaptic weight Jij is equal to Jp(i)p(j) /Np(j) . In practice this synaptic weight randomly varies depending on the local properties of the environment. Models including
this type of randomness are not covered in this paper. The scaling assumption is
necessary to ensure that the total input to a neuron does not depend on the network
size.
The network behavior is therefore governed by the following set of stochastic
differential equations:


P
X
X
Jαβ
1
Sβ (V j (t)) dt + λα (t)dBti (1.1)
dV i (t) = − V i (t) + Iα (t) +
τα
Nβ
β=1

j: p(j)=β

These equations represent a set of interacting diffusion processes. Such processes
have been studied for instance by McKean, Tanaka and Sznitman among others [40,
50, 49, 48]. This case is simpler than the different cases treated in [53] where the
intrinsic dynamics of each individual diffusion is nonlinear.
We aim at identifying the limit in law of the activity of finite sets of neurons
in the network as the number of neurons tend to infinity. The identification of this
law will imply the fact that the system satisfies the propagation of chaos property.
This property states that, provided the initial conditions are independent and identically distributed for all neurons of each population (initial conditions are said to be
chaotic ∗ ), then in the limit N → ∞, all neurons behave independently, and have the
same law which is given by an implicit equation on the law of the limiting process
(the chaos of the initial condition is propagated for all time t > 0). In details, the
law of (V i1 (t), . . . , V ik (t), t ≤ T ) for any fixed k ≥ 1 and (i1 , . . . , ik ) ∈ Nk , converges
towards νp(i1 ) ⊗ . . . ⊗ νp(ik ) when N → ∞, where να denotes the law of the solution
of equation (1.2) corresponding to population α.
This convergence and the limit equations are the subject of the following theorem:
Theorem 1.1. Let T > 0 a fixed time. Under the previous assumptions, we
have:
(i). The process V i for i in population α, solution of equation (1.1), converges in
law towards the process V̄ α solution of the mean field implicit equation:
∗ Note that the term chaos is understood here in the statistical physics sense as the Boltzmann’s
molecular chaos (”Stoßzahlansatz”), corresponding to the independence between the velocities of two
different particles before they collide. This is very different from the notion of chaos in deterministic
dynamical systems.

5

Noise effects in mean field dynamics




P
X


1
dV̄ α (t) = − V̄ α (t) + Iα (t) +
Jαβ E Sβ (V̄ β (t))  dt+λα (t)dB α (t)
τα
β=1

(1.2)
as a process for t ∈ [0, T ], in the sense that there exists (V̄ti )t≥0 distributed as
(V̄tα )t≥0 such that

E


C̃(T )
sup |Vti − V̄ti | ≤ √
N
0≤t≤T

where C̃(T ) is a finite quantity depending on the time horizon T and on the
parameters of the system. As a random variable, it converges uniformly in
time in the sense that:


C
sup E |Vti − V̄ti | ≤ √
N
0≤t≤T
where C does not depend on time. In equations (1.2), the processes (B α (t))α=1...P
are independent Brownian motions.
(ii). Equation (1.2) has a unique (trajectorial and in law) solution which is square
integrable.
(iii). The propagation of chaos applies.
Note that the expectation term in equation (1.2) is the classical expectation of a
function of a stochastic process. In details, if pβt is the probability density of V̄ β (t),


R
E Sβ (V̄ β (t)) is equal to R Sβ (x)pβt (x) dx.
The proof of this theorem essentially uses results from the works of Tanaka and
Sznitman, summarized in [48]. A distinction with these classical results is that the
network is not totally homogeneous but composed of distinct neural populations.
Thanks to the assumption that the proportion of neurons in each population is nontrivial (Nα /N → δα ∈]0, 1[), the propagation of chaos occurs simultaneously in each
population yielding our equations, as it is shown in a wider setting in [53]. The
main deep theoretical distinction is that the theorem claims a uniform convergence
in time: most of the results proved in the kinetic theory domain show propagation
of chaos properties and convergence results only for finite time intervals, and convergence estimates diverge as the interval grows. Uniform propagation of chaos is
an important property as commented in [17], and particularly in our case as we will
further comment. Methods to prove uniformity are generally involved (see e.g. [41]
where uniformity is obtained for certain models using a dual approach based on the
analysis of generator operators). Due to the linearity of the intrinsic dynamics, we
provide here an elementary proof of this property in our particular system.
Proof. The existence and uniqueness of solutions can be performed in a classical
fashion using Picard iterations of an integral form of equation (1.2) and a contraction
argument. The proof of the convergence towards this law, and of the propagation
of chaos can be performed using Sznitman’s powerful coupling method (see e.g. [48],
method which dates back from Dobrushin [23]), that consists in exhibiting an almost
sure limit of the sequence of processes Vti as N goes to infinity by coupling the mean
field equation with the network equation as follows. We define the different independent processes V̄ i solution of equation (1.2) driven by the same Brownian motion
(Bti )t as involved in the network equation (1.1), and with the same initial condition

6

J. TOUBOUL, G. HERMANN & O. FAUGERAS

V i (0) as neuron i in the network. It is clear that these processes are independent
(since the (Bti ) are pairwise independent) and have the same law as the solution of
the mean field equation (1.2). The almost sure convergence of (Vti ) towards (V̄ti ) will
therefore imply the convergence in law towards the mean field equation. For a neuron
i belonging to population α, we have:

Vti

−

V̄ti

=

P
X

Z
Jαβ

e
0

β=1

t
−(t−s)/τα

1
Nβ

(
X




Sβ (Vsj ) − Sβ (V̄sj )

j: p(j)=β

+



Sβ (V̄sj )

−E



Sβ (V̄sj )



)
ds

We have, denoting by τ the maximal value of (τβ , β = 1 . . . P ):

|Vti

−

V̄ti |

Z
≤ Kα

t

e−(t−s)/τ max |Vsj − V̄sj | ds
j=1...N



 Z t
N
X





1
+ Kα0 
Sp(j) (V̄sj ) − E Sp(j) (V̄sj ) ds , (1.3)
e−(t−s)/τα
N 0

j=1
0

P
where Kα = β |Jαβ |L. L is the largest Lipschitz constant of the sigmoids (Sβ , β =
1 . . . P ), and Kα0 = maxβ |Jαβ |N/Nβ , quantity upperbounded, for N sufficiently large,
by maxβ |Jαβ |2/δβ .
Since the righthand side of (1.3) does not depend on the index i, taking the
maximum with respect to i and the expected value of both sides of (1.3), we obtain



Z t
i
max |Vti − V̄ti | ≤ K
e−(t−s)/τ E max |Vsj − V̄sj | ds
i=1...N
j=1...N
0




 Z t
N
X


 
1
−(t−s)/τα
j
j
0 

+K E
max
e
Sp(j) (V̄s ) − E Sp(j) (V̄s ) ds (1.4)
α=1,...,P  N 0

j=1

h
E



Since the random variables Aj (s) = Sp(j) (V̄sj ) − E Sp(j) (V̄sj ) are independent and
centered (i.e. have a null expectation), using the fact that the sigmoids Sβ take their
values in the interval [0, 1], using Cauchy-Scwartz and posing τ̄ = maxα τα +1 = τ +1,

7

Noise effects in mean field dynamics

we have:




1

E max 
α
N

Z

t

e−(t−s)/τα

0

2 



Sp(j) (V̄sj ) − E Sp(j) (V̄sj ) ds 

j=1


=

N
X



1 
E max 
α
N2

Z t

e

α
−(t−s) τ̄τ̄−τ
τα



0


2 
N
X


e−(t−s)/τ̄ Aj (s) ds
j=1



≤

≤

=

≤
≤



Z t
 Z t
N
X

τ̄ −τα
1 
2
e−2(t−s) τ̄ τα ds 
E max
e−2(t−s)/τ̄
Aj (s) ds
α
N2
0
0
j=1



Z t
 Z t
N
X

1 
2
E
e−2(t−s)/(τ (τ +1)) ds 
e−2(t−s)/(τ +1)
Aj (s) ds
N2
0
0
j=1


Z t
N
X
1 τ (τ + 1)
(1 − e−2t/(τ (τ +1)) )
Aj (s)2  ds
e−2(t−s)/(τ +1) E 
N2
2
0
j=1
Z t
1 τ (τ + 1)
e−2(t−s)/(τ +1) ds
N
2
0
1 τ (τ + 1)2
τ0
=
N
4
N

By Cauchy-Schwartz inequality,
p we can upperbound the second term of the right
hand side of inequality (1.4) by τ 0 /N . Therefore, defining Mt = E maxi |Vti − V̄ti | ,
K = maxα Kα and K 0 = maxα Kα0 we have:
Z

r

t

Mt ≤ K

−(t−s)/τ

e

Ms ds + K

0

0

τ0
N

implying, using Gronwall’s lemma,
√
K 0 τ 0 eKτ
√
Mt ≤
.
N
This inequality readily yields the almost sure convergence of Vti towards V̄ti as N goes
to infinity, uniformly in time, and hence convergence in law of Vti towards V̄tα .
The almost sure convergence of (Vti )t∈[0,T ] (considered as a process) towards
can be proved in a similar fashion. Indeed, upperbounding the exponential
term in (1.3) by 1 and taking the supremum, it is easy to see that:

(V̄ti )t∈[0,T ]


E

sup

max

0≤t≤T i=1...N

|Vti



−

V̄ti |

Z
≤K

"

T

E
0

#
sup

max

s∈[0,t] j=1...N

|Vsj

−

V̄sj |

K0 T
dt + √ ,
N

8

J. TOUBOUL, G. HERMANN & O. FAUGERAS

using the fact that:



Z

1

E max sup 
α t∈[0,T ]
N

≤

T
N2

e−(t−s)/τα

0

T

Z

t

0

N
X

2 



Sp(j) (V̄sj ) − E Sp(j) (V̄sj ) ds 

j=1


2 
N


  
X
j
j
Sp(j) (V̄s ) − E Sp(j) (V̄s )   ds
E 
 j=1


=



N Z

 2
T X T

j
j
ds
(
V̄
)
−
E
S
(
V̄
)
E

S
p(j) s
p(j) s
N 2 j=1 0

≤

T2
N

using the independence of the V̄ j and Cauchy-Schwartz inequality. This last estimate
readily implies, using Gronwall’s inequality:

E

sup

max

0≤t≤T i=1...N

|Vti



−

V̄ti |

≤

K 0 T eK T
√
.
N

The propagation of chaos property (iii) stems from the almost sure convergence of
(V i1 (t), . . . , V ik (t), t ≤ T ) towards (V̄ i1 (t), . . . , V̄ ik (t), t ≤ T ) which are independent,
as a process and uniformly for fixed time, and is proved in a similar fashion.
The P equations (1.2), which are P implicit stochastic differential equations,
describe the asymptotic behavior of the network. However, the characterization and
simulation of their solutions is a challenge. Fortunately, due to their particular form
in our setting, these equations can be substantially simplified. Indeed, under some
weak assumptions, the solutions of the mean field equations are shown to be Gaussian,
allowing to exactly reduce the dynamics of the mean field equations to the study of
coupled ordinary differential equations as we now show.
Proposition 1.2. Let us assume that V̄0 = (V̄0α )α=1...P is a P-dimensional
Gaussian random variable. We have:
? The solutions of the P mean field equations (1.2) with initial conditions V̄0
are Gaussian processes for all time.
? Let µ(t) = (µα (t))α=1...P denote the mean vector of (V̄tα )α=1...P and v(t) =
(vα (t))α=1...P its variance. Let also fβ (x, y) denote the expectation of Sβ (U )
for U a Gaussian random variable of mean x and variance y. We have:
(
PP
µ̇α (t) = − τ1α µα (t) + β=1 Jαβ fβ (µβ (t), vβ (t)) + Iα (t)
v̇α (t) = − τ2α vα (t) + λ2α (t)

α = 1...P
α = 1...P
 

(1.5)
with initial conditions µα (0) = E V̄0α and vα (0) = E (V̄0α − µα (0))2 . In
equation (1.5), the dot denotes the differential with respect to time.
Proof. The unique solution of the mean field equations (1.2) starting from a
square integrable initial condition V̄0 measurable with respect to F can be written in

9

Noise effects in mean field dynamics

the form:
t

t

V̄tα = e− τα V̄0α + e− τα

Z

t

s

e τα (Iα (s) +

0

P
X



Jαβ E Sβ (V̄sβ ) )ds

β=1

Z
+

t


s
e τα λα (s) dBsα . (1.6)

0

We observe that if V̄0 is a Gaussian random variable, then the righthand side of (1.6)
is necessarily Gaussian as the sum of a scaled version of this random variable, a
deterministic term and an Itô integral of a deterministic function, and hence so is
the solution of the mean field equation. Its law is hence characterizedby its mean

and covariance functions. The formula (1.6) involves the expectation E Sβ (V̄ β (s)) ,
which, because of the Gaussian nature of Xβ , only depends on µβ (s) and vβ (s), and
is denoted by fβ (µβ (s), vβ (s)). Taking the expectation of both sides of the equality

(1.6), we obtain the equation satisfied by the mean of the process µα (t) = E V̄ α (t) :


 
Z t
P
X


s
− τtα  α
µ (0) +
e τα 
Jαβ E Sβ (V̄sβ + Iα (s) ds .
µα (t) = e
0

β=1

Taking the variance of both sides of the equality (1.6), we obtain the following equation:


Z t
2s
− τ2t
2
τ
vα (t) = e α vα (0) +
e α λα (s) ds ,
0

and this concludes the proof.
Remark.
• In order to fully characterize the law of the process V̄ , we need to compute the
covariance matrix function Cov(V̄tα1 , V̄tβ2 ) for t1 and t2 in R+∗ . For α 6= β this
covariance is clearly zero using equation (1.6), and we have:


Z t1 ∧t2
t +t
2s
− 1 2
Var(V̄α0 ) +
(1.7)
Cov(V̄tα1 , V̄tα2 ) = e τα
e τα λ2α (s) ds
0
+∗

for t1 , t2 ∈ R , hence only depends on the parameters of the system and is in
particular not coupled to the dynamics. The description of the solution given by
equations (1.5) is hence sufficient to fully characterize the solution of the mean field
equations (1.2).
• The uniformity in time of the propagation of chaos has deep implications in regard
of equations (1.5). Indeed, we observe that the solution of the mean field equation is
governed by the mean of the process, the expectation being a deterministic function
depending on the parameters of the system. The uniformity in particular implies
that, for i in population α:
 h i

h
i
C


sup E Vti − µα (t) ≤ sup E |Vti − V̄ti | ≤ √
(1.8)
N
t≥0
t≥0
implying uniform convergence of the empirical mean, as a function of time, towards
µα (t).
• If V̄0 is not Gaussian, the solution of equation (1.2) asymptotically converges exponentially towards a Gaussian solution. The important uniformity convergence
property towards the mean field equations ensures that the Gaussian solution is indeed the asymptotic regime of the network, which strengthens the interest of the
analysis of the differential system (1.5).

10

J. TOUBOUL, G. HERMANN & O. FAUGERAS

The functions fβ depend on the choice of the sigmoidal transform. A particularly
interesting choice is the erf sigmoidal function† Sα (x) = erf(gα x + γα ). In that case
we are able to express the function fβ in closed form, because of the following lemma:
Lemma 1.3. In the case where the sigmoidal transforms are of the form Sα (x) =
erf(gα x + γα ), the functions fα (µα , vα ) involved in the mean field equations (1.5) with
a Gaussian initial condition take the simple form:
!
gα µ + γα
.
(1.9)
fα (µ, v) = erf p
1 + gα2 v
Proof. The proof is given in Appendix A.
In summary, we have shown that, provided that the initial conditions of each
neuron are independent and Gaussian, the large-scale behavior of our linear model
is governed by a set of ordinary differential equations (theorem 1.1 and proposition
1.2). This is very interesting since it reduces the study of the solutions to the very
complex implicit equation (1.2) bearing on the law of a process to a much simpler
setting, ordinary differential equations. As shown below this allows us to understand
the effects of the system parameters on the solutions. For this reason we assume from
now on that the initial condition is Gaussian, and focus on the effect of the noise on
the dynamics.
2. Noise-induced phenomena. In this section we mathematically and numerically study the influence of the noise levels λα on the dynamics of the neuronal
populations. Thanks to the uniform convergence of the empirical mean towards the
mean of the mean field system (equation (1.8)) and the propagation of chaos property
for the network process, it is relevant to study such phenomena through the thorough
analysis of the solutions of the mean field equations given by the ODEs (1.5). This is
what we do in the present section.
As observed in equation (1.5), in the case of a Gaussian initial condition, the
equation of the variance v is decoupled from the equation on the mean µ in (1.5).
The variance satisfies an autonomous equation:
v̇α = −

2
vα + λ2α (t).
τα

which is easily integrated as:
Z t


2t
2s
vα (t) = e− τα vα (0) +
e τα λ2α (s) ds .
0

vα (t) is therefore independent of the mean µ. This implies that the equations on µ
are a set of non-autonomous ordinary differential equations.
These ordinary differential equations are similar to those of a single neuron. They
differ in that the terms in the sigmoidal functions depend on the external noise levels
† We

emphasize that in this article, the erf function is understood as the repartition function of
R x exp{−x2 /2}
√
a standard Gaussian random variable, i.e. erf(x) = −∞
. In particular, this function
2π
is always positive. It is distinct from the usual definition of the erf function corresponding to
R
2
Erf (x) = √2π 0x e−x . This function is not always positive (hence not a good rate function) and is
related to our function through the formula: erf(x) =

1
(Erf ( √x )
2
2

+ 1).

11

Noise effects in mean field dynamics

λα (t). They read:
µ̇α = −

µα
+
τα



P
X



gβ µβ + γβ
 + Iα
Jαβ erf  q
R t 2s/τ 2
2
−2t/τ
β (v (0) +
β λ (s) ds)
1 + gβ e
e
β=1
β
β
0

hence the slope gβ and the threshold γβ are scaled by a time-varying coefficient which
is always smaller than one.
We now focus on the stationary solutions when the noise parameter λ does not
depend upon time. In that case, the variance is equal to:
2t

vα (t) = τα λ2α /2 + e− τα (vα (0) − τα λ2α /2),
and converges exponentially fast towards the constant value τα λ2α /2. Asymptotic
regimes of the mean field equations are therefore Gaussian random variables with
constant standard deviation. Their mean is solution of the equation:


P
µα X
g
µ
+
γ
β β
β
 + Iα α = 1, · · · , P
Jαβ erf  q
µ̇α = −
+
τα
2
1 + g τ λ2 /2
β=1

β β β

In other words, the presence of noise has the effect of modifying the slope gα and
the threshold γα of the sigmoidal erf function, but the type of the equations is the
same as that of the equation of each individual neuron or cortical column, it is a rate
equation.
We observe that the larger the noise amplitude λ, the smaller the slope of the
sigmoidal transform. Noise has the effect of smoothing the sigmoidal transform. This
will have a strong influence on the bifurcations of the solutions to the mean field
equations and hence on the behaviors of the system. We demonstrate these effects for
two simple choices of parameters in one- and two-population networks.
2.1. The external noise can destroy a pitchfork bifurcation. Let us start
by considering the case of a one-population network. We drop the index α since no
confusion is possible. We assume for simplicity that the threshold γ of the sigmoid is
null and that the time constant τ is equal to one. By doing so, we do not restrict the
generality of the study, since τ can be eliminated by rescaling the time and γ can be
absorbed into I by a simple change of origin for µ. The network equations read:


N


X
J
dVti = −Vti +
erf g Vtj + I(t) dt + λdBti i = 1, · · · , N,
N j=1
and we are interested in the limit in law of their solutions as the number of neurons
N tends to infinity.
In order to analytically study the effect of the parameter λ, we set I ≡ − J2 .
In that case, and in the absence of noise, the solution V = 0 is a fixed point of
the network equations. The following proposition characterizes their solutions in the
deterministic and stochastic cases.
Proposition 2.1. In a non-stochastic finite-size
√ network, the null solution is:
• stable if J < 0 or if J > 0 and g < gc := 2π/J,
• unstable for J > 0 and g > gc .
• For J > 0, the system undergoes a pitchfork bifurcation at g = gc .

12

J. TOUBOUL, G. HERMANN & O. FAUGERAS

In the mean field limit of the same
stochastic network, the pitchfork bifurcation occurs
√
√
2π
for a new value of g = g ∗ = √J 2 −πλ
> gc if J > 0 and λ < J/ π. Furthermore the
2
null solution is:
• stable if:
? J < 0 or
√
? J > 0 and λ >√J/ π (large noise case) or
? J > 0, λ < J/ π and√g < g ∗ ,
• unstable for J > 0, λ < J/ π and g > g ∗ , and
∗
• the system
√ undergoes a pitchfork bifurcation at g = g when J > 0 and
λ < J/ π.
This proposition is a bit surprising at first sight. Indeed, it says that noise can
stabilize a fixed point which is unstable in the same non-stochastic system. Even
more interesting is the fact that if the system is driven by a sufficiently large noisy
input, the zero solution will always stabilize. It is known, see, e.g., [38], that noise
can stabilize the fixed points of a deterministic system of dimension greater than or
equal to 2. The present observation extends these results to a one-dimensional case,
in a more complicated setting because of the particular, non-standard, form of the
mean field equations. Also note that this proposition provides a precise quantification
of the value of the parameter that destabilizes the fixed point. This is a stochastic
bifurcation of the mean field equation (a P-bifurcation –P for phenomenological– in
the sense of [6]). This estimation will be used as a yardstick for the evaluation of the
behavior of the solutions to the network equations in section 3.
Proof. We start by studying the finite-size deterministic system. In the absence
of noise, it is obvious because of our assumptions that the solution V i = 0 for all
i ∈ {1, . . . , N } is a fixed point of the network equations. At this point, the Jacobian
J √g
matrix reads −IdN + N
1 , where IdN is the N ×N identity matrix and 1N is the
2π N
N × N matrix with all elements equal to one. The matrix 1N is diagonalizable, all its
eigenvalues are equal to zero except one which is equal to N . Hence, all eigenvalues of
g
the Jacobian matrix are equal to −1, except one which is equal to √J2π
−1. The solution
i
where all V are
√ equal to zero in the deterministic system is therefore stable if and
only if g J < 2π. The eigenvalue corresponding to the destabilization corresponds
→
−
to the eigenvector 1 whose components are all equal to 1. Interestingly, this vector
does not depend on the parameters, and therefore it is easy to check that at the point
g = gc the system loses stability through a pitchfork bifurcation. Indeed, because of
the symmetry of the erf function, the second derivative of the vector field projected
on this vector vanishes, while the third derivative does not (it is equal to −(1 + g 2 )).
Considering now the stochastic mean field limit, the stationary mean firing rate
in that case is solution of the equation:
!
gµ
µ̇ = −µ + Jerf p
+I
1 + g 2 λ2 /2
Here again, the null firing rate point µ = 0 is a fixed point of the mean field equations,
g
and it is stable if and only if −1 + J √
< 0. The remaining of the
2 2
2π(1+g λ /2)

proposition
readily follows from the fact that the stability changes at g = g ∗ where
∗
g
= 1.
J√
∗2 2
2π(1+g

λ /2)

Note that the results in this proposition only depend on λ and its effect on the
slope of the sigmoid. It is a general phenomenon that goes beyond the example in this

Noise effects in mean field dynamics

13

section: increasing λ decreases the slope of the sigmoidal transform and the threshold.
In section 3 we will see that this phenomenon can be observed at the network level,
and a good agreement will be found between the finite-size network behavior and the
predictions obtained from the mean field limit.
We now turn to an example in a two-dimensional network, where the presence of
oscillations will be modulated by the noise levels.
2.2. The external noise can destroy oscillations. The same phenomenon
of nonlinear interaction between the noise intensity and the sigmoid function can
lead, in higher dimensions, to more complex phenomena such as the disappearance or
appearance of oscillations. In order to study phenomena of this type, we instantiate a
simple two-populations network model in which, similarly to the one-dimensional case,
all the calculations can be performed analytically. The network we consider consists
of an excitatory population, labeled 1, and an inhibitory population, labeled 2. Both
populations are composed of the same number N/2 of neurons (N is assumed in all
the subsection to be even), and have the same parameters τ1 = τ2 = τ , g1 = g2 = g
and λ1 = λ2 = λ. We choose for simplicity the following connectivity matrix:


2
1 −1
,
M =J
1 1
N
and we assume that the inputs are set to I1 = 0 and I2 = −J. The zero solution
where all neurons have a zero voltage is a fixed point of the equations whatever the
number of neurons N in each population. We have the following result:
Proposition 2.2. In the deterministic finite-size
network, the null solution is:
√
• stable if J < 0 or if J > 0 and g < gc := 2π/J,
• unstable for J > 0 and g > gc and the solutions are oscillating on a periodic
orbit.
• For J > 0 the system undergoes a supercritical Hopf bifurcation at g = gc .
In the mean field limit of the same stochastic network,
the Hopf bifurcation occurs for
√
2π
a new value of the slope parameter g = g ∗ = √J 2 −πλ
> gc . Furthermore the null
2
solution is:
• stable if:
? J < 0 or
√
? J > 0 and λ >√J/ π (“large noise case”) or
? J > 0, λ < J/ π and √
g < g∗ ,
• unstable for J > 0, λ < J/ π and g > g ∗ , and the system features a stable
periodic orbit.
• The system√undergoes a supercritical Hopf bifurcation at g = g ∗ when J > 0
and λ < J/ π.
Note that proposition 2.2 is quite similar to proposition 2.1, the qualitative difference being that the system is oscillating. The proof is closely related and is presented
in less details.
Proof. In the deterministic network model, the Jacobian matrix at the null equilibrium can be written as
g
A = −IdN + √ M ⊗ 1N/2
2π
where ⊗ denotes the Kronecker product (see e.g. [43, 11]), i.e. the Jacobian matrix is
built from N/2 blocs of size 2 × 2 and each of these blocks is a copy of √g2π M . The

14

J. TOUBOUL, G. HERMANN & O. FAUGERAS

eigenvalues of a Kronecker product of two matrices are all possible pairwise products
of the eigenvalues of the matrices. Since the eigenvalues of M are equal to 2NJ (1 ± i)
where i2 = −1, and as noted previously, the eigenvalues of 1N/2 are 0 with multiplicity
N/2 − 1 and N/2 with multiplicity 1, we conclude that the Jacobian matrix
A has
√
N − 2 eigenvalues equal to −1, and two eigenvalues equal to −1 + g J/ 2π(1 ± i).
The null equilibrium in this deterministic system is therefore
√ stable if and only if the
2π. Therefore, for a fixed
real parts of all eigenvalues are smaller than
0,
viz.
gJ
<
√
J, the system has a bifurcation at gc = 2π/J. The analysis of the eigenvectors
allows to check the genericity and transversality conditions of the Hopf bifurcation
(see e.g. [32]) in a very similar fashion to the proof of proposition 2.1.
In the mean field model, the same analysis applies and, as in the one-dimensional
case, the bifurcation point is shifted to g ∗ when this value is well-defined, which
concludes the proof of the proposition 2.2.
We have therefore shown that noise can destroy the oscillations of the network.
The results of propositions 2.1 and 2.2 are summarized in figure 2.1.

Bo
th

M
D FS
et ta
er b
m le
in
is
tic

St

ab
le

Bo
th

U

ns

Un

ta

bl
e

st

ab
le

0
Figure 2.1. Summary of the results in propositions 2.1 and 2.2, see text. The additive noise
parameter λ smoothly modifies the pitchfork or the Hopf bifurcation curve in the (g, J) plane. For λ
large enough, the null solution of the mean field equation is always stabilized whatever g.MF: mean
field limit, Deterministic: finite-size deterministic
network. The
red) curve is one
√ blue (respectively
√
√
branch of the hyperbola of equation gJ = 2π (respectively g J 2 − πλ2 = 2π).

An even more interesting phenomenon is that noise can also produce regular
cycles in the mean part of the solution of the mean field equations, for parameters
such that the deterministic system presents a stable equilibrium. This is the subject
of the following section.
2.3. The external noise can induce oscillations. In order to uncover further effects of the noise on the dynamics, we now turn to the numerical study of
a two-populations network including excitation and inhibition. The time constant
τ , sigmoidal transforms S, noise intensity λ and the initial condition on the variance are chosen identical for both population. Under these hypotheses, the variances
of the two populations are identical and denoted by v(t). We further assume that
S(x) = erf(g x), and hence the mean field nonlinear function f (µ, v) is given by

Noise effects in mean field dynamics

15




15 −12
. The input cur16 −5
rents I1 and I2 are considered constant. The mean field equations in that case read:

µ1

µ̇1 = − τ + J11 f (µ1 , v) + J12 f (µ2 , v) + I1
µ̇2 = − µτ2 + J21 f (µ1 , v) + J22 f (µ2 , v) + I2


v̇ = −2 τv + λ2
lemma 1.3. The connectivity matrix is set to J =

1
N

The codimension two bifurcation diagram of the system for I2 = −3 is displayed
in Figure 2.2 (qualitative results turn out to change smoothly when I2 is also allowed
to vary). It features two cusps (CP) and one Bogdanov-Takens (BT) bifurcations. In
addition to these local bifurcations, we observe that the Hopf bifurcation manifold
(shown in pink in Figure 2.2) and the saddle-homoclinic bifurcation curve (green line)
present a turning point, i.e. change monotony as a function of λ.
The diagram can be decomposed into 4 different regions depending on the dynamical features (number and stability of fixed points or cycles): the “trivial” zone
where the system features a unique stable fixed point (not colored), a zone with 2
unstable and 1 stable fixed point (green zone (a)) separated by the saddle-homoclinic
bifurcation curve from region (b) (yellow) where an additional stable cycle exists.
Zone (c) (orange) features a stable cycle and an unstable fixed point, and zone (d)
(green) features 2 stable and 1 unstable fixed points.
Let us for instance fix I1 = 0. As λ is increased, several noise-induced transitions
occur leading the system successively in zone (a), (b), (c) and the trivial zone (see
codimension one bifurcation diagram in Figure 2.2 (i)). In details, for small noise levels
the system features a unique stable fixed point (zone (a)). A family of large amplitude
and small frequency periodic orbits appears from the saddle-homoclinic bifurcation
yielding a bistable regime (zone (b)) before the stable fixed point disappears through
a saddle-node bifurcation (zone (c)). The amplitude of these cycles progressively decreases and their frequency progressively increases as the noise intensity is increased,
and they eventually disappear through a supercritical Hopf bifurcation leading to the
trivial behavior with a single fixed point. We emphasize here the fact that the sudden
appearance of large amplitude slow oscillations can be compared to epileptic spikes,
which are characterized by the presence of collective oscillations of large amplitude
and small frequency suddenly appearing in a population of neurons (see [52]). This
comparison turns out to be relevant from the microscopic viewpoint: network simulations of section 3 will indeed show a sudden synchronization of all neurons at this
transition.
The diagram can also be decomposed into six different noise levels intervals (labels
(A) through (F) in Figure 2.2) corresponding to qualitatively different codimension 1
bifurcation diagrams as I1 is varied (the six corresponding bifurcation diagrams are
presented in appendix B). The presence of these different zones illustrate how the
noise influences the response of the neural assembly to external inputs. For instance,
for λ large enough, no cycles exist whatever I1 (zones (E-F)), whereas for λ small
enough (zones A-D), cycles always exist for some values of the input. Such partitions
may provide an experimental design for evaluating a noise level range as a function
of the observed dynamics when varying the input to the excitatory population for
instance.
3. Back to the network dynamics. Thus far, we studied the dynamics of the
mean field equations representing regimes of the network dynamics in the limit where

16

J. TOUBOUL, G. HERMANN & O. FAUGERAS

(A)

(i)

(F)

(C) (D) (E)

(B)

5

(a)

(b)

LP

2

BT

CP

1
0

(d)

H

-1
-2

0

-3

(c)
0

1

2

4

3

CP
-5
0

4

2

(iii)

(ii)
BT

-2.2

(C)

(D)

1.9

(a)

(b)

1.8

(d)

CP
1.7

-2.28
0

0.1

2.93

2.95

2.97

Figure 2.2. Codimension two bifurcation diagram (upper left) and zooms (subfigures (ii) and
(iii)) for the mean field equations as I1 and λ are varied. We distinguish, apart from the trivial
regime with a single fixed point, three dynamical regimes labeled (a), (b) and (c) (see text) and 6
ranges of λ, labeled (A) through (F). Blue: saddle-node bifurcations, pink: Hopf bifurcations, green:
saddle homoclinic bifurcations, BT: Bogdanov Takens bifurcation, CP: cusp. Individual behaviors
in each zone are summarized in appendix B. (i): Codimension 1 bifurcation diagram for I1 = 0 as
a function of λ: saddle-node (LP), a Hopf (H) and a saddle homoclinic bifurcation (green circles).
We observe three main different noise regimes: a high-state equilibrium regime, a periodic regime
and a low-state equilibrium regime. A small interval of values of λ corresponds to the co-existence of
cycles and a fixed point close to the saddle-homoclinic orbit. Diagrams obtained with XPPAut [26]
and MatCont package [22, 21].

the number of neurons is infinite. We now compare the regimes identified in this
analysis with simulations of the finite-size stochastic network. We are particularly
looking for potential finite-size effects, namely qualitative differences between the
solutions to the network and the mean field equations. This will provide us with
information about the accuracy of an approximation of the network dynamics by the
mean field model, as function of the size of the network.
3.1. Numerical Simulations. Numerical simulations of the network stochastic
differential equations (1.1) are performed using the usual Euler-Maruyama algorithm
(see e.g. [39, 38]) with fixed time step (less than 0.01) over an interval [0, T ]. In order

17

Noise effects in mean field dynamics
3

10

Computational Time

2

10

1

10

0

10

−1

10

0

10

1

10

2

10

3

10

4

10

5

10

6

10

Network size

Figure 3.1. Computation time for the simulation of the stochastic network in logarithmic scale
as a function of the network size.

to observe oscillations, we choose T between 50 and 70. The simulations are performed
R using a vectorized implementation that has the advantage to be very
with Matlab,
efficient even for large networks. The computation time stays below 1s for networks
up to 2 000 neurons, and appears to grow linearly with the size of the network once
the cache memory saturates (see Figure 3.1). For instance, for T = 20, dt = 0.01,
the simulation of a 2 000 neurons network takes 0.89s, and for 525 000 neurons, 600s
on a HP Z800 with 8 Intel Xeon CPU E5520 @ 2.27 GHz 17.4 Go RAM. The main
limitation preventing the simulation of very large networks is the amount of memory
required for the storage of the trajectories of all neurons.
An important property arising from theorem 1.1 is that asymptotically, neurons
behave independently and have the same probability distribution. In our numerical
simulations, we will make use of this asymptotic independence and, in order to evaluate an empirical mean of the process related to a given neuron in population α, will
compute both the empirical mean over all neurons in that population and a mean
over different independent realization of the process. This method allows to reduce
sensitively the number of independent simulations in order to obtain a given precision
in the empirical mean evaluation.
3.2. A one population case. We start by addressing the case discussed in
section 2.1 where we showed analytically that the loss of stability of the null fixed
point as the slope of the sigmoid was varied depended on the noise parameter λ. We
now investigate numerically the stability of the 0 fixed point of the network equations.
In order to check for the presence of a pitchfork bifurcation, we compute, for each
value of the noise and for each value of the slope of the sigmoid, an estimated value
of the mean of the membrane potential. This estimate is calculated by averaging
out over 500 independent realizations the empirical mean of the membrane potentials
of all neurons in the network at the final time. We display the average value then
compare these simulations with those of the mean field equations stopped at the same
time as the network. We observe that both are very similar and show some differences
with the bifurcation diagram that corresponds to the asymptotic regimes.
The results of the simulations, where we have also varied N , are shown in Figure 3.2 and reveal two interesting features. First, because we simulate over a finite
time, we tend to smooth the pitchfork bifurcation: this is perceptible for both the
network and the mean field equations. Second, we observe that the loss of stability of
the zero fixed point arises at the value of λ predicted by the analysis of the mean field
equations for networks as small as 50 neurons. The value reached by the simulations

18

J. TOUBOUL, G. HERMANN & O. FAUGERAS

of the network is very close to that related to the mean field equation as soon as N
becomes greater than 250.
0.4
0.6

0.35

0.5

0.3

0.4

0.25
0.2

0.3

0.15
0.2

0.1
0.1

0.05

0

−0.1
1.5

MeanField
Fixed Point
Finite Time
Network Size:
N=1000
N=750
N=500
N=250
N=100
N=75
N=50
N=25
N=10

0

2

2.5

3

3.5

4

4.5

5

5.5

g

(a) Network Simulations vs mean field simulations, different λ, N = 50 000

−0.05
1.5

2

2.5

3

3.5

4

4.5

5

5.5

g

(b) Network simulations vs mean field simulations, different N , λ = 0.4

Figure 3.2. Comparison of the pitchfork bifurcations with respect to the slope parameter g for
the network and the mean field equations, T = 40, dt = 0.001, number of sample paths: 100, initial
condition V 0 = 0.5 (hence we only see the positive part of the pitchfork, symmetrical solutions
are found for negative initial conditions, and are not plotted for legibility). (a): 50 000 neurons.
Continuous curves correspond to network simulations, dashed curves to mean field simulations.
When λ increases, as predicted by proposition 2.1, we observe that the value of the parameter g
related to the pitchfork bifurcation
increases as well, until the pitchfork disappears: red: λ = 0, blue:
√
λ = 0.4, green: λ = 0.8 > 2π/J ∼ 0.56. (b): λ = 0.4. The solution to the mean field equation
undergoes a pitchfork bifurcation at g = 3.55. Large dotted red: theoretical pitchfork bifurcation.
Large black: endpoint of mean field simulation at time T = 40. The other colored curves show the
results of the network simulation for different values of the size of the network N . The 0 solution,
which loses stability, is displayed in thin dashed black. We see that as N increases, the mean field
equation describes accurately the network activity. For N ≥ 50 (red, green, dotted blue and dotted
cyan curves) the bifurcation diagram is quite close to the one predicted by the mean field analysis.

3.3. Two populations case and oscillations. We now investigate the case
shown in Figure 2.2 (i) where cycles are created (through homoclinic bifurcation) or
destroyed (through Hopf bifurcation) as the additive noise intensity parameter λ is
increased.
Looking at Figure 2.2(i), we observe that for λ ∈ [1.12, 1.33], stable periodic orbits
coexist with stable fixed points in the mean field system. For smaller values of λ, the
mean field system features a unique stable fixed point, while for λ ∈ [1.33, 1.97], it
features a unique stable limit cycle, and for λ > 1.97, the dynamics is reduced to a
unique attractive fixed point. Numerical simulations confirm this analysis. Let us for
instance illustrate the fact that the network features the bistable regime, the most
complex phenomenon. Figure 3.3 shows simulations of a network composed of 5 000
neurons in each population (time step dt = 5 · 10−3 , total time T = 50). Depending
on the mean and on the standard deviation of the initial condition, we observe that
the network either converges to the mean field fixed point or the periodic orbit. Both
mean field equations show very close behaviors.
In the fixed point regime corresponding to small values of λ we observe that the
membrane potential of every neuron randomly varies around the value corresponding
to the fixed points of the mean field equation (see Figure 3.4, cases (a) and (b))), with
a standard deviation that converges toward the constant value λ2 /2 as predicted by
the mean field equations. The empirical mean and standard deviation of the voltages

19

Noise effects in mean field dynamics

6

8

5

7

4
6
3
5

2
1

4

0

3

−1
2
−2
1

−3
−4

0
0

5

10

15

20

25

30

35

40

45

50

(a) λ = 1.2. Oscillatory regime. Statistics
of the network compared to the mean field.

0

5

10

15

20

25

30

35

40

45

50

(b) λ = 1.2. Fixed-point regime Statistics
of the network compared to the mean field.

Figure 3.3. Featuring bistability. In both cases λ = 1.2. The initial conditions for the mean
field equation are chosen in agreement with the initial conditions of the network. The initial value
of the membrane potential of each individual neuron in the network is drawn independently from
a Gaussian distribution of variance 1 whose mean depends on the population: (a) mean 0.5. (b)
mean 4. Cyan (resp. magenta) curves: value of the mean variable of the mean field solution for
population 1 (resp. 2). Dashed blue (resp. red) curves: empirical mean of population 1 (resp. 2).
Yellow: value of the variance of the mean field solution. Dashed black (resp. green): empirical
variance of population 1 (resp. 2).

in the network show a very good agreement with the related mean field variables. For
larger values of λ corresponding to the oscillatory regime (Figure 3.4, cases (c) and
(d)), all neurons oscillate in phase. These synchronized oscillations yield a coherent
global oscillation of the network activity. The statistics of the network are again
in good agreement with the mean field solution. The standard deviation converges
towards the constant solution of the mean field equation. This is visible at the level
of individual trajectories, that shape a “tube” of solutions around the periodic mean
field solution, whose size increases with λ. The empirical means accurately match the
regular oscillations of the solution of the mean field equation. A progressive phase shift
is observed, likely to be related with the time step dt involved in the simulation. Note
that the phase does not depend on the realization. Indeed, according to theorem 1.1,
the solution of the mean field equations only depends on the mean and the standard
deviation of the Gaussian initial condition, which therefore governs the phase of the
oscillations on the limit cycle (see Figure 3.5).
In the fixed point regime related to large values of λ, very noisy trajectories are
obtained because of the levels of noise involved (see Figure 3.4, cases (e) and (f)).
Though the individual neurons show very fluctuating trajectories, the empirical mean
averaged out over all neurons in the network fits closely the mean field fixed point
solution.
Eventually, we study the switching between a fixed-point regime and an oscillatory regime by extensively simulating the 10 000 neurons network for different values
of λ and computing the Fourier transform of the empirical mean (see Figure 3.6).
The three-dimensional plots show that the appearance and disappearance of oscillations occur for the same values of the parameter λ as in the mean field limit, and
the route to oscillations is similar: at the homoclinic bifurcation in the mean field
system, arbitrarily small frequencies are present, this is also the case for the finitesize network. At the value of λ related to the Hopf bifurcation, the system suddenly

20

J. TOUBOUL, G. HERMANN & O. FAUGERAS

10

8

8

6

6
4
4
2
2
0
0
−2

−2

−4

0

5

10

15

20

25

30

35

40

45

50

(a) λ = 0.6. Fixed-point regime. Individual trajectories vs mean field.
8

−4

0

5

10

15

20

25

30

35

40

45

50

(b) λ = 0.6. Fixed-point regime Empirical
network statistics vs mean field.
6
5

6

4
4
3
2

2

0

1
0

−2

−1
−4
−2
−6
−8

−3

0

5

10

15

20

25

30

35

40

45

50

(c) λ = 1.2. Oscillatory regime. Individual trajectories vs mean field.

−4

0

5

10

15

20

25

30

35

40

45

50

(d) λ = 1.2. Oscillatory regime Empirical
network statistics vs mean field.
4

3

2

1

0

−1

−2

−3
0

(e) λ = 2.5. Noisy fixed point regime.
Individual trajectories vs mean field.

5

10

15

20

25

30

35

40

45

50

(f) λ = 2.5. Noisy fixed point regime
Empirical network statistics vs mean field.

Figure 3.4. Solution of the network dynamics for different values of the noise parameter λ
compared to the mean field solution. Simulations are run for 10 000 neurons, 5 000 in each population. (a), (c), (e): 40 individual trajectories of the membrane potentials of 40 neurons arbitrarily
chosen in the network (20 in each population) compared to the solution of the mean field equations.
Blue: population 1 (excitatory). Red: population 2 (inhibitory). Cyan (resp. magenta): mean of
the mean field solution for population 1 (resp. 2). (b), (d), (f ): Empirical statistics of the network
compared to the mean field. Cyan (resp. magenta): mean of the mean field solution for population
1 (resp. 2). Yellow: variance of the mean field solution. Dashed blue (resp. red): empirical mean
of population 1 (resp. 2). Dashed black (resp. green): empirical variance of population 1 (resp. 2).
For λ = 2.5, due to the amplitude of noise, the statistics were averaged over 10 realizations of the
process.

21

Noise effects in mean field dynamics

6

4

2

0

−2

−4

−6

−8
14

16

18

20

22

24

26

28

30

Figure 3.5. Different realizations of the stochastic network dynamics: the membrane potentials
of 5 neurons among 5 000 of population 1 are plotted for 12 different realizations represented in
different colors. All neurons oscillate in phase, and this phase does not depend on the realization.

switches from a non-zero frequency to a zero frequency in a form that is very similar
to the network case. Therefore we conclude that the mean field equations accurately
reproduce the network dynamics for networks as small as 10 000 neurons, and hence
provide a good model, simple to study, for networks of the scale of typical cortical
columns. As a side remark, we note that at a homoclinic bifurcation of the mean field
system, very small frequencies appear and a precise description of the spectrum of the
network activity would require very large simulation times to uncover precisely the
spectrum at this point, even more so since the large standard deviation of the process
disturbs the synchronization. In a forthcoming study focusing on mean field equations
arising in a similar system including dynamically fluctuating synaptic coefficients, an
interesting additional phenomenon will appear: in that case, the standard deviation
variable will be coupled to the mean, and this coupling will result in sharpening the
synchronization.
We conclude this section by discussing heuristic arguments explaining the observed regular oscillations. Let us start by stating that this phenomenon is a pure
collective effect: indeed, two-neurons networks (one per population) do not present
such regular oscillations as noise is varied. We observe that individual trajectories
of the membrane potential of a 2-neurons networks for small noise levels stay close
to the deterministic fixed point. However, when noise is increased, the system starts
making large excursions with a typical shape resembling the cycle observed in the
mean field limit, and these excursions occur randomly. Such excursions are typical
of the presence of a homoclinic deterministic trajectory: when perturbed, the system
catches the homoclinic orbit responsible for such large excursions. The codimension
one bifurcation diagram of the 2-neurons system indeed illustrates the presence of a
homoclinic orbit as a function of I1 (see diagram 2.2, and Figure B.1 (A))‡ . Noise
can be heuristically seen as perturbing the deterministic value of I1 . For sufficiently
small values of the noise parameter, the probability of I1 to visit regions corresponding to the presence of a cycle is small. But as the noise amplitude is increased, this
‡ Indeed, the mean field equations with λ = 0 are precisely the equations of a two-neurons network
since in that case f (µ, λ2 /2) = S(µ).

22

J. TOUBOUL, G. HERMANN & O. FAUGERAS
(b)

Squared FT Modulus

Squared FT Modulus

(a)

Freque
n

cy

Freque
n

cy

Diﬀerence

(c)

Frequen

cy

Figure 3.6. Squared moduli of the Fourier transforms of (a) the empirical mean for simulations
of the network and (b) of the mean variable of the solution to the mean field equations as functions
of the frequency (Hz) and the noise parameter λ. We observe that oscillations appear in the network
for the same value of λ as in the mean field equations (Figure 2.2), first through what appears to
be a homoclinic bifurcation (arbitrary small frequencies) and also disappear for the same value of λ
through what seems to be a Hopf bifurcation (discontinuity in the power spectrum). (c) Magnitude of
the difference between the two diagrams: we note that the frequency distribution reaches its maxima
for these same values of λ, and the main differences are observed, as expected, around the putative
homoclinic bifurcation point.

probability becomes non-negligible and individual trajectories will randomly follow
the stable cycle. Such excursions produce large input to the other neurons which will
either be inhibited or excited synchronously at this time, a phenomenon that may
trigger synchronized oscillations if the coupling is strong enough and the proportion
of neurons involved in a possible excursion large enough. If the noise parameter is too
large, the limit cycle structure will be destroyed.
Another way to understand this phenomenon consists in considering the phase
plane dynamics of the two-neurons network with no noise (see Figure 3.7). The
system presents three fixed points, one attractive, one repulsive, and a saddle. The
unstable manifold of the saddle fixed point connects with the stable fixed point in
an heteroclinic orbit. The stable manifold of the saddle fixed point is a separatrix
between trajectories that make small excursions around the stable fixed point, and
those related to large excursions close to the heteroclinic orbit. As noise is increased,
the probability distribution of each individual neuron, centered around the stable fixed
point, will grow larger until it crosses the separatrix with a non-negligible probability,
resulting in the system randomly displaying large excursions around the heteroclinic
cycle. The fact that a homoclinic path to oscillations is found in the mean field
limit can be accounted for by these observations, considering the fact that crossing
the separatrix, when noise is of small amplitude, can take an arbitrary long time.
The rhythmicity of the oscillations we found and the synchronization are related to
the coupling in a complex interplay with the probability of large excursions. These

Noise effects in mean field dynamics

23

Figure 3.7. Trajectories in the phase plane for different values of λ superimposed on the phase
diagram. Red curve: µ1 -nullcline, Green curve: µ2 -nullcline, Orange cycle: unstable manifold of
the saddle fixed point (heteroclinic orbit) and Cyan curve: stable manifold of the saddle fixed point
(note that it is almost superposed with part of the µ1 -nullcline), constituting the separatrix between
those orbits that directly return to the stable fixed point and those following the heteroclinic cycle.
Black: noisy trajectories. Upper left: λ = 0.2: no excursion, corresponds to the fixed point regime.
Upper right: λ = 1: rare excursions do occur, corresponding to the bistable regime. Bottom left:
λ = 1.6: excursions are frequent but occur irregularly (corresponding to the oscillatory regime).
Bottom right: λ = 5: the heteroclinic cycle structure is lost, corresponding to the fixed point regime.

heuristic arguments require a thorough mathematical analysis that is outside the scope
of the present paper.
4. Discussion. In this article, we have been interested in the large-scale behavior of networks of firing rate neuron models. Using a probabilistic approach, we
addressed the question of the behavior of neurons in the network as its size tends to
infinity. In that limit, we showed that all neurons behaved independently and satisfied
a mean field equation whose solutions are Gaussian processes such that their mean
and variance satisfy a closed set of nonlinear ordinary differential equations. Uniform
convergence properties were obtained.
We started by studying the solutions of the mean field equations, in particular
their dependence with respect to the noise parameter using tools from dynamical
systems theory. We showed that the noise had non-trivial effects on the dynamics
of the network, such as stabilizing fixed points, inducing or canceling oscillations. A
codimension two bifurcation diagram was obtained when simultaneously varying an
input parameter and the noise intensity, as well as simultaneously varying a total input
connectivity parameter and the noise intensity. The analysis of these diagrams yielded
several qualitatively distinct codimension one bifurcation diagrams for different ranges
of noise intensity. Noise therefore clearly induces transitions in the global behavior of
the network, structuring its Gaussian activity by inducing smooth oscillations of its

24

J. TOUBOUL, G. HERMANN & O. FAUGERAS
0.7

Probability Density

0.6

0.5

0.4

0.3

0.2

0.1

0
−2

0

2

4

6

8

10

12

14

V

Figure 4.1. Empirical distribution of the values of (V i (T ))i=1...N for N = 1000 (500 neurons
per population) in each population (blue and green filled distribution) versus theoretical mean field
distribution. The Kolmogorov-Smirnov validates the fit of the distributions (see text).

mean.
These classes of behaviors were then compared to simulations of the original finitesize networks. We obtained a very good agreement between the simulations of the
finite-size system and the solution of the mean field equations, for networks as small as
a few hundreds to few thousands of neurons. Transitions between different qualitative
behaviors of the network matched precisely the related bifurcations of the mean field
equations, and no qualitative systematic finite-size effects were encountered. Moreover, it appears that the convergence of the solution to a Gaussian process as well as
the propagation of chaos property happen for quite small values of N , as illustrated in
Figure 4.1. This figure represents the distribution of the voltage potential at a fixed
time T = 40 for N = 500, simulated for 20 sample trajectories. The KolmogorovSmirnov test validates the Gaussian nature of the solution with a p-value equal to
7 · 10−4 . In order to test for the independence, we used the Pearson, Kendall and
Spearman tests of dependence. We obtain the correlation values 0.0439 (p-value 0.33)
for the first population, 0.0212 (p-value 0.4785) for the second, and 0.0338 (p-value
0.45) for the cross-correlation between populations, all of them clearly rejecting the
dependence null hypothesis. This independence has deep implications in the efficiency
of neural coding, a concept that we will further develop in a forthcoming paper.
These findings have several implications in neuroscience and other scientific domains as we now comment.
4.1. Noise-induced phenomena. We have seen that the presence of noise in
the system induces different qualitative behaviors. For instance, regular oscillations
of the mean firing rate, linked with a synchronization of all neurons in the network,
appear in the system at some precise values of the noise parameter, in particular for
systems that feature a stable fixed point in a noiseless context. This means that noise
has a strong structuring effect on the global behavior of a cortical assembly, which is
rather a counterintuitive phenomenon, since noise is usually chiefly seen as altering
structured responses. This phenomenon adds to a few recent observations made in
other settings by various authors. For instance, coherent oscillations in one-population
spiking neural networks where exhibited by Pham and collaborators [45]. The results
obtained by Nesse and collaborators [42] also confirm the presence of noise-induced

Noise effects in mean field dynamics

25

phenomena in all-to-all coupled networks of leaky integrate-and-fire neurons with
filtered noise and slow activity-dependent currents, i.e. noise and spikes are filtered by
a second order kernel. The mean field equations they derive are based on a separation
of time scale and ergodicity properties. They show in that case the presence of noiseinduced burst oscillations similar to the case of [45], and a resonance phenomenon.
Both approaches differ from our analysis in that they are dealing with spiking neurons.
Pham and collaborators are able to reduce the dynamics a discrete-time network of
pulse-coupled spike response model neurons, using a Markovian approach, to the study
of a discrete time dynamical system. Nesse and collaborators derive their mean field
limit using relevant approximations of the Fokker-Planck equations. The interest of
our firing-rate model is that it rigorously allows the use of the mathematical theory
of propagation of chaos to reduce the system to a set of coupled differential equations
that can be studied using the dynamical systems theory.
The phenomena observed in our analysis of large-scale neuronal networks differs
from the phenomena of stochastic resonance or coherence resonance well documented
in the neuro-computational literature (see e.g. [36] for a review of the effect of noise
in excitable systems). These phenomena correspond to the fact that there exists a
particular level of noise maximizing the regularity of an oscillatory output related
to periodic forcing (stochastic resonance) or to intrinsic oscillations (coherence resonance). Such situations are evidenced through the computation of the maximal value
of the Fourier transform of the output. In our case, as we can see in the Fourier transform plots, the maximal value of the Fourier transform does not present a clear peak
as a function of the noise level (see Figure 3.6), hence the system does not exhibit
resonance. Besides this observation, the regularity of the oscillation can be expected
to be relatively high for large networks in our framework, since the mean activity is
asymptotically perfectly periodic. This type of phenomena is fundamentally related
to the randomness in the inputs, and will not be observed in the Markovian mean
field equations developed by [14, 8]. Indeed, apart from the difference inherent to the
fact that they consider Markov chains governing the firing of individual neurons as
their microscopic model, the randomness and the correlations in the activity vanishes
in the limit N → ∞ yielding the deterministic Wilson and Cowan equation.
Another important result of ours is the ability to define classes of parameter
ranges attached to a few generic bifurcation diagrams as functions of the input to a
population. This property suggests further some reverse-engineering studies allowing
to infer from measurements of the system responses to different stimuli the level of
noise it is submitted to.
The influence of noise in spiking one-population neural networks was studied
in another context by Pham and collaborators in [45] and Brunel and collaborators [12, 29]. In [45], the authors study randomly or fully connected one-population
networks of spiking neurons. They analyze the probability distribution of spike sequences and reduce this analysis to the study of the properties of a certain map under
an independence assumption and in the limit where the number of neurons is infinite
(which makes the independence assumption particularly relevant). They show that
noise can trigger oscillations for certain values of the total connectivity parameter in
a one-population case. Similar phenomena are shown in the study of sparse randomly
connected integrate-and-fire neurons as shown in [12] where the system can present
synchronous regular regimes. In the mean field model studied in the present article,
no oscillatory activity is possible in such one-population systems, since its dynamics
can be reduced to a one-dimensional autonomous dynamical system. Smooth nonlin-

26

J. TOUBOUL, G. HERMANN & O. FAUGERAS

earities in the intrinsic dynamics or discontinuities such as the presence of a spiking
threshold in [45, 12] makes the dynamics of the mean field equations more complex,
in particular prevents reduction to a one-dimensional autonomous system governing
the mean of the solution. Such intricacies may also be the source of oscillations in
one-population systems.
We eventually emphasize the fact that the noise-induced transitions presented
here are related to the nature of the mean field equations, which is not a standard
stochastic differential equation. Such phenomena do not generally occur in usual
stochastic differential equations, as for instance shown in [34].
4.2. Understanding the functional role of noise. The question of the functional role of noise in the brain is widely debated today since it clearly affects neuronal
information processing. A key point is that the presence of noise is not necessarily
a problem for neurons: as an example, stochastic resonance helps neurons detecting
and transmitting weak subthreshold signals. Furthermore neuronal networks that
have evolved in the presence of noise are bound to be more robust and able to explore
more states, which is an advantage for learning in a dynamic environment.
The fact that noise can trigger synchronized oscillations at the network level enriches the possible mechanisms leading to rhythmic oscillations in the brain, directly
relating it to the functional role of oscillations. Rhythmic patterns are ubiquitous in
the brain and take on different functional roles. Among those, we may cite visual
feature integration [47], selective attention, working memory. Abnormal neural synchronization is present in various brain disorders [54]. Oscillations themselves can
signal a pathological behavior. For instance, epileptic seizures are characterized by
the appearance of sudden, collective, slow oscillations of large amplitude, corresponding at the cell level to a synchronization of neurons, and visible at a macroscopic scale
through EEG/MEG recordings. This phenomenon is very close to the observation in
our model that, as noise is slowly increased, the solutions of the mean field equations
undergo a saddle-homoclinic bifurcation abruptly yielding large amplitude and small
frequency oscillations. Such a collective phenomenon resembles epileptic seizures.
Moreover, in our model and at the microscopic level, these regimes are characterized
by a sudden precise synchronization of all neurons in the network, consistent with
what is observed at the cell level in epileptic seizures. Our mean field model, based
on a simple description of neural activity, was able to account for such complex biologically relevant phenomena, which suggests to use this new model as a cortical mass
model and compare it to more established cortical column models such as Jansen and
Rit’s or Wendling and Chauvel’s [52, 55, 35].
4.3. Perspectives. Several extensions of the present study with applications in
neuroscience and in applied mathematics are envisioned.
The first is to expand our work to include more biologically relevant models and
to study the behavior of the solutions of the mean field equations in that mathematically much more complex setting that would include in particular nonlinear intrinsic
dynamics and different ionic populations. Another important direction in the development of this work would consist in fitting the microscopic model to biological
measurements. This would yield a new neural mass model for large scale areas and
develop studies on the appearance of stochastic seizures and rhythmic activity in relationship with different parameters of the model, integrating the presence of noise in
a mathematically and biologically relevant manner.
From this point of view, the simplicity of the model, specifically the linearity
of the intrinsic dynamics of each individual neuron, made possible an analytic and

Noise effects in mean field dynamics

27

quantitative analysis of our mean field equations, a particular case of McKean-Vlasov
equations. The drawback of this simplicity is that it does not represent precisely the
activity of individual neurons. Rate models are often considered valid at the macroscopic level as describing populations activity, and as such might not be good models
of single cells. However, defining the instantaneous firing rate as a trial average [31, 19]
can be more relevant from this viewpoint. Alternatively, our model can be seen as a
model of a hypercolumn, each diffusion process characterizing the activity of a whole
cortical column modeled by Wilson and Cowan equations. For realistic individual
neuron models the solutions to the mean field equations will not, in general, be Gaussian. General approaches to study them consist either in studying their properties as
random processes, or in describing their probability distribution. In the first case, one
is led to investigate an implicit equation in the space of stochastic processes, and in
the second case, one is led to study a complex non-local partial differential equation
(Fokker-Planck), as done in a recent paper by Caceres and collaborators [15]. In both
cases one faces a difficult challenge, and the dependency of the solutions with respect
to parameters is extremely hard to describe.
Another interesting improvement of the model would consist in considering that
the synaptic weights are random. These weights can be randomly drawn in a distribution and frozen during the evolution of the network. The propagation of chaos
would again applies in that case, and in the rather simple case discussed in the present
manuscript, the solution is a Gaussian process as proved in [28]. However, the mean
and covariance cannot be described by a set of ordinary differential equations, as in
here since the covariance depends on the whole previous history of the correlations.
Alternatively, the weights can be considered as stochastic processes themselves. In
this case, noise will appear multiplicatively and new noise-induced transitions might
appear. The question of the synaptic noise model chosen and the dynamics of such
systems will be addressed in a forthcoming paper.
The present study can be seen as a proof of concept, and it seems reasonable
to extrapolate that such noise-induced transitions do occur as well for the solutions
to the mean field equations of these more complex and more biologically plausible
systems.
Appendix A. Proof of lemma 1.3.
In this appendix we prove lemma 1.3 stating that in the case where the sigmoidal
transforms are of the form Sα (x) = erf(gα x + γα ), the functions fα (µα , vα ) involved
in the mean field equations (1.2) with a Gaussian initial condition take the simple
form (1.9).
Proof. We have, using the definition of the erf function

 e−x2 /2
 p
dx
E [Sα (Xα )(t)] =
erf gα x vα (t) + µα (t) + γα √
2π
R
 √

Z Z gα x vα (t)+µα (t) +γα −(x2 +y2 )/2
e
=
dxdy
2π
R −∞
Z



This integral is of the form:
Z Z
R

a x+b

−∞

2

e−(x

+y 2 )/2

2π

dxdy

and therefore, the integration domain has an affine shape as plotted in figure A.1. In

28

J. TOUBOUL, G. HERMANN & O. FAUGERAS

y

v

+b

y=

ax

u

x

Figure A.1. Change of variable for the erf function.

order to compute this integral, we change variables by a rotation of the axes (x, y)
and align the affine boundary of our integration domain with our new variables (u, v)
(see figure A.1). Simple geometric analysis shows that the rotation angle θ for this
change of variable is such that tan(θ) = a. The new integration domain is in the new
b
:
coordinates given by v ≤ vm = b cos(θ) = √1+a
2
Z Z
R

a x+b

e−(x

2

+y 2 )/2

2π

−∞

Z Z

√

dxdy =
R

= erf

gb
1+g 2 a2

−∞

2

e−(u +v
!

2

)/2

1
dudv
2π

gb
p
1 + g 2 a2

which reads with the parameters of the model:
fα (µ, v) = erf

g µ + γα
pα
1 + gα2 v

!

Appendix B. Bifurcations Diagram as a function of λ. In section 2, we
observed that six different bifurcation diagrams appear as I1 is varied, depending on
the value of λ characterizing the additive noise input. For the particular choice of
parameters chosen in that section, the different zones are segmented for values of λ
given in table B.1.
Type
λ

C
0.16

BT
2.934

Hom TP
2.948

H TP
2.968

C
3.74

Table B.1
Numerical values of the separation into six λ zones for Figure 2.2. C stands for Cusp, BT:
Bogdanov-Takens, Hom TP: turning point of the Homoclinic bifurcations curve, H TP: Hopf bifurcation curve turning point.

In each of these zones, typical codimension 1 bifurcation diagrams as the input
I1 is varied are depicted in figure B.1. We now describe the behavior of the system
in each of these zones.
(A) For very small values of λ, the system features four saddle-node bifurcations
and one supercritical Hopf bifurcation, associated to the presence of stable
limit cycles that disappear through saddle-homoclinic bifurcation arising from
the Bogdanov-Takens bifurcation (after the turning point.) In an extremely

29

Noise effects in mean field dynamics

(A)

2

(B)

3

SN

SN

2

1

SN

0

1

SN

0

-1

H
SN

-2

H

-1

SN
-2

-3
-3
-4
-4
-5

-3

-2

-1

0

1

-3

2

(C1)

3.5

-2

0

1

(C2)

3.5

SN

3

-1

SN

3
2.5

2.5

2

2

1.5

1.5

SN
H

1

1

SN
H

H

H
0.5

0.5
0

0

-0.5

-0.5
0.5

1

1.5

2

0.5

2.5

1

2

2.5

(E)

(D)

3.5

1.5

SN

3

4
2.5
2

2

1.5

SN
0

1
0.5

-2
0
-0.5

-4
0.5

1

1.5

2

2.5

-2

-1

0

1

2

3

4

5

Figure B.1. Typical behavior of the system in each zone (A) through (F). (A): λ = 0, (B):
λ = 1, (C): λ = 2.945, (D):λ = 2.955, (E): λ = 3, (F): λ = 4. Red stars: bifurcations, SN: SaddleNode, H: Hopf, green circle: Saddle-Homoclinic bifurcation. Thick blue line: stable fixed point, thin
blue line: unstable fixed point, thick pink line: stable cycle. See text for precise description.

limited range of parameter, the occurrence of two saddle-node bifurcation
relates to a bistable regime in that small parameter region.
(B) In zone (B), the system differs from zone (A) in that the two inferior saddlenode bifurcation disappeared through Cusp bifurcation. Globally the same
behavior are observed, except for the bistable behavior commented above
(which was not a prominent phenomenon due to the reduced parameter region
concerned).
(C) On the upper branch of saddle nodes, the systems undergoes a Bogdanov-Takens
bifurcations, yielding the presence in zone (C) of a supercritical Hopf bifurcation and of a saddle-homoclinic bifurcation curve. This BT bifurcation
accounts for the family of Hopf bifurcations observed in zones (A-B) and for
the saddle-homoclinic bifurcations, because of the turning points observed in
the full bifurcation diagrams. In region (C), two families of limit cycles coexist, both arising from supercritical Hopf bifurcation and disappearing through
saddle-homoclinic bifurcation.
(D) Because of the topology of the bifurcation diagram, the turning point of the
saddle-homoclinic bifurcations curve occurs before the turning point of the

30

J. TOUBOUL, G. HERMANN & O. FAUGERAS

Hopf bifurcations curve. This difference yields zone (D) between the two
turning points. In that zone, we still have two supercritical Hopf bifurcations,
but no more homoclinic bifurcation. The families of limit cycles corresponding
to each of the Hopf bifurcations are identical.
(E) After the turning point of the Hopf bifurcations manifold, we are left with two
saddle-node bifurcations, hence a pure bistable behavior with no cycle.
(F) Both saddle-node bifurcation disappear by merging into a cusp bifurcation. After
this cusp, the system has a trivial behavior, i.e. it features a single attractive
equilibrium whatever I1 .
Acknowledgements
This work was partially supported by the ERC grant #227747 NerVi.
REFERENCES
[1] L.F Abbott and C.A. Van Vreeswijk, Asynchronous states in networks of pulse-coupled
neuron, Phys. Rev, 48 (1993), pp. 1483–1490.
[2] A. Aldo Faisal, L.P.J. Selen, and D.M. Wolpert, Noise in the nervous system, Nat. Rev.
Neurosci., 9 (2008), pp. 292–303.
[3] S. Amari, Characteristics of random nets of analog neuron-like elements, Syst. Man Cybernet.
SMC-2, (1972).
[4] S.-I. Amari, Dynamics of pattern formation in lateral-inhibition type neural fields, Biological
Cybernetics, 27 (1977), pp. 77–87.
[5] D.J. Amit and N. Brunel, Model of global spontaneous activity and local structured delay
activity during delay periods in the cerebral cortex, Cerebral Cortex, 7 (1997), pp. 237–
252.
[6] L. Arnold, Random Dynamical Systems, Springer, 1998.
[7] M. Benayoun, J. D. Cowan, W. van Drongelen, and E. Wallace, Avalanches in a stochastic model of spiking neurons, PLoS Comput Biol, 6 (2010), p. e1000846.
[8] P.C. Bressfloff, Stochastic neural field theory and the system-size expansion, SIAM J. Appl.
Math. 70 (2009) pp. 1488-1521.
, Metastable states and quasicycles in a stochastic Wilson–Cowan model, Phys. Rev. E,
[9]
82 (2010).
[10] P.C. Bressloff, J.D. Cowan, M. Golubitsky, P.J. Thomas, and M.C. Wiener, What
Geometric Visual Hallucinations Tell Us about the Visual Cortex, Neural Computation,
14 (2002), pp. 473–491.
[11] J. Brewer, Kronecker Products and Matrix Calculus in System Theory, IEEE Trans. Circuits
Syst,, CAS-25 (1978).
[12] N. Brunel, Dynamics of sparsely connected networks of excitatory and inhibitory spiking
neurons, Journal of Computational Neuroscience, 8 (2000), pp. 183–208.
[13] N. Brunel and V. Hakim, Fast global oscillations in networks of integrate-and-fire neurons
with low firing rates, Neural Computation, 11 (1999), pp. 1621–1671.
[14] MA Buice and JD Cowan, Field-theoretic approach to fluctuation effects in neural networks,
Physical Review E, 75 (2007).
[15] M.J. Caceres, J. A. Carrillo, and B. Perthame, Analysis of nonlinear noisy integrate and
fire neuron models: blow-up and steady states, Journal of Mathematical Neuroscience, 1
(2011).
[16] D Cai, L Tao, M Shelley, and DW McLaughlin, An effective kinetic representation of
fluctuation-driven neuronal networks with application to simple and complex cells in visual
cortex, Proceedings of the National Academy of Sciences, 101 (2004), pp. 7757–7762.
[17] P. Cattiaux, A. Guillin, and F. Malrieu, Probabilistic approach for granular media equations in the non-uniformly convex case, Probability theory and related fields, 140 (2008),
pp. 19–40
[18] S. Coombes and M. R. Owen, Bumps, breathers, and waves in a neural network with spike
frequency adaptation, Phys. Rev. Lett., 94 (2005).
[19] P. Dayan and L. F. Abbott, Theoretical Neuroscience : Computational and Mathematical
Modeling of Neural Systems, MIT Press, 2001.
[20] A. Destexhe, Self-sustained asynchronous irregular states and up/down states in thalamic,

Noise effects in mean field dynamics

[21]
[22]
[23]
[24]
[25]
[26]
[27]
[28]

[29]
[30]
[31]
[32]
[33]
[34]
[35]

[36]
[37]

[38]
[39]
[40]

[41]

[42]

[43]
[44]
[45]
[46]
[47]
[48]
[49]

31

cortical and thalamocortical networks of nonlinear integrate-and-fire neurons, Journal of
Computational Neuroscience, 27 (2008), pp. 493–506.
A. Dhooge, W. Govaerts, and Y.A. Kuznetsov, Numerical Continuation of Fold Bifurcations of Limit Cycles in MATCONT, Proceedings of the ICCS, (2003), pp. 701–710.
A. Dhooge, W. Govaerts, and Yu. A. Kuznetsov, Matcont: A matlab package for numerical
bifurcation analysis of odes, ACM Trans. Math. Softw., 29 (2003), pp. 141–164.
RL Dobrushin, Prescribing a system of random variables by conditional distributions, Theory
of Probability and its Applications, 15 (1970).
S El Boustani and A Destexhe, A master equation formalism for macroscopic modeling of
asynchronous irregular activity states, Neural computation, 21 (2009), pp. 46–100.
G.B. Ermentrout, Neural networks as spatio-temporal pattern-forming systems, Reports on
Progress in Physics, 61 (1998), pp. 353–430.
,, Simulating, Analyzing, and Animating Dynamical Systems: A Guide to XPPAUT for
Researchers and Students, Society for Industrial Mathematics, 2002.
G B Ermentrout and J D Cowan, A mathematical theory of visual hallucination patterns.,
Biol Cybern, 34 (1979), pp. 137–150.
O. Faugeras, J. Touboul, and B. Cessac, A constructive mean field analysis of multi population neural networks with random synaptic weights and stochastic inputs, Frontiers in
Neuroscience, 3 (2009).
N. Fourcaud and N. Brunel, Dynamics of the firing probability of noisy integrate-and-fire
neurons, Neural Computation, 14 (2002), pp. 2057–2110.
W. Gerstner and W. Kistler, Spiking Neuron Models, Cambridge University Press, 2002.
, Mathematical formulations of hebbian learning., Biological Cybernetics, 87 (2002),
pp. 404–415.
J. Guckenheimer and P. J. Holmes, Nonlinear Oscillations, Dynamical Systems and Bifurcations of Vector Fields, vol. 42 of Applied mathematical sciences, Springer, 1983.
S. Herrmann and J. Tugaut, Non uniqueness of stationary measures for self-stabilizing diffusions, (2009).
W. Horsthemke and R. Lefever, Noise-induced transitions, vol. 1, Springer Berlin, 1984.
B. H. Jansen and V. G. Rit, Electroencephalogram and visual evoked potential generation
in a mathematical model of coupled cortical columns, Biological Cybernetics, 73 (1995),
pp. 357–366.
B. Lindner, J. Garcia-Ojalvo, A. Neiman, and L. Schimansky-Geier, Effects of noise in
excitable systems, Physics Reports, 392 (2004), pp. 321–424
C. Ly and D. Tranchina, Critical analysis of dimension reduction by a moment closure
method in a population density approach to neural network modeling., Neural Computation, 19 (2007), pp. 2032–2092.
X. Mao, Stochastic Differential Equations and Applications, Horwood publishing, 2008.
G. Maruyama, On the poisson distribution derived from independent random walks, Nat. Sci.
Rep. Ochanomiza Univ., 9 (1955), pp. 1–6.
HP McKean Jr, A class of markov processes associated with nonlinear parabolic equations,
Proceedings of the National Academy of Sciences of the United States of America, 56
(1966).
S. Mischler, C. Mouhot, and B. Wennberg, A new approach to quantitative chaos propagation estimates for drift, diffusion and jump processes, Arxiv Preprint, arXiv:1101.4727
(2011).
W.H. Nesse, A. Borisyuk, and P.C. Bressloff, Fluctuation-driven rhythmogenesis in an
excitatory neuronal network with slow adaptation, Journal of computational neuroscience,
25 (2008), pp. 317–333
H Neudecker, Some theorems on matrix differentiation with special reference to kronecker
matrix products, Journal of the American Statistical Association, 64 (1969), pp. 953–963.
T Ohira and JD Cowan, Master-equation approach to stochastic neurodynamics, Physical
Review E, 48 (1993), pp. 2259–2266.
J. Pham, K. Pakdaman, and J.F. Vibert, Noise-induced coherent oscillations in randomly
connected neural networks, Physical Review E, 58 (1998), p. 3610.
ET Rolls and G Deco, The noisy brain: stochastic dynamics as a principle of brain function,
Oxford university press, 2010.
W. Singer and C.M. Gray, Visual feature integration and the temporal correlation hypothesis,
Annual review of neuroscience, 18 (1995), pp. 555–586
AS Sznitman, Topics in propagation of chaos, Ecole d’Eté de Probabilités de Saint-Flour XIX,
(1989), pp. 165–251.
H Tanaka, Probabilistic treatment of the boltzmann equation of maxwellian molecules, Prob-

32

[50]
[51]

[52]
[53]
[54]
[55]

[56]
[57]
[58]

J. TOUBOUL, G. HERMANN & O. FAUGERAS
ability Theory and Related Fields, 46 (1978), pp. 67–105.
, Some probabilistic problems in the spatially homogeneous boltzmann equation, Theory
and Application of Random Fields, (1983), pp. 258–267.
J. Touboul and G. B. Ermentrout, Finite-size and correlation-induced effects in mean-field
dynamics finite-size and correlation-induced effects in mean-field dynamics, Arxiv preprint
arXiv:1008.2839, (2011).
J. Touboul, F. Wendling, P. Chauvel and O.D. Faugeras, Nonlinear dynamics, neural
mass activity and epilepsy, Neural Computation, (in press).
J. Touboul, D. Fasoli and O.D. Faugeras, A probabilistic approach for large populations of
conductance-based neurons, in preparation, (2011).
P.J. Uhlhaas and W. Singer, Neural synchrony in brain disorders: relevance for cognitive
dysfunctions and pathophysiology, Neuron, 52 (2006), pp. 155–168
F. Wendling and P. Chauvel, Transition to Ictal Activity in Temporal Lobe Epilepsy: Insights from Macroscopic Models, Computational Neuroscience in Epilepsy (Stolesz and
Staley Ed.), (2008), pp. 356–386.
J.A. White, J.T. Rubinstein, and A.R. Kay, Channel noise in neurons, J. Neurosci, 16,
pp. 3219–3235.
H.R. Wilson and J.D. Cowan, Excitatory and inhibitory interactions in localized populations
of model neurons, Biophys. J., 12 (1972), pp. 1–24.
H.R. Wilson and J.D. Cowan, A mathematical theory of the functional dynamics of cortical
and thalamic nervous tissue, Biological Cybernetics, 13 (1973), pp. 55–80.

