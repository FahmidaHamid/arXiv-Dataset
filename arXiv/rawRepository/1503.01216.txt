Visual decisions in the presence of
measurement and stimulus correlations
Manisha Bhardwaj

∗1

, Sam Carroll

†1

, Wei Ji Ma

‡2,3

, and Krešimir Josić

§1,4

1

arXiv:1503.01216v1 [q-bio.NC] 4 Mar 2015

Department of Mathematics, University of Houston
Department of Neuroscience, Baylor College of Medicine
3
Now at: Center for Neural Science and Department of Psychology, New
York University
4
Department of Biology and Biochemistry, University of Houston
2

Abstract
Humans and other animals base their decisions on noisy sensory input. Much work
has therefore been devoted to understanding the computations that underly such decisions. The problem has been studied in a variety of tasks and with stimuli of differing
complexity. However, the impact of correlations in sensory noise on perceptual judgments is not well understood. Here we examine how stimulus correlations together with
correlations in sensory noise impact decision making. As an example, we consider the
task of detecting the presence of a single or multiple targets amongst distractors. We
assume that both the distractors and the observer’s measurements of the stimuli are
correlated. The computations of an optimal observer in this task are nontrivial, yet
can be analyzed and understood intuitively. We find that when distractors are strongly
correlated, measurement correlations can have a strong impact on performance. When
distractor correlations are weak, measurement correlations have little impact, unless the
number of stimuli is large. Correlations in neural responses to structured stimuli can
therefore strongly impact perceptual judgments.

1

Introduction

The perceptual system has evolved to extract ecologically meaningful information from sensory
input. For example, in many mid- to high-level visual tasks the brain has to make categorical,
global judgements based on multiple stimuli where the identity of any individual stimulus
∗

manisha@math.uh.edu
srcarroll314@gmail.com
‡
weijima@nyu.edu
§
josic@math.uh.edu
†

1

is not of direct relevance. In a visual search task, the goal might be to detect whether a
predefined target object is present in a scene that contains multiple objects. Complicating
such tasks is the fact that noise corrupts sensory measurements, especially when observation
time is short, or many objects are present.
Much work has been devoted to modeling the decision processes by which the brain converts noisy sensory measurements of a set of stimuli into a judgement about a global world
state, such as the presence or absence of a target. These models often focus on various decision
rules that can be applied to the measurements. By contrast, the measurements themselves
are usually modeled in a rather stereotypical fashion, namely as independent and normally
distributed, (e.g. Peterson et al. (1954); Nolte and Jaarsma (1967); Pelli (1985); Graham
et al. (1987); Palmer et al. (1993); Baldassi and Burr (2000); Baldassi and Verghese (2002);
van den Berg et al. (2012); Ma et al. (2011); Mazyar et al. (2012)). Both the assumption
of independence and the assumption of Gaussianity can be questioned. Specifically, neural
correlations can extend to distances as long as 4mm in monkey cortex (Ecker et al., 2010;
Cohen and Kohn, 2011). This suggests that sensory measurements can be strongly correlated (Rosenbaum et al., 2010; Chen et al., 2006). Here we focus on the effects of violation of
the assumption of independent measurements on performance in categorical, global perceptual
judgements.
To make such perceptual judgments, an observer needs to take into account the statistical
structure of the stimuli and the structure of measurements. Consider a search task where
a subject is required to detect a target among distractors. The effects of measurement correlations and stimulus correlations will be intertwined: If the distractors are identical on a
given trial, then strong correlations between the measurements will help preserve their perceived similarity. Namely, an observer can group the distractor measurements and identify
the target as corresponding to the outlying measurement. By contrast, when distractors are
unstructured (independently drawn across locations), strong measurement correlations may
have no effect on performance. Thus, measurement and stimulus correlations should not be
considered in isolation.
Here we examine how measurement and stimulus correlations impact the strategy and
the performance of an ideal observer in a target detection task. We assume that on half the
trials, one or more target stimuli are presented along with a number of distractors, whereas
on the other half of trials, only distractors are presented. The task is to infer whether targets are present or not. Importantly, we assume that the distractor stimuli are not drawn
independently – for instance, in the extreme case the targets could be identical. Our ideal
observer infers target presence based on measurements of the stimuli. We assume that these
measurements are corrupted by correlated noise. In an extreme case, this noise is perfectly
correlated and all measurements are perturbed by the same, random value.
We provide an analytical study of the optimal decision rule to show that the interplay
of measurement and stimulus correlations can be intricate. In general, if the stimuli are
strongly correlated, then measurement correlations can strongly affect the performance of an
ideal observer. When stimuli are weakly correlated, measurement correlations have a smaller
impact.
We expect that these insights hold more generally: Natural stimuli are structured, and their
distributions are concentrated along low dimensional structures in stimulus space (Geisler,
2

2008). Correlations in measurement noise could thus help in the inferring parameters of
interest. Ultimately, we thus expect our results to be relevant to modeling perceptual decisionmaking in natural scenes.

2

Model description

To examine how decisions of an ideal observer are determined by the statistical structure
of measurements and stimuli we consider the following task. An observer is asked whether
target stimuli are present among a set of distractor stimuli. Each stimulus, i, is characterized
by a scalar, si ∈ R. The set of N stimuli presented on a single trial is characterized by
the vector s = (s1 , s2 , · · · , sN ). For instance, stimuli could be pure tones characterized by
their frequency, gratings characterized by their orientation, or ellipses characterized by the
orientation of their major axis. A target is a stimulus with a particular characteristic, sT . A
target could be a vertical grating, or a pure tone at 440Hz. For simplicity, we assume that, if
i is a target, then si = sT = 0, and stimulus characteristics are measured relative to that of a
target. Stimuli that are not targets are distractors. For such stimuli sj 6= 0 with probability 1.
We will consider situations with single and multiple targets.

Figure 1: A schematic representation of Bayesian inference. Information about a parameter
of interest, T , is encoded in a stimulus vector, s. The dimension of s could be much higher
than the dimension of T . The observer makes a sensory measurement, x, of the stimulus, s,
and must infer T from this measurement. This inference requires marginalization over s.
The following treatment parallels the one we used earlier for independent measurements (Mazyar et al., 2012, 2013; Bhardwaj et al., 2015). We denote target presence by T = 1, and absence
by T = 0. We assume that targets are present with probability 0.5. When T = 0 (no targets)
stimuli are drawn from a multivariate normal distribution with mean 0N = (0, 0, · · · , 0), and
covariance matrix, Σs . The subscript denotes vector length, so 0N has N components. We
can therefore write
p(s|T ) = N (s; 0N , Σs ),
(2.1)
where N (s; µ, Σ) denotes the density of the normal distribution with mean µ and covariance
Σ. For simplicity, we assume Σs has constant diagonal and off-diagonal terms, so that


1 ρs · · · ρs
ρ 1 · · · ρs 

2 s
Σs = σs  ..
(2.2)
..  .
.
.
.
. .
ρs ρs · · ·

1

The correlation coefficient, ρs , determines the relation between the components of the stimulus.
3

If T = 1, then n ≥ 1 targets are present. In this case we assign, with uniform probability,
the target characteristic to n out of the total
 of N stimuli. This subset n targets is denoted
N
by L. We denote by L the collection of all
possible choices of the sets L of targets.
n
The remaining N − n distractors are drawn from a multivariate normal distribution
with mean 0N −n , and covariance matrix Σs\L of dimension (N − n) × (N − n). Let sL =
(si1 , si2 , · · · , sin ), il ∈ L denote the target stimuli, and s\L = (sj1 , sj2 , · · · , sjN −n ), jl ∈
/ L the
distractors. We can therefore write
X
p(sL |T = 1) =
δ(si ),
and
p(s\L |T = 1) = N (s\L ; 0N −n , Σs\L ).
i∈L

Since the density p(s|T = 1, L) is singular, we introduce the auxiliary covariance,

(Σηs,L )i,j



(Σs\L )i,j ,
= η,


0,

if i, j ∈
/ L,
if i = j ∈ L,

(2.3)

if i ∈ L, or j ∈ L, and i 6= j,

where η > 0. Therefore,
p(s|T = 1, L) = lim N (s; 0N , Σηs,L ).
η↓0

We further assume that an observer makes a noisy measurement, xi , of each stimulus, si . This measurement can be thought of as the estimate of stimulus i obtained from
the activity of a population of neurons that responded to the stimulus. We denote by
x = (x1 , x2 , · · · , xN ) the vector of N measurements. It is commonly assumed that these
measurements are unbiased, and corrupted by additive, independent, normally distributed
Q
noise, so that p(x|s) = ni=1 N (xi ; si , σx2 ). Here we consider the more general situation where
the measurements are unbiased, but noise could be correlated so that
p(x|s) = N (x; s, Σx ).

(2.4)

We consider the particular case when Σx has constant diagonal terms, σx2 , and off-diagonal
terms, ρx σx2 , so that


1 ρx · · · ρx
ρx 1 · · · ρx 


Σx = σx2  ..
(2.5)
. .
..
.
. .. 
ρx ρx · · ·
Note that

1

Z
p(x|T = 1) =

p(x|s)p(s|T = 1)ds,

and similarly for p(x|T = 0). We must thus marginalize over s to obtain these distributions.
A simple computation (See Appendix A) shows that p(x|L, T = 1) = N (x; 0N , Σ0s,L + Σx ),
and p(x|T = 0) = N (x; 0N , Σs + Σx ).

4

Figure 2: Stimulus and measurement distributions in the single target detection task with
N = 2 stimuli and σs = 15, σx = 4. (A) (Top) Stimulus distributions on target present
(left) and target absent (right) trials for ρs = 0.5. On target present trials, the distribution
is constrained to the axes, since one of the stimuli is a target. On target absent trials, the
two stimuli follow a bivariate normal distribution. (Bottom) Distributions of measurements
in the absence of measurement correlations, ρx = 0. On both the target present (left), and
target absent case (right), the measurement distribution inherits its shape from the stimulus
distribution. (B) Overlap of measurement distributions for strongly correlated distractors,
ρs = 0.99. Measurement distributions on target present (black) and absent (gray) trials are
shown for ρx = 0 (top) and ρx = 0.95 (bottom). The overlap between the measurement
distributions decreases with increasing measurement correlation, ρx . The decision boundary
(solid line) therefore better separates the two distribution when ρs and ρx are both large. We
used high correlations to bring out the difference between
p the distributions. Axes represent
the total standard deviation of the measurements, σ = σs2 + σx2 .

5

3

Results

Our goal is to describe how correlations between stimuli along with correlations between
their measurements affect the decisions of an optimal observer in a target detection task.
We examine how performance changes as both correlations between distractors and between
measurements are varied.
The stimuli, s, follow different distributions depending on whether T = 0 or T = 1. Measurement noise increases the overlap between the corresponding measurement distributions,
p(x|T = 0) and p(x|T = 1). The higher the overlap between these two distributions, the more
difficult it is to tell whether a target is present or not. However, correlations in measurement
noise can reduce such overlap (See Fig. 2B), even when noise intensity is unchanged. Therefore, the estimate of a parameter from a neural response depends not only on the level, σx ,
but also on the structure of measurement noise (Abbott and Dayan, 1999; Sompolinsky et al.,
2001; Averbeck et al., 2006; Josić et al., 2009).
An ideal observer makes a decision based on the sign of the log posterior ratio,
dTD (x) = log

p(x|T = 1)
p(T = 1)
p(T = 1|x)
= log
+ log
.
p(T = 0|x)
p(x|T = 0)
p(T = 0)

(3.1)

If dTD (x) > 0, the observer infers that a target is present. Note that
X
p(x|L, T = 1)p(L).
p(x|T = 1) =
L∈L

 −1
N
Given that the prior probability that L is a set of targets is uniform gives p(L) =
,
n
and
" −1
#
X p(x|L, T = 1)
p(x|T = 1)
N
= log
.
(3.2)
dTD (x) = log
n
p(x|T = 0)
p(x|T = 0)
L∈L
The decision variable thus depends on the sum of the normalized probabilities that a measurement in x is made, given that the target set is L.
Note also that
p(L|x, T = 1) ∝ p(x|L, T = 1).
Therefore, the decision variable can also be interpreted as a sum of likelihoods that L is a
target set, given a measurement x. Thus the decision is directly related to the posterior
distribution over the target sets L: The summands in Eq. (3.2) correspond to the evidence
that L is a set of targets.
The distribution of measurements, x, depends on whether a target or targets are present.
The conditional distributions of measurements, p(x|T = 0) and p(x|L, T = 1), are Gaussian
with all means equal to 0, and covariances C = Σs + Σx and CL = Σ0s,L + Σx , respectively.
We therefore have (see Appendix A for details)
" −1 s

#
X

|C|
1
N
−1
dTD (x) = log
exp − xT C−1
x .
(3.3)
L −C
n
|CL | L∈L
2
6

This decision variable depends on the model parameters, and the measurement, x. The
total number of stimuli, N, number of targets, n, and the variability, σs2 . The correlation, ρs ,
between the distractors determine the structure of the stimulus, while the variability, σx2 , and
correlation, ρx , describe the distribution of sensory measurements. An ideal observer knows
all these parameters.
Setting the right hand side of Eq. (3.3) to 0 defines a nonlinear decision boundary in the
space of measurements. This boundary separates measurements that lead to a “target present”
decision (dTD (x) > 0) from those that lead to a “target absent” decision (dTD (x) < 0). For
example, with a single stimulus, N = n = 1, the observer needs to decide whether or not a
single measurement corresponds to a target. The decision variable has the form
s
!
σs2 + σx2
1
x21
dTD (x) = log
−
,
σx2
2σx2 1 + σx2 /σs2
If the measurement, x1 , differs sufficiently from 0, then dTD (x) < 0. A measurement close to
0, gives dTD (x) > 0.
With more stimuli, the observer needs to take into account the known correlations between
the stimuli and between the measurements. The decision variable depends in a complicated
way on the parameters that describe the structure of the stimulus and the response. The
explicit form of Eq. (3.3), can be derived under the assumption of equal variances and equal
covariances (See Appendix A)




s


 1
 1 1 + (ρ σ 2 + ρ σ 2 )N v  v n X
X


s s
x x
L
x2i
exp − σs2 (1 − ρs )vL v
dTD (x) = log 

γL
v
2

M

i∈L
L∈L


|
{z
}
I

 












2 2
2
2
X
X
X


ρ
σ
v
g
ρ
σ
v
v
v
f
x
L
x
L
L
x L
x
2
2
2

+ βv −
xi xj + 2 βv −
xi xj + βv −
xi xj  
.
γL
γL
γL


i,j∈L
i∈L,j ∈L
/
i,j ∈L
/

|
{z
} |
{z
} |
{z
} 

II

III

IV

(3.4)
The variables v and vL represent scaled inverse variances corresponding to distractor and
target stimuli. The parameters β, gL , fL , and γL are given in Eqs. (A.4) and (A.5), and are
defined in terms of σs2 , ρs , σx2 , and ρx . Eq. (3.4) has a form that can be interpreted intuitively:
1. Term I contains a sum of squares of individual measurements, x2i , over the putative set
of targets, L. The smaller this sum, the more likely that L contains targets.
2. Term II contains the sample covariance about the known target value, sT = 0, that is,
P
P
i,j∈L (xi − sT )(xj − sT ) =
i,j∈L xi xj . In the absence of measurement correlations,
ρx = 0, covariability about the target value has vanishing expectation for target measurements. Therefore the larger this sum, the less likely that the measurements come
from a set of target stimuli. In the presence of measurement correlations, ρx > 0, covariability between target measurements is expected. Hence the prefactor in term II
decreases with ρx .
7

3. Term III is similar to term II, with the sum representing the sample covariance about the
target value between putative target and non-target stimuli. The larger this covariance,
the less likely that L or its complement contain targets.
4. Term IV contains the sample covariance about the mean of measurements outside of
the putative target set. If there are no targets, then all terms in the sum are expected
to be large, regardless of the choice of L. However, if there are targets, then whenever
the complement of L contains targets, some of the terms in the sum have expectation
0. Hence the term again makes a smaller contribution if targets are present.
While this provides an intuitive interpretation of the sums in Eq. (3.4), the expression is
complex and it is difficult to precisely understand how an ideal observer uses knowledge of the
generative model and the stimulus measurements to make a decision. We therefore examine
a number of cases where Eq. (3.4) is tractable, and all the terms can be interpreted precisely.
We also numerically examine performance in a wider range of examples.

3.1

Single target, n = 1

We start with the case when a single target is present at one of N locations. This case was considered previously in the absence of correlations between the sensory measurements (Bhardwaj
et al., 2015).
We observe in Figs. 3A and 4A that the performance of an ideal observer is nearly independent of ρs and ρx when external structure is weak (ρs < 1). Performance depends strongly
on ρx when distractors are strongly correlated, ρs ≈ 1. An ideal observer performs perfectly
when ρs = ρx = 1 (Fig. 3C).
Increased performance with increasing stimulus correlations, ρs , accords with intuition
that similar distractors make it easier to detect a target. However, correlations in measurement noise can play an equally important role and significantly improve performance when
distractors are identical (See Fig. 3B).
Perfect performance when ρs = ρx = 1, can be understood intuitively. In this case measurements, xi , of the stimuli are obtained by adding the same realization of a random variable,
i.e. identical measurement noise to each stimulus value, si . In target absent trials, all measurements are hence identical. If the target is present, measurements contain a single outlier.
An ideal observer can thus distinguish the two cases perfectly.
We examine in more detail the cases of weak measurement noise, and then the case of
comparable measurement noise and distractor variability.
Weak measurement noise, σx2  σs2 with highly correlated stimuli, ρs ≈ 1. When
measurement noise is weak, discriminability between the “target absent” and “target present”
conditions is governed by external variability, i.e. trial-to-trial variability of the stimuli. As
noted, measurement correlations improve discriminability in the presence of strong stimulus
correlations (see Figs. 3A,C). When ρs ≈ 1, Eq. (3.4) can be approximated as (details in

8

Appendix B.1):

1
dTD (x) ≈ log 
N
|

r

N (1 − ρx )
N −1 }
{z

P (N,ρx )


N
X
i=1




X
1
1
(1 − N ρx )x2i + 2xi
xj −
exp −
2
 2N σx (1 − ρx )
N −1
j6=i
{z
|
αi (x,N,ρx ,σx )

X
j6=i

! 2  



xj
.


}
(3.5)

The ideal observer hence uses knowledge about the measurement correlations, ρx , in a decision.
We first confirm the intuitive observation that an ideal observer performs perfectly when
measurement noise is highly correlated, 1 − ρx  1. In this case the exponential term in
Eq. (3.5) is, to leading order in 1/(1 − ρx ),


N −1
2
αi (x, N, ρx , σx ) ≈ exp
(xi − x̄ı̂ ) ,
(3.6)
2N σx2 (1 − ρx )
P
where x̄ı̂ = 1/(N − 1) j6=i xj is the sample mean of the measurements excluding the putative
target, i. Hence, to make a decision, the ideal observer subtracts the mean of the N − 1
measurements of putative distractors from that of the putative target. In Appedix B.1 we
show that on “target absent” trials, dTD (x) → −∞, as ρx → 1, and on target present trials,
dTD (x) → ∞, as ρx → 1. Hence performance improves as measurement correlations increase.
This can also be seen in Fig. 2B, as the overlap between the distributions p(x|T = 0) and
p(x|T = 1) decreases with an increase in ρx .
When measurement correlations are absent, ρx = 0, an ideal observer performs perfectly
only when measurement noise is vanishing. The exponential in Eq. (3.5) now equals


!2 
!2
X
X
1
1
1
αi (x, N, ρx , σx ) = exp − 2 
xj  
xj −
2σx N
N
−
1
j
j6=i



1
= exp − 2 N x̄2 − (N − 1)x̄2ı̂ ,
(3.7)
2σx
where x̄ is the sample mean of all observations. The ideal observer therefore compares the
sample mean over all measurements, x̄, with the sample mean over all observations excluding
the putative target, x̄ı̂ . Since measurement noise is uncorrelated, averaging over all measurements is beneficial. This is a very different strategy than when measurement noise is highly
correlated (Eq. (3.6)), and the ideal observer compares a single putative target measurement,
xi , to the sample mean of the remaining target measurements. As shown in Appendix B.1
at intermediate values of measurement correlations, 0 < ρx < 1, the ideal observer uses a
mixture of these two strategies.
Intuitively, an increased number of distractors make it more difficult to detect the target.
An analysis of the decision variable in the limit N → ∞ shows that this is indeed the case
(See Appendix B.3).
9

Figure 3: Performance of an optimal observer on a single target detection task with weak
measurement noise, σx2  σs2 . (A) Proportion of correct responses as a function of stimulus
correlation, ρs and measurement correlation, ρx , for N = 4 stimuli. (B) Proportion of correct
responses as a function of measurement correlation, ρx , in the case of intermediate stimulus
correlation, ρs = 0.5 (left) and strong correlation, ρs = 1 (right) for N = 4 stimuli. (C)
Decision boundary, dTD (x) = 0 (black solid line) and measurement distributions on target
present (left) and target absent (right) trials for N = 2 and ρs = 0.5. Here and henceforth
dark gray dots correspond to measurements that result in a correct inference, and light gray
dots correspond to measurements leading to an incorrect inference. The stimulus will result in
a measurement that lies within the black dashed lines with probability 0.95. Other parameters
used: σs = 15 and σx = 4.

10

Weak measurement noise, non-identical distractors, ρs < 1. Measurement correlations have little effect on performance when stimulus correlations are weaker (See Fig. 3A).
Consider again ρx ≈ 1, so that measurements are corrupted by adding a random, but nearly
identical perturbation to the stimuli. An ideal observer uses the knowledge that all measurements are obtained by adding an approximately equal value to the stimulus. However, when
stimulus correlations are weak, the target measurement is no longer an outlier. Correlations
in measurement noise provide little help in this situation.
These observations are reflected in the structure of the decision boundaries (dTD (x) = 0),
and the distributions of the measurements (See Fig. 3B). In the target absent (left column)
and present trials (right column) the distributions of measurements, p(x|T ), is shaped predominantly by variability in the stimulus. Measurement correlations have little effect on this
shape, and the decision boundary therefore changes little with an increase in ρx . In contrast when ρs ≈ 1, measurement correlations significantly impact the overlap between the
distributions p(x|T = 1) and p(x|T = 0) as shown in Fig. 2B.
We can confirm this intuition about the role of measurement correlations by approximating
the decision criterion when σx2  σs2 and ρs < 1 (see details in Appendix B.4),
s

!
N
x2i
1 σs2 (1 − ρs )(1 + (N − 1)ρs ) X
.
(3.8)
dTD (x) ≈ log
exp − 2
N
σx2 (1 + (N − 2)ρs )
2σ
x
i=1
The strength of noise correlations, ρx , does not affect the decisions of an ideal observer at
highest order in (σx2 /σs2 ). This explains the approximate independence of performance on ρx
observed in Fig. 3B. In this case an ideal observer considers stimuli at each location separately,
and weighs each measurement individually by its precision, 1/σx2 (Green and Swets, 1966;
Palmer, 1999).
When the number of distractors becomes larger, Eq. (3.8) is no longer valid. The observer
compares measurements of all stimuli to make a decision. We return to this point below.
Strong measurement noise, σx2 = σs2 Increasing measurement noise trivially degrades
performance. However, in the limit of perfect stimulus and measurement correlations, an ideal
observer still performs perfectly for the reasons described earlier.
Measurement correlations affect performance differently than in the case of weak measurement noise (See Fig. 4A,C). Even with uncorrelated stimuli, ρs = 0, performance increases
slightly (approximately 5-6%) with ρx . Surprisingly, for intermediate values of stimulus correlations, e.g. ρs = 0.5, measurement correlations negatively impact performance. If measurement correlations are fixed at a high value, then the worst performance is observed at
an intermediate value, 0 < ρs < 1. The reason for this unexpected behavior is unclear, as
Eq. (3.4) is difficult to analyze in this case.
Generally, when measurement noise is strong, measurement correlations will change the
shape of the measurement distributions p(x|T = 0) and p(x|T = 1), and hence impact decisions and performance. Note that when measurement correlations increase, the region corresponding to dTD (x) > 0 is elongated along the diagonal to capture more of the mass of
the distribution p(x|T = 1) (See Fig. 4C). However, when measurement noise is high, the
interactions between measurement and stimulus correlations are intricate.
11

Figure 4: Performance of an optimal observer on a single target detection task with strong
measurement noise, σx2 = σs2 . (A) Proportion of correct responses as a function of ρs and
ρx for N = 4 stimuli. (B) Performance as a function of measurement correlation, ρx , when
ρs = 0 (left) and ρs = 0.5 (right) for N = 4 stimuli. (C) Decision boundary (black solid line)
and distribution of measurements, x on target present (left) and target absent (right) trials
for N = 2 and ρs = 0.5. Other parameters used: σs = σx = 15.

12

3.2

Multiple targets, n > 1

In the present task when multiple targets are present, they are all identical and hence perfectly
correlated. Thus, regardless of the value of ρs , on half the trials the stimuli will be strongly
structured, and the density p(s|T = 1) concentrated on a low dimensional subspace. As a
consequence, measurement correlations always impact performance.
Regardless of stimulus correlations, an ideal observer performs perfectly when ρx = 1 (See
Fig. 5A). Even when ρs < 1, performance increases with ρx (see Fig. 5A). When ρx = 1, all
target measurements are identical. Hence, an ideal observer performs perfectly by checking
whether n of the measurements, xi , are equal. We only analyze the case ρs < 1, since the case
of perfectly correlated distractors is similar.

Figure 5: Performance of an optimal observer in a multiple target search task with weak
measurement noise, σx2  σs2 . (A) Proportion of correct responses as a function of stimulus
correlation, ρs , and measurement correlation, ρx , with N = 4 stimuli and n = 3 targets. (B)
Proportion of correct responses as a function of ρx for ρs = 0 (left) and ρs = 0.5 (right) when
N = 4 and n = 3. (C) Measurement distributions and decision boundary on target present
(left) and target absent (right) trials for ρs = 0.5 and N = n = 2.

With weak measurement noise, Eq. (3.4) can be approximated as (see Appendix B.4):
 s
 2
n
1
(1 − ρx )(1 + (N − 1)ρs )
σs (1 − ρs )
dTD (x) ≈ log
M (1 + (N − n − 1)ρs )(1 + (n − 1)ρs ) σx2 (1 − ρx )



!2 



X
n
1X 2
nρx
1X


×
exp − 2
. (3.9)
xi −
xi


(1 + (n − 1)ρx ) n i∈L
 2σx (1 − ρx ) n i∈L

L∈L
13

An ideal observer takes into account measurement correlations for all values of ρs even when
measurement noise is low. Stimulus correlations only appear in the prefactor, and are not
used in the comparison of the measurements inside the exponential.
Interestingly, decisions in this case are based only on measurements of stimuli within the set
of putative targets, L. In the absence of measurement correlations, a decision is based solely
on the sample second moment of the n stimulus measurements about the target characteristic,
P
sT = 0, i.e. 1/n i∈L x2i (the underlined term in Eq. (3.9)). A low value of this sample
moment indicates that L contains targets.
In the limit of ρx → 1, the underlined term in Eq. (3.9) approaches the sample variance,
2
si∈L , i.e. the sample second moment about the sample mean,
1X 2
nρx
xi −
n i∈L
1 + ρx (n − 1)

1X
xi
n i∈L

!2

1X 2
x −
→
n i∈L i

1X
xi
n i∈L

!2
= s2i∈L .

(3.10)

If measurement correlations are strong, the measurements of a set of target stimuli will be
approximately equal (regardless of ρs ). The ideal observer makes use of this knowledge by
comparing the value of putative targets in the set L. If the sample variance of the measurements {xi }i∈L is small, then it is likely that L is a set of targets. When ρx = 1, an ideal
observer performs perfectly (See Appendix B.5 for details).
When 0 < ρx < 1, the underlined term in Eq. (3.9) shows that the ideal observer takes
an intermediate strategy by computing a second moment about a point between the target
characteristic, sT , and the sample mean. Interestingly, the larger the number of targets, the
larger the weight on the sample mean, since the prefactor nρx /(1 + (n − 1)ρx ) increases with
n for fixed ρx .
These observations are reflected in the distributions shown in Fig. 5C. The distribution of
measurements, p(x|T = 1), move closer to the diagonal (x1 = x2 ) as ρx → 1, and the overlap
with the distribution of measurements, p(x|T = 0) decreases. In higher dimensions, for N
stimuli and n targets, the measurement distributions, p(x|T = 1), is concentrated on the
union of (N − n + 1)-dimensional subspaces when ρx = 1: The target measurements lie on a
line, while the N − n distractor measurements are distributed along the remaining directions.
To conclude, when there are multiple targets part of the stimulus set is always perfectly
correlated. When measurement correlations are high, the observer checks whether the measurements are similar to each other to make a decision. When measurement correlations are
low, the observer compares the measurements to the known target value. Measurement correlations can again decrease the overlap between the conditional distributions of measurements,
and significantly impact decisions and performance. For finite N , decisions are based on the
comparison of measurements within a putative set of stimuli, L. We show next that when N
is large, this is no longer the case.

3.3

Larger number of targets and stimuli

If we assume a fixed proportion, K, of the stimuli consists of targets, so that n = KN ,
then Eq. (3.4) simplifies considerably in the limit of large N . If we let cx = σx2 (1 − ρx ) and

14

cs = σs2 (1 − ρs ), the exponential in Eq. (3.4) has the form, (See Appendix B.6)



 1 NK 2

N
N (1 − K) 2 
2


,
−
+
αL (x, N, cx , cs ) ≈ exp 
s
s
s
−
 2  cx i∈L cs + cx
cs + cx i6∈L 
{z
}
| {z } | {z } |
I

II

(3.11)

III

where s2i∈L , s2i6∈L , and s2 are the sample variances of measurements form the putative target
set, L, outside of the putative target set, and over all N measurements, respectively.
The different terms in this expression can be interpreted as earlier: Term I is the sample
variance of measurements of the putative targets. If this variance is large, then the set L
is unlikely to contain targets. Term II is the sample variance among all terms. If this term
is large then all stimuli are dissimilar, and there is evidence that targets are present: For
example, when distractors are correlated, and cs  1, then the sample variance of all stimulus
measurements is small only in the absence of targets. Finally, term III is the sample variance
among putative distractors. If distractors are correlated, this term will be small if L contains
targets, and hence stimuli outside L are distractors. The sign of the three terms agrees with
this interpretation: Terms I and III are negative, and term II is positive.
The main difference between the cases of large N , and the examples discussed previously
is that an observer takes into account putative distractor measurements, i.e measurements
outside the putative target set L. An exception is the case when measurement correlations
are much stronger than distractor correlations, cx  cs . In this case, the putative targets are
more strongly structured, and hence only their measurements are used in a decision. When
cs  1, or, equivalently, ρs ≈ 1, and distractors are strongly correlated, all three terms in
Eq. (3.11) are comparable. In this case, ideal observers base their decision on the similarity,
as measured by sample variance, of both putative distractor and target measurements.
Importantly, the decision is made using distractor measurements, even when distractors
are not perfectly correlated. Fig 6 shows that intermediate distractor correlations have an
increasingly impact decisions with an increase in distractor number. Indeed, the higher the
fraction of distractors, (1 − K), the more weight is assigned to their sample variance (term
III). This is unlike the case of small N , where distractor measurements are used only when
they are perfectly correlated.

4

Discussion

We have shown that in a simple task the statistical structure of the stimulus as well as that of
noise in perceptual measurements determine the strategy and performance of an ideal observer.
Correlations in measurement noise can significantly impact performance, particularly when
stimulus correlations are high: When the distribution of stimuli conditioned on the parameter
of interest is concentrated in a small volume of stimulus space, the statistical structure of
measurement noise can be of particular importance (Mazyar et al., 2012, 2013; Bhardwaj
et al., 2015).
The impact of noise correlations on the inference of a parameter from neural responses
has been studied in detail (Averbeck et al., 2006; Averbeck, 2009; Latham and Nirenberg,
15

Figure 6: Normalized performance of an optimal observer with respect to ρs = 0 as a function
of stimulus correlation, ρs , for different values of N in case of (A) ρx = 0 and (B) ρx = 0.5.
Intermediate correlations between distractor measurements become increasingly important as
the number of distractors increases from four to sixteen. Other parameters used: σs = 15,
σx = 4, and n = 2.

2005; Perkel et al., 1967; Schneidman et al., 2003; Sompolinsky et al., 2001). Frequently
the parameter of interest was identified with the stimulus, and both were univariate. The
estimation of the orientation of a bar in the receptive field of a population of neurons has
been a canonical example.
Reality is far more complex. Stimuli, such as a natural visual or auditory scene, are
high dimensional and highly structured. Moreover, only some of the parameters are typically
relevant. Intuitively, if noise perturbs measurements along relevant direction, i.e. along the
directions of the parameters of interest, then estimates will be corrupted. Perturbations
along irrelevant directions in parameter space have little effect (Moreno-Bote et al., 2014).
Measurement correlations can channel noise into irrelevant directions, without decreasing
overall noise magnitude, and thus improve parameter inference.
This is difficult to study using general theoretical models without putting some constraints
on the structure of measurement noise. We therefore considered a relatively simple, analytically tractable example where both measurement and stimulus structure are characterized by
a small number of parameters. We have used a similar setup to examine decision making in
controlled search experiments (Mazyar et al., 2012, 2013; Bhardwaj et al., 2015).
We assumed that measurement noise and measurement correlations can be varied independently. This is not realistic. For instance, it is known that changes in the mean, variability
and covariability of neural responses can be tightly linked (Cohen and Kohn, 2011; de la
Rocha et al., 2007; Rosenbaum and Josić, 2011). It is thus likely that the statistics of measurement noise also change in concert. However, at present this relationship has not been well
characterized.
More importantly, noise from the periphery of the nervous system will limit the performance of any observer. It is therefore not possible that a simple change in measurement
correlations can lead to perfect performance (Moreno-Bote et al., 2014). To address this question it would be necessary to provide a more accurate model of both the noise correlations in
a recurrent network encoding information about the stimuli (Beck et al., 2011), as well as the
resulting measurement correlations. This is beyond the scope of the present study.
16

We also made strong assumptions about the structure of measurement and stimulus correlations. We chose to restrict our analysis to positive correlations. The reason is that the
requirement that a covariance matrix is positive definite implies restrictions on the range of
allowable negative measurement and stimulus correlations (Horn and Johnson, 2012). These
restrictions depend on the number of stimuli, N , and complicate the analysis. To make the
model tractable, we also assumed that all off-diagonal elements in the stimulus and measurement noise covariance matrices are identical. While we did not examine it here, heterogeneity
in the correlation structure can strongly affect parameter inference (Shamir and Sompolinsky,
2006; Chelaru and Dragoi, 2008; Berens et al., 2011).

Figure 7: Performance comparison of an optimal observer as a function of measurement
correlations in (A) a mean left/right discrimination task with N = 4 stimuli and (B) a target
detection task with n = 3 targets. Other parameters used: σs = 15, σx = 4, and ρs = 0.5.
To end, we provide another illustration of the fact that measurement correlations can
affect the performance of an ideal observer in different ways depending on the task: Suppose
an observer is presented with N oriented stimuli, such as Gabor patches. The stimuli and
measurements follow the same Gaussian distributions introduced earlier in this study. The
observer is asked to perform one of the following two tasks: 1) Report whether the mean
orientation of the stimuli is to the left or right of vertical; 2) Report whether a vertically
oriented target is present or absent. In this case, a subset of the stimuli has vertical orientation
on half the trials.
The first task is a discrimination task, and the observer needs to to integrate information from different sources. When measurement correlations are high, it is more difficult to
average out the noise between the stimulus measurements (Sompolinsky et al., 2001; Zohary
et al., 1994). The estimate of the average orientation is therefore degraded and performance
decreases with an increase in measurement correlations (see Fig. 7A and Appendix C). The
second is a detection task that requires extracting information that is buried in a sea of distractors. As discussed above, in this case measurement correlations can increase performance
if there is more than one target, or if the distractors are strongly correlated (see Fig. 7B).
In this example the stimuli and the measurements have the same statistical structure on
target absent trials for the two tasks. However, the parameter of interest differs: In the
first task the observer needs to estimate the average stimulus orientation, and in the second
17

determine whether a target is present. The distributions of measurements conditioned on these
parameters are therefore also different, and are differently affected by measurement noise.
The question of how measurement correlations impact decision making and performance
does not have a simple answer (Hu et al., 2014). Correlations in measurement noise can have a
pronounced effect when the stimuli themselves are highly correlated, i.e. when they occupy a
small volume in stimulus space. We have illustrated how in this case measurement correlations
can help in separating the distribution of measurements conditioned on a parameter of interest.
Similar considerations will be important whenever we try to understand how information can
be extracted from the collective responses of neural populations to high dimensional, and
highly structured stimuli.

5

Acknowledgment

K.J. was supported by NSF award DMS-1122094. W.J.M. was supported by award number
R01EY020958 from the National Eye Institute and award number W911NF-12-1-0262 from
the Army Research Office.

Appendices
A

Derivation of Eqs. (3.3) and (3.4)

Here we present the details of some of the calculations leading to the results presented in the
main text. The computations rely on the assumption of Gaussianity. We will therefore make
repeated use of the fact that the density N (x; µ, Σ) of the normal distribution with mean µ,
and covariance Σ is


1
1
T −1
exp − (x − µ) Σ (x − µ) ,
(A.1)
N (x; µ, Σ) = p
2
(2π)N |Σ|
where |Σ| denotes
 the determinant of the matrix Σ.
N
Let M =
denote the cardinality of the set L of all possible sets L. We compute
n
p(x|T = 1) in Eq. (3.1) by marginalizing over s,
Z
p(x|T = 1) = p(x|s)p(s|T = 1)ds.
We note that
p(s|T = 1) =

X

p(s|T = 1, L)p(L) =

L∈L

18

1 X
p(s|T = 1, L).
M L∈L

Therefore,
1
p(x|T = 1) =
M

Z

X
p(x|s) p(s|T = 1, L)ds
L∈L

Z
1 X
p(x|s)p(s|T = 1, L)ds
=
M L∈L
XZ
1
=
lim
N (x; s, Σx )N (s; 0N , Σηs,L )ds
M η→0 L∈L
1 X
N (x; 0N , CL ),
=
M L∈L
Similarly,
Z
p(x|T = 0) =

p(x|s)p(s|T = 0)ds
Z

=

N (x; s, Σx )N (s; 0N , Σs )ds

= N (x; 0N , C),
where CL = Σx + Σ0s,L and C = Σx + Σs . We therefore obtain
!
p(x|T = 1)
1 X N (x; 0N , CL )
dTD (x) = log
= log
p(x|T = 0)
M L∈L N (x; 0N , C)
s

!

1 T
1
|C| X
−1
= log
exp − x C−1
.
x
L −C
M
|CL | L∈L
2

(A.2)

We note that the determinant of CL does not depend on the set L since all matrices CL can
be obtained from each other by permuting appropriate rows and columns.
In the case variances and covariances are equal, we can invert CL and C. In general,
matrix CL has the following form,


σx2 ,
if i = j ∈ L,



σ 2 + σ 2 ,
if i = j ∈
/ L,
s
x
(CL )i,j =
2

ρx σx ,
if i 6= j, and i or j ∈ L,



ρ σ 2 + ρ σ 2 , if i 6= j, and i, j ∈
/ L,
s s

x x

We will use the Sherman-Morrison-Woodbury formula (Meyer, 2000),
−1
(A + U EV )−1 = A−1 − A−1 U E −1 + V A−1 U
V A−1 ,

(A.3)

to obtain the inverses of CL and C. To do so we first rewrite CL as AL + UL EVL : When
L = {1, 2, ..., n} (targets placed at the first n out of N possible locations), we have
 2


T
(σx − b)In
0n×(N −n)
b ··· b
a ··· a
AL =
,
UL =
,
0(N −n)×n (σs2 + σx2 − a)I(N −n) (N ×N )
b · · · · · · · · · · · · b (N ×2)




0 ··· 0 1 ··· 1
1 0
VL =
, and
E=
,
1 · · · 1 0 · · · 0 (2×N )
0 1 (2×2)
19

where a = ρs σs2 + ρx σx2 , and b = ρx σx2 .
Using Eq. (A.3) we obtain

2g
bvL
L

v
−
, if

L
γ

L

2

v fL

if

v − 2 γL ,
bvL gL
−1
(CL )i,j = − γ ,
if
L


v 2 fL


− γL ,
if



− bvvL ,
if
γL

i = j ∈ L,
i=j∈
/ L,
i 6= j, and i, j ∈ L,
i 6= j, and i, j ∈
/ L,
i 6= j, i ∈ L, j ∈
/ L or i 6= j, i ∈
/ L, j ∈ L,

where
1
1
, vL = 2
, fL = a + nρs σs2 ρx σx2 vL
2
− ρs ) + σx (1 − ρx )
σx (1 − ρx )
gL = 1 + ρs σs2 (N − n)v, and γL = 1 + a(N − n)v + nρx σx2 vL gL .
v=

σs2 (1

(A.4)

Using the Matrix Determinant Lemma (Harville, 1998), we can obtain the determinant of CL
det(CL ) =

vLn

γL
.
v N −n

Similarly, we compute the inverse and determinant of

v − βv 2 −βv 2 · · ·
 −βv 2 v − βv 2 · · ·

−1
C =  ..
..
 .
.
2
2
−βv
−βv
···
where
β=

matrix C,

−βv 2
−βv 2 

,

2
v − βv

a
,
1 + aN v

and
det(C) =

(A.5)

1 + aN v
.
vN

The prefactor in Eq. (A.2), is therefore
s
s
|C|
1 + aN v  vL n
=
|CL |
γL
v
and we can compute
T

−1

2

x C x = (v − βv )

N
X
i=1

T

x

C−1
L x

x2i

− βv

2

N
X

xi xj ,

i6=j


X

X
X
1 2
1 2
1
2
=
vL − bvL gL
x i + v − v fL
xi xj
x2i − bvL2 gL
γL
γ
γ
L
k
i∈L
i,j∈L
i∈L
/

i6=j

X
X
2
1
− bvvL
xi xj − v 2 f L
xi xj .
γL
γ
k
i∈L
j ∈L
/

i,j ∈L
/
i6=j

20

A slight rearrangement of the terms therefore shows that when variances and covariances are
equal, Eq. (A.2) is equivalent to
s
(
X
1 1 + aN v  vL n X
1
x2i
dTD (x) = log
exp −
σs2 (1 − ρs )vL v
M
γL
v
2
i∈L
L∈L
 







2
2
X
X
v bgL X
vL vb
fL v
+ βv 2 − L
xi xj + 2 βv 2 −
xi xj + βv 2 −
x i xj   .

γL
γL
γL
i,j∈L

B

i∈L,j ∈L
/

i,j ∈L
/

Asymptotic analysis of Eq. (3.4)

Here we present some asymptotic results for the decision variable dTD (x) given in Eq. (3.4).
The main results are obtained for small measurement noise, σx2 . Equivalent results can be
obtained for large external variability, σs2 .

B.1

Small measurement noise and idental distractors, ρs = 1

We first concentrate on the case n = 1. The exponential terms in Eq. (3.4) simplify to the
following:
σs2 (1 − ρs )vL v = 0,
ρx σx2 vL2 gL
σs2 + ρx σx2
= 2
γL
σx (1 − ρx )[N σs2 + σx2 (1 + (N − 1)ρx )]
ρx [σs2 (N − 1) + σx2 (1 − ρx )]
− 2
σx (1 − ρx )[σs2 (N − 1) + σx2 (1 − ρx )(1 + (N − 1)ρx )]
1 − N ρx
=
+ O(1),
N σx2 (1 − ρx )
ρx σx2 vL v
σs2 + σx2 ρx
ρx
βv 2 −
= 2
− 2
2
2
2
γL
σx (1 − ρx )[N σs + σx (1 + (N − 1)ρx )] σs (N − 1) + σx (1 − ρx )(1 + (N − 1)ρx )
1
+ O(1),
=
2
N σx (1 − ρx )
v 2 fL
σs2 + σx2 ρx
βv 2 −
= 2
γL
σx (1 − ρx )[N σs2 + σx2 (1 + (N − 1)ρx )]
σs2 + ρx σx2 (1 − ρx )
− 2
σx (1 − ρx )[σs2 (N − 1) + σx2 (1 − ρx )(1 + (N − 1)ρx )]
1
=−
+ O(1).
N (N − 1)σx2 (1 − ρx )

βv 2 −

And the leading determinant term becomes:
s
s
1 + aN v vL
(1 − ρx )[N σs2 + σx2 (1 + (N − 1)ρx )]
=
γL
v
σs2 (N − 1) + σx2 (1 − ρx )(1 + (N − 1)ρx )
r
N (1 − ρx )
+ O (σx2 )
=
N −1
21

Therefore, the decision variable becomes approximately,
r
1 N (1 − ρx )
dTD (x) ≈ log
N
N −1


N

X
X
−1
1
(1 − N ρx )x2i + 2xi
xj −
exp
2
 2N σx (1 − ρx )
N −1
i=1
j6=i

X
j6=i

!2 

.

xi


After rearranging terms, this can be rewritten as
r
1 N (1 − ρx )
dTD (x) ≈ log
N
N −1
!

N
X

1
N −1
,
(xi − x̄ı̂ )2 − 2 N x̄2 − (N − 1)x̄2ı̂
exp ρx
2 (1 − ρ )
2N
σ
2σx
x
x
i=1

(B.1)

where x̄ is the sample mean of all measurements, and where x̄ı̂ is the sample mean of the
measurements excluding the putative target, i. In the limiting cases ρx = 0 and (1 − ρx )  1,
we obtain the exponents given in Eq. (3.6) and Eq. (3.7) discussed in the text.

B.2

Perfect performance when ρx = 1

We show that when ρs = 1, an ideal observer performs perfectly in the limit of identical
measurement noise. For a fixed number of stimuli, N, on “target absent” trials, xi − (N −
P
√
1)−1 j6=i xj = O( ), and hence α(x, N, ρx , σx ) = O(1). On the other hand the prefactor,
√
P (N, ρx ) = O( ), and hence dTD (x) → −∞, as  → 0, i.e. as ρx → 1. On “target
P
present” trials, when stimulus i is the target, then xi − (N − 1)−1 j6=i xj = O(1), and
√
αi (x, N, ρx , σx ) = O(1/). In this case the prefactor is still P (N, ρx ) = O( ), and the
exponential term dominates. A similar argument works for the summands for which i is not
a target.

B.3

Single target with increasing number of distractors

We still work under the assumption that measurement noise is relatively
√ weak, so that we
2
can use Eq. (B.1). Note that on “target absent” trials, x̄ = s + O(σx /√ N ), where s is the
true value of the (identical) distractors. We also have x̄ı̂ = s + O(σx2 / N −√1). Hence, the
first term in the exponential of Eq. (B.1) is O(1), while the second term is O( N ). A similar
argument holds in “taget present” trials.
On target absent trials


√

1 2
1
2
2
for all i,
− 2 N x̄ − (N − 1)x̄ı̂ = exp − 2 s + O( N σx )
2σx
2σx
to leading order in N . We abuse notation slightly and only use order notation on the terms
that include measurement noise. As stimuli become more dissimilar to the target, i.e. as s2
increases, αi decreases exponentially, dTD (x) becomes more negative, and it is hence easier to
22

√
infer that a target is absent. However, the O( N σx ) terms can be both positive and negative.
Thus performance decreases with the number of stimuli. Similarly we can see that when a
target is present


√
1 N −1 2
2
αi (x, N, σx ) = exp
s + O( N σx )
when i is the target,
(B.2)
2σx2 N


√
1 1 + N − N2 2
αi (x, N, ρx ) = exp − 2
when i is not a target, (B.3)
s + O( N σx )
2σx N − N 2
again to leading order in N . As measurement noise decreases, or s2 increases, the first term
given in Eq. (B.2) diverges exponentially, and the terms √
given in Eq. (B.3) approach 0 exponentially. As a result dTD (x) increases. However, the O( N σx ) noise term increases with N ,
and an increase in the number of stimuli again decreases performance.
If (1 − ρx )  1/N , i.e. measurement noise is strongly correlated, the first term in the
exponential of Eq. (B.1) dominates. Thus when correlations increase faster than the inverse
of the number of distractors, performance increases with the number of distractors.

B.4

Weak external structure, ρs < 1, arbitrary number of targets

We approximate each term in the exponential of Eq. (3.4) assuming σx2  1:
σs2 (1 − ρs )
σx2 (1 − ρx )[σs2 (1 − ρs ) + σx2 (1 − ρx )]
1
= 2
+ O(1),
σx (1 − ρx )
ρx σx2 vL2 gL
ρs σs2 + ρx σx2
βv 2 −
= 2
γL
[σs (1 − ρs ) + σx2 (1 − ρx )][σs2 (1 + (N − 1)ρs ) + σx2 (1 + (N − 1)ρx )]
ρx [σs2 (1 + (N − n − 1)ρs + σx2 (1 − ρx )]
− 2
σx (1 − ρx )[σs2 (1 + (n − 1)ρx )(1 + (N − n − 1)ρs ) + σx2 (1 − ρx )(1 + (N − 1)ρx ))]
ρx
=− 2
+ O(1),
σx (1 − ρx )(1 + (n − 1)ρx )
ρx σx2 vL v
ρs σs2 + ρx σx2
βv 2 −
= 2
γL
[σs (1 − ρs ) + σx2 (1 − ρx )][σs2 (1 + (N − 1)ρs ) + σx2 (1 + (N − 1)ρx )]
ρx
− 2
[σs (1 + (N − n − 1)ρs )(1 + (n − 1)ρx ) + σx2 (1 − ρx )(1 + (N − 1)ρx )]
= O(1),
σs2 (1 − ρs )vL v =

and
v 2 fL
ρs σs2 + ρx σx2
= 2
γL
[σs (1 − ρs ) + σx2 (1 − ρx )][σs2 (1 + (N − 1)ρs ) + σx2 (1 + (N − 1)ρx )]
ρs σs2 (1 + (n − 1)ρx ) + ρx σx2 (1 − ρx )
− 2
[σs (1 − ρs ) + σx2 (1 − ρx )][σs2 (1 + (N − n − 1)ρs )(1 + (n − 1)ρx ) + σx2 (1 − ρx )(1 + (N − 1)ρx )]
= O(1)

βv 2 −

23

We also approximate the leading coefficient of the exponential term in Eq. (3.4) as:
s

1 + (ρs σs2 + ρx σx2 )N v  vL n
=
γL
v

s

(1 − ρx )[σs2 (1 + (N − 1)ρs ) + σx2 (1 + (N − 1)ρx )]
[σs2 (1 + (N − n − 1)ρs )(1 + (n − 1)ρx ) + σx2 (1 − ρx )(1 + (N − 1)ρx )]
s

n
ρs (1 − ρs )
(1 − ρx )(1 + (N − 1)ρs )
+ O(1).
=
(1 + (N − n − 1)ρs )(1 + (n − 1)ρx ) σx2 (1 − ρx )



σs2 (1 − ρs ) + σx2 (1 − ρx )
σx2 (1 − ρx )

Combining above terms, Eq. (3.4) reduces to the following expression under the assumption
of σx2  σs2 , ρs < 1,
 s
n
 2
1
(1 − ρx )(1 + (N − 1)ρs )
σs (1 − ρs )
dTD (x) ≈ log
M (1 + (N − n − 1)ρs )(1 + (n − 1)ρs ) σx2 (1 − ρx )
(
!)
X
X
X
1
ρ
x
×
exp − 2
x2i −
xi xj
.
2σ
(1
−
ρ
)
1 + (n − 1)ρx i,j∈L
x
x
L∈L
i∈L
Special case: n = 1 In case of a single target, set L has only one element and L =
{1, 2, · · · , N }. In this case, Eq. (3.9) reduces to a much simpler expression
s

!
N
1 σs2 (1 − ρs )(1 + (N − 1)ρs ) X
x2i
dTD (x) ≈ log
exp − 2
.
N
σx2 (1 + (N − 2)ρs )
2σx
i=1
This expression is independent of ρx . Hence, the decision boundary, and the performance of
an ideal observer is unaffected by measurement correlations.

B.5

Near perfect performance with n > 1, and ρx ≈ 1

We first assume that T = 1. From Eq. (3.10), if LT is the set of targets, then this expression
is approximately zero, and the exponential is approximately unity. When L is a set not
consisting of all targets, then the expression in Eq. (3.10) has expectation greater than zero.
Then


!2  


X
X
X
1
n
1
x2i −
xi  =
exp − 2
 2σx (1 − ρx ) n

n i∈L
L∈L
i∈L


X
nαL2
→1
(B.4)
1+
exp − 2
2σx (1 − ρx )
L∈L\LT

The prefactor in Eq. (3.9) diverges since the exponential dominates. If T = 0, then Eq. (3.10)
will be greater than zero for all sets, L. Thus dT D (x) → −∞ with ρx → 1.

24

n

B.6

Asymptotics for large N

Here we develop asymptotic results for the decision variable when the number of stimuli, N
is large. We assume that there a fraction K of the stimuli are targets, so that there are
N K targets and (1 − K)N distractors. To simplify notation we write cx = σx2 (1 − ρx ) and
cs = σs2 (1 − ρs ). We find that
cs
(1 − ρs )vL v =
cx (cs + cx )




1 vL 2
1
1 1
1
1
v2 β −
ρx σx2 gL
=
−
+ O( 2 )
γL v
N cs + cx N K cx
N


1 vL
1
1
1
v2 β −
ρx σx2
=
+ O( 2 )
γ v
N c + cx
N
L

 s

1
1
1
1
1
2
=
−
+ O( 2 )
v β − fL
γL
N
(1 − K)N cx + cs
N

KN 



2
2
2
1 + (ρs σs + ρx σx )N v vL n
cs
1 (ρx − 1)(ρs σs + ρx σx2 )
=
1+
γL
v
cx
N (K − 1)Kρs ρx σs2
We can therefore group the terms in the exponential of the decision variable given by Eq. (3.4)
as


N
X
X
X
X
1
cs
1
1
1
1 1
1
− 
x2i +
xi xj −
xi xj −
xi xj 
2 cx (cs + cx ) i∈L
N cx + cs i,j
N K cx i,j∈L
(1 − K)N cx + cs
i,j ∈L
/

A simple reorganization of the terms yields Eq. (3.11).

C

Mean stimulus orientation - left or right discrimination task

An observer is presented with N stimuli on every trial. For concreteness, we can think of the
stimuli as bars, with orientations s = (s1 , s2 , · · · , sN ). The task is to decide whether the mean
orientation of the set is to the left (a condition we denote by C = −1) or right (C = 1) of
the vertical. The observer makes a decision based on the measurements, x = (x1 , x2 , · · · , xN ).
Stimulus orientations are drawn from a multivariate normal distribution with mean vector,
0N , and covariance matrix, Σs ,
p(s) = N (0N , Σs ),
(C.1)
with Σs defined in Eq. (2.2). As in the target detection task, we assume the measurements
follow a multivariate normal distribution with mean vector and covariance matrix specified in
Eq. (2.4).
An ideal observer performs the task by making a decision based on the log posterior ratio,
p(C = 1|x)
p(s̄ > 0|x)
= log
p(C = −1|x)
p(s̄ < 0|x)
p(x|s̄ > 0)
p(s̄ > 0)
= log
+ log
,
p(x|s̄ < 0)
p(s̄ < 0)

dMOD (x) = log

25

(C.2)

where, s̄ =

N
X

si denotes the mean stimulus orientation on a trial. If dMOD (x) > 0, the

i=1

observer infers Ĉ = 1, that is, the mean stimulus orientation is to the right of the vertical.
We compute Eq. (C.2) by marginalizing over s and applying Bayes’ rule,
Z
p(x|s̄ > 0) =
p(x|s)p(s|s̄ > 0)ds
Z
p(s)
=
p(x|s)p(s̄ > 0|s)
ds
p(s̄ > 0)
Z
1
=
p(x|s)p(s)ds
p(s̄ > 0) s̄>0
Z
1
=
N (x; s, Σx )N (s; 0N , Σs )ds
p(s̄ > 0) s̄>0
Z


 
zc
−1
−1
−1 −1
=
N s; I + Σx Σs x, Σx + Σs
ds,
p(s̄ > 0) s̄>0
where zc is a normalization constant. Similarly, we compute p(x|s̄ < 0) and obtain

 
R
−1 −1
−1
−1 −1
N
s;
(I
+
Σ
Σ
)
x,
(Σ
+
Σ
)
ds
x s
x
s
s̄>0
.

dMOD (x) = log  R
−1 )−1 x, (Σ−1 + Σ−1 )−1 ds
N
s;
(I
+
Σ
Σ
x
s
x
s
s̄<0

(C.3)

By symmetry, it is easy to see that the decision boundary in the space of measurements is
N
X
1
given by the hyperplane x̄ = 0, where x̄ = N
xi is the sample mean of the measurement.
i=1

An ideal observer therefore bases the decision only on the sample mean. Therefore, we have
x̄ > 0 ⇒ s̄ > 0.
In order to understand the negative impact of noise correlations on the performance of an
optimal observer on this task, consider x = s + ξ~ where ξ~ = (ξ1 , ξ2 , · · · , ξN ) ∼ N (0, Σx ), so
that
1 X
¯
x̄ =
(si + ξi ) = s̄ + ξ.
N i
In this case, the variance of the mean noise scales with increasing noise correlation strength,
ρx .
2
¯ = σx (1 − ρx ) + ρx σ 2 .
Var(ξ)
x
N
As the variance increases, the overlap between the conditional distributions p(x̄|s̄ > 0) and
p(x̄|s̄ < 0) increases. It is therefore more difficult to tell which condition the measurement
comes from, and performance deteriorates.

References
Abbott, L. and Dayan, P. (1999). The effect of correlated variability on the accuracy of a
population code. Neural Computation, 11(1):91–101.
26

Averbeck, B. (2009). Noise correlations and information encoding and decoding. Springer.
Averbeck, B., Latham, P., and Pouget, A. (2006). Neural correlations, population coding and
computation. Nature Neuroscience Reviews, 7(5):358–366.
Averbeck, B. and Lee, D. (2006). Effects of noise correlations on information encoding and
decoding. Journal of Neurophysiology, 95(6):3633–3644.
Baldassi, S. and Burr, D. C. (2000). Feature-based integration of orientation signals in visual
search. Vision Research, 40(10):1293–1300.
Baldassi, S. and Verghese, P. (2002). Comparing integration rules in visual search. Journal
of Vision, 2(8):3.
Bartlett, M. (1951). An inverse matrix adjustment arising in discriminant analysis. The
Annals of Mathematical Statistics, 22(1):107–111.
Beck, J., Bejjanki, V., and Pouget, A. (2011). Insights from a simple expression for linear Fisher information in a recurrently connected population of spiking neurons. Neural
Computation, pages 1–19.
Beck, J., Ma, W., Pitkow, X., Latham, P., and Pouget, A. (2012). Not noisy, just wrong: the
role of suboptimal inference in behavioral variability. Neuron, 74(1):30–39.
Berens, P., Ecker, A., Gerwinn, S., Tolias, A., and Bethge, M. (2011). Reassessing optimal
neural population codes with neurometric functions. Proceedings of the National Academy
of Sciences of the United States of America.
Bhardwaj, M., van den Berg, R., Ma, W., and Josić, K. (2015). Do people take stimulus
correlations into account in visual search? Submitted.
Brady, T. and Tenenbaum, J. (2010). Encoding higher-order structure in visual working
memory: A probabilistic model. Proceedings of the 32nd Annual Conference of the Cognitive
Science Society, pages 411–416.
Chelaru, M. and Dragoi, V. (2008). Efficient coding in heterogeneous neuronal populations. Proceedings of the National Academy of Sciences of the United States of America,
105(42):16344–16349.
Chen, Y., Geisler, W., and Seidemann, E. (2006). Optimal decoding of correlated neural
population responses in the primate visual cortex. Nature Neuroscience, 9(11):1412–1420.
Cohen, M. and Kohn, A. (2011). Measuring and interpreting neuronal correlations. Nature
Neuroscience, 14(7):811–819.
de la Rocha, J., Doiron, B., Shea-Brown, E., Josić, K., and Reyes, A. (2007). Correlation
between neural spike trains increases with firing rate. Nature, 448(7155):802–806.
Ecker, A., Berens, P., Keliris, G., Bethge, M., Logothetis, N., and Tolias, A. (2010). Decorrelated neuronal firing in cortical microcircuits. Science, 327(5965):584–587.
27

Ecker, A., Berens, P., Tolias, A., and Bethge, M. (2011). The effect of noise correlations in
populations of diversely tuned neurons. The Journal of Neuroscience, 31(40):14272–14283.
Ganmor, E., Segev, R., and Schneidman, E. (2011). Sparse low-order interaction network
underlies a highly correlated and learnable neural population code. Proceedings of the
National Academy of Sciences, 108(23):9679–9684.
Gawne, T. and Richmond, B. (1993). How independent are the messages carried by adjacent
inferior temporal cortical neurons? The Journal of Neuroscience, 13(7):2758–2771.
Geisler, W. (2008). Visual perception and the statistical properties of natural scenes. Annual
review of psychology, 59:167–192.
Graham, N., Kramer, P., and Yager, D. (1987). Signal-detection models for multidimensional stimuli: Probability distributions and combination rules. Journal of Mathematical
Psychology, 31(4):366–409.
Green, D. and Swets, J. (1966). Signal detection theory and psychophysics, volume 1. Wiley
New York.
Hansen, B., Chelaru, M., and Dragoi, V. (2012). Correlated variability in laminar cortical
circuits. Neuron, 76:590–602.
Harville, D. (1998).
40(2):164–164.

Matrix algebra from a statistician’s perspective.

Technometrics,

Horn, R. and Johnson, C. (2012). Matrix analysis. Cambridge University Press.
Hu, Y., Zylberberg, J., and Shea-Brown, E. (2014). The sign rule and beyond: boundary
effects, flexibility, and noise correlations in neural population codes. PLoS Computational
Biology, 10(2):e1003469.
Josić, K., Shea-Brown, E., Doiron, B., and de la Rocha, J. (2009). Stimulus-dependent
correlations and population codes. Neural Computation, 21(10):2774–2804.
Kohn, A. and Smith, M. (2005). Stimulus dependence of neuronal correlation in primary
visual cortex of the macaque. The Journal of Neuroscience, 25(14):3661–3673.
Latham, P. and Nirenberg, S. (2005). Synergy, redundancy, and independence in population
codes, revisited. The Journal of Neuroscience, 25(21):5195–5206.
Latham, P. and Roudi, Y. (2011). Role of correlations in population coding. arXiv preprint
arXiv:1109.6524.
Ma, W. (2010). Signal detection theory, uncertainty, and poisson-like population codes. Vision
Research, 50(22):2308–2319.
Ma, W., Navalpakkam, V., Beck, J. M., van den Berg, R., and Pouget, A. (2011). Behavior
and neural basis of near-optimal visual search. Nature Neuroscience, 14(6):783–790.
28

Mazyar, H., van den Berg, R., and Ma, W. (2012). Does precision decrease with set size?
Journal of Vision, 12(6).
Mazyar, H., van den Berg, R., Seilheimer, R., and Ma, W. (2013). Independence is elusive:
set size effects on encoding precision in visual search. Journal of Vision, 13(5):1–14.
Meyer, C. (2000). Matrix analysis and Applied Linear Algebra Book and Solutions Manual,
volume 2. SIAM.
Moreno-Bote, R., Beck, J., Kanitscheider, I., Pitkow, X., Latham, P., and Pouget, A. (2014).
Information-limiting correlations. Nature Neuroscience, 17(10):1410–1417.
Nolte, L. W. and Jaarsma, D. (1967). More on the detection of one of m orthogonal signals.
Journal of the Acoustical Society of America, 41(2):497–505.
Ohiorhenuan, I., Mechler, F., Purpura, K., Schmid, A., Hu, Q., and Victor, J. (2010). Sparse
coding and high-order correlations in fine-scale cortical network. Nature, 466:617–621.
Palmer, J., Ames, C. T., and Lindsey, D. T. (1993). Measuring the effect of attention on simple
visual search. Journal of Experimental Psychology: Human Perception and Performance,
19(1):108.
Palmer, S. (1999). Vision science: Photons to phenomenology, volume 1. MIT press Cambridge, MA.
Pelli, D. G. (1985). Uncertainty explains many aspects of visual contrast detection and
discrimination. Journal of the Optimal Society of America A, 2(9):1508–1531.
Perkel, D., Gerstein, G., and Moore, G. (1967). Neuronal spike trains and stochastic point
processes: II. Simultaneous spike trains. Biophysical Journal, 7(4):419–440.
Peterson, W. W., Birdsall, T. G., and Fox, W. (1954). The theory of signal detectability.
Information Theory, Transactions of the IRE Professional Group on, 4(4):171–212.
Romo, R., Hernandez, A., Zainos, A., and Salinas, E. (2003). Correlated neuronal discharges
that increase coding efficiency during perceptual discrimination. Neuron, 38(4):649–657.
Rosenbaum, R. and Josić, K. (2011). Mechanisms that modulate the transfer of spiking
correlations. Neural Computation, 23(5):1261–1305.
Rosenbaum, R., Trousdale, J., and Josić, K. (2010). Pooling and correlated neural activity.
Frontiers in Computational Neuroscience, 4:9.
Schneidman, E., Bialek, W., and Berry, M. (2003). Synergy, redundancy, and independence
in population codes. Journal of Neuroscience, 23(37):11539–11553.
Seriès, P., Latham, P., and Pouget, A. (2004). Tuning curve sharpening for orientation selectivity: coding efficiency and the impact of correlations. Nature Neuroscience, 7(10):1129–1135.

29

Shamir, M. and Sompolinsky, H. (2002). Correlation codes in neuronal populations. Advances
in neural information processing systems, 1:277–284.
Shamir, M. and Sompolinsky, H. (2006). Implications of neuronal diversity on population
coding. Neural Computation, 18(8):1951–1986.
Sherman, J. and Morrison, W. (1950). Adjustment of an inverse matrix corresponding to a
change in one element of a given matrix. The Annals of Mathematical Statistics, 21(1):124–
127.
Sompolinsky, H., Yoon, H., Kang, K., and Shamir, M. (2001). Population coding in neuronal
systems with correlated noise. Physical Review E, 64(5):051904.
Tkačik, G., Prentice, J., Balasubramanian, V., and Schneidman, E. (2010). Optimal population coding by noisy spiking neurons. Proceedings of the National Academy of Sciences,
107(32):14419–14424.
van den Berg, R., Vogel, M., Josić, K., and Ma, W. (2012). Optimal inference of sameness.
Proceedings of the National Academy of Sciences, 109(8):3178–83.
Zohary, E., Shadlen, M., and Newsome, W. (1994). Correlated neuronal discharge rate and
its implications for psychophysical performance. Nature, 370(6485):140–143.

30

