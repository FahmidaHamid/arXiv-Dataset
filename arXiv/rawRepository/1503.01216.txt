Visual decisions in the presence of
measurement and stimulus correlations
Manisha Bhardwaj

‚àó1

, Sam Carroll

‚Ä†1

, Wei Ji Ma

‚Ä°2,3

, and KresÃåimir JosicÃÅ

¬ß1,4

1

arXiv:1503.01216v1 [q-bio.NC] 4 Mar 2015

Department of Mathematics, University of Houston
Department of Neuroscience, Baylor College of Medicine
3
Now at: Center for Neural Science and Department of Psychology, New
York University
4
Department of Biology and Biochemistry, University of Houston
2

Abstract
Humans and other animals base their decisions on noisy sensory input. Much work
has therefore been devoted to understanding the computations that underly such decisions. The problem has been studied in a variety of tasks and with stimuli of differing
complexity. However, the impact of correlations in sensory noise on perceptual judgments is not well understood. Here we examine how stimulus correlations together with
correlations in sensory noise impact decision making. As an example, we consider the
task of detecting the presence of a single or multiple targets amongst distractors. We
assume that both the distractors and the observer‚Äôs measurements of the stimuli are
correlated. The computations of an optimal observer in this task are nontrivial, yet
can be analyzed and understood intuitively. We find that when distractors are strongly
correlated, measurement correlations can have a strong impact on performance. When
distractor correlations are weak, measurement correlations have little impact, unless the
number of stimuli is large. Correlations in neural responses to structured stimuli can
therefore strongly impact perceptual judgments.

1

Introduction

The perceptual system has evolved to extract ecologically meaningful information from sensory
input. For example, in many mid- to high-level visual tasks the brain has to make categorical,
global judgements based on multiple stimuli where the identity of any individual stimulus
‚àó

manisha@math.uh.edu
srcarroll314@gmail.com
‚Ä°
weijima@nyu.edu
¬ß
josic@math.uh.edu
‚Ä†

1

is not of direct relevance. In a visual search task, the goal might be to detect whether a
predefined target object is present in a scene that contains multiple objects. Complicating
such tasks is the fact that noise corrupts sensory measurements, especially when observation
time is short, or many objects are present.
Much work has been devoted to modeling the decision processes by which the brain converts noisy sensory measurements of a set of stimuli into a judgement about a global world
state, such as the presence or absence of a target. These models often focus on various decision
rules that can be applied to the measurements. By contrast, the measurements themselves
are usually modeled in a rather stereotypical fashion, namely as independent and normally
distributed, (e.g. Peterson et al. (1954); Nolte and Jaarsma (1967); Pelli (1985); Graham
et al. (1987); Palmer et al. (1993); Baldassi and Burr (2000); Baldassi and Verghese (2002);
van den Berg et al. (2012); Ma et al. (2011); Mazyar et al. (2012)). Both the assumption
of independence and the assumption of Gaussianity can be questioned. Specifically, neural
correlations can extend to distances as long as 4mm in monkey cortex (Ecker et al., 2010;
Cohen and Kohn, 2011). This suggests that sensory measurements can be strongly correlated (Rosenbaum et al., 2010; Chen et al., 2006). Here we focus on the effects of violation of
the assumption of independent measurements on performance in categorical, global perceptual
judgements.
To make such perceptual judgments, an observer needs to take into account the statistical
structure of the stimuli and the structure of measurements. Consider a search task where
a subject is required to detect a target among distractors. The effects of measurement correlations and stimulus correlations will be intertwined: If the distractors are identical on a
given trial, then strong correlations between the measurements will help preserve their perceived similarity. Namely, an observer can group the distractor measurements and identify
the target as corresponding to the outlying measurement. By contrast, when distractors are
unstructured (independently drawn across locations), strong measurement correlations may
have no effect on performance. Thus, measurement and stimulus correlations should not be
considered in isolation.
Here we examine how measurement and stimulus correlations impact the strategy and
the performance of an ideal observer in a target detection task. We assume that on half the
trials, one or more target stimuli are presented along with a number of distractors, whereas
on the other half of trials, only distractors are presented. The task is to infer whether targets are present or not. Importantly, we assume that the distractor stimuli are not drawn
independently ‚Äì for instance, in the extreme case the targets could be identical. Our ideal
observer infers target presence based on measurements of the stimuli. We assume that these
measurements are corrupted by correlated noise. In an extreme case, this noise is perfectly
correlated and all measurements are perturbed by the same, random value.
We provide an analytical study of the optimal decision rule to show that the interplay
of measurement and stimulus correlations can be intricate. In general, if the stimuli are
strongly correlated, then measurement correlations can strongly affect the performance of an
ideal observer. When stimuli are weakly correlated, measurement correlations have a smaller
impact.
We expect that these insights hold more generally: Natural stimuli are structured, and their
distributions are concentrated along low dimensional structures in stimulus space (Geisler,
2

2008). Correlations in measurement noise could thus help in the inferring parameters of
interest. Ultimately, we thus expect our results to be relevant to modeling perceptual decisionmaking in natural scenes.

2

Model description

To examine how decisions of an ideal observer are determined by the statistical structure
of measurements and stimuli we consider the following task. An observer is asked whether
target stimuli are present among a set of distractor stimuli. Each stimulus, i, is characterized
by a scalar, si ‚àà R. The set of N stimuli presented on a single trial is characterized by
the vector s = (s1 , s2 , ¬∑ ¬∑ ¬∑ , sN ). For instance, stimuli could be pure tones characterized by
their frequency, gratings characterized by their orientation, or ellipses characterized by the
orientation of their major axis. A target is a stimulus with a particular characteristic, sT . A
target could be a vertical grating, or a pure tone at 440Hz. For simplicity, we assume that, if
i is a target, then si = sT = 0, and stimulus characteristics are measured relative to that of a
target. Stimuli that are not targets are distractors. For such stimuli sj 6= 0 with probability 1.
We will consider situations with single and multiple targets.

Figure 1: A schematic representation of Bayesian inference. Information about a parameter
of interest, T , is encoded in a stimulus vector, s. The dimension of s could be much higher
than the dimension of T . The observer makes a sensory measurement, x, of the stimulus, s,
and must infer T from this measurement. This inference requires marginalization over s.
The following treatment parallels the one we used earlier for independent measurements (Mazyar et al., 2012, 2013; Bhardwaj et al., 2015). We denote target presence by T = 1, and absence
by T = 0. We assume that targets are present with probability 0.5. When T = 0 (no targets)
stimuli are drawn from a multivariate normal distribution with mean 0N = (0, 0, ¬∑ ¬∑ ¬∑ , 0), and
covariance matrix, Œ£s . The subscript denotes vector length, so 0N has N components. We
can therefore write
p(s|T ) = N (s; 0N , Œ£s ),
(2.1)
where N (s; ¬µ, Œ£) denotes the density of the normal distribution with mean ¬µ and covariance
Œ£. For simplicity, we assume Œ£s has constant diagonal and off-diagonal terms, so that
Ô£Æ
Ô£π
1 œÅs ¬∑ ¬∑ ¬∑ œÅs
Ô£ØœÅ 1 ¬∑ ¬∑ ¬∑ œÅs Ô£∫
Ô£∫
2Ô£Ø s
Œ£s = œÉs Ô£Ø ..
(2.2)
.. Ô£∫ .
.
.
Ô£∞.
. .Ô£ª
œÅs œÅs ¬∑ ¬∑ ¬∑

1

The correlation coefficient, œÅs , determines the relation between the components of the stimulus.
3

If T = 1, then n ‚â• 1 targets are present. In this case we assign, with uniform probability,
the target characteristic to n out of the total
 of N stimuli. This subset n targets is denoted
N
by L. We denote by L the collection of all
possible choices of the sets L of targets.
n
The remaining N ‚àí n distractors are drawn from a multivariate normal distribution
with mean 0N ‚àín , and covariance matrix Œ£s\L of dimension (N ‚àí n) √ó (N ‚àí n). Let sL =
(si1 , si2 , ¬∑ ¬∑ ¬∑ , sin ), il ‚àà L denote the target stimuli, and s\L = (sj1 , sj2 , ¬∑ ¬∑ ¬∑ , sjN ‚àín ), jl ‚àà
/ L the
distractors. We can therefore write
X
p(sL |T = 1) =
Œ¥(si ),
and
p(s\L |T = 1) = N (s\L ; 0N ‚àín , Œ£s\L ).
i‚ààL

Since the density p(s|T = 1, L) is singular, we introduce the auxiliary covariance,

(Œ£Œ∑s,L )i,j

Ô£±
Ô£¥
Ô£≤(Œ£s\L )i,j ,
= Œ∑,
Ô£¥
Ô£≥
0,

if i, j ‚àà
/ L,
if i = j ‚àà L,

(2.3)

if i ‚àà L, or j ‚àà L, and i 6= j,

where Œ∑ > 0. Therefore,
p(s|T = 1, L) = lim N (s; 0N , Œ£Œ∑s,L ).
Œ∑‚Üì0

We further assume that an observer makes a noisy measurement, xi , of each stimulus, si . This measurement can be thought of as the estimate of stimulus i obtained from
the activity of a population of neurons that responded to the stimulus. We denote by
x = (x1 , x2 , ¬∑ ¬∑ ¬∑ , xN ) the vector of N measurements. It is commonly assumed that these
measurements are unbiased, and corrupted by additive, independent, normally distributed
Q
noise, so that p(x|s) = ni=1 N (xi ; si , œÉx2 ). Here we consider the more general situation where
the measurements are unbiased, but noise could be correlated so that
p(x|s) = N (x; s, Œ£x ).

(2.4)

We consider the particular case when Œ£x has constant diagonal terms, œÉx2 , and off-diagonal
terms, œÅx œÉx2 , so that
Ô£Æ
Ô£π
1 œÅx ¬∑ ¬∑ ¬∑ œÅx
Ô£ØœÅx 1 ¬∑ ¬∑ ¬∑ œÅx Ô£∫
Ô£Ø
Ô£∫
Œ£x = œÉx2 Ô£Ø ..
(2.5)
. Ô£∫.
..
Ô£∞.
. .. Ô£ª
œÅx œÅx ¬∑ ¬∑ ¬∑
Note that

1

Z
p(x|T = 1) =

p(x|s)p(s|T = 1)ds,

and similarly for p(x|T = 0). We must thus marginalize over s to obtain these distributions.
A simple computation (See Appendix A) shows that p(x|L, T = 1) = N (x; 0N , Œ£0s,L + Œ£x ),
and p(x|T = 0) = N (x; 0N , Œ£s + Œ£x ).

4

Figure 2: Stimulus and measurement distributions in the single target detection task with
N = 2 stimuli and œÉs = 15, œÉx = 4. (A) (Top) Stimulus distributions on target present
(left) and target absent (right) trials for œÅs = 0.5. On target present trials, the distribution
is constrained to the axes, since one of the stimuli is a target. On target absent trials, the
two stimuli follow a bivariate normal distribution. (Bottom) Distributions of measurements
in the absence of measurement correlations, œÅx = 0. On both the target present (left), and
target absent case (right), the measurement distribution inherits its shape from the stimulus
distribution. (B) Overlap of measurement distributions for strongly correlated distractors,
œÅs = 0.99. Measurement distributions on target present (black) and absent (gray) trials are
shown for œÅx = 0 (top) and œÅx = 0.95 (bottom). The overlap between the measurement
distributions decreases with increasing measurement correlation, œÅx . The decision boundary
(solid line) therefore better separates the two distribution when œÅs and œÅx are both large. We
used high correlations to bring out the difference between
p the distributions. Axes represent
the total standard deviation of the measurements, œÉ = œÉs2 + œÉx2 .

5

3

Results

Our goal is to describe how correlations between stimuli along with correlations between
their measurements affect the decisions of an optimal observer in a target detection task.
We examine how performance changes as both correlations between distractors and between
measurements are varied.
The stimuli, s, follow different distributions depending on whether T = 0 or T = 1. Measurement noise increases the overlap between the corresponding measurement distributions,
p(x|T = 0) and p(x|T = 1). The higher the overlap between these two distributions, the more
difficult it is to tell whether a target is present or not. However, correlations in measurement
noise can reduce such overlap (See Fig. 2B), even when noise intensity is unchanged. Therefore, the estimate of a parameter from a neural response depends not only on the level, œÉx ,
but also on the structure of measurement noise (Abbott and Dayan, 1999; Sompolinsky et al.,
2001; Averbeck et al., 2006; JosicÃÅ et al., 2009).
An ideal observer makes a decision based on the sign of the log posterior ratio,
dTD (x) = log

p(x|T = 1)
p(T = 1)
p(T = 1|x)
= log
+ log
.
p(T = 0|x)
p(x|T = 0)
p(T = 0)

(3.1)

If dTD (x) > 0, the observer infers that a target is present. Note that
X
p(x|L, T = 1)p(L).
p(x|T = 1) =
L‚ààL

 ‚àí1
N
Given that the prior probability that L is a set of targets is uniform gives p(L) =
,
n
and
" ‚àí1
#
X p(x|L, T = 1)
p(x|T = 1)
N
= log
.
(3.2)
dTD (x) = log
n
p(x|T = 0)
p(x|T = 0)
L‚ààL
The decision variable thus depends on the sum of the normalized probabilities that a measurement in x is made, given that the target set is L.
Note also that
p(L|x, T = 1) ‚àù p(x|L, T = 1).
Therefore, the decision variable can also be interpreted as a sum of likelihoods that L is a
target set, given a measurement x. Thus the decision is directly related to the posterior
distribution over the target sets L: The summands in Eq. (3.2) correspond to the evidence
that L is a set of targets.
The distribution of measurements, x, depends on whether a target or targets are present.
The conditional distributions of measurements, p(x|T = 0) and p(x|L, T = 1), are Gaussian
with all means equal to 0, and covariances C = Œ£s + Œ£x and CL = Œ£0s,L + Œ£x , respectively.
We therefore have (see Appendix A for details)
" ‚àí1 s

#
X

|C|
1
N
‚àí1
dTD (x) = log
exp ‚àí xT C‚àí1
x .
(3.3)
L ‚àíC
n
|CL | L‚ààL
2
6

This decision variable depends on the model parameters, and the measurement, x. The
total number of stimuli, N, number of targets, n, and the variability, œÉs2 . The correlation, œÅs ,
between the distractors determine the structure of the stimulus, while the variability, œÉx2 , and
correlation, œÅx , describe the distribution of sensory measurements. An ideal observer knows
all these parameters.
Setting the right hand side of Eq. (3.3) to 0 defines a nonlinear decision boundary in the
space of measurements. This boundary separates measurements that lead to a ‚Äútarget present‚Äù
decision (dTD (x) > 0) from those that lead to a ‚Äútarget absent‚Äù decision (dTD (x) < 0). For
example, with a single stimulus, N = n = 1, the observer needs to decide whether or not a
single measurement corresponds to a target. The decision variable has the form
s
!
œÉs2 + œÉx2
1
x21
dTD (x) = log
‚àí
,
œÉx2
2œÉx2 1 + œÉx2 /œÉs2
If the measurement, x1 , differs sufficiently from 0, then dTD (x) < 0. A measurement close to
0, gives dTD (x) > 0.
With more stimuli, the observer needs to take into account the known correlations between
the stimuli and between the measurements. The decision variable depends in a complicated
way on the parameters that describe the structure of the stimulus and the response. The
explicit form of Eq. (3.3), can be derived under the assumption of equal variances and equal
covariances (See Appendix A)
Ô£±
Ô£´
Ô£´
Ô£¥
s
Ô£¥
Ô£¥
Ô£≤ 1Ô£¨
Ô£¨ 1 1 + (œÅ œÉ 2 + œÅ œÉ 2 )N v  v n X
X
Ô£¨
Ô£¨
s s
x x
L
x2i
exp ‚àí Ô£¨œÉs2 (1 ‚àí œÅs )vL v
dTD (x) = log Ô£¨
Ô£¥
Œ≥L
v
2
Ô£≠
Ô£≠M
Ô£¥
i‚ààL
L‚ààL
Ô£¥
Ô£≥
|
{z
}
I

Ô£∂ Ô£ºÔ£∂
Ô£¥
Ô£¥
Ô£¥
Ô£∑
Ô£¥Ô£∑






Ô£Ω
2 2
2
2
X
X
X
Ô£∑
Ô£∑
œÅ
œÉ
v
g
œÅ
œÉ
v
v
v
f
x
L
x
L
L
x L
x
2
2
2
Ô£∑
+ Œ≤v ‚àí
xi xj + 2 Œ≤v ‚àí
xi xj + Œ≤v ‚àí
xi xj Ô£∑ Ô£∑
Ô£∑.
Œ≥L
Œ≥L
Œ≥L
Ô£¥
Ô£∏Ô£¥
i,j‚ààL
i‚ààL,j ‚ààL
/
i,j ‚ààL
/
Ô£¥Ô£∏
|
{z
} |
{z
} |
{z
} Ô£¥
Ô£æ
II

III

IV

(3.4)
The variables v and vL represent scaled inverse variances corresponding to distractor and
target stimuli. The parameters Œ≤, gL , fL , and Œ≥L are given in Eqs. (A.4) and (A.5), and are
defined in terms of œÉs2 , œÅs , œÉx2 , and œÅx . Eq. (3.4) has a form that can be interpreted intuitively:
1. Term I contains a sum of squares of individual measurements, x2i , over the putative set
of targets, L. The smaller this sum, the more likely that L contains targets.
2. Term II contains the sample covariance about the known target value, sT = 0, that is,
P
P
i,j‚ààL (xi ‚àí sT )(xj ‚àí sT ) =
i,j‚ààL xi xj . In the absence of measurement correlations,
œÅx = 0, covariability about the target value has vanishing expectation for target measurements. Therefore the larger this sum, the less likely that the measurements come
from a set of target stimuli. In the presence of measurement correlations, œÅx > 0, covariability between target measurements is expected. Hence the prefactor in term II
decreases with œÅx .
7

3. Term III is similar to term II, with the sum representing the sample covariance about the
target value between putative target and non-target stimuli. The larger this covariance,
the less likely that L or its complement contain targets.
4. Term IV contains the sample covariance about the mean of measurements outside of
the putative target set. If there are no targets, then all terms in the sum are expected
to be large, regardless of the choice of L. However, if there are targets, then whenever
the complement of L contains targets, some of the terms in the sum have expectation
0. Hence the term again makes a smaller contribution if targets are present.
While this provides an intuitive interpretation of the sums in Eq. (3.4), the expression is
complex and it is difficult to precisely understand how an ideal observer uses knowledge of the
generative model and the stimulus measurements to make a decision. We therefore examine
a number of cases where Eq. (3.4) is tractable, and all the terms can be interpreted precisely.
We also numerically examine performance in a wider range of examples.

3.1

Single target, n = 1

We start with the case when a single target is present at one of N locations. This case was considered previously in the absence of correlations between the sensory measurements (Bhardwaj
et al., 2015).
We observe in Figs. 3A and 4A that the performance of an ideal observer is nearly independent of œÅs and œÅx when external structure is weak (œÅs < 1). Performance depends strongly
on œÅx when distractors are strongly correlated, œÅs ‚âà 1. An ideal observer performs perfectly
when œÅs = œÅx = 1 (Fig. 3C).
Increased performance with increasing stimulus correlations, œÅs , accords with intuition
that similar distractors make it easier to detect a target. However, correlations in measurement noise can play an equally important role and significantly improve performance when
distractors are identical (See Fig. 3B).
Perfect performance when œÅs = œÅx = 1, can be understood intuitively. In this case measurements, xi , of the stimuli are obtained by adding the same realization of a random variable,
i.e. identical measurement noise to each stimulus value, si . In target absent trials, all measurements are hence identical. If the target is present, measurements contain a single outlier.
An ideal observer can thus distinguish the two cases perfectly.
We examine in more detail the cases of weak measurement noise, and then the case of
comparable measurement noise and distractor variability.
Weak measurement noise, œÉx2  œÉs2 with highly correlated stimuli, œÅs ‚âà 1. When
measurement noise is weak, discriminability between the ‚Äútarget absent‚Äù and ‚Äútarget present‚Äù
conditions is governed by external variability, i.e. trial-to-trial variability of the stimuli. As
noted, measurement correlations improve discriminability in the presence of strong stimulus
correlations (see Figs. 3A,C). When œÅs ‚âà 1, Eq. (3.4) can be approximated as (details in

8

Appendix B.1):
Ô£´
Ô£¨1
dTD (x) ‚âà log Ô£¨
Ô£≠N
|

r

N (1 ‚àí œÅx )
N ‚àí1 }
{z

P (N,œÅx )

Ô£∂
N
X
i=1

Ô£±
Ô£´
Ô£≤
X
1
1
Ô£≠(1 ‚àí N œÅx )x2i + 2xi
xj ‚àí
exp ‚àí
2
Ô£≥ 2N œÉx (1 ‚àí œÅx )
N ‚àí1
j6=i
{z
|
Œ±i (x,N,œÅx ,œÉx )

X
j6=i

! 2 Ô£∂ Ô£ºÔ£∑
Ô£ΩÔ£∑
Ô£∑
Ô£∏
xj
Ô£∑.
Ô£æÔ£∑
Ô£∏
}
(3.5)

The ideal observer hence uses knowledge about the measurement correlations, œÅx , in a decision.
We first confirm the intuitive observation that an ideal observer performs perfectly when
measurement noise is highly correlated, 1 ‚àí œÅx  1. In this case the exponential term in
Eq. (3.5) is, to leading order in 1/(1 ‚àí œÅx ),


N ‚àí1
2
Œ±i (x, N, œÅx , œÉx ) ‚âà exp
(xi ‚àí xÃÑƒ±ÃÇ ) ,
(3.6)
2N œÉx2 (1 ‚àí œÅx )
P
where xÃÑƒ±ÃÇ = 1/(N ‚àí 1) j6=i xj is the sample mean of the measurements excluding the putative
target, i. Hence, to make a decision, the ideal observer subtracts the mean of the N ‚àí 1
measurements of putative distractors from that of the putative target. In Appedix B.1 we
show that on ‚Äútarget absent‚Äù trials, dTD (x) ‚Üí ‚àí‚àû, as œÅx ‚Üí 1, and on target present trials,
dTD (x) ‚Üí ‚àû, as œÅx ‚Üí 1. Hence performance improves as measurement correlations increase.
This can also be seen in Fig. 2B, as the overlap between the distributions p(x|T = 0) and
p(x|T = 1) decreases with an increase in œÅx .
When measurement correlations are absent, œÅx = 0, an ideal observer performs perfectly
only when measurement noise is vanishing. The exponential in Eq. (3.5) now equals
Ô£Æ
Ô£´
!2 Ô£∂Ô£π
!2
X
X
1
1
1
Œ±i (x, N, œÅx , œÉx ) = exp Ô£∞‚àí 2 Ô£≠
xj Ô£∏ Ô£ª
xj ‚àí
2œÉx N
N
‚àí
1
j
j6=i



1
= exp ‚àí 2 N xÃÑ2 ‚àí (N ‚àí 1)xÃÑ2ƒ±ÃÇ ,
(3.7)
2œÉx
where xÃÑ is the sample mean of all observations. The ideal observer therefore compares the
sample mean over all measurements, xÃÑ, with the sample mean over all observations excluding
the putative target, xÃÑƒ±ÃÇ . Since measurement noise is uncorrelated, averaging over all measurements is beneficial. This is a very different strategy than when measurement noise is highly
correlated (Eq. (3.6)), and the ideal observer compares a single putative target measurement,
xi , to the sample mean of the remaining target measurements. As shown in Appendix B.1
at intermediate values of measurement correlations, 0 < œÅx < 1, the ideal observer uses a
mixture of these two strategies.
Intuitively, an increased number of distractors make it more difficult to detect the target.
An analysis of the decision variable in the limit N ‚Üí ‚àû shows that this is indeed the case
(See Appendix B.3).
9

Figure 3: Performance of an optimal observer on a single target detection task with weak
measurement noise, œÉx2  œÉs2 . (A) Proportion of correct responses as a function of stimulus
correlation, œÅs and measurement correlation, œÅx , for N = 4 stimuli. (B) Proportion of correct
responses as a function of measurement correlation, œÅx , in the case of intermediate stimulus
correlation, œÅs = 0.5 (left) and strong correlation, œÅs = 1 (right) for N = 4 stimuli. (C)
Decision boundary, dTD (x) = 0 (black solid line) and measurement distributions on target
present (left) and target absent (right) trials for N = 2 and œÅs = 0.5. Here and henceforth
dark gray dots correspond to measurements that result in a correct inference, and light gray
dots correspond to measurements leading to an incorrect inference. The stimulus will result in
a measurement that lies within the black dashed lines with probability 0.95. Other parameters
used: œÉs = 15 and œÉx = 4.

10

Weak measurement noise, non-identical distractors, œÅs < 1. Measurement correlations have little effect on performance when stimulus correlations are weaker (See Fig. 3A).
Consider again œÅx ‚âà 1, so that measurements are corrupted by adding a random, but nearly
identical perturbation to the stimuli. An ideal observer uses the knowledge that all measurements are obtained by adding an approximately equal value to the stimulus. However, when
stimulus correlations are weak, the target measurement is no longer an outlier. Correlations
in measurement noise provide little help in this situation.
These observations are reflected in the structure of the decision boundaries (dTD (x) = 0),
and the distributions of the measurements (See Fig. 3B). In the target absent (left column)
and present trials (right column) the distributions of measurements, p(x|T ), is shaped predominantly by variability in the stimulus. Measurement correlations have little effect on this
shape, and the decision boundary therefore changes little with an increase in œÅx . In contrast when œÅs ‚âà 1, measurement correlations significantly impact the overlap between the
distributions p(x|T = 1) and p(x|T = 0) as shown in Fig. 2B.
We can confirm this intuition about the role of measurement correlations by approximating
the decision criterion when œÉx2  œÉs2 and œÅs < 1 (see details in Appendix B.4),
s

!
N
x2i
1 œÉs2 (1 ‚àí œÅs )(1 + (N ‚àí 1)œÅs ) X
.
(3.8)
dTD (x) ‚âà log
exp ‚àí 2
N
œÉx2 (1 + (N ‚àí 2)œÅs )
2œÉ
x
i=1
The strength of noise correlations, œÅx , does not affect the decisions of an ideal observer at
highest order in (œÉx2 /œÉs2 ). This explains the approximate independence of performance on œÅx
observed in Fig. 3B. In this case an ideal observer considers stimuli at each location separately,
and weighs each measurement individually by its precision, 1/œÉx2 (Green and Swets, 1966;
Palmer, 1999).
When the number of distractors becomes larger, Eq. (3.8) is no longer valid. The observer
compares measurements of all stimuli to make a decision. We return to this point below.
Strong measurement noise, œÉx2 = œÉs2 Increasing measurement noise trivially degrades
performance. However, in the limit of perfect stimulus and measurement correlations, an ideal
observer still performs perfectly for the reasons described earlier.
Measurement correlations affect performance differently than in the case of weak measurement noise (See Fig. 4A,C). Even with uncorrelated stimuli, œÅs = 0, performance increases
slightly (approximately 5-6%) with œÅx . Surprisingly, for intermediate values of stimulus correlations, e.g. œÅs = 0.5, measurement correlations negatively impact performance. If measurement correlations are fixed at a high value, then the worst performance is observed at
an intermediate value, 0 < œÅs < 1. The reason for this unexpected behavior is unclear, as
Eq. (3.4) is difficult to analyze in this case.
Generally, when measurement noise is strong, measurement correlations will change the
shape of the measurement distributions p(x|T = 0) and p(x|T = 1), and hence impact decisions and performance. Note that when measurement correlations increase, the region corresponding to dTD (x) > 0 is elongated along the diagonal to capture more of the mass of
the distribution p(x|T = 1) (See Fig. 4C). However, when measurement noise is high, the
interactions between measurement and stimulus correlations are intricate.
11

Figure 4: Performance of an optimal observer on a single target detection task with strong
measurement noise, œÉx2 = œÉs2 . (A) Proportion of correct responses as a function of œÅs and
œÅx for N = 4 stimuli. (B) Performance as a function of measurement correlation, œÅx , when
œÅs = 0 (left) and œÅs = 0.5 (right) for N = 4 stimuli. (C) Decision boundary (black solid line)
and distribution of measurements, x on target present (left) and target absent (right) trials
for N = 2 and œÅs = 0.5. Other parameters used: œÉs = œÉx = 15.

12

3.2

Multiple targets, n > 1

In the present task when multiple targets are present, they are all identical and hence perfectly
correlated. Thus, regardless of the value of œÅs , on half the trials the stimuli will be strongly
structured, and the density p(s|T = 1) concentrated on a low dimensional subspace. As a
consequence, measurement correlations always impact performance.
Regardless of stimulus correlations, an ideal observer performs perfectly when œÅx = 1 (See
Fig. 5A). Even when œÅs < 1, performance increases with œÅx (see Fig. 5A). When œÅx = 1, all
target measurements are identical. Hence, an ideal observer performs perfectly by checking
whether n of the measurements, xi , are equal. We only analyze the case œÅs < 1, since the case
of perfectly correlated distractors is similar.

Figure 5: Performance of an optimal observer in a multiple target search task with weak
measurement noise, œÉx2  œÉs2 . (A) Proportion of correct responses as a function of stimulus
correlation, œÅs , and measurement correlation, œÅx , with N = 4 stimuli and n = 3 targets. (B)
Proportion of correct responses as a function of œÅx for œÅs = 0 (left) and œÅs = 0.5 (right) when
N = 4 and n = 3. (C) Measurement distributions and decision boundary on target present
(left) and target absent (right) trials for œÅs = 0.5 and N = n = 2.

With weak measurement noise, Eq. (3.4) can be approximated as (see Appendix B.4):
 s
 2
n
1
(1 ‚àí œÅx )(1 + (N ‚àí 1)œÅs )
œÉs (1 ‚àí œÅs )
dTD (x) ‚âà log
M (1 + (N ‚àí n ‚àí 1)œÅs )(1 + (n ‚àí 1)œÅs ) œÉx2 (1 ‚àí œÅx )
Ô£±
Ô£º
Ô£´
!2 Ô£∂Ô£¥
Ô£¥
Ô£≤
Ô£Ω
X
n
1X 2
nœÅx
1X
Ô£≠
Ô£∏
√ó
exp ‚àí 2
. (3.9)
xi ‚àí
xi
Ô£¥
Ô£¥
(1 + (n ‚àí 1)œÅx ) n i‚ààL
Ô£≥ 2œÉx (1 ‚àí œÅx ) n i‚ààL
Ô£æ
L‚ààL
13

An ideal observer takes into account measurement correlations for all values of œÅs even when
measurement noise is low. Stimulus correlations only appear in the prefactor, and are not
used in the comparison of the measurements inside the exponential.
Interestingly, decisions in this case are based only on measurements of stimuli within the set
of putative targets, L. In the absence of measurement correlations, a decision is based solely
on the sample second moment of the n stimulus measurements about the target characteristic,
P
sT = 0, i.e. 1/n i‚ààL x2i (the underlined term in Eq. (3.9)). A low value of this sample
moment indicates that L contains targets.
In the limit of œÅx ‚Üí 1, the underlined term in Eq. (3.9) approaches the sample variance,
2
si‚ààL , i.e. the sample second moment about the sample mean,
1X 2
nœÅx
xi ‚àí
n i‚ààL
1 + œÅx (n ‚àí 1)

1X
xi
n i‚ààL

!2

1X 2
x ‚àí
‚Üí
n i‚ààL i

1X
xi
n i‚ààL

!2
= s2i‚ààL .

(3.10)

If measurement correlations are strong, the measurements of a set of target stimuli will be
approximately equal (regardless of œÅs ). The ideal observer makes use of this knowledge by
comparing the value of putative targets in the set L. If the sample variance of the measurements {xi }i‚ààL is small, then it is likely that L is a set of targets. When œÅx = 1, an ideal
observer performs perfectly (See Appendix B.5 for details).
When 0 < œÅx < 1, the underlined term in Eq. (3.9) shows that the ideal observer takes
an intermediate strategy by computing a second moment about a point between the target
characteristic, sT , and the sample mean. Interestingly, the larger the number of targets, the
larger the weight on the sample mean, since the prefactor nœÅx /(1 + (n ‚àí 1)œÅx ) increases with
n for fixed œÅx .
These observations are reflected in the distributions shown in Fig. 5C. The distribution of
measurements, p(x|T = 1), move closer to the diagonal (x1 = x2 ) as œÅx ‚Üí 1, and the overlap
with the distribution of measurements, p(x|T = 0) decreases. In higher dimensions, for N
stimuli and n targets, the measurement distributions, p(x|T = 1), is concentrated on the
union of (N ‚àí n + 1)-dimensional subspaces when œÅx = 1: The target measurements lie on a
line, while the N ‚àí n distractor measurements are distributed along the remaining directions.
To conclude, when there are multiple targets part of the stimulus set is always perfectly
correlated. When measurement correlations are high, the observer checks whether the measurements are similar to each other to make a decision. When measurement correlations are
low, the observer compares the measurements to the known target value. Measurement correlations can again decrease the overlap between the conditional distributions of measurements,
and significantly impact decisions and performance. For finite N , decisions are based on the
comparison of measurements within a putative set of stimuli, L. We show next that when N
is large, this is no longer the case.

3.3

Larger number of targets and stimuli

If we assume a fixed proportion, K, of the stimuli consists of targets, so that n = KN ,
then Eq. (3.4) simplifies considerably in the limit of large N . If we let cx = œÉx2 (1 ‚àí œÅx ) and

14

cs = œÉs2 (1 ‚àí œÅs ), the exponential in Eq. (3.4) has the form, (See Appendix B.6)
Ô£πÔ£∂
Ô£´
Ô£Æ
Ô£¨ 1 Ô£ØNK 2
Ô£∑
N
N (1 ‚àí K) 2 Ô£∫
2
Ô£∫
Ô£Ø
Ô£∑,
‚àí
+
Œ±L (x, N, cx , cs ) ‚âà exp Ô£¨
s
s
s
‚àí
Ô£≠ 2 Ô£∞ cx i‚ààL cs + cx
cs + cx i6‚ààL Ô£ªÔ£∏
{z
}
| {z } | {z } |
I

II

(3.11)

III

where s2i‚ààL , s2i6‚ààL , and s2 are the sample variances of measurements form the putative target
set, L, outside of the putative target set, and over all N measurements, respectively.
The different terms in this expression can be interpreted as earlier: Term I is the sample
variance of measurements of the putative targets. If this variance is large, then the set L
is unlikely to contain targets. Term II is the sample variance among all terms. If this term
is large then all stimuli are dissimilar, and there is evidence that targets are present: For
example, when distractors are correlated, and cs  1, then the sample variance of all stimulus
measurements is small only in the absence of targets. Finally, term III is the sample variance
among putative distractors. If distractors are correlated, this term will be small if L contains
targets, and hence stimuli outside L are distractors. The sign of the three terms agrees with
this interpretation: Terms I and III are negative, and term II is positive.
The main difference between the cases of large N , and the examples discussed previously
is that an observer takes into account putative distractor measurements, i.e measurements
outside the putative target set L. An exception is the case when measurement correlations
are much stronger than distractor correlations, cx  cs . In this case, the putative targets are
more strongly structured, and hence only their measurements are used in a decision. When
cs  1, or, equivalently, œÅs ‚âà 1, and distractors are strongly correlated, all three terms in
Eq. (3.11) are comparable. In this case, ideal observers base their decision on the similarity,
as measured by sample variance, of both putative distractor and target measurements.
Importantly, the decision is made using distractor measurements, even when distractors
are not perfectly correlated. Fig 6 shows that intermediate distractor correlations have an
increasingly impact decisions with an increase in distractor number. Indeed, the higher the
fraction of distractors, (1 ‚àí K), the more weight is assigned to their sample variance (term
III). This is unlike the case of small N , where distractor measurements are used only when
they are perfectly correlated.

4

Discussion

We have shown that in a simple task the statistical structure of the stimulus as well as that of
noise in perceptual measurements determine the strategy and performance of an ideal observer.
Correlations in measurement noise can significantly impact performance, particularly when
stimulus correlations are high: When the distribution of stimuli conditioned on the parameter
of interest is concentrated in a small volume of stimulus space, the statistical structure of
measurement noise can be of particular importance (Mazyar et al., 2012, 2013; Bhardwaj
et al., 2015).
The impact of noise correlations on the inference of a parameter from neural responses
has been studied in detail (Averbeck et al., 2006; Averbeck, 2009; Latham and Nirenberg,
15

Figure 6: Normalized performance of an optimal observer with respect to œÅs = 0 as a function
of stimulus correlation, œÅs , for different values of N in case of (A) œÅx = 0 and (B) œÅx = 0.5.
Intermediate correlations between distractor measurements become increasingly important as
the number of distractors increases from four to sixteen. Other parameters used: œÉs = 15,
œÉx = 4, and n = 2.

2005; Perkel et al., 1967; Schneidman et al., 2003; Sompolinsky et al., 2001). Frequently
the parameter of interest was identified with the stimulus, and both were univariate. The
estimation of the orientation of a bar in the receptive field of a population of neurons has
been a canonical example.
Reality is far more complex. Stimuli, such as a natural visual or auditory scene, are
high dimensional and highly structured. Moreover, only some of the parameters are typically
relevant. Intuitively, if noise perturbs measurements along relevant direction, i.e. along the
directions of the parameters of interest, then estimates will be corrupted. Perturbations
along irrelevant directions in parameter space have little effect (Moreno-Bote et al., 2014).
Measurement correlations can channel noise into irrelevant directions, without decreasing
overall noise magnitude, and thus improve parameter inference.
This is difficult to study using general theoretical models without putting some constraints
on the structure of measurement noise. We therefore considered a relatively simple, analytically tractable example where both measurement and stimulus structure are characterized by
a small number of parameters. We have used a similar setup to examine decision making in
controlled search experiments (Mazyar et al., 2012, 2013; Bhardwaj et al., 2015).
We assumed that measurement noise and measurement correlations can be varied independently. This is not realistic. For instance, it is known that changes in the mean, variability
and covariability of neural responses can be tightly linked (Cohen and Kohn, 2011; de la
Rocha et al., 2007; Rosenbaum and JosicÃÅ, 2011). It is thus likely that the statistics of measurement noise also change in concert. However, at present this relationship has not been well
characterized.
More importantly, noise from the periphery of the nervous system will limit the performance of any observer. It is therefore not possible that a simple change in measurement
correlations can lead to perfect performance (Moreno-Bote et al., 2014). To address this question it would be necessary to provide a more accurate model of both the noise correlations in
a recurrent network encoding information about the stimuli (Beck et al., 2011), as well as the
resulting measurement correlations. This is beyond the scope of the present study.
16

We also made strong assumptions about the structure of measurement and stimulus correlations. We chose to restrict our analysis to positive correlations. The reason is that the
requirement that a covariance matrix is positive definite implies restrictions on the range of
allowable negative measurement and stimulus correlations (Horn and Johnson, 2012). These
restrictions depend on the number of stimuli, N , and complicate the analysis. To make the
model tractable, we also assumed that all off-diagonal elements in the stimulus and measurement noise covariance matrices are identical. While we did not examine it here, heterogeneity
in the correlation structure can strongly affect parameter inference (Shamir and Sompolinsky,
2006; Chelaru and Dragoi, 2008; Berens et al., 2011).

Figure 7: Performance comparison of an optimal observer as a function of measurement
correlations in (A) a mean left/right discrimination task with N = 4 stimuli and (B) a target
detection task with n = 3 targets. Other parameters used: œÉs = 15, œÉx = 4, and œÅs = 0.5.
To end, we provide another illustration of the fact that measurement correlations can
affect the performance of an ideal observer in different ways depending on the task: Suppose
an observer is presented with N oriented stimuli, such as Gabor patches. The stimuli and
measurements follow the same Gaussian distributions introduced earlier in this study. The
observer is asked to perform one of the following two tasks: 1) Report whether the mean
orientation of the stimuli is to the left or right of vertical; 2) Report whether a vertically
oriented target is present or absent. In this case, a subset of the stimuli has vertical orientation
on half the trials.
The first task is a discrimination task, and the observer needs to to integrate information from different sources. When measurement correlations are high, it is more difficult to
average out the noise between the stimulus measurements (Sompolinsky et al., 2001; Zohary
et al., 1994). The estimate of the average orientation is therefore degraded and performance
decreases with an increase in measurement correlations (see Fig. 7A and Appendix C). The
second is a detection task that requires extracting information that is buried in a sea of distractors. As discussed above, in this case measurement correlations can increase performance
if there is more than one target, or if the distractors are strongly correlated (see Fig. 7B).
In this example the stimuli and the measurements have the same statistical structure on
target absent trials for the two tasks. However, the parameter of interest differs: In the
first task the observer needs to estimate the average stimulus orientation, and in the second
17

determine whether a target is present. The distributions of measurements conditioned on these
parameters are therefore also different, and are differently affected by measurement noise.
The question of how measurement correlations impact decision making and performance
does not have a simple answer (Hu et al., 2014). Correlations in measurement noise can have a
pronounced effect when the stimuli themselves are highly correlated, i.e. when they occupy a
small volume in stimulus space. We have illustrated how in this case measurement correlations
can help in separating the distribution of measurements conditioned on a parameter of interest.
Similar considerations will be important whenever we try to understand how information can
be extracted from the collective responses of neural populations to high dimensional, and
highly structured stimuli.

5

Acknowledgment

K.J. was supported by NSF award DMS-1122094. W.J.M. was supported by award number
R01EY020958 from the National Eye Institute and award number W911NF-12-1-0262 from
the Army Research Office.

Appendices
A

Derivation of Eqs. (3.3) and (3.4)

Here we present the details of some of the calculations leading to the results presented in the
main text. The computations rely on the assumption of Gaussianity. We will therefore make
repeated use of the fact that the density N (x; ¬µ, Œ£) of the normal distribution with mean ¬µ,
and covariance Œ£ is


1
1
T ‚àí1
exp ‚àí (x ‚àí ¬µ) Œ£ (x ‚àí ¬µ) ,
(A.1)
N (x; ¬µ, Œ£) = p
2
(2œÄ)N |Œ£|
where |Œ£| denotes
 the determinant of the matrix Œ£.
N
Let M =
denote the cardinality of the set L of all possible sets L. We compute
n
p(x|T = 1) in Eq. (3.1) by marginalizing over s,
Z
p(x|T = 1) = p(x|s)p(s|T = 1)ds.
We note that
p(s|T = 1) =

X

p(s|T = 1, L)p(L) =

L‚ààL

18

1 X
p(s|T = 1, L).
M L‚ààL

Therefore,
1
p(x|T = 1) =
M

Z

X
p(x|s) p(s|T = 1, L)ds
L‚ààL

Z
1 X
p(x|s)p(s|T = 1, L)ds
=
M L‚ààL
XZ
1
=
lim
N (x; s, Œ£x )N (s; 0N , Œ£Œ∑s,L )ds
M Œ∑‚Üí0 L‚ààL
1 X
N (x; 0N , CL ),
=
M L‚ààL
Similarly,
Z
p(x|T = 0) =

p(x|s)p(s|T = 0)ds
Z

=

N (x; s, Œ£x )N (s; 0N , Œ£s )ds

= N (x; 0N , C),
where CL = Œ£x + Œ£0s,L and C = Œ£x + Œ£s . We therefore obtain
!
p(x|T = 1)
1 X N (x; 0N , CL )
dTD (x) = log
= log
p(x|T = 0)
M L‚ààL N (x; 0N , C)
s

!

1 T
1
|C| X
‚àí1
= log
exp ‚àí x C‚àí1
.
x
L ‚àíC
M
|CL | L‚ààL
2

(A.2)

We note that the determinant of CL does not depend on the set L since all matrices CL can
be obtained from each other by permuting appropriate rows and columns.
In the case variances and covariances are equal, we can invert CL and C. In general,
matrix CL has the following form,
Ô£±
Ô£¥
œÉx2 ,
if i = j ‚àà L,
Ô£¥
Ô£¥
Ô£¥
Ô£≤œÉ 2 + œÉ 2 ,
if i = j ‚àà
/ L,
s
x
(CL )i,j =
2
Ô£¥
œÅx œÉx ,
if i 6= j, and i or j ‚àà L,
Ô£¥
Ô£¥
Ô£¥
Ô£≥œÅ œÉ 2 + œÅ œÉ 2 , if i 6= j, and i, j ‚àà
/ L,
s s

x x

We will use the Sherman-Morrison-Woodbury formula (Meyer, 2000),
‚àí1
(A + U EV )‚àí1 = A‚àí1 ‚àí A‚àí1 U E ‚àí1 + V A‚àí1 U
V A‚àí1 ,

(A.3)

to obtain the inverses of CL and C. To do so we first rewrite CL as AL + UL EVL : When
L = {1, 2, ..., n} (targets placed at the first n out of N possible locations), we have
 2


T
(œÉx ‚àí b)In
0n√ó(N ‚àín)
b ¬∑¬∑¬∑ b
a ¬∑¬∑¬∑ a
AL =
,
UL =
,
0(N ‚àín)√ón (œÉs2 + œÉx2 ‚àí a)I(N ‚àín) (N √óN )
b ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ b (N √ó2)




0 ¬∑¬∑¬∑ 0 1 ¬∑¬∑¬∑ 1
1 0
VL =
, and
E=
,
1 ¬∑ ¬∑ ¬∑ 1 0 ¬∑ ¬∑ ¬∑ 0 (2√óN )
0 1 (2√ó2)
19

where a = œÅs œÉs2 + œÅx œÉx2 , and b = œÅx œÉx2 .
Using Eq. (A.3) we obtain
Ô£±
2g
bvL
L
Ô£¥
v
‚àí
, if
Ô£¥
L
Œ≥
Ô£¥
L
Ô£¥
2
Ô£¥
v fL
Ô£¥
if
Ô£¥
Ô£≤v ‚àí 2 Œ≥L ,
bvL gL
‚àí1
(CL )i,j = ‚àí Œ≥ ,
if
L
Ô£¥
Ô£¥
v 2 fL
Ô£¥
Ô£¥
‚àí Œ≥L ,
if
Ô£¥
Ô£¥
Ô£¥
Ô£≥‚àí bvvL ,
if
Œ≥L

i = j ‚àà L,
i=j‚àà
/ L,
i 6= j, and i, j ‚àà L,
i 6= j, and i, j ‚àà
/ L,
i 6= j, i ‚àà L, j ‚àà
/ L or i 6= j, i ‚àà
/ L, j ‚àà L,

where
1
1
, vL = 2
, fL = a + nœÅs œÉs2 œÅx œÉx2 vL
2
‚àí œÅs ) + œÉx (1 ‚àí œÅx )
œÉx (1 ‚àí œÅx )
gL = 1 + œÅs œÉs2 (N ‚àí n)v, and Œ≥L = 1 + a(N ‚àí n)v + nœÅx œÉx2 vL gL .
v=

œÉs2 (1

(A.4)

Using the Matrix Determinant Lemma (Harville, 1998), we can obtain the determinant of CL
det(CL ) =

vLn

Œ≥L
.
v N ‚àín

Similarly, we compute the inverse and determinant of
Ô£Æ
v ‚àí Œ≤v 2 ‚àíŒ≤v 2 ¬∑ ¬∑ ¬∑
Ô£Ø ‚àíŒ≤v 2 v ‚àí Œ≤v 2 ¬∑ ¬∑ ¬∑
Ô£Ø
‚àí1
C = Ô£Ø ..
..
Ô£∞ .
.
2
2
‚àíŒ≤v
‚àíŒ≤v
¬∑¬∑¬∑
where
Œ≤=

matrix C,
Ô£π
‚àíŒ≤v 2
‚àíŒ≤v 2 Ô£∫
Ô£∫
Ô£∫,
Ô£ª
2
v ‚àí Œ≤v

a
,
1 + aN v

and
det(C) =

(A.5)

1 + aN v
.
vN

The prefactor in Eq. (A.2), is therefore
s
s
|C|
1 + aN v  vL n
=
|CL |
Œ≥L
v
and we can compute
T

‚àí1

2

x C x = (v ‚àí Œ≤v )

N
X
i=1

T

x

C‚àí1
L x

x2i

‚àí Œ≤v

2

N
X

xi xj ,

i6=j


X

X
X
1 2
1 2
1
2
=
vL ‚àí bvL gL
x i + v ‚àí v fL
xi xj
x2i ‚àí bvL2 gL
Œ≥L
Œ≥
Œ≥
L
k
i‚ààL
i,j‚ààL
i‚ààL
/

i6=j

X
X
2
1
‚àí bvvL
xi xj ‚àí v 2 f L
xi xj .
Œ≥L
Œ≥
k
i‚ààL
j ‚ààL
/

i,j ‚ààL
/
i6=j

20

A slight rearrangement of the terms therefore shows that when variances and covariances are
equal, Eq. (A.2) is equivalent to
s
(
X
1 1 + aN v  vL n X
1
x2i
dTD (x) = log
exp ‚àí
œÉs2 (1 ‚àí œÅs )vL v
M
Œ≥L
v
2
i‚ààL
L‚ààL
Ô£∂ Ô£ºÔ£∂






Ô£Ω
2
2
X
X
v bgL X
vL vb
fL v
+ Œ≤v 2 ‚àí L
xi xj + 2 Œ≤v 2 ‚àí
xi xj + Œ≤v 2 ‚àí
x i xj Ô£∏ Ô£∏ .
Ô£æ
Œ≥L
Œ≥L
Œ≥L
i,j‚ààL

B

i‚ààL,j ‚ààL
/

i,j ‚ààL
/

Asymptotic analysis of Eq. (3.4)

Here we present some asymptotic results for the decision variable dTD (x) given in Eq. (3.4).
The main results are obtained for small measurement noise, œÉx2 . Equivalent results can be
obtained for large external variability, œÉs2 .

B.1

Small measurement noise and idental distractors, œÅs = 1

We first concentrate on the case n = 1. The exponential terms in Eq. (3.4) simplify to the
following:
œÉs2 (1 ‚àí œÅs )vL v = 0,
œÅx œÉx2 vL2 gL
œÉs2 + œÅx œÉx2
= 2
Œ≥L
œÉx (1 ‚àí œÅx )[N œÉs2 + œÉx2 (1 + (N ‚àí 1)œÅx )]
œÅx [œÉs2 (N ‚àí 1) + œÉx2 (1 ‚àí œÅx )]
‚àí 2
œÉx (1 ‚àí œÅx )[œÉs2 (N ‚àí 1) + œÉx2 (1 ‚àí œÅx )(1 + (N ‚àí 1)œÅx )]
1 ‚àí N œÅx
=
+ O(1),
N œÉx2 (1 ‚àí œÅx )
œÅx œÉx2 vL v
œÉs2 + œÉx2 œÅx
œÅx
Œ≤v 2 ‚àí
= 2
‚àí 2
2
2
2
Œ≥L
œÉx (1 ‚àí œÅx )[N œÉs + œÉx (1 + (N ‚àí 1)œÅx )] œÉs (N ‚àí 1) + œÉx (1 ‚àí œÅx )(1 + (N ‚àí 1)œÅx )
1
+ O(1),
=
2
N œÉx (1 ‚àí œÅx )
v 2 fL
œÉs2 + œÉx2 œÅx
Œ≤v 2 ‚àí
= 2
Œ≥L
œÉx (1 ‚àí œÅx )[N œÉs2 + œÉx2 (1 + (N ‚àí 1)œÅx )]
œÉs2 + œÅx œÉx2 (1 ‚àí œÅx )
‚àí 2
œÉx (1 ‚àí œÅx )[œÉs2 (N ‚àí 1) + œÉx2 (1 ‚àí œÅx )(1 + (N ‚àí 1)œÅx )]
1
=‚àí
+ O(1).
N (N ‚àí 1)œÉx2 (1 ‚àí œÅx )

Œ≤v 2 ‚àí

And the leading determinant term becomes:
s
s
1 + aN v vL
(1 ‚àí œÅx )[N œÉs2 + œÉx2 (1 + (N ‚àí 1)œÅx )]
=
Œ≥L
v
œÉs2 (N ‚àí 1) + œÉx2 (1 ‚àí œÅx )(1 + (N ‚àí 1)œÅx )
r
N (1 ‚àí œÅx )
+ O (œÉx2 )
=
N ‚àí1
21

Therefore, the decision variable becomes approximately,
r
1 N (1 ‚àí œÅx )
dTD (x) ‚âà log
N
N ‚àí1
Ô£±
Ô£´
N
Ô£≤
X
X
‚àí1
1
Ô£≠(1 ‚àí N œÅx )x2i + 2xi
xj ‚àí
exp
2
Ô£≥ 2N œÉx (1 ‚àí œÅx )
N ‚àí1
i=1
j6=i

X
j6=i

!2 Ô£∂Ô£ºÔ£∂
Ô£Ω
Ô£∏.
Ô£∏
xi
Ô£æ

After rearranging terms, this can be rewritten as
r
1 N (1 ‚àí œÅx )
dTD (x) ‚âà log
N
N ‚àí1
!

N
X

1
N ‚àí1
,
(xi ‚àí xÃÑƒ±ÃÇ )2 ‚àí 2 N xÃÑ2 ‚àí (N ‚àí 1)xÃÑ2ƒ±ÃÇ
exp œÅx
2 (1 ‚àí œÅ )
2N
œÉ
2œÉx
x
x
i=1

(B.1)

where xÃÑ is the sample mean of all measurements, and where xÃÑƒ±ÃÇ is the sample mean of the
measurements excluding the putative target, i. In the limiting cases œÅx = 0 and (1 ‚àí œÅx )  1,
we obtain the exponents given in Eq. (3.6) and Eq. (3.7) discussed in the text.

B.2

Perfect performance when œÅx = 1

We show that when œÅs = 1, an ideal observer performs perfectly in the limit of identical
measurement noise. For a fixed number of stimuli, N, on ‚Äútarget absent‚Äù trials, xi ‚àí (N ‚àí
P
‚àö
1)‚àí1 j6=i xj = O( ), and hence Œ±(x, N, œÅx , œÉx ) = O(1). On the other hand the prefactor,
‚àö
P (N, œÅx ) = O( ), and hence dTD (x) ‚Üí ‚àí‚àû, as  ‚Üí 0, i.e. as œÅx ‚Üí 1. On ‚Äútarget
P
present‚Äù trials, when stimulus i is the target, then xi ‚àí (N ‚àí 1)‚àí1 j6=i xj = O(1), and
‚àö
Œ±i (x, N, œÅx , œÉx ) = O(1/). In this case the prefactor is still P (N, œÅx ) = O( ), and the
exponential term dominates. A similar argument works for the summands for which i is not
a target.

B.3

Single target with increasing number of distractors

We still work under the assumption that measurement noise is relatively
‚àö weak, so that we
2
can use Eq. (B.1). Note that on ‚Äútarget absent‚Äù trials, xÃÑ = s + O(œÉx /‚àö N ), where s is the
true value of the (identical) distractors. We also have xÃÑƒ±ÃÇ = s + O(œÉx2 / N ‚àí‚àö1). Hence, the
first term in the exponential of Eq. (B.1) is O(1), while the second term is O( N ). A similar
argument holds in ‚Äútaget present‚Äù trials.
On target absent trials


‚àö

1 2
1
2
2
for all i,
‚àí 2 N xÃÑ ‚àí (N ‚àí 1)xÃÑƒ±ÃÇ = exp ‚àí 2 s + O( N œÉx )
2œÉx
2œÉx
to leading order in N . We abuse notation slightly and only use order notation on the terms
that include measurement noise. As stimuli become more dissimilar to the target, i.e. as s2
increases, Œ±i decreases exponentially, dTD (x) becomes more negative, and it is hence easier to
22

‚àö
infer that a target is absent. However, the O( N œÉx ) terms can be both positive and negative.
Thus performance decreases with the number of stimuli. Similarly we can see that when a
target is present


‚àö
1 N ‚àí1 2
2
Œ±i (x, N, œÉx ) = exp
s + O( N œÉx )
when i is the target,
(B.2)
2œÉx2 N


‚àö
1 1 + N ‚àí N2 2
Œ±i (x, N, œÅx ) = exp ‚àí 2
when i is not a target, (B.3)
s + O( N œÉx )
2œÉx N ‚àí N 2
again to leading order in N . As measurement noise decreases, or s2 increases, the first term
given in Eq. (B.2) diverges exponentially, and the terms ‚àö
given in Eq. (B.3) approach 0 exponentially. As a result dTD (x) increases. However, the O( N œÉx ) noise term increases with N ,
and an increase in the number of stimuli again decreases performance.
If (1 ‚àí œÅx )  1/N , i.e. measurement noise is strongly correlated, the first term in the
exponential of Eq. (B.1) dominates. Thus when correlations increase faster than the inverse
of the number of distractors, performance increases with the number of distractors.

B.4

Weak external structure, œÅs < 1, arbitrary number of targets

We approximate each term in the exponential of Eq. (3.4) assuming œÉx2  1:
œÉs2 (1 ‚àí œÅs )
œÉx2 (1 ‚àí œÅx )[œÉs2 (1 ‚àí œÅs ) + œÉx2 (1 ‚àí œÅx )]
1
= 2
+ O(1),
œÉx (1 ‚àí œÅx )
œÅx œÉx2 vL2 gL
œÅs œÉs2 + œÅx œÉx2
Œ≤v 2 ‚àí
= 2
Œ≥L
[œÉs (1 ‚àí œÅs ) + œÉx2 (1 ‚àí œÅx )][œÉs2 (1 + (N ‚àí 1)œÅs ) + œÉx2 (1 + (N ‚àí 1)œÅx )]
œÅx [œÉs2 (1 + (N ‚àí n ‚àí 1)œÅs + œÉx2 (1 ‚àí œÅx )]
‚àí 2
œÉx (1 ‚àí œÅx )[œÉs2 (1 + (n ‚àí 1)œÅx )(1 + (N ‚àí n ‚àí 1)œÅs ) + œÉx2 (1 ‚àí œÅx )(1 + (N ‚àí 1)œÅx ))]
œÅx
=‚àí 2
+ O(1),
œÉx (1 ‚àí œÅx )(1 + (n ‚àí 1)œÅx )
œÅx œÉx2 vL v
œÅs œÉs2 + œÅx œÉx2
Œ≤v 2 ‚àí
= 2
Œ≥L
[œÉs (1 ‚àí œÅs ) + œÉx2 (1 ‚àí œÅx )][œÉs2 (1 + (N ‚àí 1)œÅs ) + œÉx2 (1 + (N ‚àí 1)œÅx )]
œÅx
‚àí 2
[œÉs (1 + (N ‚àí n ‚àí 1)œÅs )(1 + (n ‚àí 1)œÅx ) + œÉx2 (1 ‚àí œÅx )(1 + (N ‚àí 1)œÅx )]
= O(1),
œÉs2 (1 ‚àí œÅs )vL v =

and
v 2 fL
œÅs œÉs2 + œÅx œÉx2
= 2
Œ≥L
[œÉs (1 ‚àí œÅs ) + œÉx2 (1 ‚àí œÅx )][œÉs2 (1 + (N ‚àí 1)œÅs ) + œÉx2 (1 + (N ‚àí 1)œÅx )]
œÅs œÉs2 (1 + (n ‚àí 1)œÅx ) + œÅx œÉx2 (1 ‚àí œÅx )
‚àí 2
[œÉs (1 ‚àí œÅs ) + œÉx2 (1 ‚àí œÅx )][œÉs2 (1 + (N ‚àí n ‚àí 1)œÅs )(1 + (n ‚àí 1)œÅx ) + œÉx2 (1 ‚àí œÅx )(1 + (N ‚àí 1)œÅx )]
= O(1)

Œ≤v 2 ‚àí

23

We also approximate the leading coefficient of the exponential term in Eq. (3.4) as:
s

1 + (œÅs œÉs2 + œÅx œÉx2 )N v  vL n
=
Œ≥L
v

s

(1 ‚àí œÅx )[œÉs2 (1 + (N ‚àí 1)œÅs ) + œÉx2 (1 + (N ‚àí 1)œÅx )]
[œÉs2 (1 + (N ‚àí n ‚àí 1)œÅs )(1 + (n ‚àí 1)œÅx ) + œÉx2 (1 ‚àí œÅx )(1 + (N ‚àí 1)œÅx )]
s

n
œÅs (1 ‚àí œÅs )
(1 ‚àí œÅx )(1 + (N ‚àí 1)œÅs )
+ O(1).
=
(1 + (N ‚àí n ‚àí 1)œÅs )(1 + (n ‚àí 1)œÅx ) œÉx2 (1 ‚àí œÅx )



œÉs2 (1 ‚àí œÅs ) + œÉx2 (1 ‚àí œÅx )
œÉx2 (1 ‚àí œÅx )

Combining above terms, Eq. (3.4) reduces to the following expression under the assumption
of œÉx2  œÉs2 , œÅs < 1,
 s
n
 2
1
(1 ‚àí œÅx )(1 + (N ‚àí 1)œÅs )
œÉs (1 ‚àí œÅs )
dTD (x) ‚âà log
M (1 + (N ‚àí n ‚àí 1)œÅs )(1 + (n ‚àí 1)œÅs ) œÉx2 (1 ‚àí œÅx )
(
!)
X
X
X
1
œÅ
x
√ó
exp ‚àí 2
x2i ‚àí
xi xj
.
2œÉ
(1
‚àí
œÅ
)
1 + (n ‚àí 1)œÅx i,j‚ààL
x
x
L‚ààL
i‚ààL
Special case: n = 1 In case of a single target, set L has only one element and L =
{1, 2, ¬∑ ¬∑ ¬∑ , N }. In this case, Eq. (3.9) reduces to a much simpler expression
s

!
N
1 œÉs2 (1 ‚àí œÅs )(1 + (N ‚àí 1)œÅs ) X
x2i
dTD (x) ‚âà log
exp ‚àí 2
.
N
œÉx2 (1 + (N ‚àí 2)œÅs )
2œÉx
i=1
This expression is independent of œÅx . Hence, the decision boundary, and the performance of
an ideal observer is unaffected by measurement correlations.

B.5

Near perfect performance with n > 1, and œÅx ‚âà 1

We first assume that T = 1. From Eq. (3.10), if LT is the set of targets, then this expression
is approximately zero, and the exponential is approximately unity. When L is a set not
consisting of all targets, then the expression in Eq. (3.10) has expectation greater than zero.
Then
Ô£±
Ô£´
!2 Ô£∂ Ô£º
Ô£≤
Ô£Ω
X
X
X
1
n
Ô£≠1
x2i ‚àí
xi Ô£∏ =
exp ‚àí 2
Ô£≥ 2œÉx (1 ‚àí œÅx ) n
Ô£æ
n i‚ààL
L‚ààL
i‚ààL


X
nŒ±L2
‚Üí1
(B.4)
1+
exp ‚àí 2
2œÉx (1 ‚àí œÅx )
L‚ààL\LT

The prefactor in Eq. (3.9) diverges since the exponential dominates. If T = 0, then Eq. (3.10)
will be greater than zero for all sets, L. Thus dT D (x) ‚Üí ‚àí‚àû with œÅx ‚Üí 1.

24

n

B.6

Asymptotics for large N

Here we develop asymptotic results for the decision variable when the number of stimuli, N
is large. We assume that there a fraction K of the stimuli are targets, so that there are
N K targets and (1 ‚àí K)N distractors. To simplify notation we write cx = œÉx2 (1 ‚àí œÅx ) and
cs = œÉs2 (1 ‚àí œÅs ). We find that
cs
(1 ‚àí œÅs )vL v =
cx (cs + cx )




1 vL 2
1
1 1
1
1
v2 Œ≤ ‚àí
œÅx œÉx2 gL
=
‚àí
+ O( 2 )
Œ≥L v
N cs + cx N K cx
N


1 vL
1
1
1
v2 Œ≤ ‚àí
œÅx œÉx2
=
+ O( 2 )
Œ≥ v
N c + cx
N
L

 s

1
1
1
1
1
2
=
‚àí
+ O( 2 )
v Œ≤ ‚àí fL
Œ≥L
N
(1 ‚àí K)N cx + cs
N

KN 



2
2
2
1 + (œÅs œÉs + œÅx œÉx )N v vL n
cs
1 (œÅx ‚àí 1)(œÅs œÉs + œÅx œÉx2 )
=
1+
Œ≥L
v
cx
N (K ‚àí 1)KœÅs œÅx œÉs2
We can therefore group the terms in the exponential of the decision variable given by Eq. (3.4)
as
Ô£´
Ô£∂
N
X
X
X
X
1
cs
1
1
1
1 1
1
‚àí Ô£≠
x2i +
xi xj ‚àí
xi xj ‚àí
xi xj Ô£∏
2 cx (cs + cx ) i‚ààL
N cx + cs i,j
N K cx i,j‚ààL
(1 ‚àí K)N cx + cs
i,j ‚ààL
/

A simple reorganization of the terms yields Eq. (3.11).

C

Mean stimulus orientation - left or right discrimination task

An observer is presented with N stimuli on every trial. For concreteness, we can think of the
stimuli as bars, with orientations s = (s1 , s2 , ¬∑ ¬∑ ¬∑ , sN ). The task is to decide whether the mean
orientation of the set is to the left (a condition we denote by C = ‚àí1) or right (C = 1) of
the vertical. The observer makes a decision based on the measurements, x = (x1 , x2 , ¬∑ ¬∑ ¬∑ , xN ).
Stimulus orientations are drawn from a multivariate normal distribution with mean vector,
0N , and covariance matrix, Œ£s ,
p(s) = N (0N , Œ£s ),
(C.1)
with Œ£s defined in Eq. (2.2). As in the target detection task, we assume the measurements
follow a multivariate normal distribution with mean vector and covariance matrix specified in
Eq. (2.4).
An ideal observer performs the task by making a decision based on the log posterior ratio,
p(C = 1|x)
p(sÃÑ > 0|x)
= log
p(C = ‚àí1|x)
p(sÃÑ < 0|x)
p(x|sÃÑ > 0)
p(sÃÑ > 0)
= log
+ log
,
p(x|sÃÑ < 0)
p(sÃÑ < 0)

dMOD (x) = log

25

(C.2)

where, sÃÑ =

N
X

si denotes the mean stimulus orientation on a trial. If dMOD (x) > 0, the

i=1

observer infers CÃÇ = 1, that is, the mean stimulus orientation is to the right of the vertical.
We compute Eq. (C.2) by marginalizing over s and applying Bayes‚Äô rule,
Z
p(x|sÃÑ > 0) =
p(x|s)p(s|sÃÑ > 0)ds
Z
p(s)
=
p(x|s)p(sÃÑ > 0|s)
ds
p(sÃÑ > 0)
Z
1
=
p(x|s)p(s)ds
p(sÃÑ > 0) sÃÑ>0
Z
1
=
N (x; s, Œ£x )N (s; 0N , Œ£s )ds
p(sÃÑ > 0) sÃÑ>0
Z


 
zc
‚àí1
‚àí1
‚àí1 ‚àí1
=
N s; I + Œ£x Œ£s x, Œ£x + Œ£s
ds,
p(sÃÑ > 0) sÃÑ>0
where zc is a normalization constant. Similarly, we compute p(x|sÃÑ < 0) and obtain

 Ô£∂
Ô£´R
‚àí1 ‚àí1
‚àí1
‚àí1 ‚àí1
N
s;
(I
+
Œ£
Œ£
)
x,
(Œ£
+
Œ£
)
ds
x s
x
s
sÃÑ>0
Ô£∏.

dMOD (x) = log Ô£≠ R
‚àí1 )‚àí1 x, (Œ£‚àí1 + Œ£‚àí1 )‚àí1 ds
N
s;
(I
+
Œ£
Œ£
x
s
x
s
sÃÑ<0

(C.3)

By symmetry, it is easy to see that the decision boundary in the space of measurements is
N
X
1
given by the hyperplane xÃÑ = 0, where xÃÑ = N
xi is the sample mean of the measurement.
i=1

An ideal observer therefore bases the decision only on the sample mean. Therefore, we have
xÃÑ > 0 ‚áí sÃÑ > 0.
In order to understand the negative impact of noise correlations on the performance of an
optimal observer on this task, consider x = s + Œæ~ where Œæ~ = (Œæ1 , Œæ2 , ¬∑ ¬∑ ¬∑ , ŒæN ) ‚àº N (0, Œ£x ), so
that
1 X
¬Ø
xÃÑ =
(si + Œæi ) = sÃÑ + Œæ.
N i
In this case, the variance of the mean noise scales with increasing noise correlation strength,
œÅx .
2
¬Ø = œÉx (1 ‚àí œÅx ) + œÅx œÉ 2 .
Var(Œæ)
x
N
As the variance increases, the overlap between the conditional distributions p(xÃÑ|sÃÑ > 0) and
p(xÃÑ|sÃÑ < 0) increases. It is therefore more difficult to tell which condition the measurement
comes from, and performance deteriorates.

References
Abbott, L. and Dayan, P. (1999). The effect of correlated variability on the accuracy of a
population code. Neural Computation, 11(1):91‚Äì101.
26

Averbeck, B. (2009). Noise correlations and information encoding and decoding. Springer.
Averbeck, B., Latham, P., and Pouget, A. (2006). Neural correlations, population coding and
computation. Nature Neuroscience Reviews, 7(5):358‚Äì366.
Averbeck, B. and Lee, D. (2006). Effects of noise correlations on information encoding and
decoding. Journal of Neurophysiology, 95(6):3633‚Äì3644.
Baldassi, S. and Burr, D. C. (2000). Feature-based integration of orientation signals in visual
search. Vision Research, 40(10):1293‚Äì1300.
Baldassi, S. and Verghese, P. (2002). Comparing integration rules in visual search. Journal
of Vision, 2(8):3.
Bartlett, M. (1951). An inverse matrix adjustment arising in discriminant analysis. The
Annals of Mathematical Statistics, 22(1):107‚Äì111.
Beck, J., Bejjanki, V., and Pouget, A. (2011). Insights from a simple expression for linear Fisher information in a recurrently connected population of spiking neurons. Neural
Computation, pages 1‚Äì19.
Beck, J., Ma, W., Pitkow, X., Latham, P., and Pouget, A. (2012). Not noisy, just wrong: the
role of suboptimal inference in behavioral variability. Neuron, 74(1):30‚Äì39.
Berens, P., Ecker, A., Gerwinn, S., Tolias, A., and Bethge, M. (2011). Reassessing optimal
neural population codes with neurometric functions. Proceedings of the National Academy
of Sciences of the United States of America.
Bhardwaj, M., van den Berg, R., Ma, W., and JosicÃÅ, K. (2015). Do people take stimulus
correlations into account in visual search? Submitted.
Brady, T. and Tenenbaum, J. (2010). Encoding higher-order structure in visual working
memory: A probabilistic model. Proceedings of the 32nd Annual Conference of the Cognitive
Science Society, pages 411‚Äì416.
Chelaru, M. and Dragoi, V. (2008). Efficient coding in heterogeneous neuronal populations. Proceedings of the National Academy of Sciences of the United States of America,
105(42):16344‚Äì16349.
Chen, Y., Geisler, W., and Seidemann, E. (2006). Optimal decoding of correlated neural
population responses in the primate visual cortex. Nature Neuroscience, 9(11):1412‚Äì1420.
Cohen, M. and Kohn, A. (2011). Measuring and interpreting neuronal correlations. Nature
Neuroscience, 14(7):811‚Äì819.
de la Rocha, J., Doiron, B., Shea-Brown, E., JosicÃÅ, K., and Reyes, A. (2007). Correlation
between neural spike trains increases with firing rate. Nature, 448(7155):802‚Äì806.
Ecker, A., Berens, P., Keliris, G., Bethge, M., Logothetis, N., and Tolias, A. (2010). Decorrelated neuronal firing in cortical microcircuits. Science, 327(5965):584‚Äì587.
27

Ecker, A., Berens, P., Tolias, A., and Bethge, M. (2011). The effect of noise correlations in
populations of diversely tuned neurons. The Journal of Neuroscience, 31(40):14272‚Äì14283.
Ganmor, E., Segev, R., and Schneidman, E. (2011). Sparse low-order interaction network
underlies a highly correlated and learnable neural population code. Proceedings of the
National Academy of Sciences, 108(23):9679‚Äì9684.
Gawne, T. and Richmond, B. (1993). How independent are the messages carried by adjacent
inferior temporal cortical neurons? The Journal of Neuroscience, 13(7):2758‚Äì2771.
Geisler, W. (2008). Visual perception and the statistical properties of natural scenes. Annual
review of psychology, 59:167‚Äì192.
Graham, N., Kramer, P., and Yager, D. (1987). Signal-detection models for multidimensional stimuli: Probability distributions and combination rules. Journal of Mathematical
Psychology, 31(4):366‚Äì409.
Green, D. and Swets, J. (1966). Signal detection theory and psychophysics, volume 1. Wiley
New York.
Hansen, B., Chelaru, M., and Dragoi, V. (2012). Correlated variability in laminar cortical
circuits. Neuron, 76:590‚Äì602.
Harville, D. (1998).
40(2):164‚Äì164.

Matrix algebra from a statistician‚Äôs perspective.

Technometrics,

Horn, R. and Johnson, C. (2012). Matrix analysis. Cambridge University Press.
Hu, Y., Zylberberg, J., and Shea-Brown, E. (2014). The sign rule and beyond: boundary
effects, flexibility, and noise correlations in neural population codes. PLoS Computational
Biology, 10(2):e1003469.
JosicÃÅ, K., Shea-Brown, E., Doiron, B., and de la Rocha, J. (2009). Stimulus-dependent
correlations and population codes. Neural Computation, 21(10):2774‚Äì2804.
Kohn, A. and Smith, M. (2005). Stimulus dependence of neuronal correlation in primary
visual cortex of the macaque. The Journal of Neuroscience, 25(14):3661‚Äì3673.
Latham, P. and Nirenberg, S. (2005). Synergy, redundancy, and independence in population
codes, revisited. The Journal of Neuroscience, 25(21):5195‚Äì5206.
Latham, P. and Roudi, Y. (2011). Role of correlations in population coding. arXiv preprint
arXiv:1109.6524.
Ma, W. (2010). Signal detection theory, uncertainty, and poisson-like population codes. Vision
Research, 50(22):2308‚Äì2319.
Ma, W., Navalpakkam, V., Beck, J. M., van den Berg, R., and Pouget, A. (2011). Behavior
and neural basis of near-optimal visual search. Nature Neuroscience, 14(6):783‚Äì790.
28

Mazyar, H., van den Berg, R., and Ma, W. (2012). Does precision decrease with set size?
Journal of Vision, 12(6).
Mazyar, H., van den Berg, R., Seilheimer, R., and Ma, W. (2013). Independence is elusive:
set size effects on encoding precision in visual search. Journal of Vision, 13(5):1‚Äì14.
Meyer, C. (2000). Matrix analysis and Applied Linear Algebra Book and Solutions Manual,
volume 2. SIAM.
Moreno-Bote, R., Beck, J., Kanitscheider, I., Pitkow, X., Latham, P., and Pouget, A. (2014).
Information-limiting correlations. Nature Neuroscience, 17(10):1410‚Äì1417.
Nolte, L. W. and Jaarsma, D. (1967). More on the detection of one of m orthogonal signals.
Journal of the Acoustical Society of America, 41(2):497‚Äì505.
Ohiorhenuan, I., Mechler, F., Purpura, K., Schmid, A., Hu, Q., and Victor, J. (2010). Sparse
coding and high-order correlations in fine-scale cortical network. Nature, 466:617‚Äì621.
Palmer, J., Ames, C. T., and Lindsey, D. T. (1993). Measuring the effect of attention on simple
visual search. Journal of Experimental Psychology: Human Perception and Performance,
19(1):108.
Palmer, S. (1999). Vision science: Photons to phenomenology, volume 1. MIT press Cambridge, MA.
Pelli, D. G. (1985). Uncertainty explains many aspects of visual contrast detection and
discrimination. Journal of the Optimal Society of America A, 2(9):1508‚Äì1531.
Perkel, D., Gerstein, G., and Moore, G. (1967). Neuronal spike trains and stochastic point
processes: II. Simultaneous spike trains. Biophysical Journal, 7(4):419‚Äì440.
Peterson, W. W., Birdsall, T. G., and Fox, W. (1954). The theory of signal detectability.
Information Theory, Transactions of the IRE Professional Group on, 4(4):171‚Äì212.
Romo, R., Hernandez, A., Zainos, A., and Salinas, E. (2003). Correlated neuronal discharges
that increase coding efficiency during perceptual discrimination. Neuron, 38(4):649‚Äì657.
Rosenbaum, R. and JosicÃÅ, K. (2011). Mechanisms that modulate the transfer of spiking
correlations. Neural Computation, 23(5):1261‚Äì1305.
Rosenbaum, R., Trousdale, J., and JosicÃÅ, K. (2010). Pooling and correlated neural activity.
Frontiers in Computational Neuroscience, 4:9.
Schneidman, E., Bialek, W., and Berry, M. (2003). Synergy, redundancy, and independence
in population codes. Journal of Neuroscience, 23(37):11539‚Äì11553.
SerieÃÄs, P., Latham, P., and Pouget, A. (2004). Tuning curve sharpening for orientation selectivity: coding efficiency and the impact of correlations. Nature Neuroscience, 7(10):1129‚Äì1135.

29

Shamir, M. and Sompolinsky, H. (2002). Correlation codes in neuronal populations. Advances
in neural information processing systems, 1:277‚Äì284.
Shamir, M. and Sompolinsky, H. (2006). Implications of neuronal diversity on population
coding. Neural Computation, 18(8):1951‚Äì1986.
Sherman, J. and Morrison, W. (1950). Adjustment of an inverse matrix corresponding to a
change in one element of a given matrix. The Annals of Mathematical Statistics, 21(1):124‚Äì
127.
Sompolinsky, H., Yoon, H., Kang, K., and Shamir, M. (2001). Population coding in neuronal
systems with correlated noise. Physical Review E, 64(5):051904.
TkacÃåik, G., Prentice, J., Balasubramanian, V., and Schneidman, E. (2010). Optimal population coding by noisy spiking neurons. Proceedings of the National Academy of Sciences,
107(32):14419‚Äì14424.
van den Berg, R., Vogel, M., JosicÃÅ, K., and Ma, W. (2012). Optimal inference of sameness.
Proceedings of the National Academy of Sciences, 109(8):3178‚Äì83.
Zohary, E., Shadlen, M., and Newsome, W. (1994). Correlated neuronal discharge rate and
its implications for psychophysical performance. Nature, 370(6485):140‚Äì143.

30

